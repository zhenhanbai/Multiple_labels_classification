Experience Recommendation for Long Term Safe Learning-based Model Predictive Control in Changing Operating Conditions	Learning has propelled the cutting edge of performance in robotic control to new heights, allowing robots to operate with high performance in conditions that were previously unimaginable. The majority of the work, however, assumes that the unknown parts are static or slowly changing. This limits them to static or slowly changing environments. However, in the real world, a robot may experience various unknown conditions. This paper presents a method to extend an existing single mode GP-based safe learning controller to learn an increasing number of non-linear models for the robot dynamics. We show that this approach enables a robot to re-use past experience from a large number of previously visited operating conditions, and to safely adapt when a new and distinct operating condition is encountered. This allows the robot to achieve safety and high performance in an large number of operating conditions that do not have to be specified ahead of time. Our approach runs independently from the controller, imposing no additional computation time on the control loop regardless of the number of previous operating conditions considered. We demonstrate the effectiveness of our approach in experiment on a 900\,kg ground robot with both physical and artificial changes to its dynamics. All of our experiments are conducted using vision for localization.	1,0,0,0,0,0
Learning from Noisy Label Distributions	In this paper, we consider a novel machine learning problem, that is, learning a classifier from noisy label distributions. In this problem, each instance with a feature vector belongs to at least one group. Then, instead of the true label of each instance, we observe the label distribution of the instances associated with a group, where the label distribution is distorted by an unknown noise. Our goals are to (1) estimate the true label of each instance, and (2) learn a classifier that predicts the true label of a new instance. We propose a probabilistic model that considers true label distributions of groups and parameters that represent the noise as hidden variables. The model can be learned based on a variational Bayesian method. In numerical experiments, we show that the proposed model outperforms existing methods in terms of the estimation of the true labels of instances.	1,0,0,1,0,0
Hierarchical Learning for Modular Robots	We argue that hierarchical methods can become the key for modular robots achieving reconfigurability. We present a hierarchical approach for modular robots that allows a robot to simultaneously learn multiple tasks. Our evaluation results present an environment composed of two different modular robot configurations, namely 3 degrees-of-freedom (DoF) and 4DoF with two corresponding targets. During the training, we switch between configurations and targets aiming to evaluate the possibility of training a neural network that is able to select appropriate motor primitives and robot configuration to achieve the target. The trained neural network is then transferred and executed on a real robot with 3DoF and 4DoF configurations. We demonstrate how this technique generalizes to robots with different configurations and tasks.	1,0,0,0,0,0
Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and Blind Unmixing of Hyperspectral Images	Estimation of the number of endmembers existing in a scene constitutes a critical task in the hyperspectral unmixing process. The accuracy of this estimate plays a crucial role in subsequent unsupervised unmixing steps i.e., the derivation of the spectral signatures of the endmembers (endmembers' extraction) and the estimation of the abundance fractions of the pixels. A common practice amply followed in literature is to treat endmembers' number estimation and unmixing, independently as two separate tasks, providing the outcome of the former as input to the latter. In this paper, we go beyond this computationally demanding strategy. More precisely, we set forth a multiple constrained optimization framework, which encapsulates endmembers' number estimation and unsupervised unmixing in a single task. This is attained by suitably formulating the problem via a low-rank and sparse nonnegative matrix factorization rationale, where low-rankness is promoted with the use of a sophisticated $\ell_2/\ell_1$ norm penalty term. An alternating proximal algorithm is then proposed for minimizing the emerging cost function. The results obtained by simulated and real data experiments verify the effectiveness of the proposed approach.	1,0,0,1,0,0
Rapid processing of 85Kr/Kr ratios using Atom Trap Trace Analysis	We report a methodology for measuring 85Kr/Kr isotopic abundances using Atom Trap Trace Analysis (ATTA) that increases sample measurement throughput by over an order of magnitude to 6 samples per 24 hours. The noble gas isotope 85Kr (half-life = 10.7 yr) is a useful tracer for young groundwater in the age range of 5-50 years. ATTA, an efficient and selective laser-based atom counting method, has recently been applied to 85Kr/Kr isotopic abundance measurements, requiring 5-10 microliters of krypton gas at STP extracted from 50-100 L of water. Previously a single such measurement required 48 hours. Our new method demonstrates that we can measure 85Kr/Kr ratios with 3-5% relative uncertainty every 4 hours, on average, with the same sample requirements.	0,1,0,0,0,0
Parametrizing filters of a CNN with a GAN	It is commonly agreed that the use of relevant invariances as a good statistical bias is important in machine-learning. However, most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations, such as translations and rotations. Hence, there is a need for methods to model and extract richer transformations that capture much higher-level invariances. To that end, we introduce a tool allowing to parametrize the set of filters of a trained convolutional neural network with the latent space of a generative adversarial network. We then show that the method can capture highly non-linear invariances of the data by visualizing their effect in the data space.	1,0,0,1,0,0
Distributed Optimization of Multi-Beam Directional Communication Networks	We formulate an optimization problem for maximizing the data rate of a common message transmitted from nodes within an airborne network broadcast to a central station receiver while maintaining a set of intra-network rate demands. Assuming that the network has full-duplex links with multi-beam directional capability, we obtain a convex multi-commodity flow problem and use a distributed augmented Lagrangian algorithm to solve for the optimal flows associated with each beam in the network. For each augmented Lagrangian iteration, we propose a scaled gradient projection method to minimize the local Lagrangian function that incorporates the local topology of each node in the network. Simulation results show fast convergence of the algorithm in comparison to simple distributed primal dual methods and highlight performance gains over standard minimum distance-based routing.	0,0,1,0,0,0
Preprint Déjà Vu: an FAQ	I give a brief overview of arXiv history, and describe the current state of arXiv practice, both technical and sociological. This commentary originally appeared in the EMBO Journal, 19 Oct 2016. It was intended as an update on comments from the late 1990s regarding use of preprints by biologists (or lack thereof), but may be of interest to practitioners of other disciplines. It is based largely on a keynote presentation I gave to the ASAPbio inaugural meeting in Feb 2016, and responds as well to some follow-up questions.	1,1,0,0,0,0
Dirac and Chiral Quantum Spin Liquids on the Honeycomb Lattice in a Magnetic Field	Motivated by recent experimental observations in $\alpha$-RuCl$_3$, we study the $K$-$\Gamma$ model on the honeycomb lattice in an external magnetic field. By a slave-particle representation and Variational Monte Carlo calculations, we reproduce the phase transition from zigzag magnetic order to a field-induced disordered phase. The nature of this state depends crucially on the field orientation. For particular field directions in the honeycomb plane, we find a gapless Dirac spin liquid, in agreement with recent experiments on $\alpha$-RuCl$_3$. For a range of out-of-plane fields, we predict the existence of a Kalmeyer-Laughlin-type chiral spin liquid, which would show an integer-quantized thermal Hall effect.	0,1,0,0,0,0
Wandering domains for diffeomorphisms of the k-torus: a remark on a theorem by Norton and Sullivan	We show that there is no C^{k+1} diffeomorphism of the k-torus which is semiconjugate to a minimal translation and has a wandering domain all of whose iterates are Euclidean balls.	0,0,1,0,0,0
Dense families of modular curves, prime numbers and uniform symmetric tensor rank of multiplication in certain finite fields	We obtain new uniform bounds for the symmetric tensor rank of multiplication in finite extensions of any finite field Fp or Fp2 where p denotes a prime number greater or equal than 5. In this aim, we use the symmetric Chudnovsky-type generalized algorithm applied on sufficiently dense families of modular curves defined over Fp2 attaining the Drinfeld-Vladuts bound and on the descent of these families to the definition field Fp. These families are obtained thanks to prime number density theorems of type Hoheisel, in particular a result due to Dudek (2016).	0,0,1,0,0,0
Data-Driven Decentralized Optimal Power Flow	The implementation of optimal power flow (OPF) methods to perform voltage and power flow regulation in electric networks is generally believed to require communication. We consider distribution systems with multiple controllable Distributed Energy Resources (DERs) and present a data-driven approach to learn control policies for each DER to reconstruct and mimic the solution to a centralized OPF problem from solely locally available information. Collectively, all local controllers closely match the centralized OPF solution, providing near-optimal performance and satisfaction of system constraints. A rate distortion framework facilitates the analysis of how well the resulting fully decentralized control policies are able to reconstruct the OPF solution. Our methodology provides a natural extension to decide what buses a DER should communicate with to improve the reconstruction of its individual policy. The method is applied on both single- and three-phase test feeder networks using data from real loads and distributed generators. It provides a framework for Distribution System Operators to efficiently plan and operate the contributions of DERs to active distribution networks.	0,0,0,1,0,0
ARTENOLIS: Automated Reproducibility and Testing Environment for Licensed Software	Motivation: Automatically testing changes to code is an essential feature of continuous integration. For open-source code, without licensed dependencies, a variety of continuous integration services exist. The COnstraint-Based Reconstruction and Analysis (COBRA) Toolbox is a suite of open-source code for computational modelling with dependencies on licensed software. A novel automated framework of continuous integration in a semi-licensed environment is required for the development of the COBRA Toolbox and related tools of the COBRA community. Results: ARTENOLIS is a general-purpose infrastructure software application that implements continuous integration for open-source software with licensed dependencies. It uses a master-slave framework, tests code on multiple operating systems, and multiple versions of licensed software dependencies. ARTENOLIS ensures the stability, integrity, and cross-platform compatibility of code in the COBRA Toolbox and related tools. Availability and Implementation: The continuous integration server, core of the reproducibility and testing infrastructure, can be freely accessed under artenolis.lcsb.uni.lu. The continuous integration framework code is located in the /.ci directory and at the root of the repository freely available under github.com/opencobra/cobratoolbox.	1,0,0,0,0,0
Projection Theorems Using Effective Dimension	In this paper we use the theory of computing to study fractal dimensions of projections in Euclidean spaces. A fundamental result in fractal geometry is Marstrand's projection theorem, which shows that for every analytic set E, for almost every line L, the Hausdorff dimension of the orthogonal projection of E onto L is maximal. We use Kolmogorov complexity to give two new results on the Hausdorff and packing dimensions of orthogonal projections onto lines. The first shows that the conclusion of Marstrand's theorem holds whenever the Hausdorff and packing dimensions agree on the set E, even if E is not analytic. Our second result gives a lower bound on the packing dimension of projections of arbitrary sets. Finally, we give a new proof of Marstrand's theorem using the theory of computing.	1,0,1,0,0,0
Efficient Data-Driven Geologic Feature Detection from Pre-stack Seismic Measurements using Randomized Machine-Learning Algorithm	Conventional seismic techniques for detecting the subsurface geologic features are challenged by limited data coverage, computational inefficiency, and subjective human factors. We developed a novel data-driven geological feature detection approach based on pre-stack seismic measurements. Our detection method employs an efficient and accurate machine-learning detection approach to extract useful subsurface geologic features automatically. Specifically, our method is based on kernel ridge regression model. The conventional kernel ridge regression can be computationally prohibited because of the large volume of seismic measurements. We employ a data reduction technique in combination with the conventional kernel ridge regression method to improve the computational efficiency and reduce memory usage. In particular, we utilize a randomized numerical linear algebra technique, named Nyström method, to effectively reduce the dimensionality of the feature space without compromising the information content required for accurate detection. We provide thorough computational cost analysis to show efficiency of our new geological feature detection methods. We further validate the performance of our new subsurface geologic feature detection method using synthetic surface seismic data for 2D acoustic and elastic velocity models. Our numerical examples demonstrate that our new detection method significantly improves the computational efficiency while maintaining comparable accuracy. Interestingly, we show that our method yields a speed-up ratio on the order of $\sim10^2$ to $\sim 10^3$ in a multi-core computational environment.	1,0,0,1,0,0
Elongation and shape changes in organisms with cell walls: a dialogue between experiments and models	The generation of anisotropic shapes occurs during morphogenesis of almost all organisms. With the recent renewal of the interest in mechanical aspects of morphogenesis, it has become clear that mechanics contributes to anisotropic forms in a subtle interaction with various molecular actors. Here, we consider plants, fungi, oomycetes, and bacteria, and we review the mechanisms by which elongated shapes are generated and maintained. We focus on theoretical models of the interplay between growth and mechanics, in relation with experimental data, and discuss how models may help us improve our understanding of the underlying biological mechanisms.	0,0,0,0,1,0
Robustness of semiparametric efficiency in nearly-true models for two-phase samples	We examine the performance of efficient and AIPW estimators under two-phase sampling when the complete-data model is nearly correctly specified, in the sense that the misspecification is not reliably detectable from the data by any possible diagnostic or test. Asymptotic results for these nearly true models are obtained by representing them as sequences of misspecified models that are mutually contiguous with a correctly specified model. We find that for the least-favourable direction of model misspecification the bias in the efficient estimator induced can be comparable to the extra variability in the AIPW estimator, so that the mean squared error of the efficient estimator is no longer lower. This can happen when the most-powerful test for the model misspecification still has modest power. We verify that the theoretical results agree with simulation in three examples: a simple informative-sampling model for a Normal mean, logistic regression in the classical case-control design, and linear regression in a two-phase design.	0,0,1,1,0,0
Towards a Deeper Understanding of Adversarial Losses	Recent work has proposed various adversarial losses for training generative adversarial networks. Yet, it remains unclear what certain types of functions are valid adversarial loss functions, and how these loss functions perform against one another. In this paper, we aim to gain a deeper understanding of adversarial losses by decoupling the effects of their component functions and regularization terms. We first derive some necessary and sufficient conditions of the component functions such that the adversarial loss is a divergence-like measure between the data and the model distributions. In order to systematically compare different adversarial losses, we then propose DANTest, a new, simple framework based on discriminative adversarial networks. With this framework, we evaluate an extensive set of adversarial losses by combining different component functions and regularization approaches. This study leads to some new insights into the adversarial losses. For reproducibility, all source code is available at this https URL .	1,0,0,1,0,0
Tracy-Widom at each edge of real covariance and MANOVA estimators	We study the sample covariance matrix for real-valued data with general population covariance, as well as MANOVA-type covariance estimators in variance components models under null hypotheses of global sphericity. In the limit as matrix dimensions increase proportionally, the asymptotic spectra of such estimators may have multiple disjoint intervals of support, possibly intersecting the negative half line. We show that the distribution of the extremal eigenvalue at each regular edge of the support has a GOE Tracy-Widom limit. Our proof extends a comparison argument of Ji Oon Lee and Kevin Schnelli, replacing a continuous Green function flow by a discrete Lindeberg swapping scheme.	0,0,1,1,0,0
Ensemble dependence of fluctuations and the canonical/micro-canonical equivalence of ensembles	We study the equivalence of microcanonical and canonical ensembles in continuous systems, in the sense of the convergence of the corresponding Gibbs measures. This is obtained by proving a local central limit theorem and a local large deviations principle. As an application we prove a formula due to Lebowitz-Percus-Verlet. It gives mean square fluctuations of an extensive observable, like the kinetic energy, in a classical micro canonical ensemble at fixed energy.	0,1,1,0,0,0
Invariance in Constrained Switching	We study discrete time linear constrained switching systems with additive disturbances, in which the switching may be on the system matrices, the disturbance sets, the state constraint sets or a combination of the above. In our general setting, a switching sequence is admissible if it is accepted by an automaton. For this family of systems, stability does not necessarily imply the existence of an invariant set. Nevertheless, it does imply the existence of an invariant multi-set, which is a relaxation of invariance and the object of our work. First, we establish basic results concerning the characterization, approximation and computation of the minimal and the maximal admissible invariant multi-set. Second, by exploiting the topological properties of the directed graph which defines the switching constraints, we propose invariant multi-set constructions with several benefits. We illustrate our results in benchmark problems in control.	1,0,1,0,0,0
Few-shot Learning by Exploiting Visual Concepts within CNNs	Convolutional neural networks (CNNs) are one of the driving forces for the advancement of computer vision. Despite their promising performances on many tasks, CNNs still face major obstacles on the road to achieving ideal machine intelligence. One is that CNNs are complex and hard to interpret. Another is that standard CNNs require large amounts of annotated data, which is sometimes hard to obtain, and it is desirable to learn to recognize objects from few examples. In this work, we address these limitations of CNNs by developing novel, flexible, and interpretable models for few-shot learning. Our models are based on the idea of encoding objects in terms of visual concepts (VCs), which are interpretable visual cues represented by the feature vectors within CNNs. We first adapt the learning of VCs to the few-shot setting, and then uncover two key properties of feature encoding using VCs, which we call category sensitivity and spatial pattern. Motivated by these properties, we present two intuitive models for the problem of few-shot learning. Experiments show that our models achieve competitive performances, while being more flexible and interpretable than alternative state-of-the-art few-shot learning methods. We conclude that using VCs helps expose the natural capability of CNNs for few-shot learning.	1,0,0,1,0,0
Universal and shape dependent features of surface superconductivity	We analyze the response of a type II superconducting wire to an external magnetic field parallel to it in the framework of Ginzburg-Landau theory. We focus on the surface superconductivity regime of applied field between the second and third critical values, where the superconducting state survives only close to the sample's boundary. Our first finding is that, in first approximation, the shape of the boundary plays no role in determining the density of superconducting electrons. A second order term is however isolated, directly proportional to the mean curvature of the boundary. This demonstrates that points of higher boundary curvature (counted inwards) attract superconducting electrons.	0,1,1,0,0,0
Mendelian randomization with fine-mapped genetic data: choosing from large numbers of correlated instrumental variables	Mendelian randomization uses genetic variants to make causal inferences about the effect of a risk factor on an outcome. With fine-mapped genetic data, there may be hundreds of genetic variants in a single gene region any of which could be used to assess this causal relationship. However, using too many genetic variants in the analysis can lead to spurious estimates and inflated Type 1 error rates. But if only a few genetic variants are used, then the majority of the data is ignored and estimates are highly sensitive to the particular choice of variants. We propose an approach based on summarized data only (genetic association and correlation estimates) that uses principal components analysis to form instruments. This approach has desirable theoretical properties: it takes the totality of data into account and does not suffer from numerical instabilities. It also has good properties in simulation studies: it is not particularly sensitive to varying the genetic variants included in the analysis or the genetic correlation matrix, and it does not have greatly inflated Type 1 error rates. Overall, the method gives estimates that are not so precise as those from variable selection approaches (such as using a conditional analysis or pruning approach to select variants), but are more robust to seemingly arbitrary choices in the variable selection step. Methods are illustrated by an example using genetic associations with testosterone for 320 genetic variants to assess the effect of sex hormone-related pathways on coronary artery disease risk, in which variable selection approaches give inconsistent inferences.	0,0,0,1,0,0
Any cyclic quadrilateral can be inscribed in any closed convex smooth curve	We prove that any cyclic quadrilateral can be inscribed in any closed convex $C^1$-curve. The smoothness condition is not required if the quadrilateral is a rectangle.	0,0,1,0,0,0
Adaptive clustering procedure for continuous gravitational wave searches	In hierarchical searches for continuous gravitational waves, clustering of candidates is an important postprocessing step because it reduces the number of noise candidates that are followed-up at successive stages [1][7][12]. Previous clustering procedures bundled together nearby candidates ascribing them to the same root cause (be it a signal or a disturbance), based on a predefined cluster volume. In this paper, we present a procedure that adapts the cluster volume to the data itself and checks for consistency of such volume with what is expected from a signal. This significantly improves the noise rejection capabilities at fixed detection threshold, and at fixed computing resources for the follow-up stages, this results in an overall more sensitive search. This new procedure was employed in the first Einstein@Home search on data from the first science run of the advanced LIGO detectors (O1) [11].	0,1,1,0,0,0
Estimates of the Reconstruction Error in Partially Redressed Warped Frames Expansions	In recent work, redressed warped frames have been introduced for the analysis and synthesis of audio signals with non-uniform frequency and time resolutions. In these frames, the allocation of frequency bands or time intervals of the elements of the representation can be uniquely described by means of a warping map. Inverse warping applied after time-frequency sampling provides the key to reduce or eliminate dispersion of the warped frame elements in the conjugate variable, making it possible, e.g., to construct frequency warped frames with synchronous time alignment through frequency. The redressing procedure is however exact only when the analysis and synthesis windows have compact support in the domain where warping is applied. This implies that frequency warped frames cannot have compact support in the time domain. This property is undesirable when online computation is required. Approximations in which the time support is finite are however possible, which lead to small reconstruction errors. In this paper we study the approximation error for compactly supported frequency warped analysis-synthesis elements, providing a few examples and case studies.	1,0,0,0,0,0
Generic Dynamical Phase Transition in One-Dimensional Bulk-Driven Lattice Gases with Exclusion	Dynamical phase transitions are crucial features of the fluctuations of statistical systems, corresponding to boundaries between qualitatively different mechanisms of maintaining unlikely values of dynamical observables over long periods of time. They manifest themselves in the form of non-analyticities in the large deviation function of those observables. In this paper, we look at bulk-driven exclusion processes with open boundaries. It is known that the standard asymmetric simple exclusion process exhibits a dynamical phase transition in the large deviations of the current of particles flowing through it. That phase transition has been described thanks to specific calculation methods relying on the model being exactly solvable, but more general methods have also been used to describe the extreme large deviations of that current, far from the phase transition. We extend those methods to a large class of models based on the ASEP, where we add arbitrary spatial inhomogeneities in the rates and short-range potentials between the particles. We show that, as for the regular ASEP, the large deviation function of the current scales differently with the size of the system if one considers very high or very low currents, pointing to the existence of a dynamical phase transition between those two regimes: high current large deviations are extensive in the system size, and the typical states associated to them are Coulomb gases, which are correlated ; low current large deviations do not depend on the system size, and the typical states associated to them are anti-shocks, consistently with a hydrodynamic behaviour. Finally, we illustrate our results numerically on a simple example, and we interpret the transition in terms of the current pushing beyond its maximal hydrodynamic value, as well as relate it to the appearance of Tracy-Widom distributions in the relaxation statistics of such models.	0,1,0,0,0,0
Deep Residual Networks and Weight Initialization	Residual Network (ResNet) is the state-of-the-art architecture that realizes successful training of really deep neural network. It is also known that good weight initialization of neural network avoids problem of vanishing/exploding gradients. In this paper, simplified models of ResNets are analyzed. We argue that goodness of ResNet is correlated with the fact that ResNets are relatively insensitive to choice of initial weights. We also demonstrate how batch normalization improves backpropagation of deep ResNets without tuning initial values of weights.	1,0,0,1,0,0
Information Diffusion in Social Networks: Friendship Paradox based Models and Statistical Inference	Dynamic models and statistical inference for the diffusion of information in social networks is an area which has witnessed remarkable progress in the last decade due to the proliferation of social networks. Modeling and inference of diffusion of information has applications in targeted advertising and marketing, forecasting elections, predicting investor sentiment and identifying epidemic outbreaks. This chapter discusses three important aspects related to information diffusion in social networks: (i) How does observation bias named friendship paradox (a graph theoretic consequence) and monophilic contagion (influence of friends of friends) affect information diffusion dynamics. (ii) How can social networks adapt their structural connectivity depending on the state of information diffusion. (iii) How one can estimate the state of the network induced by information diffusion. The motivation for all three topics considered in this chapter stems from recent findings in network science and social sensing. Further, several directions for future research that arise from these topics are also discussed.	1,0,0,0,0,0
Thermodynamics of BTZ Black Holes in Gravity's Rainbow	In this paper, we deform the thermodynamics of a BTZ black hole from rainbow functions in gravity's rainbow. The rainbow functions will be motivated from results in loop quantum gravity and Noncommutative geometry. It will be observed that the thermodynamics gets deformed due to these rainbow functions, indicating the existence of a remnant. However, the Gibbs free energy does not get deformed due to these rainbow functions, and so the critical behaviour from Gibbs does not change by this deformation.This is because the deformation in the entropy cancel's out the temperature deformation.	0,1,0,0,0,0
Beta-rhythm oscillations and synchronization transition in network models of Izhikevich neurons: effect of topology and synaptic type	Despite their significant functional roles, beta-band oscillations are least understood. Synchronization in neuronal networks have attracted much attention in recent years with the main focus on transition type. Whether one obtains explosive transition or a continuous transition is an important feature of the neuronal network which can depend on network structure as well as synaptic types. In this study we consider the effect of synaptic interaction (electrical and chemical) as well as structural connectivity on synchronization transition in network models of Izhikevich neurons which spike regularly with beta rhythms. We find a wide range of behavior including continuous transition, explosive transition, as well as lack of global order. The stronger electrical synapses are more conducive to synchronization and can even lead to explosive synchronization. The key network element which determines the order of transition is found to be the clustering coefficient and not the small world effect, or the existence of hubs in a network. These results are in contrast to previous results which use phase oscillator models such as the Kuramoto model. Furthermore, we show that the patterns of synchronization changes when one goes to the gamma band. We attribute such a change to the change in the refractory period of Izhikevich neurons which changes significantly with frequency.	0,0,0,0,1,0
Weighted boundedness of maximal functions and fractional Bergman operators	The aim of this paper is to study two-weight norm inequalities for fractional maximal functions and fractional Bergman operator defined on the upper-half space. Namely, we characterize those pairs of weights for which these maximal operators satisfy strong and weak type inequalities. Our characterizations are in terms of Sawyer and Békollé-Bonami type conditions. We also obtain a $\Phi$-bump characterization for these maximal functions, where $\Phi$ is a Orlicz function. As a consequence, we obtain two-weight norm inequalities for fractional Bergman operators. Finally, we provide some sharp weighted inequalities for the fractional maximal functions.	0,0,1,0,0,0
Parametric Gaussian Process Regression for Big Data	This work introduces the concept of parametric Gaussian processes (PGPs), which is built upon the seemingly self-contradictory idea of making Gaussian processes parametric. Parametric Gaussian processes, by construction, are designed to operate in "big data" regimes where one is interested in quantifying the uncertainty associated with noisy data. The proposed methodology circumvents the well-established need for stochastic variational inference, a scalable algorithm for approximating posterior distributions. The effectiveness of the proposed approach is demonstrated using an illustrative example with simulated data and a benchmark dataset in the airline industry with approximately 6 million records.	0,0,0,1,0,0
A Channel-Based Perspective on Conjugate Priors	A desired closure property in Bayesian probability is that an updated posterior distribution be in the same class of distributions --- say Gaussians --- as the prior distribution. When the updating takes place via a statistical model, one calls the class of prior distributions the `conjugate priors' of the model. This paper gives (1) an abstract formulation of this notion of conjugate prior, using channels, in a graphical language, (2) a simple abstract proof that such conjugate priors yield Bayesian inversions, and (3) a logical description of conjugate priors that highlights the required closure of the priors under updating. The theory is illustrated with several standard examples, also covering multiple updating.	1,0,0,0,0,0
Efficient Spatial Variation Characterization via Matrix Completion	In this paper, we propose a novel method to estimate and characterize spatial variations on dies or wafers. This new technique exploits recent developments in matrix completion, enabling estimation of spatial variation across wafers or dies with a small number of randomly picked sampling points while still achieving fairly high accuracy. This new approach can be easily generalized, including for estimation of mixed spatial and structure or device type information.	1,0,0,0,0,0
An adverse selection approach to power pricing	We study the optimal design of electricity contracts among a population of consumers with different needs. This question is tackled within the framework of Principal-Agent problems in presence of adverse selection. The particular features of electricity induce an unusual structure on the production cost, with no decreasing return to scale. We are nevertheless able to provide an explicit solution for the problem at hand. The optimal contracts are either linear or polynomial with respect to the consumption. Whenever the outside options offered by competitors are not uniform among the different type of consumers, we exhibit situations where the electricity provider should contract with consumers with either low or high appetite for electricity.	0,0,1,0,0,0
Semigroup C*-algebras and toric varieties	Let S be a finitely generated subsemigroup of Z^2. We derive a general formula for the K-theory of the left regular C*-algebra for S.	0,0,1,0,0,0
Bias Correction For Paid Search In Media Mix Modeling	Evaluating the return on ad spend (ROAS), the causal effect of advertising on sales, is critical to advertisers for understanding the performance of their existing marketing strategy as well as how to improve and optimize it. Media Mix Modeling (MMM) has been used as a convenient analytical tool to address the problem using observational data. However it is well recognized that MMM suffers from various fundamental challenges: data collection, model specification and selection bias due to ad targeting, among others \citep{chan2017,wolfe2016}. In this paper, we study the challenge associated with measuring the impact of search ads in MMM, namely the selection bias due to ad targeting. Using causal diagrams of the search ad environment, we derive a statistically principled method for bias correction based on the \textit{back-door} criterion \citep{pearl2013causality}. We use case studies to show that the method provides promising results by comparison with results from randomized experiments. We also report a more complex case study where the advertiser had spent on more than a dozen media channels but results from a randomized experiment are not available. Both our theory and empirical studies suggest that in some common, practical scenarios, one may be able to obtain an approximately unbiased estimate of search ad ROAS.	0,0,0,1,0,0
Cyclic Dominance in the Spatial Coevolutionary Optional Prisoner's Dilemma Game	This paper studies scenarios of cyclic dominance in a coevolutionary spatial model in which game strategies and links between agents adaptively evolve over time. The Optional Prisoner's Dilemma (OPD) game is employed. The OPD is an extended version of the traditional Prisoner's Dilemma where players have a third option to abstain from playing the game. We adopt an agent-based simulation approach and use Monte Carlo methods to perform the OPD with coevolutionary rules. The necessary conditions to break the scenarios of cyclic dominance are also investigated. This work highlights that cyclic dominance is essential in the sustenance of biodiversity. Moreover, we also discuss the importance of a spatial coevolutionary model in maintaining cyclic dominance in adverse conditions.	1,1,1,0,0,0
Missing dust signature in the cosmic microwave background	I examine a possible spectral distortion of the Cosmic Microwave Background (CMB) due to its absorption by galactic and intergalactic dust. I show that even subtle intergalactic opacity of $1 \times 10^{-7}\, \mathrm{mag}\, h\, \mathrm{Gpc}^{-1}$ at the CMB wavelengths in the local Universe causes non-negligible CMB absorption and decline of the CMB intensity because the opacity steeply increases with redshift. The CMB should be distorted even during the epoch of the Universe defined by redshifts $z < 10$. For this epoch, the maximum spectral distortion of the CMB is at least $20 \times 10^{-22} \,\mathrm{Wm}^{-2}\, \mathrm{Hz}^{-1}\, \mathrm{sr}^{-1}$ at 300 GHz being well above the sensitivity of the COBE/FIRAS, WMAP or Planck flux measurements. If dust mass is considered to be redshift dependent with noticeable dust abundance at redshifts 2-4, the predicted CMB distortion is even higher. The CMB would be distorted also in a perfectly transparent universe due to dust in galaxies but this effect is lower by one order than that due to intergalactic opacity. The fact that the distortion of the CMB by dust is not observed is intriguing and questions either opacity and extinction law measurements or validity of the current model of the Universe.	0,1,0,0,0,0
Recovering sparse graphs	We construct a fixed parameter algorithm parameterized by d and k that takes as an input a graph G' obtained from a d-degenerate graph G by complementing on at most k arbitrary subsets of the vertex set of G and outputs a graph H such that G and H agree on all but f(d,k) vertices. Our work is motivated by the first order model checking in graph classes that are first order interpretable in classes of sparse graphs. We derive as a corollary that if G_0 is a graph class with bounded expansion, then the first order model checking is fixed parameter tractable in the class of all graphs that can obtained from a graph G from G_0 by complementing on at most k arbitrary subsets of the vertex set of G; this implies an earlier result that the first order model checking is fixed parameter tractable in graph classes interpretable in classes of graphs with bounded maximum degree.	1,0,0,0,0,0
The boundary value problem for Yang--Mills--Higgs fields	We show the existence of Yang--Mills--Higgs (YMH) fields over a Riemann surface with boundary where a free boundary condition is imposed on the section and a Neumann boundary condition on the connection. In technical terms, we study the convergence and blow-up behavior of a sequence of Sacks-Uhlenbeck type $\alpha$-YMH fields as $\alpha\to 1$. For $\alpha>1$, each $\alpha$-YMH field is shown to be smooth up to the boundary under some gauge transformation. This is achieved by showing a regularity theorem for more general coupled systems, which extends the classical results of Ladyzhenskaya-Ural'ceva and Morrey.	0,0,1,0,0,0
Lagrangian Flow Network approach to an open flow model	Concepts and tools from network theory, the so-called Lagrangian Flow Network framework, have been successfully used to obtain a coarse-grained description of transport by closed fluid flows. Here we explore the application of this methodology to open chaotic flows, and check it with numerical results for a model open flow, namely a jet with a localized wave perturbation. We find that network nodes with high values of out-degree and of finite-time entropy in the forward-in-time direction identify the location of the chaotic saddle and its stable manifold, whereas nodes with high in-degree and backwards finite-time entropy highlight the location of the saddle and its unstable manifold. The cyclic clustering coefficient, associated to the presence of periodic orbits, takes non-vanishing values at the location of the saddle itself.	0,1,0,0,0,0
ADN: An Information-Centric Networking Architecture for the Internet of Things	Forwarding data by name has been assumed to be a necessary aspect of an information-centric redesign of the current Internet architecture that makes content access, dissemination, and storage more efficient. The Named Data Networking (NDN) and Content-Centric Networking (CCNx) architectures are the leading examples of such an approach. However, forwarding data by name incurs storage and communication complexities that are orders of magnitude larger than solutions based on forwarding data using addresses. Furthermore, the specific algorithms used in NDN and CCNx have been shown to have a number of limitations. The Addressable Data Networking (ADN) architecture is introduced as an alternative to NDN and CCNx. ADN is particularly attractive for large-scale deployments of the Internet of Things (IoT), because it requires far less storage and processing in relaying nodes than NDN. ADN allows things and data to be denoted by names, just like NDN and CCNx do. However, instead of replacing the waist of the Internet with named-data forwarding, ADN uses an address-based forwarding plane and introduces an information plane that seamlessly maps names to addresses without the involvement of end-user applications. Simulation results illustrate the order of magnitude savings in complexity that can be attained with ADN compared to NDN.	1,0,0,0,0,0
RNN-based Early Cyber-Attack Detection for the Tennessee Eastman Process	An RNN-based forecasting approach is used to early detect anomalies in industrial multivariate time series data from a simulated Tennessee Eastman Process (TEP) with many cyber-attacks. This work continues a previously proposed LSTM-based approach to the fault detection in simpler data. It is considered necessary to adapt the RNN network to deal with data containing stochastic, stationary, transitive and a rich variety of anomalous behaviours. There is particular focus on early detection with special NAB-metric. A comparison with the DPCA approach is provided. The generated data set is made publicly available.	1,0,0,0,0,0
Quadrature Compound: An approximating family of distributions	Compound distributions allow construction of a rich set of distributions. Typically they involve an intractable integral. Here we use a quadrature approximation to that integral to define the quadrature compound family. Special care is taken that this approximation is suitable for computation of gradients with respect to distribution parameters. This technique is applied to discrete (Poisson LogNormal) and continuous distributions. In the continuous case, quadrature compound family naturally makes use of parameterized transformations of unparameterized distributions (a.k.a "reparameterization"), allowing for gradients of expectations to be estimated as the gradient of a sample mean. This is demonstrated in a novel distribution, the diffeomixture, which is is a reparameterizable approximation to a mixture distribution.	0,0,0,1,0,0
Comparison of hidden Markov chain models and hidden Markov random field models in estimation of computed tomography images	There is an interest to replace computed tomography (CT) images with magnetic resonance (MR) images for a number of diagnostic and therapeutic workflows. In this article, predicting CT images from a number of magnetic resonance imaging (MRI) sequences using regression approach is explored. Two principal areas of application for estimated CT images are dose calculations in MRI-based radiotherapy treatment planning and attenuation correction for positron emission tomography (PET)/MRI. The main purpose of this work is to investigate the performance of hidden Markov (chain) models (HMMs) in comparison to hidden Markov random field (HMRF) models when predicting CT images of head. Our study shows that HMMs have clear advantages over HMRF models in this particular application. Obtained results suggest that HMMs deserve a further study for investigating their potential in modelling applications where the most natural theoretical choice would be the class of HMRF models.	0,0,0,1,0,0
A Generalized Zero-Forcing Precoder with Successive Dirty-Paper Coding in MISO Broadcast Channels	In this paper, we consider precoder designs for multiuser multiple-input-single-output (MISO) broadcasting channels. Instead of using a traditional linear zero-forcing (ZF) precoder, we propose a generalized ZF (GZF) precoder in conjunction with successive dirty-paper coding (DPC) for data-transmissions, namely, the GZF-DP precoder, where the suffix \lq{}DP\rq{} stands for \lq{}dirty-paper\rq{}. The GZF-DP precoder is designed to generate a band-shaped and lower-triangular effective channel $\vec{F}$ such that only the entries along the main diagonal and the $\nu$ first lower-diagonals can take non-zero values. Utilizing the successive DPC, the known non-causal inter-user interferences from the other (up to) $\nu$ users are canceled through successive encoding. We analyze optimal GZF-DP precoder designs both for sum-rate and minimum user-rate maximizations. Utilizing Lagrange multipliers, the optimal precoders for both cases are solved in closed-forms in relation to optimal power allocations. For the sum-rate maximization, the optimal power allocation can be found through water-filling, but with modified water-levels depending on the parameter $\nu$. While for the minimum user-rate maximization that measures the quality of the service (QoS), the optimal power allocation is directly solved in closed-form which also depends on $\nu$. Moreover, we propose two low-complexity user-ordering algorithms for the GZF-DP precoder designs for both maximizations, respectively. We show through numerical results that, the proposed GZF-DP precoder with a small $\nu$ ($\leq\!3$) renders significant rate increments compared to the previous precoder designs such as the linear ZF and user-grouping based DPC (UG-DP) precoders.	1,0,0,0,0,0
End-to-End Musical Key Estimation Using a Convolutional Neural Network	We present an end-to-end system for musical key estimation, based on a convolutional neural network. The proposed system not only out-performs existing key estimation methods proposed in the academic literature; it is also capable of learning a unified model for diverse musical genres that performs comparably to existing systems specialised for specific genres. Our experiments confirm that different genres do differ in their interpretation of tonality, and thus a system tuned e.g. for pop music performs subpar on pieces of electronic music. They also reveal that such cross-genre setups evoke specific types of error (predicting the relative or parallel minor). However, using the data-driven approach proposed in this paper, we can train models that deal with multiple musical styles adequately, and without major losses in accuracy.	1,0,0,0,0,0
About Synchronized Globular Cluster Formation over Supra-galactic Scales	Observational and theoretical arguments support the idea that violent events connected with $AGN$ activity and/or intense star forming episodes have played a significant role in the early phases of galaxy formation at high red shifts. Being old stellar systems, globular clusters seem adequate candidates to search for the eventual signatures that might have been left by those energetic phenomena. The analysis of the colour distributions of several thousands of globular clusters in the Virgo and Fornax galaxy clusters reveals the existence of some interesting and previously undetected features. A simple pattern recognition technique, indicates the presence of "colour modulations", distinctive for each galaxy cluster. The results suggest that the globular cluster formation process has not been completely stochastic but, rather, included a significant fraction of globulars that formed in a synchronized way and over supra-galactic spatial scales.	0,1,0,0,0,0
Sequential Checking: Reallocation-Free Data-Distribution Algorithm for Scale-out Storage	Using tape or optical devices for scale-out storage is one option for storing a vast amount of data. However, it is impossible or almost impossible to rewrite data with such devices. Thus, scale-out storage using such devices cannot use standard data-distribution algorithms because they rewrite data for moving between servers constituting the scale-out storage when the server configuration is changed. Although using rewritable devices for scale-out storage, when server capacity is huge, rewriting data is very hard when server constitution is changed. In this paper, a data-distribution algorithm called Sequential Checking is proposed, which can be used for scale-out storage composed of devices that are hardly able to rewrite data. Sequential Checking 1) does not need to move data between servers when the server configuration is changed, 2) distribute data, the amount of which depends on the server's volume, 3) select a unique server when datum is written, and 4) select servers when datum is read (there are few such server(s) in most cases) and find out a unique server that stores the newest datum from them. These basic characteristics were confirmed through proofs and simulations. Data can be read by accessing 1.98 servers on average from a storage comprising 256 servers under a realistic condition. And it is confirmed by evaluations in real environment that access time is acceptable. Sequential Checking makes selecting scale-out storage using tape or optical devices or using huge capacity servers realistic.	1,0,0,0,0,0
A General Framework of Multi-Armed Bandit Processes by Arm Switch Restrictions	This paper proposes a general framework of multi-armed bandit (MAB) processes by introducing a type of restrictions on the switches among arms evolving in continuous time. The Gittins index process is constructed for any single arm subject to the restrictions on switches and then the optimality of the corresponding Gittins index rule is established. The Gittins indices defined in this paper are consistent with the ones for MAB processes in continuous time, integer time, semi-Markovian setting as well as general discrete time setting, so that the new theory covers the classical models as special cases and also applies to many other situations that have not yet been touched in the literature. While the proof of the optimality of Gittins index policies benefits from ideas in the existing theory of MAB processes in continuous time, new techniques are introduced which drastically simplify the proof.	0,0,0,1,0,0
Relativistic Spacecraft Propelled by Directed Energy	Achieving relativistic flight to enable extrasolar exploration is one of the dreams of humanity and the long term goal of our NASA Starlight program. We derive a fully relativistic solution for the motion of a spacecraft propelled by radiation pressure from a directed energy system. Depending on the system parameters, low mass spacecraft can achieve relativistic speeds; thereby enabling interstellar exploration. The diffraction of the directed energy system plays an important role and limits the maximum speed of the spacecraft. We consider 'photon recycling' as a possible method to achieving higher speeds. We also discuss recent claims that our previous work on this topic is incorrect and show that these claims arise from an improper treatment of causality.	0,1,0,0,0,0
A Semantics Comparison Workbench for a Concurrent, Asynchronous, Distributed Programming Language	A number of high-level languages and libraries have been proposed that offer novel and simple to use abstractions for concurrent, asynchronous, and distributed programming. The execution models that realise them, however, often change over time---whether to improve performance, or to extend them to new language features---potentially affecting behavioural and safety properties of existing programs. This is exemplified by SCOOP, a message-passing approach to concurrent object-oriented programming that has seen multiple changes proposed and implemented, with demonstrable consequences for an idiomatic usage of its core abstraction. We propose a semantics comparison workbench for SCOOP with fully and semi-automatic tools for analysing and comparing the state spaces of programs with respect to different execution models or semantics. We demonstrate its use in checking the consistency of properties across semantics by applying it to a set of representative programs, and highlighting a deadlock-related discrepancy between the principal execution models of SCOOP. Furthermore, we demonstrate the extensibility of the workbench by generalising the formalisation of an execution model to support recently proposed extensions for distributed programming. Our workbench is based on a modular and parameterisable graph transformation semantics implemented in the GROOVE tool. We discuss how graph transformations are leveraged to atomically model intricate language abstractions, how the visual yet algebraic nature of the model can be used to ascertain soundness, and highlight how the approach could be applied to similar languages.	1,0,0,0,0,0
Multiferroic Quantum Criticality	The zero-temperature limit of a continuous phase transition is marked by a quantum critical point, which can generate exotic physics that extends to elevated temperatures. Magnetic quantum criticality is now well known, and has been explored in systems ranging from heavy fermion metals to quantum Ising materials. Ferroelectric quantum critical behaviour has also been recently established, motivating a flurry of research investigating its consequences. Here, we introduce the concept of multiferroic quantum criticality, in which both magnetic and ferroelectric quantum criticality occur in the same system. We develop the phenomenology of multiferroic quantum critical behaviour, describe the associated experimental signatures, and propose material systems and schemes to realize it.	0,1,0,0,0,0
TF Boosted Trees: A scalable TensorFlow based framework for gradient boosting	TF Boosted Trees (TFBT) is a new open-sourced frame-work for the distributed training of gradient boosted trees. It is based on TensorFlow, and its distinguishing features include a novel architecture, automatic loss differentiation, layer-by-layer boosting that results in smaller ensembles and faster prediction, principled multi-class handling, and a number of regularization techniques to prevent overfitting.	1,0,0,1,0,0
Spherical Planetary Robot for Rugged Terrain Traversal	Wheeled planetary rovers such as the Mars Exploration Rovers (MERs) and Mars Science Laboratory (MSL) have provided unprecedented, detailed images of the Mars surface. However, these rovers are large and are of high-cost as they need to carry sophisticated instruments and science laboratories. We propose the development of low-cost planetary rovers that are the size and shape of cantaloupes and that can be deployed from a larger rover. The rover named SphereX is 2 kg in mass, is spherical, holonomic and contains a hopping mechanism to jump over rugged terrain. A small low-cost rover complements a larger rover, particularly to traverse rugged terrain or roll down a canyon, cliff or crater to obtain images and science data. While it may be a one-way journey for these small robots, they could be used tactically to obtain high-reward science data. The robot is equipped with a pair of stereo cameras to perform visual navigation and has room for a science payload. In this paper, we analyze the design and development of a laboratory prototype. The results show a promising pathway towards development of a field system.	1,1,0,0,0,0
A Concave Optimization Algorithm for Matching Partially Overlapping Point Sets	Point matching refers to the process of finding spatial transformation and correspondences between two sets of points. In this paper, we focus on the case that there is only partial overlap between two point sets. Following the approach of the robust point matching method, we model point matching as a mixed linear assignment-least square problem and show that after eliminating the transformation variable, the resulting problem of minimization with respect to point correspondence is a concave optimization problem. Furthermore, this problem has the property that the objective function can be converted into a form with few nonlinear terms via a linear transformation. Based on these properties, we employ the branch-and-bound (BnB) algorithm to optimize the resulting problem where the dimension of the search space is small. To further improve efficiency of the BnB algorithm where computation of the lower bound is the bottleneck, we propose a new lower bounding scheme which has a k-cardinality linear assignment formulation and can be efficiently solved. Experimental results show that the proposed algorithm outperforms state-of-the-art methods in terms of robustness to disturbances and point matching accuracy.	1,0,0,0,0,0
A Pliable Index Coding Approach to Data Shuffling	A promising research area that has recently emerged, is on how to use index coding to improve the communication efficiency in distributed computing systems, especially for data shuffling in iterative computations. In this paper, we posit that pliable index coding can offer a more efficient framework for data shuffling, as it can better leverage the many possible shuffling choices to reduce the number of transmissions. We theoretically analyze pliable index coding under data shuffling constraints, and design a hierarchical data-shuffling scheme that uses pliable coding as a component. We find benefits up to $O(ns/m)$ over index coding, where $ns/m$ is the average number of workers caching a message, and $m$, $n$, and $s$ are the numbers of messages, workers, and cache size, respectively.	1,0,0,0,0,0
On the representation of finite convex geometries with convex sets	Very recently Richter and Rogers proved that any convex geometry can be represented by a family of convex polygons in the plane. We shall generalize their construction and obtain a wide variety of convex shapes for representing convex geometries. We present an Erdos-Szekeres type obstruction, which answers a question of Czedli negatively, that is general convex geometries cannot be represented with ellipses in the plane. Moreover, we shall prove that one cannot even bound the number of common supporting lines of the pairs of the representing convex sets. In higher dimensions we prove that all convex geometries can be represented with ellipsoids.	0,0,1,0,0,0
Understanding and predicting travel time with spatio-temporal features of network traffic flow, weather and incidents	Travel time on a route varies substantially by time of day and from day to day. It is critical to understand to what extent this variation is correlated with various factors, such as weather, incidents, events or travel demand level in the context of dynamic networks. This helps a better decision making for infrastructure planning and real-time traffic operation. We propose a data-driven approach to understand and predict highway travel time using spatio-temporal features of those factors, all of which are acquired from multiple data sources. The prediction model holistically selects the most related features from a high-dimensional feature space by correlation analysis, principle component analysis and LASSO. We test and compare the performance of several regression models in predicting travel time 30 min in advance via two case studies: (1) a 6-mile highway corridor of I-270N in D.C. region, and (2) a 2.3-mile corridor of I-376E in Pittsburgh region. We found that some bottlenecks scattered in the network can imply congestion on those corridors at least 30 minutes in advance, including those on the alternative route to the corridors of study. In addition, real-time travel time is statistically related to incidents on some specific locations, morning/afternoon travel demand, visibility, precipitation, wind speed/gust and the weather type. All those spatio-temporal information together help improve prediction accuracy, comparing to using only speed data. In both case studies, random forest shows the most promise, reaching a root-mean-squared error of 16.6\% and 17.0\% respectively in afternoon peak hours for the entire year of 2014.	0,0,0,1,0,0
Saliency Detection by Forward and Backward Cues in Deep-CNNs	As prior knowledge of objects or object features helps us make relations for similar objects on attentional tasks, pre-trained deep convolutional neural networks (CNNs) can be used to detect salient objects on images regardless of the object class is in the network knowledge or not. In this paper, we propose a top-down saliency model using CNN, a weakly supervised CNN model trained for 1000 object labelling task from RGB images. The model detects attentive regions based on their objectness scores predicted by selected features from CNNs. To estimate the salient objects effectively, we combine both forward and backward features, while demonstrating that partially-guided backpropagation will provide sufficient information for selecting the features from forward run of CNN model. Finally, these top-down cues are enhanced with a state-of-the-art bottom-up model as complementing the overall saliency. As the proposed model is an effective integration of forward and backward cues through objectness without any supervision or regression to ground truth data, it gives promising results compared to state-of-the-art models in two different datasets.	1,0,0,0,0,0
Duality and Universal Transport in a Mixed-Dimension Electrodynamics	We consider a theory of a two-component Dirac fermion localized on a (2+1) dimensional brane coupled to a (3+1) dimensional bulk. Using the fermionic particle-vortex duality, we show that the theory has a strong-weak duality that maps the coupling $e$ to $\tilde e=(8\pi)/e$. We explore the theory at $e^2=8\pi$ where it is self-dual. The electrical conductivity of the theory is a constant independent of frequency. When the system is at finite density and magnetic field at filling factor $\nu=\frac12$, the longitudinal and Hall conductivity satisfies a semicircle law, and the ratio of the longitudinal and Hall thermal electric coefficients is completely determined by the Hall angle. The thermal Hall conductivity is directly related to the thermal electric coefficients.	0,1,0,0,0,0
Extended superalgebras from twistor and Killing spinors	The basic first-order differential operators of spin geometry that are Dirac operator and twistor operator are considered. Special types of spinors defined from these operators such as twistor spinors and Killing spinors are discussed. Symmetry operators of massless and massive Dirac equations are introduced and relevant symmetry operators of twistor spinors and Killing spinors are constructed from Killing-Yano (KY) and conformal Killing-Yano (CKY) forms in constant curvature and Einstein manifolds. The squaring map of spinors gives KY and CKY forms for Killing and twistor spinors respectively. They constitute a graded Lie algebra structure in some special cases. By using the graded Lie algebra structure of KY and CKY forms, extended Killing and conformal superalgebras are constructed in constant curvature and Einstein manifolds.	0,0,1,0,0,0
An FPT Algorithm Beating 2-Approximation for $k$-Cut	In the $k$-Cut problem, we are given an edge-weighted graph $G$ and an integer $k$, and have to remove a set of edges with minimum total weight so that $G$ has at least $k$ connected components. Prior work on this problem gives, for all $h \in [2,k]$, a $(2-h/k)$-approximation algorithm for $k$-cut that runs in time $n^{O(h)}$. Hence to get a $(2 - \varepsilon)$-approximation algorithm for some absolute constant $\varepsilon$, the best runtime using prior techniques is $n^{O(k\varepsilon)}$. Moreover, it was recently shown that getting a $(2 - \varepsilon)$-approximation for general $k$ is NP-hard, assuming the Small Set Expansion Hypothesis. If we use the size of the cut as the parameter, an FPT algorithm to find the exact $k$-Cut is known, but solving the $k$-Cut problem exactly is $W[1]$-hard if we parameterize only by the natural parameter of $k$. An immediate question is: \emph{can we approximate $k$-Cut better in FPT-time, using $k$ as the parameter?} We answer this question positively. We show that for some absolute constant $\varepsilon > 0$, there exists a $(2 - \varepsilon)$-approximation algorithm that runs in time $2^{O(k^6)} \cdot \widetilde{O} (n^4)$. This is the first FPT algorithm that is parameterized only by $k$ and strictly improves the $2$-approximation.	1,0,0,0,0,0
Sums of two cubes as twisted perfect powers, revisited	In this paper, we sharpen earlier work of the first author, Luca and Mulholland, showing that the Diophantine equation $$ A^3+B^3 = q^\alpha C^p, \, \, ABC \neq 0, \, \, \gcd (A,B) =1, $$ has, for "most" primes $q$ and suitably large prime exponents $p$, no solutions. We handle a number of (presumably infinite) families where no such conclusion was hitherto known. Through further application of certain {\it symplectic criteria}, we are able to make some conditional statements about still more values of $q$, a sample such result is that, for all but $O(\sqrt{x}/\log x)$ primes $q$ up to $x$, the equation $$ A^3 + B^3 = q C^p. $$ has no solutions in coprime, nonzero integers $A, B$ and $C$, for a positive proportion of prime exponents $p$.	0,0,1,0,0,0
End-to-End Attention based Text-Dependent Speaker Verification	A new type of End-to-End system for text-dependent speaker verification is presented in this paper. Previously, using the phonetically discriminative/speaker discriminative DNNs as feature extractors for speaker verification has shown promising results. The extracted frame-level (DNN bottleneck, posterior or d-vector) features are equally weighted and aggregated to compute an utterance-level speaker representation (d-vector or i-vector). In this work we use speaker discriminative CNNs to extract the noise-robust frame-level features. These features are smartly combined to form an utterance-level speaker vector through an attention mechanism. The proposed attention model takes the speaker discriminative information and the phonetic information to learn the weights. The whole system, including the CNN and attention model, is joint optimized using an end-to-end criterion. The training algorithm imitates exactly the evaluation process --- directly mapping a test utterance and a few target speaker utterances into a single verification score. The algorithm can automatically select the most similar impostor for each target speaker to train the network. We demonstrated the effectiveness of the proposed end-to-end system on Windows $10$ "Hey Cortana" speaker verification task.	1,0,0,1,0,0
Neural Networks for Predicting Algorithm Runtime Distributions	Many state-of-the-art algorithms for solving hard combinatorial problems in artificial intelligence (AI) include elements of stochasticity that lead to high variations in runtime, even for a fixed problem instance. Knowledge about the resulting runtime distributions (RTDs) of algorithms on given problem instances can be exploited in various meta-algorithmic procedures, such as algorithm selection, portfolios, and randomized restarts. Previous work has shown that machine learning can be used to individually predict mean, median and variance of RTDs. To establish a new state-of-the-art in predicting RTDs, we demonstrate that the parameters of an RTD should be learned jointly and that neural networks can do this well by directly optimizing the likelihood of an RTD given runtime observations. In an empirical study involving five algorithms for SAT solving and AI planning, we show that neural networks predict the true RTDs of unseen instances better than previous methods, and can even do so when only few runtime observations are available per training instance.	1,0,0,0,0,0
A bootstrap test to detect prominent Granger-causalities across frequencies	Granger-causality in the frequency domain is an emerging tool to analyze the causal relationship between two time series. We propose a bootstrap test on unconditional and conditional Granger-causality spectra, as well as on their difference, to catch particularly prominent causality cycles in relative terms. In particular, we consider a stochastic process derived applying independently the stationary bootstrap to the original series. Our null hypothesis is that each causality or causality difference is equal to the median across frequencies computed on that process. In this way, we are able to disambiguate causalities which depart significantly from the median one obtained ignoring the causality structure. Our test shows power one as the process tends to non-stationarity, thus being more conservative than parametric alternatives. As an example, we infer about the relationship between money stock and GDP in the Euro Area via our approach, considering inflation, unemployment and interest rates as conditioning variables. We point out that during the period 1999-2017 the money stock aggregate M1 had a significant impact on economic output at all frequencies, while the opposite relationship is significant only at high frequencies.	0,0,0,1,0,1
Effect of stellar flares on the upper atmospheres of HD 189733b and HD 209458b	Stellar flares are a frequent occurrence on young low-mass stars around which many detected exoplanets orbit. Flares are energetic, impulsive events, and their impact on exoplanetary atmospheres needs to be taken into account when interpreting transit observations. We have developed a model to describe the upper atmosphere of Extrasolar Giant Planets (EGPs) orbiting flaring stars. The model simulates thermal escape from the upper atmospheres of close-in EGPs. Ionisation by solar radiation and electron impact is included and photochemical and diffusive transport processes are simulated. This model is used to study the effect of stellar flares from the solar-like G star HD209458 and the young K star HD189733 on their respective planets. A hypothetical HD209458b-like planet orbiting the active M star AU Mic is also simulated. We find that the neutral upper atmosphere of EGPs is not significantly affected by typical flares. Therefore, stellar flares alone would not cause large enough changes in planetary mass loss to explain the variations in HD189733b transit depth seen in previous studies, although we show that it may be possible that an extreme stellar proton event could result in the required mass loss. Our simulations do however reveal an enhancement in electron number density in the ionosphere of these planets, the peak of which is located in the layer where stellar X-rays are absorbed. Electron densities are found to reach 2.2 to 3.5 times pre-flare levels and enhanced electron densities last from about 3 to 10 hours after the onset of the flare. The strength of the flare and the width of its spectral energy distribution affect the range of altitudes that see enhancements in ionisation. A large broadband continuum component in the XUV portion of the flaring spectrum in very young flare stars, such as AU Mic, results in a broad range of altitudes affected in planets orbiting this star.	0,1,0,0,0,0
SuperMinHash - A New Minwise Hashing Algorithm for Jaccard Similarity Estimation	This paper presents a new algorithm for calculating hash signatures of sets which can be directly used for Jaccard similarity estimation. The new approach is an improvement over the MinHash algorithm, because it has a better runtime behavior and the resulting signatures allow a more precise estimation of the Jaccard index.	1,0,0,0,0,0
Deep Multiple Instance Feature Learning via Variational Autoencoder	We describe a novel weakly supervised deep learning framework that combines both the discriminative and generative models to learn meaningful representation in the multiple instance learning (MIL) setting. MIL is a weakly supervised learning problem where labels are associated with groups of instances (referred as bags) instead of individual instances. To address the essential challenge in MIL problems raised from the uncertainty of positive instances label, we use a discriminative model regularized by variational autoencoders (VAEs) to maximize the differences between latent representations of all instances and negative instances. As a result, the hidden layer of the variational autoencoder learns meaningful representation. This representation can effectively be used for MIL problems as illustrated by better performance on the standard benchmark datasets comparing to the state-of-the-art approaches. More importantly, unlike most related studies, the proposed framework can be easily scaled to large dataset problems, as illustrated by the audio event detection and segmentation task. Visualization also confirms the effectiveness of the latent representation in discriminating positive and negative classes.	0,0,0,1,0,0
A stronger version of a question proposed by K. Mahler	In 1902, P. Stäckel proved the existence of a transcendental function $f(z)$, analytic in a neighbourhood of the origin, and with the property that both $f(z)$ and its inverse function assume, in this neighbourhood, algebraic values at all algebraic points. Based on this result, in 1976, K. Mahler raised the question of the existence of such functions which are analytic in $\mathbb{C}$. Recently, the authors answered positively this question. In this paper, we prove a much stronger version of this result by considering other subsets of $\mathbb{C}$.	0,0,1,0,0,0
Performance Measurements of Supercomputing and Cloud Storage Solutions	Increasing amounts of data from varied sources, particularly in the fields of machine learning and graph analytics, are causing storage requirements to grow rapidly. A variety of technologies exist for storing and sharing these data, ranging from parallel file systems used by supercomputers to distributed block storage systems found in clouds. Relatively few comparative measurements exist to inform decisions about which storage systems are best suited for particular tasks. This work provides these measurements for two of the most popular storage technologies: Lustre and Amazon S3. Lustre is an open-source, high performance, parallel file system used by many of the largest supercomputers in the world. Amazon's Simple Storage Service, or S3, is part of the Amazon Web Services offering, and offers a scalable, distributed option to store and retrieve data from anywhere on the Internet. Parallel processing is essential for achieving high performance on modern storage systems. The performance tests used span the gamut of parallel I/O scenarios, ranging from single-client, single-node Amazon S3 and Lustre performance to a large-scale, multi-client test designed to demonstrate the capabilities of a modern storage appliance under heavy load. These results show that, when parallel I/O is used correctly (i.e., many simultaneous read or write processes), full network bandwidth performance is achievable and ranged from 10 gigabits/s over a 10 GigE S3 connection to 0.35 terabits/s using Lustre on a 1200 port 10 GigE switch. These results demonstrate that S3 is well-suited to sharing vast quantities of data over the Internet, while Lustre is well-suited to processing large quantities of data locally.	1,1,0,0,0,0
Exploring Features for Predicting Policy Citations	In this study we performed an initial investigation and evaluation of altmetrics and their relationship with public policy citation of research papers. We examined methods for using altmetrics and other data to predict whether a research paper is cited in public policy and applied receiver operating characteristic curve on various feature groups in order to evaluate their potential usefulness. From the methods we tested, classifying based on tweet count provided the best results, achieving an area under the ROC curve of 0.91.	1,0,0,0,0,0
Neural Network Multitask Learning for Traffic Flow Forecasting	Traditional neural network approaches for traffic flow forecasting are usually single task learning (STL) models, which do not take advantage of the information provided by related tasks. In contrast to STL, multitask learning (MTL) has the potential to improve generalization by transferring information in training signals of extra tasks. In this paper, MTL based neural networks are used for traffic flow forecasting. For neural network MTL, a backpropagation (BP) network is constructed by incorporating traffic flows at several contiguous time instants into an output layer. Nodes in the output layer can be seen as outputs of different but closely related STL tasks. Comprehensive experiments on urban vehicular traffic flow data and comparisons with STL show that MTL in BP neural networks is a promising and effective approach for traffic flow forecasting.	1,0,0,0,0,0
Predicting Demographics, Moral Foundations, and Human Values from Digital Behaviors	Personal electronic devices including smartphones give access to behavioural signals that can be used to learn about the characteristics and preferences of individuals. In this study, we explore the connection between demographic and psychological attributes and the digital behavioural records, for a cohort of 7,633 people, closely representative of the US population with respect to gender, age, geographical distribution, education, and income. Along with the demographic data, we collected self-reported assessments on validated psychometric questionnaires for moral traits and basic human values and combined this information with passively collected multi-modal digital data from web browsing behaviour and smartphone usage. A machine learning framework was then designed to infer both the demographic and psychological attributes from the behavioural data. In a cross-validated setting, our models predicted demographic attributes with good accuracy as measured by the weighted AUROC score (Area Under the Receiver Operating Characteristic), but were less performant for the moral traits and human values. These results call for further investigation since they are still far from unveiling individuals' psychological fabric. This connection, along with the most predictive features that we provide for each attribute, might prove useful for designing personalised services, communication strategies, and interventions, and can be used to sketch a portrait of people with a similar worldview.	1,0,0,0,0,0
Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy	Over half a million individuals are diagnosed with head and neck cancer each year worldwide. Radiotherapy is an important curative treatment for this disease, but it requires manually intensive delineation of radiosensitive organs at risk (OARs). This planning process can delay treatment commencement. While auto-segmentation algorithms offer a potentially time-saving solution, the challenges in defining, quantifying and achieving expert performance remain. Adopting a deep learning approach, we demonstrate a 3D U-Net architecture that achieves performance similar to experts in delineating a wide range of head and neck OARs. The model was trained on a dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and segmented according to consensus OAR definitions. We demonstrate its generalisability through application to an independent test set of 24 CT scans available from The Cancer Imaging Archive collected at multiple international sites previously unseen to the model, each segmented by two independent experts and consisting of 21 OARs commonly segmented in clinical practice. With appropriate validation studies and regulatory approvals, this system could improve the effectiveness of radiotherapy pathways.	0,0,0,1,0,0
Protein Pattern Formation	Protein pattern formation is essential for the spatial organization of many intracellular processes like cell division, flagellum positioning, and chemotaxis. A prominent example of intracellular patterns are the oscillatory pole-to-pole oscillations of Min proteins in \textit{E. coli} whose biological function is to ensure precise cell division. Cell polarization, a prerequisite for processes such as stem cell differentiation and cell polarity in yeast, is also mediated by a diffusion-reaction process. More generally, these functional modules of cells serve as model systems for self-organization, one of the core principles of life. Under which conditions spatio-temporal patterns emerge, and how these patterns are regulated by biochemical and geometrical factors are major aspects of current research. Here we review recent theoretical and experimental advances in the field of intracellular pattern formation, focusing on general design principles and fundamental physical mechanisms.	0,0,0,0,1,0
$\mathcal{R}_{0}$ fails to predict the outbreak potential in the presence of natural-boosting immunity	Time varying susceptibility of host at individual level due to waning and boosting immunity is known to induce rich long-term behavior of disease transmission dynamics. Meanwhile, the impact of the time varying heterogeneity of host susceptibility on the shot-term behavior of epidemics is not well-studied, even though the large amount of the available epidemiological data are the short-term epidemics. Here we constructed a parsimonious mathematical model describing the short-term transmission dynamics taking into account natural-boosting immunity by reinfection, and obtained the explicit solution for our model. We found that our system show "the delayed epidemic", the epidemic takes off after negative slope of the epidemic curve at the initial phase of epidemic, in addition to the common classification in the standard SIR model, i.e., "no epidemic" as $\mathcal{R}_{0}\leq1$ or normal epidemic as $\mathcal{R}_{0}>1$. Employing the explicit solution we derived the condition for each classification.	0,0,0,0,1,0
Unexpected 3+ valence of iron in FeO$_2$, a geologically important material lying "in between" oxides and peroxides	Recent discovery of pyrite FeO$_2$, which can be an important ingredient of the Earth's lower mantle and which in particular may serve as an extra source of water in the Earth's interior, opens new perspectives for geophysics and geochemistry, but this is also an extremely interesting material from physical point of view. We found that in contrast to naive expectations Fe is nearly 3+ in this material, which strongly affects its magnetic properties and makes it qualitatively different from well known sulfide analogue - FeS$_2$. Doping, which is most likely to occur in the Earth's mantle, makes FeO$_2$ much more magnetic. In addition we show that unique electronic structure places FeO$_2$ "in between" the usual dioxides and peroxides making this system interesting both for physics and solid state chemistry.	0,1,0,0,0,0
A Bayesian Filtering Algorithm for Gaussian Mixture Models	A Bayesian filtering algorithm is developed for a class of state-space systems that can be modelled via Gaussian mixtures. In general, the exact solution to this filtering problem involves an exponential growth in the number of mixture terms and this is handled here by utilising a Gaussian mixture reduction step after both the time and measurement updates. In addition, a square-root implementation of the unified algorithm is presented and this algorithm is profiled on several simulated systems. This includes the state estimation for two non-linear systems that are strictly outside the class considered in this paper.	1,0,0,1,0,0
Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts	Understanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statistics --- cooccurrence within documents and prevalence correlation over time --- our approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other's prevalence over time, and yet rarely cooccur, almost like a "cold war" scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.	1,1,0,0,0,0
Towards security defect prediction with AI	In this study, we investigate the limits of the current state of the art AI system for detecting buffer overflows and compare it with current static analysis tools. To do so, we developed a code generator, s-bAbI, capable of producing an arbitrarily large number of code samples of controlled complexity. We found that the static analysis engines we examined have good precision, but poor recall on this dataset, except for a sound static analyzer that has good precision and recall. We found that the state of the art AI system, a memory network modeled after Choi et al. [1], can achieve similar performance to the static analysis engines, but requires an exhaustive amount of training data in order to do so. Our work points towards future approaches that may solve these problems; namely, using representations of code that can capture appropriate scope information and using deep learning methods that are able to perform arithmetic operations.	0,0,0,1,0,0
Performance of Range Separated Hybrids: Study within BECKE88 family and Semilocal Exchange Hole based Range Separated Hybrid	A long range corrected range separated hybrid functional is developed based on the density matrix expansion (DME) based semilocal exchange hole with Lee-Yang-Parr (LYP) correlation. An extensive study involving the proposed range separated hybrid for thermodynamic as well as properties related to the fractional occupation number is compared with different BECKE88 family semilocal, hybrid and range separated hybrids. It has been observed that using Kohn-Sham kinetic energy dependent exchange hole several properties related to the fractional occupation number can be improved without hindering the thermochemical accuracy. The newly constructed range separated hybrid accurately describe the hydrogen and non-hydrogen reaction barrier heights. The present range separated functional has been constructed using full semilocal meta-GGA type exchange hole having exact properties related to exchange hole therefore, it has a strong physical basis.	0,1,0,0,0,0
Learning Neural Networks with Two Nonlinear Layers in Polynomial Time	We give a polynomial-time algorithm for learning neural networks with one layer of sigmoids feeding into any Lipschitz, monotone activation function (e.g., sigmoid or ReLU). We make no assumptions on the structure of the network, and the algorithm succeeds with respect to {\em any} distribution on the unit ball in $n$ dimensions (hidden weight vectors also have unit norm). This is the first assumption-free, provably efficient algorithm for learning neural networks with two nonlinear layers. Our algorithm-- {\em Alphatron}-- is a simple, iterative update rule that combines isotonic regression with kernel methods. It outputs a hypothesis that yields efficient oracle access to interpretable features. It also suggests a new approach to Boolean learning problems via real-valued conditional-mean functions, sidestepping traditional hardness results from computational learning theory. Along these lines, we subsume and improve many longstanding results for PAC learning Boolean functions to the more general, real-valued setting of {\em probabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d. noise-tolerance.	1,0,0,1,0,0
Viconmavlink: A software tool for indoor positioning using a motion capture system	Motion capture is a widely-used technology in robotics research thanks to its precise posi tional measurements with real-time performance. This paper presents ViconMAVLink, a cross-platform open-source software tool that provides indoor positioning services to networked robots. ViconMAVLink converts Vicon motion capture data into proper pose and motion data formats and send localization information to robots using the MAVLink protocol. The software is a convenient tool for mobile robotics researchers to conduct experiments in a controlled indoor environment.	1,0,0,0,0,0
RDV: Register, Deposit, Vote: Secure and Decentralized Consensus Mechanism for Blockchain Networks	A decentralized payment system is not secure if transactions are transferred directly between clients. In such a situation it is not possible to prevent a client from redeeming some coins twice in separate transactions that means a double-spending attack. Bitcoin uses a simple method to preventing this attack i.e. all transactions are published in a unique log (blockchain). This approach requires a global consensus on the blockchain that because of significant latency for transaction confirmation is vulnerable against double-spending. The solution is to accelerate confirmations. In this paper, we try to introduce an alternative for PoW because of all its major and significant problems that lead to collapsing decentralization of the Bitcoin, while a full decentralized payment system is the main goal of Bitcoin idea. As the network is growing and becoming larger day-today , Bitcoin is approaching this risk. The method we introduce is based on a distributed voting process: RDV: Register, Deposit, Vote.	1,0,0,0,0,0
On Gromov--Witten invariants of $\mathbb{P}^1$	We propose a conjectural explicit formula of generating series of a new type for Gromov--Witten invariants of $\mathbb{P}^1$ of all degrees in full genera.	0,1,1,0,0,0
The cost of fairness in classification	We study the problem of learning classifiers with a fairness constraint, with three main contributions towards the goal of quantifying the problem's inherent tradeoffs. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for cost-sensitive classification and fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we show how the tradeoff between accuracy and fairness is determined by the alignment between the class-probabilities for the target and sensitive features. Underpinning our analysis is a general framework that casts the problem of learning with a fairness requirement as one of minimising the difference of two statistical risks.	1,0,0,0,0,0
Petri Nets and Machines of Things That Flow	Petri nets are an established graphical formalism for modeling and analyzing the behavior of systems. An important consideration of the value of Petri nets is their use in describing both the syntax and semantics of modeling formalisms. Describing a modeling notation in terms of a formal technique such as Petri nets provides a way to minimize ambiguity. Accordingly, it is imperative to develop a deep and diverse understanding of Petri nets. This paper is directed toward a new, but preliminary, exploration of the semantics of such an important tool. Specifically, the concern in this paper is with the semantics of Petri nets interpreted in a modeling language based on the notion of machines of things that flow. The semantics of several Petri net diagrams are analyzed in terms of flow of things. The results point to the viability of the approach for exploring the underlying assumptions of Petri nets.	1,0,0,0,0,0
A step towards Twist Conjecture	Under the assumption that a defining graph of a Coxeter group admits only twists in $\mathbb{Z}_2$ and is of type FC, we prove Mühlherr's Twist Conjecture.	0,0,1,0,0,0
Marked Temporal Dynamics Modeling based on Recurrent Neural Network	We are now witnessing the increasing availability of event stream data, i.e., a sequence of events with each event typically being denoted by the time it occurs and its mark information (e.g., event type). A fundamental problem is to model and predict such kind of marked temporal dynamics, i.e., when the next event will take place and what its mark will be. Existing methods either predict only the mark or the time of the next event, or predict both of them, yet separately. Indeed, in marked temporal dynamics, the time and the mark of the next event are highly dependent on each other, requiring a method that could simultaneously predict both of them. To tackle this problem, in this paper, we propose to model marked temporal dynamics by using a mark-specific intensity function to explicitly capture the dependency between the mark and the time of the next event. Extensive experiments on two datasets demonstrate that the proposed method outperforms state-of-the-art methods at predicting marked temporal dynamics.	1,0,0,0,0,0
Cuntz semigroups of compact-type Hopf C*-algebras	The classical Cuntz semigroup has an important role in the study of C*-algebras, being one of the main invariants used to classify recalcitrant C*-algebras up to isomorphism. We consider C*-algebras that have Hopf algebra structure, and find additional structure in their Cuntz semigroups, thus generalizing the equivariant Cuntz semigroup. We develop various aspects of the theory of such semigroups, and in particular, we give general results allowing classification results of the Elliott classification program to be extended to classification results for C*-algebraic quantum groups.	0,0,1,0,0,0
Optimizing noise level for perturbing geo-location data	With the tremendous increase in the number of smart phones, app stores have been overwhelmed with applications requiring geo-location access in order to provide their users better services through personalization. Revealing a user's location to these third party apps, no matter at what frequency, is a severe privacy breach which can have unpleasant social consequences. In order to prevent inference attacks derived from geo-location data, a number of location obfuscation techniques have been proposed in the literature. However, none of them provides any objective measure of privacy guarantee. Some work has been done to define differential privacy for geo-location data in the form of geo-indistinguishability with l privacy guarantee. These techniques do not utilize any prior background information about the Points of Interest (PoIs) of a user and apply Laplacian noise to perturb all the location coordinates. Intuitively, the utility of such a mechanism can be improved if the noise distribution is derived after considering some prior information about PoIs. In this paper, we apply the standard definition of differential privacy on geo-location data. We use first principles to model various privacy and utility constraints, prior background information available about the PoIs (distribution of PoI locations in a 1D plane) and the granularity of the input required by different types of apps, in order to produce a more accurate and a utility maximizing differentially private algorithm for geo-location data at the OS level. We investigate this for a particular category of apps and for some specific scenarios. This will also help us to verify that whether Laplacian noise is still the optimal perturbation when we have such prior information.	1,0,0,0,0,0
Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks	We propose a method, called Label Embedding Network, which can learn label representation (label embedding) during the training process of deep networks. With the proposed method, the label embedding is adaptively and automatically learned through back propagation. The original one-hot represented loss function is converted into a new loss function with soft distributions, such that the originally unrelated labels have continuous interactions with each other during the training process. As a result, the trained model can achieve substantially higher accuracy and with faster convergence speed. Experimental results based on competitive tasks demonstrate the effectiveness of the proposed method, and the learned label embedding is reasonable and interpretable. The proposed method achieves comparable or even better results than the state-of-the-art systems. The source code is available at \url{this https URL}.	1,0,0,0,0,0
Finding Submodularity Hidden in Symmetric Difference	A set function $f$ on a finite set $V$ is submodular if $f(X) + f(Y) \geq f(X \cup Y) + f(X \cap Y)$ for any pair $X, Y \subseteq V$. The symmetric difference transformation (SD-transformation) of $f$ by a canonical set $S \subseteq V$ is a set function $g$ given by $g(X) = f(X \vartriangle S)$ for $X \subseteq V$,where $X \vartriangle S = (X \setminus S) \cup (S \setminus X)$ denotes the symmetric difference between $X$ and $S$. Submodularity and SD-transformations are regarded as the counterparts of convexity and affine transformations in a discrete space, respectively. However, submodularity is not preserved under SD-transformations, in contrast to the fact that convexity is invariant under affine transformations. This paper presents a characterization of SD-stransformations preserving submodularity. Then, we are concerned with the problem of discovering a canonical set $S$, given the SD-transformation $g$ of a submodular function $f$ by $S$, provided that $g(X)$ is given by a function value oracle. A submodular function $f$ on $V$ is said to be strict if $f(X) + f(Y) > f(X \cup Y) + f(X \cap Y)$ holds whenever both $X \setminus Y$ and $Y \setminus X$ are nonempty. We show that the problem is solved by using ${\rm O}(|V|)$ oracle calls when $f$ is strictly submodular, although it requires exponentially many oracle calls in general.	1,0,0,0,0,0
Deriving Enhanced Geographical Representations via Similarity-based Spectral Analysis: Predicting Colorectal Cancer Survival Curves in Iowa	Neural networks are capable of learning rich, nonlinear feature representations shown to be beneficial in many predictive tasks. In this work, we use such models to explore different geographical feature representations in the context of predicting colorectal cancer survival curves for patients in the state of Iowa, spanning the years 1989 to 2013. Specifically, we compare model performance using "area between the curves" (ABC) to assess (a) whether survival curves can be reasonably predicted for colorectal cancer patients in the state of Iowa, (b) whether geographical features improve predictive performance, (c) whether a simple binary representation, or a richer, spectral analysis-elicited representation perform better, and (d) whether spectral analysis-based representations can be improved upon by leveraging geographically-descriptive features. In exploring (d), we devise a similarity-based spectral analysis procedure, which allows for the combination of geographically relational and geographically descriptive features. Our findings suggest that survival curves can be reasonably estimated on average, with predictive performance deviating at the five-year survival mark among all models. We also find that geographical features improve predictive performance, and that better performance is obtained using richer, spectral analysis-elicited features. Furthermore, we find that similarity-based spectral analysis-elicited representations improve upon the original spectral analysis results by approximately 40%.	0,0,0,1,0,0
An Applied Knowledge Framework to Study Complex Systems	The complexity of knowledge production on complex systems is well-known, but there still lacks knowledge framework that would both account for a certain structure of knowledge production at an epistemological level and be directly applicable to the study and management of complex systems. We set a basis for such a framework, by first analyzing in detail a case study of the construction of a geographical theory of complex territorial systems, through mixed methods, namely qualitative interview analysis and quantitative citation network analysis. We can therethrough inductively build a framework that considers knowledge entreprises as perspectives, with co-evolving components within complementary knowledge domains. We finally discuss potential applications and developments.	1,1,0,0,0,0
Temporally Evolving Community Detection and Prediction in Content-Centric Networks	In this work, we consider the problem of combining link, content and temporal analysis for community detection and prediction in evolving networks. Such temporal and content-rich networks occur in many real-life settings, such as bibliographic networks and question answering forums. Most of the work in the literature (that uses both content and structure) deals with static snapshots of networks, and they do not reflect the dynamic changes occurring over multiple snapshots. Incorporating dynamic changes in the communities into the analysis can also provide useful insights about the changes in the network such as the migration of authors across communities. In this work, we propose Chimera, a shared factorization model that can simultaneously account for graph links, content, and temporal analysis. This approach works by extracting the latent semantic structure of the network in multidimensional form, but in a way that takes into account the temporal continuity of these embeddings. Such an approach simplifies temporal analysis of the underlying network by using the embedding as a surrogate. A consequence of this simplification is that it is also possible to use this temporal sequence of embeddings to predict future communities. We present experimental results illustrating the effectiveness of the approach.	1,0,0,1,0,0
Light yield determination in large sodium iodide detectors applied in the search for dark matter	Application of NaI(Tl) detectors in the search for galactic dark matter particles through their elastic scattering off the target nuclei is well motivated because of the long standing DAMA/LIBRA highly significant positive result on annual modulation, still requiring confirmation. For such a goal, it is mandatory to reach very low threshold in energy (at or below the keV level), very low radioactive background (at a few counts/keV/kg/day), and high detection mass (at or above the 100 kg scale). One of the most relevant technical issues is the optimization of the crystal intrinsic scintillation light yield and the efficiency of the light collecting system for large mass crystals. In the frame of the ANAIS (Annual modulation with NaI Scintillators) dark matter search project large NaI(Tl) crystals from different providers coupled to two photomultiplier tubes (PMTs) have been tested at the Canfranc Underground Laboratory. In this paper we present the estimates of the NaI(Tl) scintillation light collected using full-absorption peaks at very low energy from external and internal sources emitting gammas/electrons, and single-photoelectron events populations selected by using very low energy pulses tails. Outstanding scintillation light collection at the level of 15~photoelectrons/keV can be reported for the final design and provider chosen for ANAIS detectors. Taking into account the Quantum Efficiency of the PMT units used, the intrinsic scintillation light yield in these NaI(Tl) crystals is above 40~photoelectrons/keV for energy depositions in the range from 3 up to 25~keV. This very high light output of ANAIS crystals allows triggering below 1~keV, which is very important in order to increase the sensitivity in the direct detection of dark matter.	0,1,0,0,0,0
On the existence of homoclinic type solutions of inhomogenous Lagrangian systems	We study the existence of homoclinic type solutions for second order Lagrangian systems of the type $\ddot{q}(t)-q(t)+a(t)\nabla G(q(t))=f(t)$, where $t\in\mathbb{R}$, $q\in\mathbb{R}^n$, $a\colon\mathbb{R}\to\mathbb{R}$ is a continuous positive bounded function, $G\colon\mathbb{R}^n\to\mathbb{R}$ is a $C^1$-smooth potential satisfying the Ambrosetti-Rabinowitz superquadratic growth condition and $f\colon\mathbb{R}\to\mathbb{R}^n$ is a continuous bounded square integrable forcing term. A homoclinic type solution is obtained as limit of $2k$-periodic solutions of an approximative sequence of second order differential equations.	0,0,1,0,0,0
End-to-End Optimized Transmission over Dispersive Intensity-Modulated Channels Using Bidirectional Recurrent Neural Networks	We propose an autoencoding sequence-based transceiver for communication over dispersive channels with intensity modulation and direct detection (IM/DD), designed as a bidirectional deep recurrent neural network (BRNN). The receiver uses a sliding window technique to allow for efficient data stream estimation. We find that this sliding window BRNN (SBRNN), based on end-to-end deep learning of the communication system, achieves a significant bit-error-rate reduction at all examined distances in comparison to previous block-based autoencoders implemented as feed-forward neural networks (FFNNs), leading to an increase of the transmission distance. We also compare the end-to-end SBRNN with a state-of-the-art IM/DD solution based on two level pulse amplitude modulation with an FFNN receiver, simultaneously processing multiple received symbols and approximating nonlinear Volterra equalization. Our results show that the SBRNN outperforms such systems at both 42 and 84\,Gb/s, while training fewer parameters. Our novel SBRNN design aims at tailoring the end-to-end deep learning-based systems for communication over nonlinear channels with memory, such as the optical IM/DD fiber channel.	1,0,0,1,0,0
A novel distribution-free hybrid regression model for manufacturing process efficiency improvement	This work is motivated by a particular problem of a modern paper manufacturing industry, in which maximum efficiency of the fiber-filler recovery process is desired. A lot of unwanted materials along with valuable fibers and fillers come out as a by-product of the paper manufacturing process and mostly goes as waste. The job of an efficient Krofta supracell is to separate the unwanted materials from the valuable ones so that fibers and fillers can be collected from the waste materials and reused in the manufacturing process. The efficiency of Krofta depends on several crucial process parameters and monitoring them is a difficult proposition. To solve this problem, we propose a novel hybridization of regression trees (RT) and artificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of low recovery percentage of the supracell. This model is used to achieve the goal of improving supracell efficiency, viz., gain in percentage recovery. In addition, theoretical results for the universal consistency of the proposed model are given with the optimal value of a vital model parameter. Experimental findings show that the proposed hybrid RT-ANN model achieves higher accuracy in predicting Krofta recovery percentage than other conventional regression models for solving the Krofta efficiency problem. This work will help the paper manufacturing company to become environmentally friendly with minimal ecological damage and improved waste recovery.	0,0,0,1,0,0
Preduals for spaces of operators involving Hilbert spaces and trace-class operators	Continuing the study of preduals of spaces $\mathcal{L}(H,Y)$ of bounded, linear maps, we consider the situation that $H$ is a Hilbert space. We establish a natural correspondence between isometric preduals of $\mathcal{L}(H,Y)$ and isometric preduals of $Y$. The main ingredient is a Tomiyama-type result which shows that every contractive projection that complements $\mathcal{L}(H,Y)$ in its bidual is automatically a right $\mathcal{L}(H)$-module map. As an application, we show that isometric preduals of $\mathcal{L}(\mathcal{S}_1)$, the algebra of operators on the space of trace-class operators, correspond to isometric preduals of $\mathcal{S}_1$ itself (and there is an abundance of them). On the other hand, the compact operators are the unique predual of $\mathcal{S}_1$ making its multiplication separately weak* continuous.	0,0,1,0,0,0
Improved Point Source Detection in Crowded Fields using Probabilistic Cataloging	Cataloging is challenging in crowded fields because sources are extremely covariant with their neighbors and blending makes even the number of sources ambiguous. We present the first optical probabilistic catalog, cataloging a crowded (~0.1 sources per pixel brighter than 22nd magnitude in F606W) Sloan Digital Sky Survey r band image from M2. Probabilistic cataloging returns an ensemble of catalogs inferred from the image and thus can capture source-source covariance and deblending ambiguities. By comparing to a traditional catalog of the same image and a Hubble Space Telescope catalog of the same region, we show that our catalog ensemble better recovers sources from the image. It goes more than a magnitude deeper than the traditional catalog while having a lower false discovery rate brighter than 20th magnitude. We also present an algorithm for reducing this catalog ensemble to a condensed catalog that is similar to a traditional catalog, except it explicitly marginalizes over source-source covariances and nuisance parameters. We show that this condensed catalog has a similar completeness and false discovery rate to the catalog ensemble. Future telescopes will be more sensitive, and thus more of their images will be crowded. Probabilistic cataloging performs better than existing software in crowded fields and so should be considered when creating photometric pipelines in the Large Synoptic Space Telescope era.	0,1,0,0,0,0
Exponential Source/Channel Duality	We propose a source/channel duality in the exponential regime, where success/failure in source coding parallels error/correctness in channel coding, and a distortion constraint becomes a log-likelihood ratio (LLR) threshold. We establish this duality by first deriving exact exponents for lossy coding of a memoryless source P, at distortion D, for a general i.i.d. codebook distribution Q, for both encoding success (R < R(P,Q,D)) and failure (R > R(P,Q,D)). We then turn to maximum likelihood (ML) decoding over a memoryless channel P with an i.i.d. input Q, and show that if we substitute P=QP, Q=Q, and D=0 under the LLR distortion measure, then the exact exponents for decoding-error (R < I(Q, P)) and strict correct-decoding (R > I(Q, P)) follow as special cases of the exponents for source encoding success/failure, respectively. Moreover, by letting the threshold D take general values, the exact random-coding exponents for erasure (D > 0) and list decoding (D < 0) under the simplified Forney decoder are obtained. Finally, we derive the exact random-coding exponent for Forney's optimum tradeoff erasure/list decoder, and show that at the erasure regime it coincides with Forney's lower bound and with the simplified decoder exponent.	1,0,0,0,0,0
One Password: An Encryption Scheme for Hiding Users' Register Information	In recent years, the attack which leverages register information (e.g. accounts and passwords) leaked from 3rd party applications to try other applications is popular and serious. We call this attack "database collision". Traditionally, people have to keep dozens of accounts and passwords for different applications to prevent this attack. In this paper, we propose a novel encryption scheme for hiding users' register information and preventing this attack. Specifically, we first hash the register information using existing safe hash function. Then the hash string is hidden, instead a coefficient vector is stored for verification. Coefficient vectors of the same register information are generated randomly for different applications. Hence, the original information is hardly cracked by dictionary based attack or database collision in practice. Using our encryption scheme, each user only needs to keep one password for dozens of applications.	1,0,0,0,0,0
Using Artificial Neural Networks (ANN) to Control Chaos	Controlling Chaos could be a big factor in getting great stable amounts of energy out of small amounts of not necessarily stable resources. By definition, Chaos is getting huge changes in the system's output due to unpredictable small changes in initial conditions, and that means we could take advantage of this fact and select the proper control system to manipulate system's initial conditions and inputs in general and get a desirable output out of otherwise a Chaotic system. That was accomplished by first building some known chaotic circuit (Chua circuit) and the NI's MultiSim was used to simulate the ANN control system. It was shown that this technique can also be used to stabilize some hard to stabilize electronic systems.	1,1,0,0,0,0
A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization	This paper aims to explore models based on the extreme gradient boosting (XGBoost) approach for business risk classification. Feature selection (FS) algorithms and hyper-parameter optimizations are simultaneously considered during model training. The five most commonly used FS methods including weight by Gini, weight by Chi-square, hierarchical variable clustering, weight by correlation, and weight by information are applied to alleviate the effect of redundant features. Two hyper-parameter optimization approaches, random search (RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in XGBoost. The effect of different FS and hyper-parameter optimization methods on the model performance are investigated by the Wilcoxon Signed Rank Test. The performance of XGBoost is compared to the traditionally utilized logistic regression (LR) model in terms of classification accuracy, area under the curve (AUC), recall, and F1 score obtained from the 10-fold cross validation. Results show that hierarchical clustering is the optimal FS method for LR while weight by Chi-square achieves the best performance in XG-Boost. Both TPE and RS optimization in XGBoost outperform LR significantly. TPE optimization shows a superiority over RS since it results in a significantly higher accuracy and a marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE tuning shows a lower variability than the RS method. Finally, the ranking of feature importance based on XGBoost enhances the model interpretation. Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an operative while powerful approach for business risk modeling.	1,0,0,1,0,0
FEAST Eigensolver for Nonlinear Eigenvalue Problems	The linear FEAST algorithm is a method for solving linear eigenvalue problems. It uses complex contour integration to calculate the eigenvectors whose eigenvalues that are located inside some user-defined region in the complex plane. This makes it possible to parallelize the process of solving eigenvalue problems by simply dividing the complex plane into a collection of disjoint regions and calculating the eigenpairs in each region independently of the eigenpairs in the other regions. In this paper we present a generalization of the linear FEAST algorithm that can be used to solve nonlinear eigenvalue problems. Like its linear progenitor, the nonlinear FEAST algorithm can be used to solve nonlinear eigenvalue problems for the eigenpairs whose eigenvalues lie in a user-defined region in the complex plane, thereby allowing for the calculation of large numbers of eigenpairs in parallel. We describe the nonlinear FEAST algorithm, and use several physically-motivated examples to demonstrate its properties.	1,0,0,0,0,0
Simultaneous non-vanishing for Dirichlet L-functions	We extend the work of Fouvry, Kowalski and Michel on correlation between Hecke eigenvalues of modular forms and algebraic trace functions in order to establish an asymptotic formula for a generalized cubic moment of modular L-functions at the central point s = 1/2 and for prime moduli q. As an application, we exploit our recent result on the mollification of the fourth moment of Dirichlet L-functions to derive that for any pair $(\omega_1,\omega_2)$ of multiplicative characters modulo q, there is a positive proportion of $\chi$ (mod q) such that $L(\chi, 1/2 ), L(\chi\omega_1, 1/2 )$ and $L(\chi\omega_2, 1/2)$ are simultaneously not too small.	0,0,1,0,0,0
Stable basic sets for finite special linear and unitary group	In this paper we show, using Deligne-Lusztig theory and Kawanaka's theory of generalised Gelfand-Graev representations, that the decomposition matrix of the special linear and unitary group in non defining characteristic can be made unitriangular with respect to a basic set that is stable under the action of automorphisms.	0,0,1,0,0,0
A second-order stochastic maximum principle for generalized mean-field control problem	In this paper, we study the generalized mean-field stochastic control problem when the usual stochastic maximum principle (SMP) is not applicable due to the singularity of the Hamiltonian function. In this case, we derive a second order SMP. We introduce the adjoint process by the generalized mean-field backward stochastic differential equation. The keys in the proofs are the expansion of the cost functional in terms of a perturbation parameter, and the use of the range theorem for vector-valued measures.	0,0,1,0,0,0
Porosity and regularity in metric measure spaces	This is a report of a joint work with E. Järvenpää, M. Järvenpää, T. Rajala, S. Rogovin, and V. Suomala. In [3], we characterized uniformly porous sets in $s$-regular metric spaces in terms of regular sets by verifying that a set $A$ is uniformly porous if and only if there is $t < s$ and a $t$-regular set $F \supset A$. Here we outline the main idea of the proof and also present an alternative proof for the crucial lemma needed in the proof of the result.	0,0,1,0,0,0
Human-in-the-loop Artificial Intelligence	Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future has a dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, these workers are digging their own graves. In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Robin Hoods, HIT-AI researchers should fight for a fairer Artificial Intelligence that gives back what it steals.	1,0,0,0,0,0
Arrow Categories of Monoidal Model Categories	We prove that the arrow category of a monoidal model category, equipped with the pushout product monoidal structure and the projective model structure, is a monoidal model category. This answers a question posed by Mark Hovey, and has the important consequence that it allows for the consideration of a monoidal product in cubical homotopy theory. As illustrations we include numerous examples of non-cofibrantly generated monoidal model categories, including chain complexes, small categories, topological spaces, and pro-categories.	0,0,1,0,0,0
The rational SPDE approach for Gaussian random fields with general smoothness	A popular approach for modeling and inference in spatial statistics is to represent Gaussian random fields as solutions to stochastic partial differential equations (SPDEs) of the form $L^{\beta}u = \mathcal{W}$, where $\mathcal{W}$ is Gaussian white noise, $L$ is a second-order differential operator, and $\beta>0$ is a parameter that determines the smoothness of $u$. However, this approach has been limited to the case $2\beta\in\mathbb{N}$, which excludes several important covariance models and makes it necessary to keep $\beta$ fixed during inference. We introduce a new method, the rational SPDE approach, which is applicable for any $\beta>0$ and therefore remedies the mentioned limitation. The presented scheme combines a finite element discretization in space with a rational approximation of the function $x^{-\beta}$ to approximate $u$. For the resulting approximation, an explicit rate of strong convergence to $u$ is derived and we show that the method has the same computational benefits as in the restricted case $2\beta\in\mathbb{N}$ when used for statistical inference and prediction. Several numerical experiments are performed to illustrate the accuracy of the method, and to show how it can be used for likelihood-based inference for all model parameters including $\beta$.	0,0,0,1,0,0
Size scaling of failure strength with fat-tailed disorder in a fiber bundle model	We investigate the size scaling of the macroscopic fracture strength of heterogeneous materials when microscopic disorder is controlled by fat-tailed distributions. We consider a fiber bundle model where the strength of single fibers is described by a power law distribution over a finite range. Tuning the amount of disorder by varying the power law exponent and the upper cutoff of fibers' strength, in the limit of equal load sharing an astonishing size effect is revealed: For small system sizes the bundle strength increases with the number of fibers and the usual decreasing size effect of heterogeneous materials is only restored beyond a characteristic size. We show analytically that the extreme order statistics of fibers' strength is responsible for this peculiar behavior. Analyzing the results of computer simulations we deduce a scaling form which describes the dependence of the macroscopic strength of fiber bundles on the parameters of microscopic disorder over the entire range of system sizes.	0,1,0,0,0,0
Bingham flow in porous media with obstacles of different size	By using the unfolding operators for periodic homogenization, we give a general compactness result for a class of functions defined on bounded domains presenting perforations of two different size. Then we apply this result to the homogenization of the flow of a Bingham fluid in a porous medium with solid obstacles of different size. Next we give the interpretation of the limit problem in term of a non linear Darcy law.	0,0,1,0,0,0
K-Means Clustering using Tabu Search with Quantized Means	The Tabu Search (TS) metaheuristic has been proposed for K-Means clustering as an alternative to Lloyd's algorithm, which for all its ease of implementation and fast runtime, has the major drawback of being trapped at local optima. While the TS approach can yield superior performance, it involves a high computational complexity. Moreover, the difficulty in parameter selection in the existing TS approach does not make it any more attractive. This paper presents an alternative, low-complexity formulation of the TS optimization procedure for K-Means clustering. This approach does not require many parameter settings. We initially constrain the centers to points in the dataset. We then aim at evolving these centers using a unique neighborhood structure that makes use of gradient information of the objective function. This results in an efficient exploration of the search space, after which the means are refined. The proposed scheme is implemented in MATLAB and tested on four real-world datasets, and it achieves a significant improvement over the existing TS approach in terms of the intra cluster sum of squares and computational time.	1,0,0,0,0,0
PriMaL: A Privacy-Preserving Machine Learning Method for Event Detection in Distributed Sensor Networks	This paper introduces PriMaL, a general PRIvacy-preserving MAchine-Learning method for reducing the privacy cost of information transmitted through a network. Distributed sensor networks are often used for automated classification and detection of abnormal events in high-stakes situations, e.g. fire in buildings, earthquakes, or crowd disasters. Such networks might transmit privacy-sensitive information, e.g. GPS location of smartphones, which might be disclosed if the network is compromised. Privacy concerns might slow down the adoption of the technology, in particular in the scenario of social sensing where participation is voluntary, thus solutions are needed which improve privacy without compromising on the event detection accuracy. PriMaL is implemented as a machine-learning layer that works on top of an existing event detection algorithm. Experiments are run in a general simulation framework, for several network topologies and parameter values. The privacy footprint of state-of-the-art event detection algorithms is compared within the proposed framework. Results show that PriMaL is able to reduce the privacy cost of a distributed event detection algorithm below that of the corresponding centralized algorithm, within the bounds of some assumptions about the protocol. Moreover the performance of the distributed algorithm is not statistically worse than that of the centralized algorithm.	1,0,0,0,0,0
The Structure Transfer Machine Theory and Applications	Representation learning is a fundamental but challenging problem, especially when the distribution of data is unknown. We propose a new representation learning method, termed Structure Transfer Machine (STM), which enables feature learning process to converge at the representation expectation in a probabilistic way. We theoretically show that such an expected value of the representation (mean) is achievable if the manifold structure can be transferred from the data space to the feature space. The resulting structure regularization term, named manifold loss, is incorporated into the loss function of the typical deep learning pipeline. The STM architecture is constructed to enforce the learned deep representation to satisfy the intrinsic manifold structure from the data, which results in robust features that suit various application scenarios, such as digit recognition, image classification and object tracking. Compared to state-of-the-art CNN architectures, we achieve the better results on several commonly used benchmarks\footnote{The source code is available. this https URL }.	0,0,0,1,0,0
Cold-Start Reinforcement Learning with Softmax Policy Gradient	Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.	1,0,0,0,0,0
A critical analysis of resampling strategies for the regularized particle filter	We analyze the performance of different resampling strategies for the regularized particle filter regarding parameter estimation. We show in particular, building on analytical insight obtained in the linear Gaussian case, that resampling systematically can prevent the filtered density from converging towards the true posterior distribution. We discuss several means to overcome this limitation, including kernel bandwidth modulation, and provide evidence that the resulting particle filter clearly outperforms traditional bootstrap particle filters. Our results are supported by numerical simulations on a linear textbook example, the logistic map and a non-linear plant growth model.	0,0,1,1,0,0
Knowledge distillation using unlabeled mismatched images	Current approaches for Knowledge Distillation (KD) either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance. Our examples include use of various datasets for stimulating MNIST and CIFAR teachers.	1,0,0,1,0,0
Continuous User Authentication via Unlabeled Phone Movement Patterns	In this paper, we propose a novel continuous authentication system for smartphone users. The proposed system entirely relies on unlabeled phone movement patterns collected through smartphone accelerometer. The data was collected in a completely unconstrained environment over five to twelve days. The contexts of phone usage were identified using k-means clustering. Multiple profiles, one for each context, were created for every user. Five machine learning algorithms were employed for classification of genuine and impostors. The performance of the system was evaluated over a diverse population of 57 users. The mean equal error rates achieved by Logistic Regression, Neural Network, kNN, SVM, and Random Forest were 13.7%, 13.5%, 12.1%, 10.7%, and 5.6% respectively. A series of statistical tests were conducted to compare the performance of the classifiers. The suitability of the proposed system for different types of users was also investigated using the failure to enroll policy.	1,0,0,0,0,0
Simulated Tornado Optimization	We propose a swarm-based optimization algorithm inspired by air currents of a tornado. Two main air currents - spiral and updraft - are mimicked. Spiral motion is designed for exploration of new search areas and updraft movements is deployed for exploitation of a promising candidate solution. Assignment of just one search direction to each particle at each iteration, leads to low computational complexity of the proposed algorithm respect to the conventional algorithms. Regardless of the step size parameters, the only parameter of the proposed algorithm, called tornado diameter, can be efficiently adjusted by randomization. Numerical results over six different benchmark cost functions indicate comparable and, in some cases, better performance of the proposed algorithm respect to some other metaheuristics.	1,0,1,0,0,0
Constructing and Understanding New and Old Scales on Slide Rules	We discuss the practical problems arising when constructing any (new or old) scales on slide rules, i.e. realizing the theory in the practice. This might help anyone in planning and realizing (mainly the magnitude and labeling of) new scales on slide rules in the future. In Sections 1-7 we deal with technical problems, Section 8 is devoted to the relationship among different scales. In the last Section we provide an interesting fact as a surprise to those readers who wish to skip this long article.	0,0,1,0,0,0
A multiplier inclusion theorem on product domains	In this note it is shown that the class of all multipliers from the $d$-parameter Hardy space $H^1_{\mathrm{prod}} (\mathbb{T}^d)$ to $L^2 (\mathbb{T}^d)$ is properly contained in the class of all multipliers from $L \log^{d/2} L (\mathbb{T}^d)$ to $L^2(\mathbb{T}^d)$.	0,0,1,0,0,0
Towards Adversarial Retinal Image Synthesis	Synthesizing images of the eye fundus is a challenging task that has been previously approached by formulating complex models of the anatomy of the eye. New images can then be generated by sampling a suitable parameter space. In this work, we propose a method that learns to synthesize eye fundus images directly from data. For that, we pair true eye fundus images with their respective vessel trees, by means of a vessel segmentation technique. These pairs are then used to learn a mapping from a binary vessel tree to a new retinal image. For this purpose, we use a recent image-to-image translation technique, based on the idea of adversarial learning. Experimental results show that the original and the generated images are visually different in terms of their global appearance, in spite of sharing the same vessel tree. Additionally, a quantitative quality analysis of the synthetic retinal images confirms that the produced images retain a high proportion of the true image set quality.	1,0,0,1,0,0
Causal Queries from Observational Data in Biological Systems via Bayesian Networks: An Empirical Study in Small Networks	Biological networks are a very convenient modelling and visualisation tool to discover knowledge from modern high-throughput genomics and postgenomics data sets. Indeed, biological entities are not isolated, but are components of complex multi-level systems. We go one step further and advocate for the consideration of causal representations of the interactions in living systems.We present the causal formalism and bring it out in the context of biological networks, when the data is observational. We also discuss its ability to decipher the causal information flow as observed in gene expression. We also illustrate our exploration by experiments on small simulated networks as well as on a real biological data set.	0,0,0,1,1,0
A mean-field approach to Kondo-attractive-Hubbard model	With the purpose of investigating coexistence between magnetic order and superconductivity, we consider a model in which conduction electrons interact with each other, via an attractive Hubbard on-site coupling $U$, and with local moments on every site, via a Kondo-like coupling, $J$. The model is solved on a simple cubic lattice through a Hartree-Fock approximation, within a `semi-classical' framework which allows spiral magnetic modes to be stabilized. For a fixed electronic density, $n_c$, the small $J$ region of the ground state ($T=0$) phase diagram displays spiral antiferromagnetic (SAFM) states for small $U$. Upon increasing $U$, a state with coexistence between superconductivity (SC) and SAFM sets in; further increase in $U$ turns the spiral mode into a Néel antiferromagnet. The large $J$ region is a (singlet) Kondo phase. At finite temperatures, and in the region of coexistence, thermal fluctuations suppress the different ordered phases in succession: the SAFM phase at lower temperatures and SC at higher temperatures; also, reentrant behaviour is found to be induced by temperature. Our results provide a qualitative description of the competition between local moment magnetism and superconductivity in the borocarbides family.	0,1,0,0,0,0
Gradient Flows in Uncertainty Propagation and Filtering of Linear Gaussian Systems	The purpose of this work is mostly expository and aims to elucidate the Jordan-Kinderlehrer-Otto (JKO) scheme for uncertainty propagation, and a variant, the Laugesen-Mehta-Meyn-Raginsky (LMMR) scheme for filtering. We point out that these variational schemes can be understood as proximal operators in the space of density functions, realizing gradient flows. These schemes hold the promise of leading to efficient ways for solving the Fokker-Planck equation as well as the equations of non-linear filtering. Our aim in this paper is to develop in detail the underlying ideas in the setting of linear stochastic systems with Gaussian noise and recover known results.	1,0,1,0,0,0
Fast-neutron and gamma-ray imaging with a capillary liquid xenon converter coupled to a gaseous photomultiplier	Gamma-ray and fast-neutron imaging was performed with a novel liquid xenon (LXe) scintillation detector read out by a Gaseous Photomultiplier (GPM). The 100 mm diameter detector prototype comprised a capillary-filled LXe converter/scintillator, coupled to a triple-THGEM imaging-GPM, with its first electrode coated by a CsI UV-photocathode, operated in Ne/5%CH4 cryogenic temperatures. Radiation localization in 2D was derived from scintillation-induced photoelectron avalanches, measured on the GPM's segmented anode. The localization properties of Co-60 gamma-rays and a mixed fast-neutron/gamma-ray field from an AmBe neutron source were derived from irradiation of a Pb edge absorber. Spatial resolutions of 12+/-2 mm and 10+/-2 mm (FWHM) were reached with Co-60 and AmBe sources, respectively. The experimental results are in good agreement with GEANT4 simulations. The calculated ultimate expected resolutions for our application-relevant 4.4 and 15.1 MeV gamma-rays and 1-15 MeV neutrons are 2-4 mm and ~2 mm (FWHM), respectively. These results indicate the potential applicability of the new detector concept to Fast-Neutron Resonance Radiography (FNRR) and Dual-Discrete-Energy Gamma Radiography (DDEGR) of large objects.	0,1,0,0,0,0
Structure preserving schemes for mean-field equations of collective behavior	In this paper we consider the development of numerical schemes for mean-field equations describing the collective behavior of a large group of interacting agents. The schemes are based on a generalization of the classical Chang-Cooper approach and are capable to preserve the main structural properties of the systems, namely nonnegativity of the solution, physical conservation laws, entropy dissipation and stationary solutions. In particular, the methods here derived are second order accurate in transient regimes whereas they can reach arbitrary accuracy asymptotically for large times. Several examples are reported to show the generality of the approach.	0,1,0,0,0,0
Decentralization of Multiagent Policies by Learning What to Communicate	Effective communication is required for teams of robots to solve sophisticated collaborative tasks. In practice it is typical for both the encoding and semantics of communication to be manually defined by an expert; this is true regardless of whether the behaviors themselves are bespoke, optimization based, or learned. We present an agent architecture and training methodology using neural networks to learn task-oriented communication semantics based on the example of a communication-unaware expert policy. A perimeter defense game illustrates the system's ability to handle dynamically changing numbers of agents and its graceful degradation in performance as communication constraints are tightened or the expert's observability assumptions are broken.	1,0,0,0,0,0
Optical signature of Weyl electronic structures in tantalum pnictides Ta$Pn$ ($Pn=$ P, As)	To investigate the electronic structure of Weyl semimetals Ta$Pn$ ($Pn=$P, As), optical conductivity [$\sigma(\omega)$] spectra are measured over a wide range of photon energies and temperatures, and these measured values are compared with band calculations. Two significant structures can be observed: a bending structure at $\hbar\omega\sim$85 meV in TaAs, and peaks at $\hbar\omega\sim$ 50 meV (TaP) and $\sim$30 meV (TaAs). The bending structure can be explained by the interband transition between saddle points connecting a set of $W_2$ Weyl points. The temperature dependence of the peak intensity can be fitted by assuming the interband transition between saddle points connecting a set of $W_1$ Weyl points. Owing to the different temperature dependence of the Drude weight in both materials, it is found that the Weyl points of TaAs are located near the Fermi level, whereas those of TaP are further away.	0,1,0,0,0,0
Guided Unfoldings for Finding Loops in Standard Term Rewriting	In this paper, we reconsider the unfolding-based technique that we have introduced previously for detecting loops in standard term rewriting. We improve it by guiding the unfolding process, using distinguished positions in the rewrite rules. This results in a depth-first computation of the unfoldings, whereas the original technique was breadth-first. We have implemented this new approach in our tool NTI and compared it to the previous one on a bunch of rewrite systems. The results we get are promising (better times, more successful proofs).	1,0,0,0,0,0
Numerically modeling Brownian thermal noise in amorphous and crystalline thin coatings	Thermal noise is expected to be one of the noise sources limiting the astrophysical reach of Advanced LIGO (once commissioning is complete) and third-generation detectors. Adopting crystalline materials for thin, reflecting mirror coatings, rather than the amorphous coatings used in current-generation detectors, could potentially reduce thermal noise. Understanding and reducing thermal noise requires accurate theoretical models, but modeling thermal noise analytically is especially challenging with crystalline materials. Thermal noise models typically rely on the fluctuation-dissipation theorem, which relates the power spectral density of the thermal noise to an auxiliary elastic problem. In this paper, we present results from a new, open-source tool that numerically solves the auxiliary elastic problem to compute the Brownian thermal noise for both amorphous and crystalline coatings. We employ open-source frameworks to solve the auxiliary elastic problem using a finite-element method, adaptive mesh refinement, and parallel processing that enables us to use high resolutions capable of resolving the thin reflective coating. We compare with approximate analytic solutions for amorphous materials, and we verify that our solutions scale as expected. Finally, we model the crystalline coating thermal noise in an experiment reported by Cole and collaborators (2013), comparing our results to a simpler numerical calculation that treats the coating as an "effectively amorphous" material. We find that treating the coating as a cubic crystal instead of as an effectively amorphous material increases the thermal noise by about 3%. Our results are a step toward better understanding and reducing thermal noise to increase the reach of future gravitational-wave detectors. (Abstract abbreviated.)	0,1,0,0,0,0
Counting points on hyperelliptic curves with explicit real multiplication in arbitrary genus	We present a probabilistic Las Vegas algorithm for computing the local zeta function of a genus-$g$ hyperelliptic curve defined over $\mathbb F_q$ with explicit real multiplication (RM) by an order $\Z[\eta]$ in a degree-$g$ totally real number field. It is based on the approaches by Schoof and Pila in a more favorable case where we can split the $\ell$-torsion into $g$ kernels of endomorphisms, as introduced by Gaudry, Kohel, and Smith in genus 2. To deal with these kernels in any genus, we adapt a technique that the author, Gaudry, and Spaenlehauer introduced to model the $\ell$-torsion by structured polynomial systems. Applying this technique to the kernels, the systems we obtain are much smaller and so is the complexity of solving them. Our main result is that there exists a constant $c>0$ such that, for any fixed $g$, this algorithm has expected time and space complexity $O((\log q)^{c})$ as $q$ grows and the characteristic is large enough. We prove that $c\le 8$ and we also conjecture that the result still holds for $c=6$.	1,0,0,0,0,0
Robust quantum switch with Rydberg excitations	We develop an approach to realize a quantum switch for Rydberg excitation in atoms with $Y$-typed level configuration. We find that the steady population on two different Rydberg states can be reversibly exchanged in a controllable way by properly tuning the Rydberg-Rydberg interaction. Moreover, our numerical simulations verify that the switching scheme is robust against spontaneous decay, environmental disturbance, as well as the duration of operation on the interaction, and also a high switching efficiency is quite attainable, which makes it have potential applications in quantum information processing and other Rydberg-based quantum technologies.	0,1,0,0,0,0
Fiber Orientation Estimation Guided by a Deep Network	Diffusion magnetic resonance imaging (dMRI) is currently the only tool for noninvasively imaging the brain's white matter tracts. The fiber orientation (FO) is a key feature computed from dMRI for fiber tract reconstruction. Because the number of FOs in a voxel is usually small, dictionary-based sparse reconstruction has been used to estimate FOs with a relatively small number of diffusion gradients. However, accurate FO estimation in regions with complex FO configurations in the presence of noise can still be challenging. In this work we explore the use of a deep network for FO estimation in a dictionary-based framework and propose an algorithm named Fiber Orientation Reconstruction guided by a Deep Network (FORDN). FORDN consists of two steps. First, we use a smaller dictionary encoding coarse basis FOs to represent the diffusion signals. To estimate the mixture fractions of the dictionary atoms (and thus coarse FOs), a deep network is designed specifically for solving the sparse reconstruction problem. Here, the smaller dictionary is used to reduce the computational cost of training. Second, the coarse FOs inform the final FO estimation, where a larger dictionary encoding dense basis FOs is used and a weighted l1-norm regularized least squares problem is solved to encourage FOs that are consistent with the network output. FORDN was evaluated and compared with state-of-the-art algorithms that estimate FOs using sparse reconstruction on simulated and real dMRI data, and the results demonstrate the benefit of using a deep network for FO estimation.	1,0,0,0,0,0
Overcoming data scarcity with transfer learning	Despite increasing focus on data publication and discovery in materials science and related fields, the global view of materials data is highly sparse. This sparsity encourages training models on the union of multiple datasets, but simple unions can prove problematic as (ostensibly) equivalent properties may be measured or computed differently depending on the data source. These hidden contextual differences introduce irreducible errors into analyses, fundamentally limiting their accuracy. Transfer learning, where information from one dataset is used to inform a model on another, can be an effective tool for bridging sparse data while preserving the contextual differences in the underlying measurements. Here, we describe and compare three techniques for transfer learning: multi-task, difference, and explicit latent variable architectures. We show that difference architectures are most accurate in the multi-fidelity case of mixed DFT and experimental band gaps, while multi-task most improves classification performance of color with band gaps. For activation energies of steps in NO reduction, the explicit latent variable method is not only the most accurate, but also enjoys cancellation of errors in functions that depend on multiple tasks. These results motivate the publication of high quality materials datasets that encode transferable information, independent of industrial or academic interest in the particular labels, and encourage further development and application of transfer learning methods to materials informatics problems.	1,0,0,1,0,0
Computational Eco-Systems for Handwritten Digits Recognition	Inspired by the importance of diversity in biological system, we built an heterogeneous system that could achieve this goal. Our architecture could be summarized in two basic steps. First, we generate a diverse set of classification hypothesis using both Convolutional Neural Networks, currently the state-of-the-art technique for this task, among with other traditional and innovative machine learning techniques. Then, we optimally combine them through Meta-Nets, a family of recently developed and performing ensemble methods.	0,0,0,1,0,0
Learning Feature Nonlinearities with Non-Convex Regularized Binned Regression	For various applications, the relations between the dependent and independent variables are highly nonlinear. Consequently, for large scale complex problems, neural networks and regression trees are commonly preferred over linear models such as Lasso. This work proposes learning the feature nonlinearities by binning feature values and finding the best fit in each quantile using non-convex regularized linear regression. The algorithm first captures the dependence between neighboring quantiles by enforcing smoothness via piecewise-constant/linear approximation and then selects a sparse subset of good features. We prove that the proposed algorithm is statistically and computationally efficient. In particular, it achieves linear rate of convergence while requiring near-minimal number of samples. Evaluations on synthetic and real datasets demonstrate that algorithm is competitive with current state-of-the-art and accurately learns feature nonlinearities. Finally, we explore an interesting connection between the binning stage of our algorithm and sparse Johnson-Lindenstrauss matrices.	1,0,1,1,0,0
Dynamical phase transitions in sampling complexity	We make the case for studying the complexity of approximately simulating (sampling) quantum systems for reasons beyond that of quantum computational supremacy, such as diagnosing phase transitions. We consider the sampling complexity as a function of time $t$ due to evolution generated by spatially local quadratic bosonic Hamiltonians. We obtain an upper bound on the scaling of $t$ with the number of bosons $n$ for which approximate sampling is classically efficient. We also obtain a lower bound on the scaling of $t$ with $n$ for which any instance of the boson sampling problem reduces to this problem and hence implies that the problem is hard, assuming the conjectures of Aaronson and Arkhipov [Proc. 43rd Annu. ACM Symp. Theory Comput. STOC '11]. This establishes a dynamical phase transition in sampling complexity. Further, we show that systems in the Anderson-localized phase are always easy to sample from at arbitrarily long times. We view these results in the light of classifying phases of physical systems based on parameters in the Hamiltonian. In doing so, we combine ideas from mathematical physics and computational complexity to gain insight into the behavior of condensed matter, atomic, molecular and optical systems.	1,1,0,0,0,0
Electron Cloud Trapping In Recycler Combined Function Dipole Magnets	Electron cloud can lead to a fast instability in intense proton and positron beams in circular accelerators. In the Fermilab Recycler the electron cloud is confined within its combined function magnets. We show that the field of combined function magnets traps the electron cloud, present the results of analytical estimates of trapping, and compare them to numerical simulations of electron cloud formation. The electron cloud is located at the beam center and up to 1% of the particles can be trapped by the magnetic field. Since the process of electron cloud build-up is exponential, once trapped this amount of electrons significantly increases the density of the cloud on the next revolution. In a Recycler combined function dipole this multi-turn accumulation allows the electron cloud reaching final intensities orders of magnitude greater than in a pure dipole. The multi-turn build-up can be stopped by injection of a clearing bunch of $10^{10}$ p at any position in the ring.	0,1,0,0,0,0
A Simple Solution for Maximum Range Flight	Within the standard framework of quasi-steady flight, this paper derives a speed that realizes the maximal obtainable range per unit of fuel. If this speed is chosen at each instant of a flight plan $h(x)$ giving altitude $h$ as a function of distance $x$, a variational problem for finding an optimal $h(x)$ can be formulated and solved. It yields flight plans with maximal range, and these turn out to consist of mainly three phases using the optimal speed: starting with a climb at maximal continuous admissible thrust, ending with a continuous descent at idle thrust, and in between with a transition based on a solution of the Euler-Lagrange equation for the variational problem. A similar variational problem is derived and solved for speed-restricted flights, e.g. at 250 KIAS below 10000 ft. In contrast to the literature, the approach of this paper does not need more than standard ordinary differential equations solving variational problems to derive range-optimal trajectories. Various numerical examplesbased on a Standard Business Jet are added for illustration.	0,0,1,0,0,0
Contact Localization through Spatially Overlapping Piezoresistive Signals	Achieving high spatial resolution in contact sensing for robotic manipulation often comes at the price of increased complexity in fabrication and integration. One traditional approach is to fabricate a large number of taxels, each delivering an individual, isolated response to a stimulus. In contrast, we propose a method where the sensor simply consists of a continuous volume of piezoresistive elastomer with a number of electrodes embedded inside. We measure piezoresistive effects between all pairs of electrodes in the set, and count on this rich signal set containing the information needed to pinpoint contact location with high accuracy using regression algorithms. In our validation experiments, we demonstrate submillimeter median accuracy in locating contact on a 10mm by 16mm sensor using only four electrodes (creating six unique pairs). In addition to extracting more information from fewer wires, this approach lends itself to simple fabrication methods and makes no assumptions about the underlying geometry, simplifying future integration on robot fingers.	1,0,0,0,0,0
Model-Based Clustering of Nonparametric Weighted Networks	Water pollution is a major global environmental problem, and it poses a great environmental risk to public health and biological diversity. This work is motivated by assessing the potential environmental threat of coal mining through increased sulfate concentrations in river networks, which do not belong to any simple parametric distribution. However, existing network models mainly focus on binary or discrete networks and weighted networks with known parametric weight distributions. We propose a principled nonparametric weighted network model based on exponential-family random graph models and local likelihood estimation and study its model-based clustering with application to large-scale water pollution network analysis. We do not require any parametric distribution assumption on network weights. The proposed method greatly extends the methodology and applicability of statistical network models. Furthermore, it is scalable to large and complex networks in large-scale environmental studies and geoscientific research. The power of our proposed methods is demonstrated in simulation studies.	0,0,0,1,0,0
On the Uniqueness of FROG Methods	The problem of recovering a signal from its power spectrum, called phase retrieval, arises in many scientific fields. One of many examples is ultra-short laser pulse characterization in which the electromagnetic field is oscillating with ~10^15 Hz and phase information cannot be measured directly due to limitations of the electronic sensors. Phase retrieval is ill-posed in most cases as there are many different signals with the same Fourier transform magnitude. To overcome this fundamental ill-posedness, several measurement techniques are used in practice. One of the most popular methods for complete characterization of ultra-short laser pulses is the Frequency-Resolved Optical Gating (FROG). In FROG, the acquired data is the power spectrum of the product of the unknown pulse with its delayed replica. Therefore the measured signal is a quartic function of the unknown pulse. A generalized version of FROG, where the delayed replica is replaced by a second unknown pulse, is called blind FROG. In this case, the measured signal is quadratic with respect to both pulses. In this letter we introduce and formulate FROG-type techniques. We then show that almost all band-limited signals are determined uniquely, up to trivial ambiguities, by blind FROG measurements (and thus also by FROG), if in addition we have access to the signals power spectrum.	1,0,1,0,0,0
Computing LPMLN Using ASP and MLN Solvers	LPMLN is a recent addition to probabilistic logic programming languages. Its main idea is to overcome the rigid nature of the stable model semantics by assigning a weight to each rule in a way similar to Markov Logic is defined. We present two implementations of LPMLN, $\text{LPMLN2ASP}$ and $\text{LPMLN2MLN}$. System $\text{LPMLN2ASP}$ translates LPMLN programs into the input language of answer set solver $\text{CLINGO}$, and using weak constraints and stable model enumeration, it can compute most probable stable models as well as exact conditional and marginal probabilities. System $\text{LPMLN2MLN}$ translates LPMLN programs into the input language of Markov Logic solvers, such as $\text{ALCHEMY}$, $\text{TUFFY}$, and $\text{ROCKIT}$, and allows for performing approximate probabilistic inference on LPMLN programs. We also demonstrate the usefulness of the LPMLN systems for computing other languages, such as ProbLog and Pearl's Causal Models, that are shown to be translatable into LPMLN. (Under consideration for acceptance in TPLP)	1,0,0,0,0,0
Breaking the 3/2 barrier for unit distances in three dimensions	We prove that every set of $n$ points in $\mathbb{R}^3$ spans $O(n^{295/197+\epsilon})$ unit distances. This is an improvement over the previous bound of $O(n^{3/2})$. A key ingredient in the proof is a new result for cutting circles in $\mathbb{R}^3$ into pseudo-segments.	1,0,1,0,0,0
Stripe-Based Fragility Analysis of Concrete Bridge Classes Using Machine Learning Techniques	A framework for the generation of bridge-specific fragility utilizing the capabilities of machine learning and stripe-based approach is presented in this paper. The proposed methodology using random forests helps to generate or update fragility curves for a new set of input parameters with less computational effort and expensive re-simulation. The methodology does not place any assumptions on the demand model of various components and helps to identify the relative importance of each uncertain variable in their seismic demand model. The methodology is demonstrated through the case studies of multi-span concrete bridges in California. Geometric, material and structural uncertainties are accounted for in the generation of bridge models and fragility curves. It is also noted that the traditional lognormality assumption on the demand model leads to unrealistic fragility estimates. Fragility results obtained the proposed methodology curves can be deployed in risk assessment platform such as HAZUS for regional loss estimation.	0,0,0,1,0,0
Connection Scan Algorithm	We introduce the Connection Scan Algorithm (CSA) to efficiently answer queries to timetable information systems. The input consists, in the simplest setting, of a source position and a desired target position. The output consist is a sequence of vehicles such as trains or buses that a traveler should take to get from the source to the target. We study several problem variations such as the earliest arrival and profile problems. We present algorithm variants that only optimize the arrival time or additionally optimize the number of transfers in the Pareto sense. An advantage of CSA is that is can easily adjust to changes in the timetable, allowing the easy incorporation of known vehicle delays. We additionally introduce the Minimum Expected Arrival Time (MEAT) problem to handle possible, uncertain, future vehicle delays. We present a solution to the MEAT problem that is based upon CSA. Finally, we extend CSA using the multilevel overlay paradigm to answer complex queries on nation-wide integrated timetables with trains and buses.	1,0,0,0,0,0
Minimax Optimal Rates of Estimation in Functional ANOVA Models with Derivatives	We establish minimax optimal rates of convergence for nonparametric estimation in functional ANOVA models when data from first-order partial derivatives are available. Our results reveal that partial derivatives can improve convergence rates for function estimation with deterministic or random designs. In particular, for full $d$-interaction models, the optimal rates with first-order partial derivatives on $p$ covariates are identical to those for $(d-p)$-interaction models without partial derivatives. For additive models, the rates by using all first-order partial derivatives are root-$n$ to achieve the "parametric rate". We also investigate the minimax optimal rates for first-order partial derivative estimations when derivative data are available. Those rates coincide with the optimal rate for estimating the first-order derivative of a univariate function.	0,0,1,1,0,0
School bus routing by maximizing trip compatibility	School bus planning is usually divided into routing and scheduling due to the complexity of solving them concurrently. However, the separation between these two steps may lead to worse solutions with higher overall costs than that from solving them together. When finding the minimal number of trips in the routing problem, neglecting the importance of trip compatibility may increase the number of buses actually needed in the scheduling problem. This paper proposes a new formulation for the multi-school homogeneous fleet routing problem that maximizes trip compatibility while minimizing total travel time. This incorporates the trip compatibility for the scheduling problem in the routing problem. Since the problem is inherently just a routing problem, finding a good solution is not cumbersome. To compare the performance of the model with traditional routing problems, we generate eight mid-size data sets. Through importing the generated trips of the routing problems into the bus scheduling (blocking) problem, it is shown that the proposed model uses up to 13% fewer buses than the common traditional routing models.	1,0,0,0,0,0
Prospects for detection of intermediate-mass black holes in globular clusters using integrated-light spectroscopy	The detection of intermediate mass black holes (IMBHs) in Galactic globular clusters (GCs) has so far been controversial. In order to characterize the effectiveness of integrated-light spectroscopy through integral field units, we analyze realistic mock data generated from state-of-the-art Monte Carlo simulations of GCs with a central IMBH, considering different setups and conditions varying IMBH mass, cluster distance, and accuracy in determination of the center. The mock observations are modeled with isotropic Jeans models to assess the success rate in identifying the IMBH presence, which we find to be primarily dependent on IMBH mass. However, even for a IMBH of considerable mass (3% of the total GC mass), the analysis does not yield conclusive results in 1 out of 5 cases, because of shot noise due to bright stars close to the IMBH line-of-sight. This stochastic variability in the modeling outcome grows with decreasing BH mass, with approximately 3 failures out of 4 for IMBHs with 0.1% of total GC mass. Finally, we find that our analysis is generally unable to exclude at 68% confidence an IMBH with mass of $10^3~M_\odot$ in snapshots without a central BH. Interestingly, our results are not sensitive to GC distance within 5-20 kpc, nor to mis-identification of the GC center by less than 2'' (<20% of the core radius). These findings highlight the value of ground-based integral field spectroscopy for large GC surveys, where systematic failures can be accounted for, but stress the importance of discrete kinematic measurements that are less affected by stochasticity induced by bright stars.	0,1,0,0,0,0
Dynamic Laplace: Efficient Centrality Measure for Weighted or Unweighted Evolving Networks	With its origin in sociology, Social Network Analysis (SNA), quickly emerged and spread to other areas of research, including anthropology, biology, information science, organizational studies, political science, and computer science. Being it's objective the investigation of social structures through the use of networks and graph theory, Social Network Analysis is, nowadays, an important research area in several domains. Social Network Analysis cope with different problems namely network metrics, models, visualization and information spreading, each one with several approaches, methods and algorithms. One of the critical areas of Social Network Analysis involves the calculation of different centrality measures (i.e.: the most important vertices within a graph). Today, the challenge is how to do this fast and efficiently, as many increasingly larger datasets are available. Recently, the need to apply such centrality algorithms to non static networks (i.e.: networks that evolve over time) is also a new challenge. Incremental and dynamic versions of centrality measures are starting to emerge (betweenness, closeness, etc). Our contribution is the proposal of two incremental versions of the Laplacian Centrality measure, that can be applied not only to large graphs but also to, weighted or unweighted, dynamically changing networks. The experimental evaluation was performed with several tests in different types of evolving networks, incremental or fully dynamic. Results have shown that our incremental versions of the algorithm can calculate node centralities in large networks, faster and efficiently than the corresponding batch version in both incremental and full dynamic network setups.	1,0,0,0,0,0
On Invariant Random Subgroups of Block-Diagonal Limits of Symmetric Groups	We classify the ergodic invariant random subgroups of block-diagonal limits of symmetric groups in the cases when the groups are simple and the associated dimension groups have finite dimensional state spaces. These block-diagonal limits arise as the transformation groups (full groups) of Bratteli diagrams that preserve the cofinality of infinite paths in the diagram. Given a simple full group $G$ admitting only a finite number of ergodic measures on the path-space $X$ of the associated Bratteli digram, we prove that every non-Dirac ergodic invariant random subgroup of $G$ arises as the stabilizer distribution of the diagonal action on $X^n$ for some $n\geq 1$. As a corollary, we establish that every group character $\chi$ of $G$ has the form $\chi(g) = Prob(g\in K)$, where $K$ is a conjugation-invariant random subgroup of $G$.	0,0,1,0,0,0
Microscopic Description of Electric and Magnetic Toroidal Multipoles in Hybrid Orbitals	We present a general formalism of multipole descriptions under the space-time inversion group. We elucidate that two types of atomic toroidal multipoles, i.e., electric and magnetic, are fundamental pieces to express electronic order parameters in addition to ordinary electric and magnetic multipoles. By deriving quantum-mechanical operators for both toroidal multipoles, we show that electric (magnetic) toroidal multipole higher than dipole (monopole) can become a primary order parameter in a hybridized-orbital system. We also demonstrate emergent cross-correlated couplings between electric, magnetic, and elastic degrees of freedom, such as magneto-electric and magneto(electro)-elastic couplings, under toroidal multipole orders.	0,1,0,0,0,0
Merging real and virtual worlds: An analysis of the state of the art and practical evaluation of Microsoft Hololens	Achieving a symbiotic blending between reality and virtuality is a dream that has been lying in the minds of many people for a long time. Advances in various domains constantly bring us closer to making that dream come true. Augmented reality as well as virtual reality are in fact trending terms and are expected to further progress in the years to come. This master's thesis aims to explore these areas and starts by defining necessary terms such as augmented reality (AR) or virtual reality (VR). Usual taxonomies to classify and compare the corresponding experiences are then discussed. In order to enable those applications, many technical challenges need to be tackled, such as accurate motion tracking with 6 degrees of freedom (positional and rotational), that is necessary for compelling experiences and to prevent user sickness. Additionally, augmented reality experiences typically rely on image processing to position the superimposed content. To do so, "paper" markers or features extracted from the environment are often employed. Both sets of techniques are explored and common solutions and algorithms are presented. After investigating those technical aspects, I carry out an objective comparison of the existing state-of-the-art and state-of-the-practice in those domains, and I discuss present and potential applications in these areas. As a practical validation, I present the results of an application that I have developed using Microsoft HoloLens, one of the more advanced affordable technologies for augmented reality that is available today. Based on the experience and lessons learned during this development, I discuss the limitations of current technologies and present some avenues of future research.	1,0,0,0,0,0
General Robust Bayes Pseudo-Posterior: Exponential Convergence results with Applications	Although Bayesian inference is an immensely popular paradigm among a large segment of scientists including statisticians, most of the applications consider the objective priors and need critical investigations (Efron, 2013, Science). And although it has several optimal properties, one major drawback of Bayesian inference is the lack of robustness against data contamination and model misspecification, which becomes pernicious in the use of objective priors. This paper presents the general formulation of a Bayes pseudo-posterior distribution yielding robust inference. Exponential convergence results related to the new pseudo-posterior and the corresponding Bayes estimators are established under the general parametric set-up and illustrations are provided for the independent stationary models and the independent non-homogenous models. For the first case, the discrete priors and the corresponding maximum posterior estimators are discussed with additional details. We further apply this new pseudo-posterior to propose robust versions of the Bayes predictive density estimators and the expected Bayes estimator for the fixed-design (normal) linear regression models; their properties are illustrated both theoretically as well as empirically.	0,0,1,1,0,0
Marginal Release Under Local Differential Privacy	Many analysis and machine learning tasks require the availability of marginal statistics on multidimensional datasets while providing strong privacy guarantees for the data subjects. Applications for these statistics range from finding correlations in the data to fitting sophisticated prediction models. In this paper, we provide a set of algorithms for materializing marginal statistics under the strong model of local differential privacy. We prove the first tight theoretical bounds on the accuracy of marginals compiled under each approach, perform empirical evaluation to confirm these bounds, and evaluate them for tasks such as modeling and correlation testing. Our results show that releasing information based on (local) Fourier transformations of the input is preferable to alternatives based directly on (local) marginals.	1,0,0,0,0,0
On a problem of Bharanedhar and Ponnusamy involving planar harmonic mappings	In this paper, we give a negative answer to a problem presented by Bharanedhar and Ponnusamy (Rocky Mountain J. Math. 44: 753--777, 2014) concerning univalency of a class of harmonic mappings. More precisely, we show that for all values of the involved parameter, this class contains a non-univalent function. Moreover, several results on a new subclass of close-to-convex harmonic mappings, which is motivated by work of Ponnusamy and Sairam Kaliraj (Mediterr. J. Math. 12: 647--665, 2015), are obtained.	0,0,1,0,0,0
Exact diagonalization of cubic lattice models in commensurate Abelian magnetic fluxes and translational invariant non-Abelian potentials	We present a general analytical formalism to determine the energy spectrum of a quantum particle in a cubic lattice subject to translationally invariant commensurate magnetic fluxes and in the presence of a general space-independent non-Abelian gauge potential. We first review and analyze the case of purely Abelian potentials, showing also that the so-called Hasegawa gauge yields a decomposition of the Hamiltonian into sub-matrices having minimal dimension. Explicit expressions for such matrices are derived, also for general anisotropic fluxes. Later on, we show that the introduction of a translational invariant non-Abelian coupling for multi-component spinors does not affect the dimension of the minimal Hamiltonian blocks, nor the dimension of the magnetic Brillouin zone. General formulas are presented for the U(2) case and explicit examples are investigated involving $\pi$ and $2\pi/3$ magnetic fluxes. Finally, we numerically study the effect of random flux perturbations.	0,1,0,0,0,0
ViP-CNN: Visual Phrase Guided Convolutional Neural Network	As the intermediate level task connecting image captioning and object detection, visual relationship detection started to catch researchers' attention because of its descriptive power and clear structure. It detects the objects and captures their pair-wise interactions with a subject-predicate-object triplet, e.g. person-ride-horse. In this paper, each visual relationship is considered as a phrase with three components. We formulate the visual relationship detection as three inter-connected recognition problems and propose a Visual Phrase guided Convolutional Neural Network (ViP-CNN) to address them simultaneously. In ViP-CNN, we present a Phrase-guided Message Passing Structure (PMPS) to establish the connection among relationship components and help the model consider the three problems jointly. Corresponding non-maximum suppression method and model training strategy are also proposed. Experimental results show that our ViP-CNN outperforms the state-of-art method both in speed and accuracy. We further pretrain ViP-CNN on our cleansed Visual Genome Relationship dataset, which is found to perform better than the pretraining on the ImageNet for this task.	1,0,0,0,0,0
Network flow of mobile agents enhances the evolution of cooperation	We study the effect of contingent movement on the persistence of cooperation on complex networks with empty nodes. Each agent plays Prisoner's Dilemma game with its neighbors and then it either updates the strategy depending on the payoff difference with neighbors or it moves to another empty node if not satisfied with its own payoff. If no neighboring node is empty, each agent stays at the same site. By extensive evolutionary simulations, we show that the medium density of agents enhances cooperation where the network flow of mobile agents is also medium. Moreover, if the movements of agents are more frequent than the strategy updating, cooperation is further promoted. In scale-free networks, the optimal density for cooperation is lower than other networks because agents get stuck at hubs. Our study suggests that keeping a smooth network flow is significant for the persistence of cooperation in ever-changing societies.	0,0,0,0,1,0
Transfer Learning for Neural Semantic Parsing	The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with a focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence modeling and compare their performance with an independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to a target task with smaller labeled data. We see absolute accuracy gains ranging from 1.0% to 4.4% in our in- house data set, and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.	1,0,0,0,0,0
Novel Universality Classes in Ferroelectric Liquid Crystals	Starting from a Langevin formulation of a thermally perturbed nonlinear elastic model of the ferroelectric smectic-C$^*$ (SmC${*}$) liquid crystals in the presence of an electric field, this article characterizes the hitherto unexplored dynamical phase transition from a thermo-electrically forced ferroelectric SmC${}^{*}$ phase to a chiral nematic liquid crystalline phase and vice versa. The theoretical analysis is based on a combination of dynamic renormalization (DRG) and numerical simulation of the emergent model. While the DRG architecture predicts a generic transition to the Kardar-Parisi-Zhang (KPZ) universality class at dynamic equilibrium, in agreement with recent experiments, the numerical simulations of the model show simultaneous existence of two phases, one a "subdiffusive" (SD) phase characterized by a dynamical exponent value of 1, and the other a KPZ phase, characterized by a dynamical exponent value of 1.5. The SD phase flows over to the KPZ phase with increased external forcing, offering a new universality paradigm, hitherto unexplored in the context of ferroelectric liquid crystals.	0,1,0,0,0,0
Understanding Career Progression in Baseball Through Machine Learning	Professional baseball players are increasingly guaranteed expensive long-term contracts, with over 70 deals signed in excess of \$90 million, mostly in the last decade. These are substantial sums compared to a typical franchise valuation of \$1-2 billion. Hence, the players to whom a team chooses to give such a contract can have an enormous impact on both competitiveness and profit. Despite this, most published approaches examining career progression in baseball are fairly simplistic. We applied four machine learning algorithms to the problem and soundly improved upon existing approaches, particularly for batting data.	1,0,0,1,0,0
Secants, bitangents, and their congruences	A congruence is a surface in the Grassmannian $\mathrm{Gr}(1,\mathbb{P}^3)$ of lines in projective $3$-space. To a space curve $C$, we associate the Chow hypersurface in $\mathrm{Gr}(1,\mathbb{P}^3)$ consisting of all lines which intersect $C$. We compute the singular locus of this hypersurface, which contains the congruence of all secants to $C$. A surface $S$ in $\mathbb{P}^3$ defines the Hurwitz hypersurface in $\mathrm{Gr}(1,\mathbb{P}^3)$ of all lines which are tangent to $S$. We show that its singular locus has two components for general enough $S$: the congruence of bitangents and the congruence of inflectional tangents. We give new proofs for the bidegrees of the secant, bitangent and inflectional congruences, using geometric techniques such as duality, polar loci and projections. We also study the singularities of these congruences.	0,0,1,0,0,0
Depth Creates No Bad Local Minima	In deep learning, \textit{depth}, as well as \textit{nonlinearity}, create non-convex loss surfaces. Then, does depth alone create bad local minima? In this paper, we prove that without nonlinearity, depth alone does not create bad local minima, although it induces non-convex loss surface. Using this insight, we greatly simplify a recently proposed proof to show that all of the local minima of feedforward deep linear neural networks are global minima. Our theoretical results generalize previous results with fewer assumptions, and this analysis provides a method to show similar results beyond square loss in deep linear models.	1,0,1,1,0,0
Statistical mechanics of low-rank tensor decomposition	Often, large, high dimensional datasets collected across multiple modalities can be organized as a higher order tensor. Low-rank tensor decomposition then arises as a powerful and widely used tool to discover simple low dimensional structures underlying such data. However, we currently lack a theoretical understanding of the algorithmic behavior of low-rank tensor decompositions. We derive Bayesian approximate message passing (AMP) algorithms for recovering arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic mean field theory to precisely characterize their performance. Our theory reveals the existence of phase transitions between easy, hard and impossible inference regimes, and displays an excellent match with simulations. Moreover, it reveals several qualitative surprises compared to the behavior of symmetric, cubic tensor decomposition. Finally, we compare our AMP algorithm to the most commonly used algorithm, alternating least squares (ALS), and demonstrate that AMP significantly outperforms ALS in the presence of noise.	0,0,0,0,1,0
Effects of the Mach number on the evolution of vortex-surface fields in compressible Taylor--Green flows	We investigate the evolution of vortex-surface fields (VSFs) in compressible Taylor--Green flows at Mach numbers ($Ma$) ranging from 0.5 to 2.0 using direct numerical simulation. The formulation of VSFs in incompressible flows is extended to compressible flows, and a mass-based renormalization of VSFs is used to facilitate characterizing the evolution of a particular vortex surface. The effects of the Mach number on the VSF evolution are different in three stages. In the early stage, the jumps of the compressive velocity component near shocklets generate sinks to contract surrounding vortex surfaces, which shrink vortex volume and distort vortex surfaces. The subsequent reconnection of vortex surfaces, quantified by the minimal distance between approaching vortex surfaces and the exchange of vorticity fluxes, occurs earlier and has a higher reconnection degree for larger $Ma$ owing to the dilatational dissipation and shocklet-induced reconnection of vortex lines. In the late stage, the positive dissipation rate and negative pressure work accelerate the loss of kinetic energy and suppress vortex twisting with increasing $Ma$.	0,1,0,0,0,0
Turbulence Hierarchy in a Random Fibre Laser	Turbulence is a challenging feature common to a wide range of complex phenomena. Random fibre lasers are a special class of lasers in which the feedback arises from multiple scattering in a one-dimensional disordered cavity-less medium. Here, we report on statistical signatures of turbulence in the distribution of intensity fluctuations in a continuous-wave-pumped erbium-based random fibre laser, with random Bragg grating scatterers. The distribution of intensity fluctuations in an extensive data set exhibits three qualitatively distinct behaviours: a Gaussian regime below threshold, a mixture of two distributions with exponentially decaying tails near the threshold, and a mixture of distributions with stretched-exponential tails above threshold. All distributions are well described by a hierarchical stochastic model that incorporates Kolmogorov's theory of turbulence, which includes energy cascade and the intermittence phenomenon. Our findings have implications for explaining the remarkably challenging turbulent behaviour in photonics, using a random fibre laser as the experimental platform.	0,1,0,0,0,0
Modulation of High-Energy Particles and the Heliospheric Current Sheet Tilts throughout 1976-2014	Cosmic ray intensities (CRIs) recorded by sixteen neutron monitors have been used to study its dependence on the tilt angles (TA) of the heliospheric current sheet (HCS) during period 1976-2014, which covers three solar activity cycles 21, 22 and 23. The median primary rigidity covers the range 16-33 GV. Our results have indicated that the CRIs are directly sensitive to, and organized by, the interplanetary magnetic field (IMF) and its neutral sheet inclinations. The observed differences in the sensitivity of cosmic ray intensity to changes in the neutral sheet tilt angles before and after the reversal of interplanetary magnetic field polarity have been studied. Much stronger intensity-tilt angle correlation was found when the solar magnetic field in the North Polar Region was directed inward than it was outward. The rigidity dependence of sensitivities of cosmic rays differs according to the IMF polarity, for the periods 1981-1988 and 2001-2008 (qA < 0) it was R-1.00 and R-1.48 respectively, while for the 1991-1998 epoch (qA > 0) it was R-1.35. Hysteresis loops between TA and CRIs have been examined during three solar activity cycles 21, 22 and 23. A consider differences in time lags during qA > 0 and qA < 0 polarity states of the heliosphere have been observed. We also found that the cosmic ray intensity decreases at much faster rate with increase of tilt angle during qA < 0 than qA > 0, indicating stronger response to the tilt angle changes during qA < 0. Our results are discussed in the light of 3D modulation models including the gradient, curvature drifts and the tilt of the heliospheric current sheet.	0,1,0,0,0,0
Why optional stopping is a problem for Bayesians	Recently, optional stopping has been a subject of debate in the Bayesian psychology community. Rouder (2014) argues that optional stopping is no problem for Bayesians, and even recommends the use of optional stopping in practice, as do Wagenmakers et al. (2012). This article addresses the question whether optional stopping is problematic for Bayesian methods, and specifies under which circumstances and in which sense it is and is not. By slightly varying and extending Rouder's (2014) experiment, we illustrate that, as soon as the parameters of interest are equipped with default or pragmatic priors - which means, in most practical applications of Bayes Factor hypothesis testing - resilience to optional stopping can break down. We distinguish between four types of default priors, each having their own specific issues with optional stopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type II and III priors).	0,0,1,1,0,0
Enhancing Quality for VVC Compressed Videos by Jointly Exploiting Spatial Details and Temporal Structure	In this paper, we propose a quality enhancement network for Versatile Video Coding (VVC) compressed videos by jointly exploiting spatial details and temporal structure (SDTS). The network consists of a temporal structure prediction subnet and a spatial detail enhancement subnet. The former subnet is used to estimate and compensate the temporal motion across frames, and the spatial detail subnet is used to reduce the compression artifacts and enhance the reconstruction quality of the VVC compressed video. Experimental results demonstrate the effectiveness of our SDTS-based approach. It offers over 7.82$\%$ BD-rate saving on the common test video sequences and achieves the state-of-the-art performance.	1,0,0,0,0,0
Two properties of Müntz spaces	We show that Müntz spaces, as subspaces of $C[0,1]$, contain asymptotically isometric copies of $c_0$ and that their dual spaces are octahedral.	0,0,1,0,0,0
A natural framework for isogeometric fluid-structure interaction based on BEM-shell coupling	The interaction between thin structures and incompressible Newtonian fluids is ubiquitous both in nature and in industrial applications. In this paper we present an isogeometric formulation of such problems which exploits a boundary integral formulation of Stokes equations to model the surrounding flow, and a non linear Kirchhoff-Love shell theory to model the elastic behaviour of the structure. We propose three different coupling strategies: a monolithic, fully implicit coupling, a staggered, elasticity driven coupling, and a novel semi-implicit coupling, where the effect of the surrounding flow is incorporated in the non-linear terms of the solid solver through its damping characteristics. The novel semi-implicit approach is then used to demonstrate the power and robustness of our method, which fits ideally in the isogeometric paradigm, by exploiting only the boundary representation (B-Rep) of the thin structure middle surface.	0,1,1,0,0,0
Consistency of the Predicative Calculus of Cumulative Inductive Constructions (pCuIC)	In order to avoid well-know paradoxes associated with self-referential definitions, higher-order dependent type theories stratify the theory using a countably infinite hierarchy of universes (also known as sorts), Type$_0$ : Type$_1$ : $\cdots$ . Such type systems are called cumulative if for any type $A$ we have that $A$ : Type$_{i}$ implies $A$ : Type$_{i+1}$. The predicative calculus of inductive constructions (pCIC) which forms the basis of the Coq proof assistant, is one such system. In this paper we present and establish the soundness of the predicative calculus of cumulative inductive constructions (pCuIC) which extends the cumulativity relation to inductive types.	1,0,0,0,0,0
Glass+Skin: An Empirical Evaluation of the Added Value of Finger Identification to Basic Single-Touch Interaction on Touch Screens	The usability of small devices such as smartphones or interactive watches is often hampered by the limited size of command vocabularies. This paper is an attempt at better understanding how finger identification may help users invoke commands on touch screens, even without recourse to multi-touch input. We describe how finger identification can increase the size of input vocabularies under the constraint of limited real estate, and we discuss some visual cues to communicate this novel modality to novice users. We report a controlled experiment that evaluated, over a large range of input-vocabulary sizes, the efficiency of single-touch command selections with vs. without finger identification. We analyzed the data not only in terms of traditional time and error metrics, but also in terms of a throughput measure based on Shannon's theory, which we show offers a synthetic and parsimonious account of users' performance. The results show that the larger the input vocabulary needed by the designer, the more promising the identification of individual fingers.	1,0,0,0,0,0
Fréchet ChemNet Distance: A metric for generative models for molecules in drug discovery	The new wave of successful generative models in machine learning has increased the interest in deep learning driven de novo drug design. However, assessing the performance of such generative models is notoriously difficult. Metrics that are typically used to assess the performance of such generative models are the percentage of chemically valid molecules or the similarity to real molecules in terms of particular descriptors, such as the partition coefficient (logP) or druglikeness. However, method comparison is difficult because of the inconsistent use of evaluation metrics, the necessity for multiple metrics, and the fact that some of these measures can easily be tricked by simple rule-based systems. We propose a novel distance measure between two sets of molecules, called Fréchet ChemNet distance (FCD), that can be used as an evaluation metric for generative models. The FCD is similar to a recently established performance metric for comparing image generation methods, the Fréchet Inception Distance (FID). Whereas the FID uses one of the hidden layers of InceptionNet, the FCD utilizes the penultimate layer of a deep neural network called ChemNet, which was trained to predict drug activities. Thus, the FCD metric takes into account chemically and biologically relevant information about molecules, and also measures the diversity of the set via the distribution of generated molecules. The FCD's advantage over previous metrics is that it can detect if generated molecules are a) diverse and have similar b) chemical and c) biological properties as real molecules. We further provide an easy-to-use implementation that only requires the SMILES representation of the generated molecules as input to calculate the FCD. Implementations are available at: this https URL	0,0,0,1,1,0
Hybrid simulation scheme for volatility modulated moving average fields	We develop a simulation scheme for a class of spatial stochastic processes called volatility modulated moving averages. A characteristic feature of this model is that the behaviour of the moving average kernel at zero governs the roughness of realisations, whereas its behaviour away from zero determines the global properties of the process, such as long range dependence. Our simulation scheme takes this into account and approximates the moving average kernel by a power function around zero and by a step function elsewhere. For this type of approach the authors of [8], who considered an analogous model in one dimension, coined the expression hybrid simulation scheme. We derive the asymptotic mean square error of the simulation scheme and compare it in a simulation study with several other simulation techniques and exemplify its favourable performance in a simulation study.	0,0,0,1,0,0
Virtual Breakpoints for x86/64	Efficient, reliable trapping of execution in a program at the desired location is a hot area of research for security professionals. The progression of debuggers and malware is akin to a game of cat and mouse - each are constantly in a state of trying to thwart one another. At the core of most efficient debuggers today is a combination of virtual machines and traditional binary modification breakpoints (int3). In this paper, we present a design for Virtual Breakpoints, a modification to the x86 MMU which brings breakpoint management into hardware alongside page tables. We demonstrate the fundamental abstraction failures of current trapping methods, and rebuild the mechanism from the ground up. Our design delivers fast, reliable trapping without the pitfalls of binary modification.	1,0,0,0,0,0
Learning Less-Overlapping Representations	In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues. To address these problems, we study a new type of regulariza- tion approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector. We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regu- larized SC. Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.	1,0,0,1,0,0
The influence of contrarians in the dynamics of opinion formation	In this work we consider the presence of contrarian agents in discrete 3-state kinetic exchange opinion models. The contrarians are individuals that adopt the choice opposite to the prevailing choice of their contacts, whatever this choice is. We consider binary as well as three-agent interactions, with stochastic parameters, in a fully-connected population. Our numerical results suggest that the presence of contrarians destroys the absorbing state of the original model, changing the transition to the para-ferromagnetic type. In this case, the consequence for the society is that the three opinions coexist in the population, in both phases (ordered and disordered). Furthermore, the order-disorder transition is suppressed for a sufficient large fraction of contrarians. In some cases the transition is discontinuous, and it changes to continuous before it is suppressed. Some of our results are complemented by analytical calculations based on the master equation.	0,1,0,0,0,0
Improving brain computer interface performance by data augmentation with conditional Deep Convolutional Generative Adversarial Networks	One of the big restrictions in brain computer interface field is the very limited training samples, it is difficult to build a reliable and usable system with such limited data. Inspired by generative adversarial networks, we propose a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks method to generate more artificial EEG signal automatically for data augmentation to improve the performance of convolutional neural networks in brain computer interface field and overcome the small training dataset problems. We evaluate the proposed cDCGAN method on BCI competition dataset of motor imagery. The results show that the generated artificial EEG data from Gaussian noise can learn the features from raw EEG data and has no less than the classification accuracy of raw EEG data in the testing dataset. Also by using generated artificial data can effectively improve classification accuracy at the same model with limited training data.	0,0,0,1,1,0
The Cost of Transportation : Spatial Analysis of US Fuel Prices	The geography of fuel prices has many various implications, from its significant impact on accessibility to being an indicator of territorial equity and transportation policy. In this paper, we study the spatio-temporal patterns of fuel price in the US at a very high resolution using a newly constructed dataset collecting daily oil prices for two months, on a significant proportion of US gas facilities. These data have been collected using a specifically-designed large scale data crawling technology that we describe. We study the influence of socio-economic variables, by using complementary methods: Geographically Weighted Regression to take into account spatial non-stationarity, and linear econometric modeling to condition at the state and test county level characteristics. The former yields an optimal spatial range roughly corresponding to stationarity scale, and significant influence of variables such as median income or wage per job, with a non-simple spatial behavior that confirms the importance of geographical particularities. On the other hand, multi-level modeling reveals a strong state fixed effect, while county specific characteristics still have significant impact. Through the combination of such methods, we unveil the superposition of a governance process with a local socio- economical spatial process. We discuss one important application that is the elaboration of locally parametrized car-regulation policies.	0,1,0,1,0,0
Polar factorization of conformal and projective maps of the sphere in the sense of optimal mass transport	Let M be a compact Riemannian manifold and let $\mu$,d be the associated measure and distance on M. Robert McCann obtained, generalizing results for the Euclidean case by Yann Brenier, the polar factorization of Borel maps S : M -> M pushing forward $\mu$ to a measure $\nu$: each S factors uniquely a.e. into the composition S = T \circ U, where U : M -> M is volume preserving and T : M -> M is the optimal map transporting $\mu$ to $\nu$ with respect to the cost function d^2/2. In this article we study the polar factorization of conformal and projective maps of the sphere S^n. For conformal maps, which may be identified with elements of the identity component of O(1,n+1), we prove that the polar factorization in the sense of optimal mass transport coincides with the algebraic polar factorization (Cartan decomposition) of this Lie group. For the projective case, where the group GL_+(n+1) is involved, we find necessary and sufficient conditions for these two factorizations to agree.	0,0,1,0,0,0
High-power closed-cycle $^4$He cryostat with top-loading sample exchange	We report on the development of a versatile cryogen-free laboratory cryostat based upon a commercial pulse tube cryocooler. It provides enough cooling power for continuous recondensation of circulating $^4$He gas at a condensation pressure of approximately 250~mbar. Moreover, the cryostat allows for exchange of different cryostat-inserts as well as fast and easy "wet" top-loading of samples directly into the 1 K pot with a turn-over time of less than 75~min. Starting from room temperature and using a $^4$He cryostat-insert, a base temperature of 1.0~K is reached within approximately seven hours and a cooling power of 250~mW is established at 1.24~K.	0,1,0,0,0,0
Nonlinear stage of Benjamin-Feir instability in forced/damped deep water waves	We study a three-wave truncation of a recently proposed damped/forced high-order nonlinear Schrödinger equation for deep-water gravity waves under the effect of wind and viscosity. The evolution of the norm (wave-action) and spectral mean of the full model are well captured by the reduced dynamics. Three regimes are found for the wind-viscosity balance: we classify them according to the attractor in the phase-plane of the truncated system and to the shift of the spectral mean. A downshift can coexist with both net forcing and damping, i.e., attraction to period-1 or period-2 solutions. Upshift is associated with stronger winds, i.e., to a net forcing where the attractor is always a period-1 solution. The applicability of our classification to experiments in long wave-tanks is verified.	0,1,0,0,0,0
On Reliability-Aware Server Consolidation in Cloud Datacenters	In the past few years, datacenter (DC) energy consumption has become an important issue in technology world. Server consolidation using virtualization and virtual machine (VM) live migration allows cloud DCs to improve resource utilization and hence energy efficiency. In order to save energy, consolidation techniques try to turn off the idle servers, while because of workload fluctuations, these offline servers should be turned on to support the increased resource demands. These repeated on-off cycles could affect the hardware reliability and wear-and-tear of servers and as a result, increase the maintenance and replacement costs. In this paper we propose a holistic mathematical model for reliability-aware server consolidation with the objective of minimizing total DC costs including energy and reliability costs. In fact, we try to minimize the number of active PMs and racks, in a reliability-aware manner. We formulate the problem as a Mixed Integer Linear Programming (MILP) model which is in form of NP-complete. Finally, we evaluate the performance of our approach in different scenarios using extensive numerical MATLAB simulations.	1,0,0,0,0,0
Changing Fashion Cultures	The paper presents a novel concept that analyzes and visualizes worldwide fashion trends. Our goal is to reveal cutting-edge fashion trends without displaying an ordinary fashion style. To achieve the fashion-based analysis, we created a new fashion culture database (FCDB), which consists of 76 million geo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of mixed fashion styles,the paper also proposes an unsupervised fashion trend descriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal analysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD effectively emphasizes consecutive features between two different times. In experiments, we clearly show the analysis of fashion trends and fashion-based city similarity. As the result of large-scale data collection and an unsupervised analyzer, the proposed approach achieves world-level fashion visualization in a time series. The code, model, and FCDB will be publicly available after the construction of the project page.	1,0,0,0,0,0
Dispersion for the wave equation outside a ball and counterexamples	The purpose of this note is to prove dispersive estimates for the wave equation outside a ball in R^d. If d = 3, we show that the linear flow satisfies the dispersive estimates as in R^3. In higher dimensions d $\ge$ 4 we show that losses in dispersion do appear and this happens at the Poisson spot.	0,0,1,0,0,0
Temporal Pattern Discovery for Accurate Sepsis Diagnosis in ICU Patients	Sepsis is a condition caused by the body's overwhelming and life-threatening response to infection, which can lead to tissue damage, organ failure, and finally death. Common signs and symptoms include fever, increased heart rate, increased breathing rate, and confusion. Sepsis is difficult to predict, diagnose, and treat. Patients who develop sepsis have an increased risk of complications and death and face higher health care costs and longer hospitalization. Today, sepsis is one of the leading causes of mortality among populations in intensive care units (ICUs). In this paper, we look at the problem of early detection of sepsis by using temporal data mining. We focus on the use of knowledge-based temporal abstraction to create meaningful interval-based abstractions, and on time-interval mining to discover frequent interval-based patterns. We used 2,560 cases derived from the MIMIC-III database. We found that the distribution of the temporal patterns whose frequency is above 10% discovered in the records of septic patients during the last 6 and 12 hours before onset of sepsis is significantly different from that distribution within a similar period, during an equivalent time window during hospitalization, in the records of non-septic patients. This discovery is encouraging for the purpose of performing an early diagnosis of sepsis using the discovered patterns as constructed features.	1,0,0,1,0,0
Marginal likelihood based model comparison in Fuzzy Bayesian Learning	In a recent paper [1] we introduced the Fuzzy Bayesian Learning (FBL) paradigm where expert opinions can be encoded in the form of fuzzy rule bases and the hyper-parameters of the fuzzy sets can be learned from data using a Bayesian approach. The present paper extends this work for selecting the most appropriate rule base among a set of competing alternatives, which best explains the data, by calculating the model evidence or marginal likelihood. We explain why this is an attractive alternative over simply minimizing a mean squared error metric of prediction and show the validity of the proposition using synthetic examples and a real world case study in the financial services sector.	0,0,0,1,0,0
Microscopic theory of refractive index applied to metamaterials: Effective current response tensor corresponding to standard relation $n^2 = \varepsilon_{\mathrm{eff}} μ_{\mathrm{eff}}$	In this article, we first derive the wavevector- and frequency-dependent, microscopic current response tensor which corresponds to the "macroscopic" ansatz $\vec D = \varepsilon_0 \varepsilon_{\mathrm{eff}} \vec E$ and $\vec B = \mu_0 \mu_{\mathrm{eff}} \vec H$ with wavevector- and frequency-independent, "effective" material constants $\varepsilon_{\mathrm{eff}}$ and $\mu_{\mathrm{eff}}$. We then deduce the electromagnetic and optical properties of this effective material model by employing exact, microscopic response relations. In particular, we argue that for recovering the standard relation $n^2 = \varepsilon_{\mathrm{eff}} \mu_{\mathrm{eff}}$ between the refractive index and the effective material constants, it is imperative to start from the microscopic wave equation in terms of the transverse dielectric function, $\varepsilon_{\mathrm{T}}(\vec k, \omega) = 0$. On the phenomenological side, our result is especially relevant for metamaterials research, which draws directly on the standard relation for the refractive index in terms of effective material constants. Since for a wide class of materials the current response tensor can be calculated from first principles and compared to the model expression derived here, this work also paves the way for a systematic search for new metamaterials.	0,1,0,0,0,0
Wait For It: Identifying "On-Hold" Self-Admitted Technical Debt	Self-admitted technical debt refers to situations where a software developer knows that their current implementation is not optimal and indicates this using a source code comment. In this work, we hypothesize that it is possible to develop automated techniques to understand a subset of these comments in more detail, and to propose tool support that can help developers manage self-admitted technical debt more effectively. Based on a qualitative study of 335 comments indicating self-admitted technical debt, we first identify one particular class of debt amenable to automated management: "on-hold" self-admitted technical debt, i.e., debt which contains a condition to indicate that a developer is waiting for a certain event or an updated functionality having been implemented elsewhere. We then design and evaluate an automated classifier which can automatically identify these "on-hold" instances with a precision of 0.81 as well as detect the specific conditions that developers are waiting for. Our work presents a first step towards automated tool support that is able to indicate when certain instances of self-admitted technical debt are ready to be addressed.	1,0,0,0,0,0
Bayes Minimax Competitors of Preliminary Test Estimators in k Sample Problems	In this paper, we consider the estimation of a mean vector of a multivariate normal population where the mean vector is suspected to be nearly equal to mean vectors of $k-1$ other populations. As an alternative to the preliminary test estimator based on the test statistic for testing hypothesis of equal means, we derive empirical and hierarchical Bayes estimators which shrink the sample mean vector toward a pooled mean estimator given under the hypothesis. The minimaxity of those Bayesian estimators are shown, and their performances are investigated by simulation.	0,0,1,1,0,0
MOBILITY21: Strategic Investments for Transportation Infrastructure & Technology	America's transportation infrastructure is the backbone of our economy. A strong infrastructure means a strong America - an America that competes globally, supports local and regional economic development, and creates jobs. Strategic investments in our transportation infrastructure are vital to our national security, economic growth, transportation safety and our technology leadership. This document outlines critical needs for our transportation infrastructure, identifies new technology drivers and proposes strategic investments for safe and efficient air, ground, rail and marine mobility of people and goods.	1,0,0,0,0,0
Beyond Planar Symmetry: Modeling human perception of reflection and rotation symmetries in the wild	Humans take advantage of real world symmetries for various tasks, yet capturing their superb symmetry perception mechanism with a computational model remains elusive. Motivated by a new study demonstrating the extremely high inter-person accuracy of human perceived symmetries in the wild, we have constructed the first deep-learning neural network for reflection and rotation symmetry detection (Sym-NET), trained on photos from MS-COCO (Microsoft-Common Object in COntext) dataset with nearly 11K consistent symmetry-labels from more than 400 human observers. We employ novel methods to convert discrete human labels into symmetry heatmaps, capture symmetry densely in an image and quantitatively evaluate Sym-NET against multiple existing computer vision algorithms. On CVPR 2013 symmetry competition testsets and unseen MS-COCO photos, Sym-NET significantly outperforms all other competitors. Beyond mathematically well-defined symmetries on a plane, Sym-NET demonstrates abilities to identify viewpoint-varied 3D symmetries, partially occluded symmetrical objects, and symmetries at a semantic level.	1,0,0,1,0,0
SAND: An automated VLBI imaging and analysing pipeline - I. Stripping component trajectories	We present our implementation of an automated VLBI data reduction pipeline dedicated to interferometric data imaging and analysis. The pipeline can handle massive VLBI data efficiently which makes it an appropriate tool to investigate multi-epoch multiband VLBI data. Compared to traditional manual data reduction, our pipeline provides more objective results since less human interference is involved. Source extraction is done in the image plane, while deconvolution and model fitting are done in both the image plane and the uv plane for parallel comparison. The output from the pipeline includes catalogues of CLEANed images and reconstructed models, polarisation maps, proper motion estimates, core light curves and multi-band spectra. We have developed a regression strip algorithm to automatically detect linear or non-linear patterns in the jet component trajectories. This algorithm offers an objective method to match jet components at different epochs and determine their proper motions.	0,1,0,0,0,0
Possible Evidence for the Stochastic Acceleration of Secondary Antiprotons by Supernova Remnants	The antiproton-to-proton ratio in the cosmic-ray spectrum is a sensitive probe of new physics. Using recent measurements of the cosmic-ray antiproton and proton fluxes in the energy range of 1-1000 GeV, we study the contribution to the $\bar{p}/p$ ratio from secondary antiprotons that are produced and subsequently accelerated within individual supernova remnants. We consider several well-motivated models for cosmic-ray propagation in the interstellar medium and marginalize our results over the uncertainties related to the antiproton production cross section and the time-, charge-, and energy-dependent effects of solar modulation. We find that the increase in the $\bar{p}/p$ ratio observed at rigidities above $\sim$ 100 GV cannot be accounted for within the context of conventional cosmic-ray propagation models, but is consistent with scenarios in which cosmic-ray antiprotons are produced and subsequently accelerated by shocks within a given supernova remnant. In light of this, the acceleration of secondary cosmic rays in supernova remnants is predicted to substantially contribute to the cosmic-ray positron spectrum, accounting for a significant fraction of the observed positron excess.	0,1,0,0,0,0
A Note on the Spectral Transfer Morphisms for Affine Hecke Algebras	E. Opdam introduced the tool of spectral transfer morphism (STM) of affine Hecke algebras to study the formal degrees of unipotent discrete series representations. He established a uniqueness property of STM for the affine Hecke algebras associated of unipotent discrete series representations. Based on this result, Opdam gave an explanation for Lusztig's arithmetic/geometric correspondence (in Lusztig's classification of unipotent representations of $p$-adic adjoint simple groups) in terms of harmonic analysis, and partitioned the unipotent discrete series representations into $L$-packets based on the Lusztig-Langlands parameters. The present paper provides some omitted details for the argument of the uniqueness property of STM. In the last section, we prove that three finite morphisms of algebraic tori are spectral transfer morphisms, and hence complete the proof of the uniqueness property.	0,0,1,0,0,0
On Structured Prediction Theory with Calibrated Convex Surrogate Losses	We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called "calibration function" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for general structured prediction.	1,0,0,1,0,0
Microplasma generation by slow microwave in an electromagnetically induced transparency-like metasurface	Microplasma generation using microwaves in an electromagnetically induced transparency (EIT)-like metasurface composed of two types of radiatively coupled cut-wire resonators with slightly different resonance frequencies is investigated. Microplasma is generated in either of the gaps of the cut-wire resonators as a result of strong enhancement of the local electric field associated with resonance and slow microwave effect. The threshold microwave power for plasma ignition is found to reach a minimum at the EIT-like transmission peak frequency, where the group index is maximized. A pump-probe measurement of the metasurface reveals that the transmission properties can be significantly varied by varying the properties of the generated microplasma near the EIT-like transmission peak frequency and the resonance frequency. The electron density of the microplasma is roughly estimated to be of order $1\times 10^{10}\,\mathrm{cm}^{-3}$ for a pump power of $15.8\,\mathrm{W}$ by comparing the measured transmission spectrum for the probe wave with the numerically calculated spectrum. In the calculation, we assumed that the plasma is uniformly generated in the resonator gap, that the electron temperature is $2\,\mathrm{eV}$, and that the elastic scattering cross section is $20 \times 10^{-16}\,\mathrm{cm}^2$.	0,1,0,0,0,0
Cross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data	We analyse multimodal time-series data corresponding to weight, sleep and steps measurements. We focus on predicting whether a user will successfully achieve his/her weight objective. For this, we design several deep long short-term memory (LSTM) architectures, including a novel cross-modal LSTM (X-LSTM), and demonstrate their superiority over baseline approaches. The X-LSTM improves parameter efficiency by processing each modality separately and allowing for information flow between them by way of recurrent cross-connections. We present a general hyperparameter optimisation technique for X-LSTMs, which allows us to significantly improve on the LSTM and a prior state-of-the-art cross-modal approach, using a comparable number of parameters. Finally, we visualise the model's predictions, revealing implications about latent variables in this task.	1,0,0,1,0,0
Clusters of Integers with Equal Total Stopping Times in the 3x + 1 Problem	The clustering of integers with equal total stopping times has long been observed in the 3x + 1 Problem, and a number of elementary results about it have been used repeatedly in the literature. In this paper we introduce a simple recursively defined function C(n), and we use it to give a necessary and sufficient condition for pairs of consecutive even and odd integers to have trajectories which coincide after a specific pair-dependent number of steps. Then we derive a number of standard total stopping time equalities, including the ones in Garner (1985), as well as several novel results.	0,0,1,0,0,0
ALMA constraints on star-forming gas in a prototypical z=1.5 clumpy galaxy: the dearth of CO(5-4) emission from UV-bright clumps	We present deep ALMA CO(5-4) observations of a main sequence, clumpy galaxy at z=1.5 in the HUDF. Thanks to the ~0.5" resolution of the ALMA data, we can link stellar population properties to the CO(5-4) emission on scales of a few kpc. We detect strong CO(5-4) emission from the nuclear region of the galaxy, consistent with the observed $L_{\rm IR}$-$L^{\prime}_{\rm CO(5-4)}$ correlation and indicating on-going nuclear star formation. The CO(5-4) gas component appears more concentrated than other star formation tracers or the dust distribution in this galaxy. We discuss possible implications of this difference in terms of star formation efficiency and mass build-up at the galaxy centre. Conversely, we do not detect any CO(5-4) emission from the UV-bright clumps. This might imply that clumps have a high star formation efficiency (although they do not display unusually high specific star formation rates) and are not entirely gas dominated, with gas fractions no larger than that of their host galaxy (~50%). Stellar feedback and disk instability torques funnelling gas towards the galaxy centre could contribute to the relatively low gas content. Alternatively, clumps could fall in a more standard star formation efficiency regime if their actual star-formation rates are lower than generally assumed. We find that clump star-formation rates derived with several different, plausible methods can vary by up to an order of magnitude. The lowest estimates would be compatible with a CO(5-4) non-detection even for main-sequence like values of star formation efficiency and gas content.	0,1,0,0,0,0
Image-derived generative modeling of pseudo-macromolecular structures - towards the statistical assessment of Electron CryoTomography template matching	Cellular Electron CryoTomography (CECT) is a 3D imaging technique that captures information about the structure and spatial organization of macromolecular complexes within single cells, in near-native state and at sub-molecular resolution. Although template matching is often used to locate macromolecules in a CECT image, it is insufficient as it only measures the relative structural similarity. Therefore, it is preferable to assess the statistical credibility of the decision through hypothesis testing, requiring many templates derived from a diverse population of macromolecular structures. Due to the very limited number of known structures, we need a generative model to efficiently and reliably sample pseudo-structures from the complex distribution of macromolecular structures. To address this challenge, we propose a novel image-derived approach for performing hypothesis testing for template matching by constructing generative models using the generative adversarial network. Finally, we conducted hypothesis testing experiments for template matching on both simulated and experimental subtomograms, allowing us to conclude the identity of subtomograms with high statistical credibility and significantly reducing false positives.	0,0,0,1,1,0
Human-Level Intelligence or Animal-Like Abilities?	The vision systems of the eagle and the snake outperform everything that we can make in the laboratory, but snakes and eagles cannot build an eyeglass or a telescope or a microscope. (Judea Pearl)	1,0,0,1,0,0
Emergence of superconductivity in the canonical heavy-electron metal YbRh2Si2	We report magnetic and calorimetric measurements down to T = 1 mK on the canonical heavy-electron metal YbRh2Si2. The data reveal the development of nuclear antiferromagnetic order slightly above 2 mK. The latter weakens the primary electronic antiferromagnetism, thereby paving the way for heavy-electron superconductivity below Tc = 2 mK. Our results demonstrate that superconductivity driven by quantum criticality is a general phenomenon.	0,1,0,0,0,0
Simply Exponential Approximation of the Permanent of Positive Semidefinite Matrices	We design a deterministic polynomial time $c^n$ approximation algorithm for the permanent of positive semidefinite matrices where $c=e^{\gamma+1}\simeq 4.84$. We write a natural convex relaxation and show that its optimum solution gives a $c^n$ approximation of the permanent. We further show that this factor is asymptotically tight by constructing a family of positive semidefinite matrices.	1,0,0,0,0,0
OSIRIS-REx Contamination Control Strategy and Implementation	OSIRIS-REx will return pristine samples of carbonaceous asteroid Bennu. This article describes how pristine was defined based on expectations of Bennu and on a realistic understanding of what is achievable with a constrained schedule and budget, and how that definition flowed to requirements and implementation. To return a pristine sample, the OSIRIS- REx spacecraft sampling hardware was maintained at level 100 A/2 and <180 ng/cm2 of amino acids and hydrazine on the sampler head through precision cleaning, control of materials, and vigilance. Contamination is further characterized via witness material exposed to the spacecraft assembly and testing environment as well as in space. This characterization provided knowledge of the expected background and will be used in conjunction with archived spacecraft components for comparison with the samples when they are delivered to Earth for analysis. Most of all, the cleanliness of the OSIRIS-REx spacecraft was achieved through communication among scientists, engineers, managers, and technicians.	0,1,0,0,0,0
On the benefits of output sparsity for multi-label classification	The multi-label classification framework, where each observation can be associated with a set of labels, has generated a tremendous amount of attention over recent years. The modern multi-label problems are typically large-scale in terms of number of observations, features and labels, and the amount of labels can even be comparable with the amount of observations. In this context, different remedies have been proposed to overcome the curse of dimensionality. In this work, we aim at exploiting the output sparsity by introducing a new loss, called the sparse weighted Hamming loss. This proposed loss can be seen as a weighted version of classical ones, where active and inactive labels are weighted separately. Leveraging the influence of sparsity in the loss function, we provide improved generalization bounds for the empirical risk minimizer, a suitable property for large-scale problems. For this new loss, we derive rates of convergence linear in the underlying output-sparsity rather than linear in the number of labels. In practice, minimizing the associated risk can be performed efficiently by using convex surrogates and modern convex optimization algorithms. We provide experiments on various real-world datasets demonstrating the pertinence of our approach when compared to non-weighted techniques.	1,0,1,1,0,0
Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification	Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.	1,0,0,1,0,0
On a local invariant of elliptic curves with a p-isogeny	An elliptic curve $E$ defined over a $p$-adic field $K$ with a $p$-isogeny $\phi:E\rightarrow E^\prime$ comes equipped with an invariant $\alpha_{\phi/K}$ that measures the valuation of the leading term of the formal group homomorphism $\Phi:\hat E \rightarrow \hat E^\prime$. We prove that if $K/\mathbb{Q}_p$ is unramified and $E$ has additive, potentially supersingular reduction, then $\alpha_{\phi/K}$ is determined by the number of distinct geometric components on the special fibers of the minimal proper regular models of $E$ and $E^\prime$.	0,0,1,0,0,0
Spin Precession Experiments for Light Axionic Dark Matter	Axion-like particles are promising candidates to make up the dark matter of the universe, but it is challenging to design experiments that can detect them over their entire allowed mass range. Dark matter in general, and in particular axion-like particles and hidden photons, can be as light as roughly $10^{-22} \;\rm{eV}$ ($\sim 10^{-8} \;\rm{Hz}$), with astrophysical anomalies providing motivation for the lightest masses ("fuzzy dark matter"). We propose experimental techniques for direct detection of axion-like dark matter in the mass range from roughly $10^{-13} \;\rm{eV}$ ($\sim 10^2 \;\rm{Hz}$) down to the lowest possible masses. In this range, these axion-like particles act as a time-oscillating magnetic field coupling only to spin, inducing effects such as a time-oscillating torque and periodic variations in the spin-precession frequency with the frequency and direction set by fundamental physics. We show how these signals can be measured using existing experimental technology, including torsion pendulums, atomic magnetometers, and atom interferometry. These experiments demonstrate a strong discovery capability, with future iterations of these experiments capable of pushing several orders of magnitude past current astrophysical bounds.	0,1,0,0,0,0
Tests for comparing time-invariant and time-varying spectra based on the Anderson-Darling statistic	Based on periodogram-ratios of two univariate time series at different frequency points, two tests are proposed for comparing their spectra. One is an Anderson-Darling-like statistic for testing the equality of two time-invariant spectra. The other is the maximum of Anderson-Darling-like statistics for testing the equality of two spectra no matter that they are time-invariant and time-varying. Both of two tests are applicable for independent or dependent time series. Several simulation examples show that the proposed statistics outperform those that are also based on periodogram-ratios but constructed by the Pearson-like statistics.	0,0,0,1,0,0
Vanishing in stable motivic homotopy sheaves	We determine systematic regions in which the bigraded homotopy sheaves of the motivic sphere spectrum vanish.	0,0,1,0,0,0
Holomorphy of Osborn loops	Let $(L,\cdot)$ be any loop and let $A(L)$ be a group of automorphisms of $(L,\cdot)$ such that $\alpha$ and $\phi$ are elements of $A(L)$. It is shown that, for all $x,y,z\in L$, the $A(L)$-holomorph $(H,\circ)=H(L)$ of $(L,\cdot)$ is an Osborn loop if and only if $x\alpha (yz\cdot x\phi^{-1})= x\alpha (yx^\lambda\cdot x) \cdot zx\phi^{-1}$. Furthermore, it is shown that for all $x\in L$, $H(L)$ is an Osborn loop if and only if $(L,\cdot)$ is an Osborn loop, $(x\alpha\cdot x^{\rho})x=x\alpha$, $x(x^{\lambda}\cdot x\phi^{-1})=x\phi^{-1}$ and every pair of automorphisms in $A(L)$ is nuclear (i.e. $x\alpha\cdot x^{\rho},x^{\lambda}\cdot x\phi\in N(L,\cdot )$). It is shown that if $H(L)$ is an Osborn loop, then $A(L,\cdot)= \mathcal{P}(L,\cdot)\cap\Lambda(L,\cdot)\cap\Phi(L,\cdot)\cap\Psi(L,\cdot)$ and for any $\alpha\in A(L)$, $\alpha= L_{e\pi}=R^{-1}_{e\varrho}$ for some $\pi\in \Phi(L,\cdot)$ and some $\varrho\in \Psi(L,\cdot)$. Some commutative diagrams are deduced by considering isomorphisms among the various groups of regular bijections (whose intersection is $A(L)$) and the nucleus of $(L,\cdot)$.	0,0,1,0,0,0
Finding Archetypal Spaces for Data Using Neural Networks	Archetypal analysis is a type of factor analysis where data is fit by a convex polytope whose corners are "archetypes" of the data, with the data represented as a convex combination of these archetypal points. While archetypal analysis has been used on biological data, it has not achieved widespread adoption because most data are not well fit by a convex polytope in either the ambient space or after standard data transformations. We propose a new approach to archetypal analysis. Instead of fitting a convex polytope directly on data or after a specific data transformation, we train a neural network (AAnet) to learn a transformation under which the data can best fit into a polytope. We validate this approach on synthetic data where we add nonlinearity. Here, AAnet is the only method that correctly identifies the archetypes. We also demonstrate AAnet on two biological datasets. In a T cell dataset measured with single cell RNA-sequencing, AAnet identifies several archetypal states corresponding to naive, memory, and cytotoxic T cells. In a dataset of gut microbiome profiles, AAnet recovers both previously described microbiome states and identifies novel extrema in the data. Finally, we show that AAnet has generative properties allowing us to uniformly sample from the data geometry even when the input data is not uniformly distributed.	1,0,0,1,0,0
A new construction of universal spaces for asymptotic dimension	For each $n$, we construct a separable metric space $\mathbb{U}_n$ that is universal in the coarse category of separable metric spaces with asymptotic dimension ($\mathop{asdim}$) at most $n$ and universal in the uniform category of separable metric spaces with uniform dimension ($\mathop{udim}$) at most $n$. Thus, $\mathbb{U}_n$ serves as a universal space for dimension $n$ in both the large-scale and infinitesimal topology. More precisely, we prove: \[ \mathop{asdim} \mathbb{U}_n = \mathop{udim} \mathbb{U}_n = n \] and such that for each separable metric space $X$, a) if $\mathop{asdim} X \leq n$, then $X$ is coarsely equivalent to a subset of $\mathbb{U}_n$; b) if $\mathop{udim} X \leq n$, then $X$ is uniformly homeomorphic to a subset of $\mathbb{U}_n$.	0,0,1,0,0,0
Dykes for filtering ocean waves using c-shaped vertical cylinders	The present study investigates a way to design dykes which can filter the wavelengths of ocean surface waves. This offers the possibility to achieve a structure that can attenuate waves associated with storm swell, without affecting coastline in other conditions. Our approach is based on low frequency resonances in metamaterials combined with Bragg frequencies for which waves cannot propagate in periodic lattices.	0,1,0,0,0,0
Intense automorphisms of finite groups	Let $G$ be a group. An automorphism of $G$ is called intense if it sends each subgroup of $G$ to a conjugate; the collection of such automorphisms is denoted by $\mathrm{Int}(G)$. In the special case in which $p$ is a prime number and $G$ is a finite $p$-group, one can show that $\mathrm{Int}(G)$ is the semidirect product of a normal $p$-Sylow and a cyclic subgroup of order dividing $p-1$. In this thesis we classify the finite $p$-groups whose groups of intense automorphisms are not themselves $p$-groups. It emerges from our investigation that the structure of such groups is almost completely determined by their nilpotency class: for $p>3$, they share a quotient, growing with their class, with a uniquely determined infinite $2$-generated pro-$p$ group.	0,0,1,0,0,0
Global existence in the 1D quasilinear parabolic-elliptic chemotaxis system with critical nonlinearity	The paper should be viewed as complement of an earlier result in [8]. In the paper just mentioned it is shown that 1d case of a quasilinear parabolic-elliptic Keller-Segel system is very special. Namely, unlike in higher dimensions, there is no critical nonlinearity. Indeed, for the nonlinear diffusion of the form 1/u all the solutions, independently on the magnitude of initial mass, stay bounded. However, the argument presented in [8] deals with the Jager-Luckhaus type system. And is very sensitive to this restriction. Namely, the change of variables introduced in [8], being a main step of the method, works only for the Jager-Luckhaus modification. It does not seem to be applicable in the usual version of the parabolic-elliptic Keller-Segel system. The present paper fulfils this gap and deals with the case of the usual parabolic-elliptic version. To handle it we establish a new Lyapunov-like functional (it is related to what was done in [8]), which leads to global existence of the initial-boundary value problem for any initial mass.	0,0,1,0,0,0
Temporal Action Localization by Structured Maximal Sums	We address the problem of temporal action localization in videos. We pose action localization as a structured prediction over arbitrary-length temporal windows, where each window is scored as the sum of frame-wise classification scores. Additionally, our model classifies the start, middle, and end of each action as separate components, allowing our system to explicitly model each action's temporal evolution and take advantage of informative temporal dependencies present in this structure. In this framework, we localize actions by searching for the structured maximal sum, a problem for which we develop a novel, provably-efficient algorithmic solution. The frame-wise classification scores are computed using features from a deep Convolutional Neural Network (CNN), which are trained end-to-end to directly optimize for a novel structured objective. We evaluate our system on the THUMOS 14 action detection benchmark and achieve competitive performance.	1,0,0,0,0,0
On the Optimization Landscape of Tensor Decompositions	Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that "all local optima are (approximately) global optima", and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult. In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised learning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant $\epsilon > 0$, among the set of points with function values $(1+\epsilon)$-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem. Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.	1,0,1,1,0,0
Triangulum II: Not Especially Dense After All	Among the Milky Way satellites discovered in the past three years, Triangulum II has presented the most difficulty in revealing its dynamical status. Kirby et al. (2015a) identified it as the most dark matter-dominated galaxy known, with a mass-to-light ratio within the half-light radius of 3600 +3500 -2100 M_sun/L_sun. On the other hand, Martin et al. (2016) measured an outer velocity dispersion that is 3.5 +/- 2.1 times larger than the central velocity dispersion, suggesting that the system might not be in equilibrium. From new multi-epoch Keck/DEIMOS measurements of 13 member stars in Triangulum II, we constrain the velocity dispersion to be sigma_v < 3.4 km/s (90% C.L.). Our previous measurement of sigma_v, based on six stars, was inflated by the presence of a binary star with variable radial velocity. We find no evidence that the velocity dispersion increases with radius. The stars display a wide range of metallicities, indicating that Triangulum II retained supernova ejecta and therefore possesses or once possessed a massive dark matter halo. However, the detection of a metallicity dispersion hinges on the membership of the two most metal-rich stars. The stellar mass is lower than galaxies of similar mean stellar metallicity, which might indicate that Triangulum II is either a star cluster or a tidally stripped dwarf galaxy. Detailed abundances of one star show heavily depressed neutron-capture abundances, similar to stars in most other ultra-faint dwarf galaxies but unlike stars in globular clusters.	0,1,0,0,0,0
Set-Based Tests for Genetic Association Using the Generalized Berk-Jones Statistic	Studying the effects of groups of Single Nucleotide Polymorphisms (SNPs), as in a gene, genetic pathway, or network, can provide novel insight into complex diseases, above that which can be gleaned from studying SNPs individually. Common challenges in set-based genetic association testing include weak effect sizes, correlation between SNPs in a SNP-set, and scarcity of signals, with single-SNP effects often ranging from extremely sparse to moderately sparse in number. Motivated by these challenges, we propose the Generalized Berk-Jones (GBJ) test for the association between a SNP-set and outcome. The GBJ extends the Berk-Jones (BJ) statistic by accounting for correlation among SNPs, and it provides advantages over the Generalized Higher Criticism (GHC) test when signals in a SNP-set are moderately sparse. We also provide an analytic p-value calculation procedure for SNP-sets of any finite size. Using this p-value calculation, we illustrate that the rejection region for GBJ can be described as a compromise of those for BJ and GHC. We develop an omnibus statistic as well, and we show that this omnibus test is robust to the degree of signal sparsity. An additional advantage of our method is the ability to conduct inference using individual SNP summary statistics from a genome-wide association study. We evaluate the finite sample performance of the GBJ though simulation studies and application to gene-level association analysis of breast cancer risk.	0,0,0,1,0,0
SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability	We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: this https URL	1,0,0,1,0,0
Towards a general theory for non-linear locally stationary processes	In this paper some general theory is presented for locally stationary processes based on the stationary approximation and the stationary derivative. Laws of large numbers, central limit theorems as well as deterministic and stochastic bias expansions are proved for processes obeying an expansion in terms of the stationary approximation and derivative. In addition it is shown that this applies to some general nonlinear non-stationary Markov-models. In addition the results are applied to derive the asymptotic properties of maximum likelihood estimates of parameter curves in such models.	0,0,1,1,0,0
Percent Change Estimation in Large Scale Online Experiments	Online experiments are a fundamental component of the development of web-facing products. Given the large user-base, even small product improvements can have a large impact on an absolute scale. As a result, accurately estimating the relative impact of these changes is extremely important. I propose an approach based on an objective Bayesian model to improve the sensitivity of percent change estimation in A/B experiments. Leveraging pre-period information, this approach produces more robust and accurate point estimates and up to 50% tighter credible intervals than traditional methods. The R package abpackage provides an implementation of the approach.	0,0,0,1,0,0
Trading the Twitter Sentiment with Reinforcement Learning	This paper is to explore the possibility to use alternative data and artificial intelligence techniques to trade stocks. The efficacy of the daily Twitter sentiment on predicting the stock return is examined using machine learning methods. Reinforcement learning(Q-learning) is applied to generate the optimal trading policy based on the sentiment signal. The predicting power of the sentiment signal is more significant if the stock price is driven by the expectation of the company growth and when the company has a major event that draws the public attention. The optimal trading strategy based on reinforcement learning outperforms the trading strategy based on the machine learning prediction.	1,0,0,0,0,0
Symmetric structure for the endomorphism algebra of projective-injective module in parabolic category	We show that for any singular dominant integral weight $\lambda$ of a complex semisimple Lie algebra $\mathfrak{g}$, the endomorphism algebra $B$ of any projective-injective module of the parabolic BGG category $\mathcal{O}_\lambda^{\mathfrak{p}}$ is a symmetric algebra (as conjectured by Khovanov) extending the results of Mazorchuk and Stroppel for the regular dominant integral weight. Moreover, the endomorphism algebra $B$ is equipped with a homogeneous (non-degenerate) symmetrizing form. In the appendix, there is a short proof due to K. Coulembier and V. Mazorchuk showing that the endomorphism algebra $B_\lambda^{\mathfrak{p}}$ of the basic projective-injective module of $\mathcal{O}_\lambda^{\mathfrak{p}}$ is a symmetric algebra.	0,0,1,0,0,0
Discovering Visual Concept Structure with Sparse and Incomplete Tags	Discovering automatically the semantic structure of tagged visual data (e.g. web videos and images) is important for visual data analysis and interpretation, enabling the machine intelligence for effectively processing the fast-growing amount of multi-media data. However, this is non-trivial due to the need for jointly learning underlying correlations between heterogeneous visual and tag data. The task is made more challenging by inherently sparse and incomplete tags. In this work, we develop a method for modelling the inherent visual data concept structures based on a novel Hierarchical-Multi-Label Random Forest model capable of correlating structured visual and tag information so as to more accurately interpret the visual semantics, e.g. disclosing meaningful visual groups with similar high-level concepts, and recovering missing tags for individual visual data samples. Specifically, our model exploits hierarchically structured tags of different semantic abstractness and multiple tag statistical correlations in addition to modelling visual and tag interactions. As a result, our model is able to discover more accurate semantic correlation between textual tags and visual features, and finally providing favourable visual semantics interpretation even with highly sparse and incomplete tags. We demonstrate the advantages of our proposed approach in two fundamental applications, visual data clustering and missing tag completion, on benchmarking video (i.e. TRECVID MED 2011) and image (i.e. NUS-WIDE) datasets.	1,0,0,0,0,0
Memory-augmented Neural Machine Translation	Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by $9.0$ and $2.7$ BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.	1,0,0,0,0,0
Purity and separation for oriented matroids	Leclerc and Zelevinsky, motivated by the study of quasi-commuting quantum flag minors, introduced the notions of strongly separated and weakly separated collections. These notions are closely related to the theory of cluster algebras, to the combinatorics of the double Bruhat cells, and to the totally positive Grassmannian. A key feature, called the purity phenomenon, is that every maximal by inclusion strongly (resp., weakly) separated collection of subsets in $[n]$ has the same cardinality. In this paper, we extend these notions and define $\mathcal{M}$-separated collections, for any oriented matroid $\mathcal{M}$. We show that maximal by size $\mathcal{M}$-separated collections are in bijection with fine zonotopal tilings (if $\mathcal{M}$ is a realizable oriented matroid), or with one-element liftings of $\mathcal{M}$ in general position (for an arbitrary oriented matroid). We introduce the class of pure oriented matroids for which the purity phenomenon holds: an oriented matroid $\mathcal{M}$ is pure if $\mathcal{M}$-separated collections form a pure simplicial complex, i.e., any maximal by inclusion $\mathcal{M}$-separated collection is also maximal by size. We pay closer attention to several special classes of oriented matroids: oriented matroids of rank $3$, graphical oriented matroids, and uniform oriented matroids. We classify pure oriented matroids in these cases. An oriented matroid of rank $3$ is pure if and only if it is a positroid (up to reorienting and relabeling its ground set). A graphical oriented matroid is pure if and only if its underlying graph is an outerplanar graph, that is, a subgraph of a triangulation of an $n$-gon. We give a simple conjectural characterization of pure oriented matroids by forbidden minors and prove it for the above classes of matroids (rank $3$, graphical, uniform).	0,0,1,0,0,0
Causal Regularization	In application domains such as healthcare, we want accurate predictive models that are also causally interpretable. In pursuit of such models, we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties. In a large-scale analysis of Electronic Health Records (EHR), our causally-regularized model outperforms its L1-regularized counterpart in causal accuracy and is competitive in predictive performance. We perform non-linear causality analysis by causally regularizing a special neural network architecture. We also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20% improvement over multilayer perceptron in detecting multivariate causation, a situation common in healthcare, where many causal factors should occur simultaneously to have an effect on the target variable.	1,0,0,1,0,0
Learning to Fly by Crashing	How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in conjunction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see: this https URL	1,0,0,0,0,0
Using the Tsetlin Machine to Learn Human-Interpretable Rules for High-Accuracy Text Categorization with Medical Applications	Medical applications challenge today's text categorization techniques by demanding both high accuracy and ease-of-interpretation. Although deep learning has provided a leap ahead in accuracy, this leap comes at the sacrifice of interpretability. To address this accuracy-interpretability challenge, we here introduce, for the first time, a text categorization approach that leverages the recently introduced Tsetlin Machine. In all brevity, we represent the terms of a text as propositional variables. From these, we capture categories using simple propositional formulae, such as: if "rash" and "reaction" and "penicillin" then Allergy. The Tsetlin Machine learns these formulae from a labelled text, utilizing conjunctive clauses to represent the particular facets of each category. Indeed, even the absence of terms (negated features) can be used for categorization purposes. Our empirical comparison with Naïve Bayes, decision trees, linear support vector machines (SVMs), random forest, long short-term memory (LSTM) neural networks, and other techniques, is quite conclusive. The Tsetlin Machine either performs on par with or outperforms all of the evaluated methods on both the 20 Newsgroups and IMDb datasets, as well as on a non-public clinical dataset. On average, the Tsetlin Machine delivers the best recall and precision scores across the datasets. Finally, our GPU implementation of the Tsetlin Machine executes 5 to 15 times faster than the CPU implementation, depending on the dataset. We thus believe that our novel approach can have a significant impact on a wide range of text analysis applications, forming a promising starting point for deeper natural language understanding with the Tsetlin Machine.	0,0,0,1,0,0
Evaluation of Direct Haptic 4D Volume Rendering of Partially Segmented Data for Liver Puncture Simulation	This work presents an evaluation study using a force feedback evaluation framework for a novel direct needle force volume rendering concept in the context of liver puncture simulation. PTC/PTCD puncture interventions targeting the bile ducts have been selected to illustrate this concept. The haptic algorithms of the simulator system are based on (1) partially segmented patient image data and (2) a non-linear spring model effective at organ borders. The primary aim is to quantitatively evaluate force errors caused by our patient modeling approach, in comparison to haptic force output obtained from using gold-standard, completely manually-segmented data. The evaluation of the force algorithms compared to a force output from fully manually segmented gold-standard patient models, yields a low mean of 0.12 N root mean squared force error and up to 1.6 N for systematic maximum absolute errors. Force errors were evaluated on 31,222 preplanned test paths from 10 patients. Only twelve percent of the emitted forces along these paths were affected by errors. This is the first study evaluating haptic algorithms with deformable virtual patients in silico. We prove haptic rendering plausibility on a very high number of test paths. Important errors are below just noticeable differences for the hand-arm system.	1,1,0,0,0,0
Improved Kernels and Algorithms for Claw and Diamond Free Edge Deletion Based on Refined Observations	In the {claw, diamond}-free edge deletion problem, we are given a graph $G$ and an integer $k>0$, the question is whether there are at most $k$ edges whose deletion results in a graph without claws and diamonds as induced graphs. Based on some refined observations, we propose a kernel of $O(k^3)$ vertices and $O(k^4)$ edges, significantly improving the previous kernel of $O(k^{12})$ vertices and $O(k^{24})$ edges. In addition, we derive an $O^*(3.792^k)$-time algorithm for the {claw, diamond}-free edge deletion problem.	1,0,0,0,0,0
Self-consistent calculation of the flux-flow conductivity in diffusive superconductors	In the framework of Keldysh-Usadel kinetic theory, we study the temperature dependence of flux-flow conductivity (FFC) in diffusive superconductors. By using self-consistent vortex solutions we find the exact values of dimensionless parameters that determine the diffusion-controlled FFC both in the limit of the low temperatures and close to the critical one. Taking into account the electron-phonon scattering we study the transition between flux-flow regimes controlled either by the diffusion or the inelastic relaxation of non-equilibrium quasiparticles. We demonstrate that the inelastic electron-phonon relaxation leads to the strong suppression of FFC as compared to the previous estimates making it possible to obtain the numerical agreement with experimental results.	0,1,0,0,0,0
Linearization of the box-ball system: an elementary approach	Kuniba, Okado, Takagi and Yamada have found that the time-evolution of the Takahashi-Satsuma box-ball system can be linearized by considering rigged configurations associated with states of the box-ball system. We introduce a simple way to understand the rigged configuration of $\mathfrak{sl}_2$-type, and give an elementary proof of the linearization property. Our approach can be applied to a box-ball system with finite carrier, which is related to a discrete modified KdV equation, and also to the combinatorial $R$-matrix of $A_1^{(1)}$-type. We also discuss combinatorial statistics and related fermionic formulas associated with the states of the box-ball systems. A fermionic-type formula we obtain for the finite carrier case seems to be new.	0,1,0,0,0,0
Quadratic automaton algebras and intermediate growth	We present an example of a quadratic algebra given by three generators and three relations, which is automaton (the set of normal words forms a regular language) and such that its ideal of relations does not possess a finite Gröbner basis with respect to any choice of generators and any choice of a well-ordering of monomials compatible with multiplication. This answers a question of Ufnarovski. Another result is a simple example (4 generators and 7 relations) of a quadratic algebra of intermediate growth.	0,0,1,0,0,0
Grassmanians and Pseudosphere Arrangements	We extend vector configurations to more general objects that have nicer combinatorial and topological properties, called weighted pseudosphere arrangements. These are defined as a weighted variant of arrangements of pseudospheres, as in the Topological Representation Theorem for oriented matroids. We show that in rank 3, the real Stiefel manifold, Grassmanian, and oriented Grassmanian are homotopy equivalent to the analagously defined spaces of weighted pseudosphere arrangements. We also show for all rank 3 oriented matroids, that the space of realizations by weighted pseudosphere arrangements is contractible. This is a sharp contrast with vector configurations, where the space of realizations can have the homotopy type of any primary semialgebraic set.	0,0,1,0,0,0
Proportional Closeness Estimation of Probability of Contamination Under Group Testing	The paper is focused on the problem of estimating the probability $p$ of individual contaminated sample, under group testing. The precision of the estimator is given by the probability of proportional closeness, a concept defined in the Introduction. Two-stage and sequential sampling procedures are characterized. An adaptive procedure is examined.	0,0,1,1,0,0
It's Like Python But: Towards Supporting Transfer of Programming Language Knowledge	Expertise in programming traditionally assumes a binary novice-expert divide. Learning resources typically target programmers who are learning programming for the first time, or expert programmers for that language. An underrepresented, yet important group of programmers are those that are experienced in one programming language, but desire to author code in a different language. For this scenario, we postulate that an effective form of feedback is presented as a transfer from concepts in the first language to the second. Current programming environments do not support this form of feedback. In this study, we apply the theory of learning transfer to teach a language that programmers are less familiar with--such as R--in terms of a programming language they already know--such as Python. We investigate learning transfer using a new tool called Transfer Tutor that presents explanations for R code in terms of the equivalent Python code. Our study found that participants leveraged learning transfer as a cognitive strategy, even when unprompted. Participants found Transfer Tutor to be useful across a number of affordances like stepping through and highlighting facts that may have been missed or misunderstood. However, participants were reluctant to accept facts without code execution or sometimes had difficulty reading explanations that are verbose or complex. These results provide guidance for future designs and research directions that can support learning transfer when learning new programming languages.	1,0,0,0,0,0
Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs	Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from 'serialised' MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data.	1,0,0,1,0,0
Context encoding enables machine learning-based quantitative photoacoustics	Real-time monitoring of functional tissue parameters, such as local blood oxygenation, based on optical imaging could provide groundbreaking advances in the diagnosis and interventional therapy of various diseases. While photoacoustic (PA) imaging is a novel modality with great potential to measure optical absorption deep inside tissue, quantification of the measurements remains a major challenge. In this paper, we introduce the first machine learning based approach to quantitative PA imaging (qPAI), which relies on learning the fluence in a voxel to deduce the corresponding optical absorption. The method encodes relevant information of the measured signal and the characteristics of the imaging system in voxel-based feature vectors, which allow the generation of thousands of training samples from a single simulated PA image. Comprehensive in silico experiments suggest that context encoding (CE)-qPAI enables highly accurate and robust quantification of the local fluence and thereby the optical absorption from PA images.	1,1,0,0,0,0
Strong Landau-quantization effects in high-magnetic-field superconductivity of a two-dimensional multiple-band metal near the Lifshitz transition	We investigate the onset of superconductivity in magnetic field for a clean two-dimensional multiple-band superconductor in the vicinity of the Lifshitz transition when one of the bands is very shallow. Due to small number of carriers in this band, the quasiclassical Werthamer-Helfand approximation breaks down and Landau quantization has to be taken into account. We found that the transition temperature TC2(H) has giant oscillations and is resonantly enhanced at the magnetic fields corresponding to full occupancy of the Landau levels in the shallow band. This enhancement is especially pronounced for the lowest Landau level. As a consequence, the reentrant superconducting regions in the temperature-field phase diagram emerge at low temperatures near the magnetic fields at which the chemical potential matches the Landau levels. The specific behavior depends on the relative strength of the intraband and interband pairing interactions and the reentrance is most pronounced in the purely interband coupling scenario. The reentrant behavior is suppressed by the Zeeman spin splitting in the shallow band, the separated regions disappear already for very small spin-splitting factors. On the other hand, the reentrance is restored in the resonance cases when the spin-splitting energy exactly matches the separation between the Landau levels. The predicted behavior may realize in the gate-tuned FeSe monolayer.	0,1,0,0,0,0
Strong instability of standing waves for nonlinear Schrödinger equations with a partial confinement	We study the instability of standing wave solutions for nonlinear Schrödinger equations with a one-dimensional harmonic potential in dimension $N\ge 2$. We prove that if the nonlinearity is $L^2$-critical or supercritical in dimension $N-1$, then any ground states are strongly unstable by blowup.	0,0,1,0,0,0
Synthesis versus analysis in patch-based image priors	In global models/priors (for example, using wavelet frames), there is a well known analysis vs synthesis dichotomy in the way signal/image priors are formulated. In patch-based image models/priors, this dichotomy is also present in the choice of how each patch is modeled. This paper shows that there is another analysis vs synthesis dichotomy, in terms of how the whole image is related to the patches, and that all existing patch-based formulations that provide a global image prior belong to the analysis category. We then propose a synthesis formulation, where the image is explicitly modeled as being synthesized by additively combining a collection of independent patches. We formally establish that these analysis and synthesis formulations are not equivalent in general and that both formulations are compatible with analysis and synthesis formulations at the patch level. Finally, we present an instance of the alternating direction method of multipliers (ADMM) that can be used to perform image denoising under the proposed synthesis formulation, showing its computational feasibility. Rather than showing the superiority of the synthesis or analysis formulations, the contributions of this paper is to establish the existence of both alternatives, thus closing the corresponding gap in the field of patch-based image processing.	1,0,0,0,0,0
An extensive impurity-scattering study on the pairing symmetry of monolayer FeSe films on SrTiO3	Determination of the pairing symmetry in monolayer FeSe films on SrTiO3 is a requisite for understanding the high superconducting transition temperature in this system, which has attracted intense theoretical and experimental studies but remains controversial. Here, by introducing several types of point defects in FeSe monolayer films, we conduct a systematic investigation on the impurity-induced electronic states by spatially resolved scanning tunneling spectroscopy. Ranging from surface adsorption, chemical substitution to intrinsic structural modification, these defects generate a variety of scattering strength, which renders new insights on the pairing symmetry.	0,1,0,0,0,0
Images of Ideals under Derivations and $\mathcal E$-Derivations of Univariate Polynomial Algebras over a Field of Characteristic Zero	Let $K$ be a field of characteristic zero and $x$ a free variable. A $K$-$\mathcal E$-derivation of $K[x]$ is a $K$-linear map of the form $\operatorname{I}-\phi$ for some $K$-algebra endomorphism $\phi$ of $K[x]$, where $\operatorname{I}$ denotes the identity map of $K[x]$. In this paper we study the image of an ideal of $K[x]$ under some $K$-derivations and $K$-$\mathcal E$-derivations of $K[x]$. We show that the LFED conjecture proposed in [Z4] holds for all $K$-$\mathcal E$-derivations and all locally finite $K$-derivations of $K[x]$. We also show that the LNED conjecture proposed in [Z4] holds for all locally nilpotent $K$-derivations of $K[x]$, and also for all locally nilpotent $K$-$\mathcal E$-derivations of $K[x]$ and the ideals $uK[x]$ such that either $u=0$, or $\operatorname{deg}\, u\le 1$, or $u$ has at least one repeated root in the algebraic closure of $K$. As a bi-product, the homogeneous Mathieu subspaces (Mathieu-Zhao spaces) of the univariate polynomial algebra over an arbitrary field have also been classified.	0,0,1,0,0,0
The Kinematics of the Permitted C II $λ$ 6578 Line in a Large Sample of Planetary Nebulae	We present spectroscopic observations of the C II $\lambda$6578 permitted line for 83 lines of sight in 76 planetary nebulae at high spectral resolution, most of them obtained with the Manchester Echelle Spectrograph on the 2.1\,m telescope at the Observatorio Astronómico Nacional on the Sierra San Pedro Mártir. We study the kinematics of the C II $\lambda$6578 permitted line with respect to other permitted and collisionally-excited lines. Statistically, we find that the kinematics of the C II $\lambda$6578 line are not those expected if this line arises from the recombination of C$^{2+}$ ions or the fluorescence of C$^+$ ions in ionization equilibrium in a chemically-homogeneous nebular plasma, but instead its kinematics are those appropriate for a volume more internal than expected. The planetary nebulae in this sample have well-defined morphology and are restricted to a limited range in H$\alpha$ line widths (no large values) compared to their counterparts in the Milky Way bulge, both of which could be interpreted as the result of young nebular shells, an inference that is also supported by nebular modeling. Concerning the long-standing discrepancy between chemical abundances inferred from permitted and collisionally-excited emission lines in photoionized nebulae, our results imply that multiple plasma components occur commonly in planetary nebulae.	0,1,0,0,0,0
Numerical investigation of supersonic shock-wave/boundary-layer interaction in transitional and turbulent regime	We perform direct numerical simulations of shock-wave/boundary-layer interactions (SBLI) at Mach number M = 1.7 to investigate the influence of the state of the incoming boundary layer on the interaction properties. We reproduce and extend the flow conditions of the experiments performed by Giepman et al., in which a spatially evolving laminar boundary layer over a flat plate is initially tripped by an array of distributed roughness elements and impinged further downstream by an oblique shock wave. Four SBLI cases are considered, based on two different shock impingement locations along the streamwise direction, corresponding to transitional and turbulent interactions, and two different shock strengths, corresponding to flow deflection angles 3 degreees and 6 degrees. We find that, for all flow cases, shock induced separation is not observed, the boundary layer remains attached for the 3 degrees case and close to incipient separation for the 6 degrees case, independent of the state of the incoming boundary layer. The findings of this work suggest that a transitional interaction might be the optimal solution for practical SBLI applications, as it removes the large separation bubble typical of laminar interactions and reduces the extent of the high-friction region associated with an incoming turbulent boundary layer.	0,1,0,0,0,0
Towards the study of least squares estimators with convex penalty	Penalized least squares estimation is a popular technique in high-dimensional statistics. It includes such methods as the LASSO, the group LASSO, and the nuclear norm penalized least squares. The existing theory of these methods is not fully satisfying since it allows one to prove oracle inequalities with fixed high probability only for the estimators depending on this probability. Furthermore, the control of compatibility factors appearing in the oracle bounds is often not explicit. Some very recent developments suggest that the theory of oracle inequalities can be revised in an improved way. In this paper, we provide an overview of ideas and tools leading to such an improved theory. We show that, along with overcoming the disadvantages mentioned above, the methodology extends to the hilbertian framework and it applies to a large class of convex penalties. This paper is partly expository. In particular, we provide adapted proofs of some results from other recent work.	0,0,1,1,0,0
VQABQ: Visual Question Answering by Basic Questions	Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset and yields state-of-the-art accuracy, 60.34% in open-ended task.	1,0,0,0,0,0
Malaria Likelihood Prediction By Effectively Surveying Households Using Deep Reinforcement Learning	We build a deep reinforcement learning (RL) agent that can predict the likelihood of an individual testing positive for malaria by asking questions about their household. The RL agent learns to determine which survey question to ask next and when to stop to make a prediction about their likelihood of malaria based on their responses hitherto. The agent incurs a small penalty for each question asked, and a large reward/penalty for making the correct/wrong prediction; it thus has to learn to balance the length of the survey with the accuracy of its final predictions. Our RL agent is a Deep Q-network that learns a policy directly from the responses to the questions, with an action defined for each possible survey question and for each possible prediction class. We focus on Kenya, where malaria is a massive health burden, and train the RL agent on a dataset of 6481 households from the Kenya Malaria Indicator Survey 2015. To investigate the importance of having survey questions be adaptive to responses, we compare our RL agent to a supervised learning (SL) baseline that fixes its set of survey questions a priori. We evaluate on prediction accuracy and on the number of survey questions asked on a holdout set and find that the RL agent is able to predict with 80% accuracy, using only 2.5 questions on average. In addition, the RL agent learns to survey adaptively to responses and is able to match the SL baseline in prediction accuracy while significantly reducing survey length.	1,0,0,1,0,0
On the risk of convex-constrained least squares estimators under misspecification	We consider the problem of estimating the mean of a noisy vector. When the mean lies in a convex constraint set, the least squares projection of the random vector onto the set is a natural estimator. Properties of the risk of this estimator, such as its asymptotic behavior as the noise tends to zero, have been well studied. We instead study the behavior of this estimator under misspecification, that is, without the assumption that the mean lies in the constraint set. For appropriately defined notions of risk in the misspecified setting, we prove a generalization of a low noise characterization of the risk due to Oymak and Hassibi in the case of a polyhedral constraint set. An interesting consequence of our results is that the risk can be much smaller in the misspecified setting than in the well-specified setting. We also discuss consequences of our result for isotonic regression.	0,0,1,1,0,0
Audio style transfer	'Style transfer' among images has recently emerged as a very active research topic, fuelled by the power of convolution neural networks (CNNs), and has become fast a very popular technology in social media. This paper investigates the analogous problem in the audio domain: How to transfer the style of a reference audio signal to a target audio content? We propose a flexible framework for the task, which uses a sound texture model to extract statistics characterizing the reference audio style, followed by an optimization-based audio texture synthesis to modify the target content. In contrast to mainstream optimization-based visual transfer method, the proposed process is initialized by the target content instead of random noise and the optimized loss is only about texture, not structure. These differences proved key for audio style transfer in our experiments. In order to extract features of interest, we investigate different architectures, whether pre-trained on other tasks, as done in image style transfer, or engineered based on the human auditory system. Experimental results on different types of audio signal confirm the potential of the proposed approach.	1,1,0,0,0,0
Distinction of representations via Bruhat-Tits buildings of p-adic groups	Introductory and pedagogical treatmeant of the article : P. Broussous "Distinction of the Steinberg representation", with an appendix by François Courtès, IMRN 2014, no 11, 3140-3157. To appear in Proceedings of Chaire Jean Morlet, Dipendra Prasad, Volker Heiermann Ed. 2017. Contains modified and simplified proofs of loc. cit. This article is written in memory of François Courtès who passed away in september 2016.	0,0,1,0,0,0
An open-source platform to study uniaxial stress effects on nanoscale devices	We present an automatic measurement platform that enables the characterization of nanodevices by electrical transport and optical spectroscopy as a function of uniaxial stress. We provide insights into and detailed descriptions of the mechanical device, the substrate design and fabrication, and the instrument control software, which is provided under open-source license. The capability of the platform is demonstrated by characterizing the piezo-resistance of an InAs nanowire device using a combination of electrical transport and Raman spectroscopy. The advantages of this measurement platform are highlighted by comparison with state-of-the-art piezo-resistance measurements in InAs nanowires. We envision that the systematic application of this methodology will provide new insights into the physics of nanoscale devices and novel materials for electronics, and thus contribute to the assessment of the potential of strain as a technology booster for nanoscale electronics.	0,1,0,0,0,0
Global well-posedness for 2-D Boussinesq system with the temperature-dependent viscosity and supercritical dissipation	The present paper is dedicated to the global well-posedness issue for the Boussinesq system with the temperature-dependent viscosity in $\mathbb{R}^2.$ We aim at extending the work by Abidi and Zhang ( Adv. Math. 2017 (305) 1202--1249 ) to a supercritical dissipation for temperature.	0,0,1,0,0,0
A Separation Between Run-Length SLPs and LZ77	In this paper we give an infinite family of strings for which the length of the Lempel-Ziv'77 parse is a factor $\Omega(\log n/\log\log n)$ smaller than the smallest run-length grammar.	1,0,0,0,0,0
Truncated Variational EM for Semi-Supervised Neural Simpletrons	Inference and learning for probabilistic generative networks is often very challenging and typically prevents scalability to as large networks as used for deep discriminative approaches. To obtain efficiently trainable, large-scale and well performing generative networks for semi-supervised learning, we here combine two recent developments: a neural network reformulation of hierarchical Poisson mixtures (Neural Simpletrons), and a novel truncated variational EM approach (TV-EM). TV-EM provides theoretical guarantees for learning in generative networks, and its application to Neural Simpletrons results in particularly compact, yet approximately optimal, modifications of learning equations. If applied to standard benchmarks, we empirically find, that learning converges in fewer EM iterations, that the complexity per EM iteration is reduced, and that final likelihood values are higher on average. For the task of classification on data sets with few labels, learning improvements result in consistently lower error rates if compared to applications without truncation. Experiments on the MNIST data set herein allow for comparison to standard and state-of-the-art models in the semi-supervised setting. Further experiments on the NIST SD19 data set show the scalability of the approach when a manifold of additional unlabeled data is available.	0,0,0,1,0,0
Conceptual Frameworks for Building Online Citizen Science Projects	In recent years, citizen science has grown in popularity due to a number of reasons, including the emphasis on informal learning and creativity potential associated with these initiatives. Citizen science projects address research questions from various domains, ranging from Ecology to Astronomy. Due to the advancement of communication technologies, which makes outreach and engagement of wider communities easier, scientists are keen to turn their own research into citizen science projects. However, the development, deployment and management of these projects remains challenging. One of the most important challenges is building the project itself. There is no single tool or framework, which guides the step-by-step development of the project, since every project has specific characteristics, such as geographical constraints or volunteers' mode of participation. Therefore, in this article, we present a series of conceptual frameworks for categorisation, decision and deployment, which guide a citizen science project creator in every step of creating a new project starting from the research question to project deployment. The frameworks are designed with consideration to the properties of already existing citizen science projects and could be easily extended to include other dimensions, which are not currently perceived.	1,0,0,0,0,0
Ultraslow fluctuations in the pseudogap states of HgBa$_{2}$CaCu$_{2}$O$_{6+d}$	We report the transverse relaxation rates 1/$T_2$'s of the $^{63}$Cu nuclear spin-echo envelope for double-layer high-$T_c$ cuprate superconductors HgBa$_{2}$CaCu$_{2}$O$_{6+d}$ from underdoped to overdoped. The relaxation rate 1/$T_{2L}$ of the exponential function (Lorentzian component) shows a peak at 220$-$240 K in the underdoped ($T_c$ = 103 K) and the optimally doped ($T_c$ = 127 K) samples but no peak in the overdoped ($T_c$ = 93 K) sample. The enhancement in 1/$T_{2L}$ suggests development of the zero frequency components of local field fluctuations. Ultraslow fluctuations are hidden in the pseudogap states.	0,1,0,0,0,0
The Mira-Titan Universe II: Matter Power Spectrum Emulation	We introduce a new cosmic emulator for the matter power spectrum covering eight cosmological parameters. Targeted at optical surveys, the emulator provides accurate predictions out to a wavenumber k~5/Mpc and redshift z<=2. Besides covering the standard set of LCDM parameters, massive neutrinos and a dynamical dark energy of state are included. The emulator is built on a sample set of 36 cosmological models, carefully chosen to provide accurate predictions over the wide and large parameter space. For each model, we have performed a high-resolution simulation, augmented with sixteen medium-resolution simulations and TimeRG perturbation theory results to provide accurate coverage of a wide k-range; the dataset generated as part of this project is more than 1.2Pbyte. With the current set of simulated models, we achieve an accuracy of approximately 4%. Because the sampling approach used here has established convergence and error-control properties, follow-on results with more than a hundred cosmological models will soon achieve ~1% accuracy. We compare our approach with other prediction schemes that are based on halo model ideas and remapping approaches. The new emulator code is publicly available.	0,1,0,0,0,0
Self-duality and scattering map for the hyperbolic van Diejen systems with two coupling parameters (with an appendix by S. Ruijsenaars)	In this paper, we construct global action-angle variables for a certain two-parameter family of hyperbolic van Diejen systems. Following Ruijsenaars' ideas on the translation invariant models, the proposed action-angle variables come from a thorough analysis of the commutation relation obeyed by the Lax matrix, whereas the proof of their canonicity is based on the study of the scattering theory. As a consequence, we show that the van Diejen system of our interest is self-dual with a factorized scattering map. Also, in an appendix by S. Ruijsenaars, a novel proof of the spectral asymptotics of certain exponential type matrix flows is presented. This result is of crucial importance in our scattering-theoretical analysis.	0,1,1,0,0,0
Training Multi-Task Adversarial Network For Extracting Noise-Robust Speaker Embedding	Under noisy environments, to achieve the robust performance of speaker recognition is still a challenging task. Motivated by the promising performance of multi-task training in a variety of image processing tasks, we explore the potential of multi-task adversarial training for learning a noise-robust speaker embedding. In this paper we present a novel framework which consists of three components: an encoder that extracts noise-robust speaker embedding; a classifier that classifies the speakers; a discriminator that discriminates the noise type of the speaker embedding. Besides, we propose a training strategy using the training accuracy as an indicator to stabilize the multi-class adversarial optimization process. We conduct our experiments on the English and Mandarin corpus and the experimental results demonstrate that our proposed multi-task adversarial training method could greatly outperform the other methods without adversarial training in noisy environments. Furthermore, experiments indicate that our method is also able to improve the speaker verification performance the clean condition.	1,0,0,0,0,0
Measuring Item Similarity in Introductory Programming: Python and Robot Programming Case Studies	A personalized learning system needs a large pool of items for learners to solve. When working with a large pool of items, it is useful to measure the similarity of items. We outline a general approach to measuring the similarity of items and discuss specific measures for items used in introductory programming. Evaluation of quality of similarity measures is difficult. To this end, we propose an evaluation approach utilizing three levels of abstraction. We illustrate our approach to measuring similarity and provide evaluation using items from three diverse programming environments.	0,0,0,1,0,0
Quench-induced entanglement and relaxation dynamics in Luttinger liquids	We investigate the time evolution towards the asymptotic steady state of a one dimensional interacting system after a quantum quench. We show that at finite time the latter induces entanglement between right- and left- moving density excitations, encoded in their cross-correlators, which vanishes in the long-time limit. This behavior results in a universal time-decay in system spectral properties $ \propto t^{-2} $, in addition to non-universal power-law contributions typical of Luttinger liquids. Importantly, we argue that the presence of quench-induced entanglement clearly emerges in transport properties, such as charge and energy currents injected in the system from a biased probe, and determines their long-time dynamics. In particular, energy fractionalization phenomenon turns out to be a promising platform to observe the universal power-law decay $ \propto t^{-2} $ induced by entanglement and represents a novel way to study the corresponding relaxation mechanism.	0,1,0,0,0,0
On consequences of measurements of turbulent Lewis number from observations	Almost all parameterizations of turbulence in NWP models and GCM make the assumption of equality of exchange coefficients for heat $K_h$ and water $K_w$. However, large uncertainties exists in old papers published in the 1950s, 1960s and 1970s, where the turbulent Lewis number Le_t $= K_h / K_w$ have been evaluated from observations and then set to Le_t$=1$. The aim of this note is: 1) to trust the recommendations of Richardson (1919), who suggested to use the moist-air entropy as a variable on which the turbulence is acting; 2) to compute a new exchange coefficients $K_s$ for the moist-air entropy; 3) to determine the values of the new entropy-Lewis number Le_ts $= K_s / K_w$ from observations (Météopole-Flux and Cabauw masts) and from LES and SCM outputs for the IHOP case (Couvreux et al., 2005). It is shown that values of Le_ts significantly different from $1$ are frequently observed and may have large consequences on the way the turbulence fluxes are computed in NWP models and GCMs.	0,1,0,0,0,0
Dynamic transport in a quantum wire driven by spin-orbit interaction	We consider a gated one-dimensional (1D) quantum wire disturbed in a contactless manner by an alternating electric field produced by a tip of a scanning probe microscope. In this schematic 1D electrons are driven not by a pulling electric field but rather by a non-stationary spin-orbit interaction (SOI) created by the tip. We show that a charge current appears in the wire in the presence of the Rashba SOI produced by the gate net charge and image charges of 1D electrons induced on the gate (iSOI). The iSOI contributes to the charge susceptibility by breaking the spin-charge separation between the charge- and spin collective excitations, generated by the probe. The velocity of the excitations is strongly renormalized by SOI, which opens a way to fine-tune the charge and spin response of 1D electrons by changing the gate potential. One of the modes softens upon increasing the gate potential to enhance the current response as well as the power dissipated in the system.	0,1,0,0,0,0
A comment on Stein's unbiased risk estimate for reduced rank estimators	In the framework of matrix valued observables with low rank means, Stein's unbiased risk estimate (SURE) can be useful for risk estimation and for tuning the amount of shrinkage towards low rank matrices. This was demonstrated by Candès et al. (2013) for singular value soft thresholding, which is a Lipschitz continuous estimator. SURE provides an unbiased risk estimate for an estimator whenever the differentiability requirements for Stein's lemma are satisfied. Lipschitz continuity of the estimator is sufficient, but it is emphasized that differentiability Lebesgue almost everywhere isn't. The reduced rank estimator, which gives the best approximation of the observation with a fixed rank, is an example of a discontinuous estimator for which Stein's lemma actually applies. This was observed by Mukherjee et al. (2015), but the proof was incomplete. This brief note gives a sufficient condition for Stein's lemma to hold for estimators with discontinuities, which is then shown to be fulfilled for a class of spectral function estimators including the reduced rank estimator. Singular value hard thresholding does, however, not satisfy the condition, and Stein's lemma does not apply to this estimator.	0,0,1,1,0,0
Analysis of universal adversarial perturbations	Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we propose the first quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exists shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.	1,0,0,1,0,0
Conformal Twists, Yang-Baxter $σ$-models & Holographic Noncommutativity	Expanding upon earlier results [arXiv:1702.02861], we present a compendium of $\sigma$-models associated with integrable deformations of AdS$_5$ generated by solutions to homogenous classical Yang-Baxter equation. Each example we study from four viewpoints: conformal (Drinfeld) twists, closed string gravity backgrounds, open string parameters and proposed dual noncommutative (NC) gauge theory. Irrespective of whether the deformed background is a solution to supergravity or generalized supergravity, we show that the open string metric associated with each gravity background is undeformed AdS$_5$ with constant open string coupling and the NC structure $\Theta$ is directly related to the conformal twist. One novel feature is that $\Theta$ exhibits "holographic noncommutativity": while it may exhibit non-trivial dependence on the holographic direction, its value everywhere in the bulk is uniquely determined by its value at the boundary, thus facilitating introduction of a dual NC gauge theory. We show that the divergence of the NC structure $\Theta$ is directly related to the unimodularity of the twist. We discuss the implementation of an outer automorphism of the conformal algebra as a coordinate transformation in the AdS bulk and discuss its implications for Yang-Baxter $\sigma$-models and self-T-duality based on fermionic T-duality. Finally, we comment on implications of our results for the integrability of associated open strings and planar integrability of dual NC gauge theories.	0,1,0,0,0,0
The Geometry of Limit State Function Graphs and Subset Simulation	In the last fifteen the subset sampling method has often been used in reliability problems as a tool for calculating small probabilities. This method is extrapolating from an initial Monte Carlo estimate for the probability content of a failure domain found by a suitable higher level of the original limit state function. Then iteratively conditional probabilities are estimated for failures domains decreasing to the original failure domain. But there are assumptions not immediately obvious about the structure of the failure domains which must be fulfilled that the method works properly. Here examples are studied that show that at least in some cases if these premises are not fulfilled, inaccurate results may be obtained. For the further development of the subset sampling method it is certainly desirable to find approaches where it is possible to check that these implicit assumptions are not violated. Also it would be probably important to develop further improvements of the concept to get rid of these limitations.	0,0,0,1,0,0
Elliptic Determinantal Processes and Elliptic Dyson Models	We introduce seven families of stochastic systems of interacting particles in one-dimension corresponding to the seven families of irreducible reduced affine root systems. We prove that they are determinantal in the sense that all spatio-temporal correlation functions are given by determinants controlled by a single function called the spatio-temporal correlation kernel. For the four families ${A}_{N-1}$, ${B}_N$, ${C}_N$ and ${D}_N$, we identify the systems of stochastic differential equations solved by these determinantal processes, which will be regarded as the elliptic extensions of the Dyson model. Here we use the notion of martingales in probability theory and the elliptic determinant evaluations of the Macdonald denominators of irreducible reduced affine root systems given by Rosengren and Schlosser.	0,1,1,0,0,0
Inverse Moment Methods for Sufficient Forecasting using High-Dimensional Predictors	We consider forecasting a single time series using high-dimensional predictors in the presence of a possible nonlinear forecast function. The sufficient forecasting (Fan et al., 2016) used sliced inverse regression to estimate lower-dimensional sufficient indices for nonparametric forecasting using factor models. However, Fan et al. (2016) is fundamentally limited to the inverse first-moment method, by assuming the restricted fixed number of factors, linearity condition for factors, and monotone effect of factors on the response. In this work, we study the inverse second-moment method using directional regression and the inverse third-moment method to extend the methodology and applicability of the sufficient forecasting. As the number of factors diverges with the dimension of predictors, the proposed method relaxes the distributional assumption of the predictor and enhances the capability of capturing the non-monotone effect of factors on the response. We not only provide a high-dimensional analysis of inverse moment methods such as exhaustiveness and rate of convergence, but also prove their model selection consistency. The power of our proposed methods is demonstrated in both simulation studies and an empirical study of forecasting monthly macroeconomic data from Q1 1959 to Q1 2016. During our theoretical development, we prove an invariance result for inverse moment methods, which make a separate contribution to the sufficient dimension reduction.	0,0,1,1,0,0
Distributed Online Learning of Event Definitions	Logic-based event recognition systems infer occurrences of events in time using a set of event definitions in the form of first-order rules. The Event Calculus is a temporal logic that has been used as a basis in event recognition applications, providing among others, direct connections to machine learning, via Inductive Logic Programming (ILP). OLED is a recently proposed ILP system that learns event definitions in the form of Event Calculus theories, in a single pass over a data stream. In this work we present a version of OLED that allows for distributed, online learning. We evaluate our approach on a benchmark activity recognition dataset and show that we can significantly reduce training times, exchanging minimal information between processing nodes.	1,0,0,0,0,0
U-SLADS: Unsupervised Learning Approach for Dynamic Dendrite Sampling	Novel data acquisition schemes have been an emerging need for scanning microscopy based imaging techniques to reduce the time in data acquisition and to minimize probing radiation in sample exposure. Varies sparse sampling schemes have been studied and are ideally suited for such applications where the images can be reconstructed from a sparse set of measurements. Dynamic sparse sampling methods, particularly supervised learning based iterative sampling algorithms, have shown promising results for sampling pixel locations on the edges or boundaries during imaging. However, dynamic sampling for imaging skeleton-like objects such as metal dendrites remains difficult. Here, we address a new unsupervised learning approach using Hierarchical Gaussian Mixture Mod- els (HGMM) to dynamically sample metal dendrites. This technique is very useful if the users are interested in fast imaging the primary and secondary arms of metal dendrites in solidification process in materials science.	0,0,0,1,0,0
A Simple Convolutional Generative Network for Next Item Recommendation	Convolutional Neural Networks (CNNs) have been recently introduced in the domain of session-based next item recommendation. An ordered collection of past items the user has interacted with in a session (or sequence) are embedded into a 2-dimensional latent matrix, and treated as an image. The convolution and pooling operations are then applied to the mapped item embeddings. In this paper, we first examine the typical session-based CNN recommender and show that both the generative model and network architecture are suboptimal when modeling long-range dependencies in the item sequence. To address the issues, we introduce a simple, but very effective generative model that is capable of learning high-level representation from both short- and long-range item dependencies. The network architecture of the proposed model is formed of a stack of \emph{holed} convolutional layers, which can efficiently increase the receptive fields without relying on the pooling operation. Another contribution is the effective use of residual block structure in recommender systems, which can ease the optimization for much deeper networks. The proposed generative model attains state-of-the-art accuracy with less training time in the next item recommendation task. It accordingly can be used as a powerful recommendation baseline to beat in future, especially when there are long sequences of user feedback.	0,0,0,1,0,0
Multi-scale analysis of lead-lag relationships in high-frequency financial markets	We propose a novel estimation procedure for scale-by-scale lead-lag relationships of financial assets observed at a high-frequency in a non-synchronous manner. The proposed estimation procedure does not require any interpolation processing of the original data and is applicable to quite fine resolution data. The validity of the proposed estimators is shown under the continuous-time framework developed in our previous work Hayashi and Koike (2016). An empirical application shows promising results of the proposed approach.	0,0,0,1,0,0
On dimensions supporting a rational projective plane	A rational projective plane ($\mathbb{QP}^2$) is a simply connected, smooth, closed manifold $M$ such that $H^*(M;\mathbb{Q}) \cong \mathbb{Q}[\alpha]/\langle \alpha^3 \rangle$. An open problem is to classify the dimensions at which such a manifold exists. The Barge-Sullivan rational surgery realization theorem provides necessary and sufficient conditions that include the Hattori-Stong integrality conditions on the Pontryagin numbers. In this article, we simplify these conditions and combine them with the signature equation to give a single quadratic residue equation that determines whether a given dimension supports a $\mathbb{QP}^2$. We then confirm existence of a $\mathbb{QP}^2$ in two new dimensions and prove several non-existence results using factorizations of numerators of divided Bernoulli numbers. We also resolve the existence question in the Spin case, and we discuss existence results for the more general class of rational projective spaces.	0,0,1,0,0,0
Pumping Lemma for Higher-order Languages	We study a pumping lemma for the word/tree languages generated by higher-order grammars. Pumping lemmas are known up to order-2 word languages (i.e., for regular/context-free/indexed languages), and have been used to show that a given language does not belong to the classes of regular/context-free/indexed languages. We prove a pumping lemma for word/tree languages of arbitrary orders, modulo a conjecture that a higher-order version of Kruskal's tree theorem holds. We also show that the conjecture indeed holds for the order-2 case, which yields a pumping lemma for order-2 tree languages and order-3 word languages.	1,0,0,0,0,0
Efficient Algorithms for Moral Lineage Tracing	Lineage tracing, the joint segmentation and tracking of living cells as they move and divide in a sequence of light microscopy images, is a challenging task. Jug et al. have proposed a mathematical abstraction of this task, the moral lineage tracing problem (MLTP), whose feasible solutions define both a segmentation of every image and a lineage forest of cells. Their branch-and-cut algorithm, however, is prone to many cuts and slow convergence for large instances. To address this problem, we make three contributions: (i) we devise the first efficient primal feasible local search algorithms for the MLTP, (ii) we improve the branch-and-cut algorithm by separating tighter cutting planes and by incorporating our primal algorithms, (iii) we show in experiments that our algorithms find accurate solutions on the problem instances of Jug et al. and scale to larger instances, leveraging moral lineage tracing to practical significance.	1,0,0,0,0,0
Divergence-free positive symmetric tensors and fluid dynamics	We consider $d\times d$ tensors $A(x)$ that are symmetric, positive semi-definite, and whose row-divergence vanishes identically. We establish sharp inequalities for the integral of $(\det A)^{\frac1{d-1}}$. We apply them to models of compressible inviscid fluids: Euler equations, Euler--Fourier, relativistic Euler, Boltzman, BGK, etc... We deduce an {\em a priori} estimate for a new quantity, namely the space-time integral of $\rho^{\frac1n}p$, where $\rho$ is the mass density, $p$ the pressure and $n$ the space dimension. For kinetic models, the corresponding quantity generalizes Bony's functional.	0,1,1,0,0,0
Impurity band conduction in group-IV ferromagnetic semiconductor Ge1-xFex with nanoscale fluctuations in Fe concentration	We study the carrier transport and magnetic properties of group-IV-based ferromagnetic semiconductor Ge1-xFex thin films (Fe concentration x = 2.3 - 14 %) with and without boron (B) doping, by measuring their transport characteristics; the temperature dependence of resistivity, hole concentration, mobility, and the relation between the anomalous Hall conductivity versus conductivity. At relatively low x (= 2.3 %), the transport in the undoped Ge1-xFex film is dominated by hole hopping between Fe-rich hopping sites in the Fe impurity band, whereas that in the B-doped Ge1-xFex film is dominated by the holes in the valence band in the degenerated Fe-poor regions. As x increases (x = 2.3 - 14 %), the transport in the both undoped and B-doped Ge1-xFex films is dominated by hole hopping between the Fe-rich hopping sites of the impurity band. The magnetic properties of the Ge1-xFex films are studied by various methods including magnetic circular dichroism, magnetization and anomalous Hall resistance, and are not influenced by B-doping. We show band profile models of both undoped and B-doped Ge1-xFex films, which can explain the transport and the magnetic properties of the Ge1-xFex films.	0,1,0,0,0,0
Detecting Changes in Time Series Data using Volatility Filters	This work develops techniques for the sequential detection and location estimation of transient changes in the volatility (standard deviation) of time series data. In particular, we introduce a class of change detection algorithms based on the windowed volatility filter. The first method detects changes by employing a convex combination of two such filters with differing window sizes, such that the adaptively updated convex weight parameter is then used as an indicator for the detection of instantaneous power changes. Moreover, the proposed adaptive filtering based method is readily extended to the multivariate case by using recent advances in distributed adaptive filters, thereby using cooperation between the data channels for more effective detection of change points. Furthermore, this work also develops a novel change point location estimator based on the differenced output of the volatility filter. Finally, the performance of the proposed methods were evaluated on both synthetic and real world data.	1,0,0,0,0,0
A stellar census of the nearby, young 32 Orionis group	The 32 Orionis group was discovered almost a decade ago and despite the fact that it represents the first northern, young (age ~ 25 Myr) stellar aggregate within 100 pc of the Sun ($d \simeq 93$ pc), a comprehensive survey for members and detailed characterisation of the group has yet to be performed. We present the first large-scale spectroscopic survey for new (predominantly M-type) members of the group after combining kinematic and photometric data to select candidates with Galactic space motion and positions in colour-magnitude space consistent with membership. We identify 30 new members, increasing the number of known 32 Ori group members by a factor of three and bringing the total number of identified members to 46, spanning spectral types B5 to L1. We also identify the lithium depletion boundary (LDB) of the group, i.e. the luminosity at which lithium remains unburnt in a coeval population. We estimate the age of the 32 Ori group independently using both isochronal fitting and LDB analyses and find it is essentially coeval with the {\beta} Pictoris moving group, with an age of $24\pm4$ Myr. Finally, we have also searched for circumstellar disc hosts utilising the AllWISE catalogue. Although we find no evidence for warm, dusty discs, we identify several stars with excess emission in the WISE W4-band at 22 {\mu}m. Based on the limited number of W4 detections we estimate a debris disc fraction of $32^{+12}_{-8}$ per cent for the 32 Ori group.	0,1,0,0,0,0
Accelerating Imitation Learning with Predictive Models	Sample efficiency is critical in solving real-world reinforcement learning problems, where agent-environment interactions can be costly. Imitation learning from expert advice has proved to be an effective strategy for reducing the number of interactions required to train a policy. Online imitation learning, which interleaves policy evaluation and policy optimization, is a particularly effective technique with provable performance guarantees. In this work, we seek to further accelerate the convergence rate of online imitation learning, thereby making it more sample efficient. We propose two model-based algorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based on solving variational inequalities and MoBIL-Prox based on stochastic first-order updates. These two methods leverage a model to predict future gradients to speed up policy learning. When the model oracle is learned online, these algorithms can provably accelerate the best known convergence rate up to an order. Our algorithms can be viewed as a generalization of stochastic Mirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style analysis of performance.	0,0,0,1,0,0
Kernel Implicit Variational Inference	Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high-dimensional latent variables. In this paper, we present a new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.	1,0,0,1,0,0
Intensity estimation of transaction arrivals on the intraday electricity market	In the following paper we present a simple intensity estimation method of transaction arrivals on the intraday electricity market. Assuming the interarrival times distribution, we utilize a maximum likelihood estimation. The method's performance is briefly tested using German Intraday Continuous data. Despite the simplicity of the method, the results are encouraging. The supplementary materials containing the R-codes and the data are attached to this paper.	0,0,0,1,0,1
Chiral Topological Superconductors Enhanced by Long-Range Interactions	We study the phase diagram and edge states of a two-dimensional p-wave superconductor with long-range hopping and pairing amplitudes. New topological phases and quasiparticles different from the usual short-range model are obtained. When both hopping and pairing terms decay with the same exponent, one of the topological chiral phases with propagating Majorana edge states gets significantly enhanced by long-range couplings. On the other hand, when the long-range pairing amplitude decays more slowly than the hopping, we discover new topological phases where propagating Majorana fermions at each edge pair nonlocally and become gapped even in the thermodynamic limit. Remarkably, these nonlocal edge states are still robust, remain separated from the bulk, and are localized at both edges at the same time. The inclusion of long-range effects is potentially applicable to recent experiments with magnetic impurities and islands in 2D superconductors.	0,1,0,0,0,0
The stability of tightly-packed, evenly-spaced systems of Earth-mass planets orbiting a Sun-like star	Many of the multi-planet systems discovered to date have been notable for their compactness, with neighbouring planets closer together than any in the Solar System. Interestingly, planet-hosting stars have a wide range of ages, suggesting that such compact systems can survive for extended periods of time. We have used numerical simulations to investigate how quickly systems go unstable in relation to the spacing between planets, focusing on hypothetical systems of Earth-mass planets on evenly-spaced orbits (in mutual Hill radii). In general, the further apart the planets are initially, the longer it takes for a pair of planets to undergo a close encounter. We recover the results of previous studies, showing a linear trend in the initial planet spacing between 3 and 8 mutual Hill radii and the logarithm of the stability time. Investigating thousands of simulations with spacings up to 13 mutual Hill radii reveals distinct modulations superimposed on this relationship in the vicinity of first and second-order mean motion resonances of adjacent and next-adjacent planets. We discuss the impact of this structure and the implications on the stability of compact multi-planet systems. Applying the outcomes of our simulations, we show that isolated systems of up to five Earth-mass planets can fit in the habitable zone of a Sun-like star without close encounters for at least $10^9$ orbits.	0,1,0,0,0,0
Proof of an entropy conjecture of Leighton and Moitra	We prove the following conjecture of Leighton and Moitra. Let $T$ be a tournament on $[n]$ and $S_n$ the set of permutations of $[n]$. For an arc $uv$ of $T$, let $A_{uv}=\{\sigma \in S_n \, : \, \sigma(u)<\sigma(v) \}$. $\textbf{Theorem.}$ For a fixed $\varepsilon>0$, if $\mathbb{P}$ is a probability distribution on $S_n$ such that $\mathbb{P}(A_{uv})>1/2+\varepsilon$ for every arc $uv$ of $T$, then the binary entropy of $\mathbb{P}$ is at most $(1-\vartheta_{\varepsilon})\log_2 n!$ for some (fixed) positive $\vartheta_\varepsilon$. When $T$ is transitive the theorem is due to Leighton and Moitra; for this case we give a short proof with a better $\vartheta_\varepsilon$.	0,0,1,0,0,0
Equilibria, information and frustration in heterogeneous network games with conflicting preferences	Interactions between people are the basis on which the structure of our society arises as a complex system and, at the same time, are the starting point of any physical description of it. In the last few years, much theoretical research has addressed this issue by combining the physics of complex networks with a description of interactions in terms of evolutionary game theory. We here take this research a step further by introducing a most salient societal factor such as the individuals' preferences, a characteristic that is key to understand much of the social phenomenology these days. We consider a heterogeneous, agent-based model in which agents interact strategically with their neighbors but their preferences and payoffs for the possible actions differ. We study how such a heterogeneous network behaves under evolutionary dynamics and different strategic interactions, namely coordination games and best shot games. With this model we study the emergence of the equilibria predicted analytically in random graphs under best response dynamics, and we extend this test to unexplored contexts like proportional imitation and scale free networks. We show that some theoretically predicted equilibria do not arise in simulations with incomplete Information, and we demonstrate the importance of the graph topology and the payoff function parameters for some games. Finally, we discuss our results with available experimental evidence on coordination games, showing that our model agrees better with the experiment that standard economic theories, and draw hints as to how to maximize social efficiency in situations of conflicting preferences.	1,1,0,0,0,0
Model-based Clustering with Sparse Covariance Matrices	Finite Gaussian mixture models are widely used for model-based clustering of continuous data. Nevertheless, since the number of model parameters scales quadratically with the number of variables, these models can be easily over-parameterized. For this reason, parsimonious models have been developed via covariance matrix decompositions or assuming local independence. However, these remedies do not allow for direct estimation of sparse covariance matrices nor do they take into account that the structure of association among the variables can vary from one cluster to the other. To this end, we introduce mixtures of Gaussian covariance graph models for model-based clustering with sparse covariance matrices. A penalized likelihood approach is employed for estimation and a general penalty term on the graph configurations can be used to induce different levels of sparsity and incorporate prior knowledge. Model estimation is carried out using a structural-EM algorithm for parameters and graph structure estimation, where two alternative strategies based on a genetic algorithm and an efficient stepwise search are proposed for inference. With this approach, sparse component covariance matrices are directly obtained. The framework results in a parsimonious model-based clustering of the data via a flexible model for the within-group joint distribution of the variables. Extensive simulated data experiments and application to illustrative datasets show that the method attains good classification performance and model quality.	0,0,0,1,0,0
On the origin of the crescent-shaped distributions observed by MMS at the magnetopause	MMS observations recently confirmed that crescent-shaped electron velocity distributions in the plane perpendicular to the magnetic field occur in the electron diffusion region near reconnection sites at Earth's magnetopause. In this paper, we re-examine the origin of the crescent-shaped distributions in the light of our new finding that ions and electrons are drifting in opposite directions when displayed in magnetopause boundary-normal coordinates. Therefore, ExB drifts cannot cause the crescent shapes. We performed a high-resolution multi-scale simulation capturing sub-electron skin depth scales. The results suggest that the crescent-shaped distributions are caused by meandering orbits without necessarily requiring any additional processes found at the magnetopause such as the highly asymmetric magnetopause ambipolar electric field. We use an adiabatic Hamiltonian model of particle motion to confirm that conservation of canonical momentum in the presence of magnetic field gradients causes the formation of crescent shapes without invoking asymmetries or the presence of an ExB drift. An important consequence of this finding is that we expect crescent-shaped distributions also to be observed in the magnetotail, a prediction that MMS will soon be able to test.	0,1,0,0,0,0
Neurology-as-a-Service for the Developing World	Electroencephalography (EEG) is an extensively-used and well-studied technique in the field of medical diagnostics and treatment for brain disorders, including epilepsy, migraines, and tumors. The analysis and interpretation of EEGs require physicians to have specialized training, which is not common even among most doctors in the developed world, let alone the developing world where physician shortages plague society. This problem can be addressed by teleEEG that uses remote EEG analysis by experts or by local computer processing of EEGs. However, both of these options are prohibitively expensive and the second option requires abundant computing resources and infrastructure, which is another concern in developing countries where there are resource constraints on capital and computing infrastructure. In this work, we present a cloud-based deep neural network approach to provide decision support for non-specialist physicians in EEG analysis and interpretation. Named `neurology-as-a-service,' the approach requires almost no manual intervention in feature engineering and in the selection of an optimal architecture and hyperparameters of the neural network. In this study, we deploy a pipeline that includes moving EEG data to the cloud and getting optimal models for various classification tasks. Our initial prototype has been tested only in developed world environments to-date, but our intention is to test it in developing world environments in future work. We demonstrate the performance of our proposed approach using the BCI2000 EEG MMI dataset, on which our service attains 63.4% accuracy for the task of classifying real vs. imaginary activity performed by the subject, which is significantly higher than what is obtained with a shallow approach such as support vector machines.	1,0,0,1,0,0
Critical well-posedness and scattering results for fractional Hartree-type equations	Scattering for the mass-critical fractional Schrödinger equation with a cubic Hartree-type nonlinearity for initial data in a small ball in the scale-invariant space of three-dimensional radial and square-integrable initial data is established. For this, we prove a bilinear estimate for free solutions and extend it to perturbations of bounded quadratic variation. This result is shown to be sharp by proving the unboundedness of a third order derivative of the flow map in the super-critical range.	0,0,1,0,0,0
Sparse phase retrieval of one-dimensional signals by Prony's method	In this paper, we show that sparse signals f representable as a linear combination of a finite number N of spikes at arbitrary real locations or as a finite linear combination of B-splines of order m with arbitrary real knots can be almost surely recovered from O(N^2) Fourier intensity measurements up to trivial ambiguities. The constructive proof consists of two steps, where in the first step the Prony method is applied to recover all parameters of the autocorrelation function and in the second step the parameters of f are derived. Moreover, we present an algorithm to evaluate f from its Fourier intensities and illustrate it at different numerical examples.	1,0,1,0,0,0
On the Complexity of Model Checking for Syntactically Maximal Fragments of the Interval Temporal Logic HS with Regular Expressions	In this paper, we investigate the model checking (MC) problem for Halpern and Shoham's interval temporal logic HS. In the last years, interval temporal logic MC has received an increasing attention as a viable alternative to the traditional (point-based) temporal logic MC, which can be recovered as a special case. Most results have been obtained under the homogeneity assumption, that constrains a proposition letter to hold over an interval if and only if it holds over each component state. Recently, Lomuscio and Michaliszyn proposed a way to relax such an assumption by exploiting regular expressions to define the behaviour of proposition letters over intervals in terms of their component states. When homogeneity is assumed, the exact complexity of MC is a difficult open question for full HS and for its two syntactically maximal fragments AA'BB'E' and AA'EB'E'. In this paper, we provide an asymptotically optimal bound to the complexity of these two fragments under the more expressive semantic variant based on regular expressions by showing that their MC problem is AEXP_pol-complete, where AEXP_pol denotes the complexity class of problems decided by exponential-time bounded alternating Turing Machines making a polynomially bounded number of alternations.	1,0,0,0,0,0
A Non-standard Standard Model	This paper examines the Standard Model under the strong-electroweak gauge group $SU_S(3)\times U_{EW}(2)$ subject to the condition $u_{EW}(2)\not\cong su_I(2)\oplus u_Y(1)$. Physically, the condition ensures that all electroweak gauge bosons interact with each other prior to symmetry breaking --- as one might expect from $U(2)$ invariance. This represents a crucial shift in the notion of physical gauge bosons: Unlike the Standard Model which posits a change of Lie algebra basis induced by spontaneous symmetry breaking, here the basis is unaltered and $A,\,Z^0,\,W^\pm$ represent (modulo $U_{EW}(2)$ gauge transformations) the physical bosons both \emph{before} and after spontaneous symmetry breaking. Our choice of $u_{EW}(2)$ basis requires some modification of the matter field sector of the Standard Model. Careful attention to the product group structure calls for strong-electroweak degrees of freedom in the $(\mathbf{3},\mathbf{2})$ and the $(\mathbf{3},\overline{\mathbf{2}})$ of $SU_S(3)\times U_{EW}(2)$ that possess integer electric charge just like leptons. These degrees of freedom play the role of quarks, and they lead to a modified Lagrangian that nevertheless reproduces transition rates and cross sections equivalent to the Standard Model. The close resemblance between quark and lepton electroweak doublets in this picture suggests a mechanism for a phase transition between quarks and leptons that stems from the product structure of the gauge group. Our hypothesis is that the strong and electroweak bosons see each other as a source of decoherence. In effect, leptons get identified with the $SU_S(3)$-trace of quark representations. This mechanism allows for possible extensions of the Standard Model that don't require large inclusive multiplets of matter fields. As an example, we propose and investigate a model that turns out to have some promising cosmological implications.	0,1,0,0,0,0
A split step Fourier/discontinuous Galerkin scheme for the Kadomtsev--Petviashvili equation	In this paper we propose a method to solve the Kadomtsev--Petviashvili equation based on splitting the linear part of the equation from the nonlinear part. The linear part is treated using FFTs, while the nonlinear part is approximated using a semi-Lagrangian discontinuous Galerkin approach of arbitrary order. We demonstrate the efficiency and accuracy of the numerical method by providing a range of numerical simulations. In particular, we find that our approach can outperform the numerical methods considered in the literature by up to a factor of five. Although we focus on the Kadomtsev--Petviashvili equation in this paper, the proposed numerical scheme can be extended to a range of related models as well.	0,1,1,0,0,0
MuLoG, or How to apply Gaussian denoisers to multi-channel SAR speckle reduction?	Speckle reduction is a longstanding topic in synthetic aperture radar (SAR) imaging. Since most current and planned SAR imaging satellites operate in polarimetric, interferometric or tomographic modes, SAR images are multi-channel and speckle reduction techniques must jointly process all channels to recover polarimetric and interferometric information. The distinctive nature of SAR signal (complex-valued, corrupted by multiplicative fluctuations) calls for the development of specialized methods for speckle reduction. Image denoising is a very active topic in image processing with a wide variety of approaches and many denoising algorithms available, almost always designed for additive Gaussian noise suppression. This paper proposes a general scheme, called MuLoG (MUlti-channel LOgarithm with Gaussian denoising), to include such Gaussian denoisers within a multi-channel SAR speckle reduction technique. A new family of speckle reduction algorithms can thus be obtained, benefiting from the ongoing progress in Gaussian denoising, and offering several speckle reduction results often displaying method-specific artifacts that can be dismissed by comparison between results.	0,0,1,1,0,0
Navigability of Random Geometric Graphs in the Universe and Other Spacetimes	Random geometric graphs in hyperbolic spaces explain many common structural and dynamical properties of real networks, yet they fail to predict the correct values of the exponents of power-law degree distributions observed in real networks. In that respect, random geometric graphs in asymptotically de Sitter spacetimes, such as the Lorentzian spacetime of our accelerating universe, are more attractive as their predictions are more consistent with observations in real networks. Yet another important property of hyperbolic graphs is their navigability, and it remains unclear if de Sitter graphs are as navigable as hyperbolic ones. Here we study the navigability of random geometric graphs in three Lorentzian manifolds corresponding to universes filled only with dark energy (de Sitter spacetime), only with matter, and with a mixture of dark energy and matter as in our universe. We find that these graphs are navigable only in the manifolds with dark energy. This result implies that, in terms of navigability, random geometric graphs in asymptotically de Sitter spacetimes are as good as random hyperbolic graphs. It also establishes a connection between the presence of dark energy and navigability of the discretized causal structure of spacetime, which provides a basis for a different approach to the dark energy problem in cosmology.	1,1,0,0,0,0
A refined count of Coxeter element factorizations	For well-generated complex reflection groups, Chapuy and Stump gave a simple product for a generating function counting reflection factorizations of a Coxeter element by their length. This is refined here to record the number of reflections used from each orbit of hyperplanes. The proof is case-by-case via the classification of well-generated groups. It implies a new expression for the Coxeter number, expressed via data coming from a hyperplane orbit; a case-free proof of this due to J. Michel is included.	0,0,1,0,0,0
Variance bounding of delayed-acceptance kernels	A delayed-acceptance version of a Metropolis--Hastings algorithm can be useful for Bayesian inference when it is computationally expensive to calculate the true posterior, but a computationally cheap approximation is available; the delayed-acceptance kernel targets the same posterior as its parent Metropolis-Hastings kernel. Although the asymptotic variance of any functional of the chain cannot be less than that obtained using its parent, the average computational time per iteration can be much smaller and so for a given computational budget the delayed-acceptance kernel can be more efficient. When the asymptotic variance of all $L^2$ functionals of the chain is finite, the kernel is said to be variance bounding. It has recently been noted that a delayed-acceptance kernel need not be variance bounding even when its parent is. We provide sufficient conditions for inheritance: for global algorithms, such as the independence sampler, the error in the approximation should be bounded; for local algorithms, two alternative sets of conditions are provided. As a by-product of our initial, general result we also supply sufficient conditions on any pair of proposals such that, for any shared target distribution, if a Metropolis-Hastings kernel using one of the proposals is variance bounding then so is the Metropolis-Hastings kernel using the other proposal.	0,0,1,1,0,0
Euler characteristics of cominuscule quantum K-theory	We prove an identity relating the product of two opposite Schubert varieties in the (equivariant) quantum K-theory ring of a cominuscule flag variety to the minimal degree of a rational curve connecting the Schubert varieties. We deduce that the sum of the structure constants associated to any product of Schubert classes is equal to one. Equivalently, the sheaf Euler characteristic map extends to a ring homomorphism defined on the quantum K-theory ring.	0,0,1,0,0,0
On quasi-hereditary algebras	In this paper we introduce an easily verifiable sufficient condition to determine whether an algebra is quasi-hereditary. In the case of monomial algebras, we give conditions that are both necessary and sufficient to show whether an algebra is quasi-hereditary.	0,0,1,0,0,0
Efficient method for estimating the number of communities in a network	While there exist a wide range of effective methods for community detection in networks, most of them require one to know in advance how many communities one is looking for. Here we present a method for estimating the number of communities in a network using a combination of Bayesian inference with a novel prior and an efficient Monte Carlo sampling scheme. We test the method extensively on both real and computer-generated networks, showing that it performs accurately and consistently, even in cases where groups are widely varying in size or structure.	1,1,0,0,0,0
Rigidity and trace properties of divergence-measure vector fields	We show some rigidity properties of divergence-free vector fields defined on half-spaces. As an application, we prove the existence of the classical trace for a bounded, divergence-measure vector field $\xi$ defined on the Euclidean plane, at almost every point of a locally oriented rectifiable set $S$, under the assumption that its weak normal trace $[\xi\cdot \nu_S]$ attains a local maximum for the norm of $\xi$ at the point.	0,0,1,0,0,0
Analysing Temporal Evolution of Interlingual Wikipedia Article Pairs	Wikipedia articles representing an entity or a topic in different language editions evolve independently within the scope of the language-specific user communities. This can lead to different points of views reflected in the articles, as well as complementary and inconsistent information. An analysis of how the information is propagated across the Wikipedia language editions can provide important insights in the article evolution along the temporal and cultural dimensions and support quality control. To facilitate such analysis, we present MultiWiki - a novel web-based user interface that provides an overview of the similarities and differences across the article pairs originating from different language editions on a timeline. MultiWiki enables users to observe the changes in the interlingual article similarity over time and to perform a detailed visual comparison of the article snapshots at a particular time point.	1,0,0,0,0,0
Multivariant Assertion-based Guidance in Abstract Interpretation	Approximations during program analysis are a necessary evil, as they ensure essential properties, such as soundness and termination of the analysis, but they also imply not always producing useful results. Automatic techniques have been studied to prevent precision loss, typically at the expense of larger resource consumption. In both cases (i.e., when analysis produces inaccurate results and when resource consumption is too high), it is necessary to have some means for users to provide information to guide analysis and thus improve precision and/or performance. We present techniques for supporting within an abstract interpretation framework a rich set of assertions that can deal with multivariance/context-sensitivity, and can handle different run-time semantics for those assertions that cannot be discharged at compile time. We show how the proposed approach can be applied to both improving precision and accelerating analysis. We also provide some formal results on the effects of such assertions on the analysis results.	1,0,0,0,0,0
A vehicle-to-infrastructure communication based algorithm for urban traffic control	We present in this paper a new algorithm for urban traffic light control with mixed traffic (communicating and non communicating vehicles) and mixed infrastructure (equipped and unequipped junctions). We call equipped junction here a junction with a traffic light signal (TLS) controlled by a road side unit (RSU). On such a junction, the RSU manifests its connectedness to equipped vehicles by broadcasting its communication address and geographical coordinates. The RSU builds a map of connected vehicles approaching and leaving the junction. The algorithm allows the RSU to select a traffic phase, based on the built map. The selected traffic phase is applied by the TLS; and both equipped and unequipped vehicles must respect it. The traffic management is in feedback on the traffic demand of communicating vehicles. We simulated the vehicular traffic as well as the communications. The two simulations are combined in a closed loop with visualization and monitoring interfaces. Several indicators on vehicular traffic (mean travel time, ended vehicles) and IEEE 802.11p communication performances (end-to-end delay, throughput) are derived and illustrated in three dimension maps. We then extended the traffic control to a urban road network where we also varied the number of equipped junctions. Other indicators are shown for road traffic performances in the road network case, where high gains are experienced in the simulation results.	1,0,1,0,0,0
Adaptive pixel-super-resolved lensfree holography for wide-field on-chip microscopy	High-resolution wide field-of-view (FOV) microscopic imaging plays an essential role in various fields of biomedicine, engineering, and physical sciences. As an alternative to conventional lens-based scanning techniques, lensfree holography provides a new way to effectively bypass the intrinsical trade-off between the spatial resolution and FOV of conventional microscopes. Unfortunately, due to the limited sensor pixel-size, unpredictable disturbance during image acquisition, and sub-optimum solution to the phase retrieval problem, typical lensfree microscopes only produce compromised imaging quality in terms of lateral resolution and signal-to-noise ratio (SNR). Here, we propose an adaptive pixel-super-resolved lensfree imaging (APLI) method which can solve, or at least partially alleviate these limitations. Our approach addresses the pixel aliasing problem by Z-scanning only, without resorting to subpixel shifting or beam-angle manipulation. Automatic positional error correction algorithm and adaptive relaxation strategy are introduced to enhance the robustness and SNR of reconstruction significantly. Based on APLI, we perform full-FOV reconstruction of a USAF resolution target ($\sim$29.85 $m{m^2}$) and achieve half-pitch lateral resolution of 770 $nm$, surpassing 2.17 times of the theoretical Nyquist-Shannon sampling resolution limit imposed by the sensor pixel-size (1.67 $\mu m$). Full-FOV imaging result of a typical dicot root is also provided to demonstrate its promising potential applications in biologic imaging.	0,1,0,0,0,0
Deep Learning Assisted Heuristic Tree Search for the Container Pre-marshalling Problem	One of the key challenges for operations researchers solving real-world problems is designing and implementing high-quality heuristics to guide their search procedures. In the past, machine learning techniques have failed to play a major role in operations research approaches, especially in terms of guiding branching and pruning decisions. We integrate deep neural networks into a heuristic tree search procedure to decide which branch to choose next and to estimate a bound for pruning the search tree of an optimization problem. We call our approach Deep Learning assisted heuristic Tree Search (DLTS) and apply it to a well-known problem from the container terminals literature, the container pre-marshalling problem (CPMP). Our approach is able to learn heuristics customized to the CPMP solely through analyzing the solutions to CPMP instances, and applies this knowledge within a heuristic tree search to produce the highest quality heuristic solutions to the CPMP to date.	1,0,0,0,0,0
Bayesian shape modelling of cross-sectional geological data	Shape information is of great importance in many applications. For example, the oil-bearing capacity of sand bodies, the subterranean remnants of ancient rivers, is related to their cross-sectional shapes. The analysis of these shapes is therefore of some interest, but current classifications are simplistic and ad hoc. In this paper, we describe the first steps towards a coherent statistical analysis of these shapes by deriving the integrated likelihood for data shapes given class parameters. The result is of interest beyond this particular application.	0,0,0,1,0,0
Improving Adversarial Robustness via Promoting Ensemble Diversity	Though deep neural networks have achieved significant progress on various tasks, often enhanced by model ensemble, existing high-performance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the outputs, which ignores the interaction among networks. This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members, and present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members. Our method is computationally efficient and compatible with the defense methods acting on individual networks. Empirical results on various datasets verify that our method can improve adversarial robustness while maintaining state-of-the-art accuracy on normal examples.	1,0,0,1,0,0
CNNs are Globally Optimal Given Multi-Layer Support	Stochastic Gradient Descent (SGD) is the central workhorse for training modern CNNs. Although giving impressive empirical performance it can be slow to converge. In this paper we explore a novel strategy for training a CNN using an alternation strategy that offers substantial speedups during training. We make the following contributions: (i) replace the ReLU non-linearity within a CNN with positive hard-thresholding, (ii) reinterpret this non-linearity as a binary state vector making the entire CNN linear if the multi-layer support is known, and (iii) demonstrate that under certain conditions a global optima to the CNN can be found through local descent. We then employ a novel alternation strategy (between weights and support) for CNN training that leads to substantially faster convergence rates, nice theoretical properties, and achieving state of the art results across large scale datasets (e.g. ImageNet) as well as other standard benchmarks.	1,0,0,0,0,0
Propensity score estimation using classification and regression trees in the presence of missing covariate data	Data mining and machine learning techniques such as classification and regression trees (CART) represent a promising alternative to conventional logistic regression for propensity score estimation. Whereas incomplete data preclude the fitting of a logistic regression on all subjects, CART is appealing in part because some implementations allow for incomplete records to be incorporated in the tree fitting and provide propensity score estimates for all subjects. Based on theoretical considerations, we argue that the automatic handling of missing data by CART may however not be appropriate. Using a series of simulation experiments, we examined the performance of different approaches to handling missing covariate data; (i) applying the CART algorithm directly to the (partially) incomplete data, (ii) complete case analysis, and (iii) multiple imputation. Performance was assessed in terms of bias in estimating exposure-outcome effects \add{among the exposed}, standard error, mean squared error and coverage. Applying the CART algorithm directly to incomplete data resulted in bias, even in scenarios where data were missing completely at random. Overall, multiple imputation followed by CART resulted in the best performance. Our study showed that automatic handling of missing data in CART can cause serious bias and does not outperform multiple imputation as a means to account for missing data.	0,0,0,1,0,0
Smooth and Efficient Policy Exploration for Robot Trajectory Learning	Many policy search algorithms have been proposed for robot learning and proved to be practical in real robot applications. However, there are still hyperparameters in the algorithms, such as the exploration rate, which requires manual tuning. The existing methods to design the exploration rate manually or automatically may not be general enough or hard to apply in the real robot. In this paper, we propose a learning model to update the exploration rate adaptively. The overall algorithm is a combination of methods proposed by other researchers. Smooth trajectories for the robot can be produced by the algorithm and the updated exploration rate maximizes the lower bound of the expected return. Our method is tested in the ball-in-cup problem. The results show that our method can receive the same learning outcome as the previous methods but with fewer iterations.	1,0,0,0,0,0
Anyonic Entanglement and Topological Entanglement Entropy	We study the properties of entanglement in two-dimensional topologically ordered phases of matter. Such phases support anyons, quasiparticles with exotic exchange statistics. The emergent nonlocal state spaces of anyonic systems admit a particular form of entanglement that does not exist in conventional quantum mechanical systems. We study this entanglement by adapting standard notions of entropy to anyonic systems. We use the algebraic theory of anyon models (modular tensor categories) to illustrate the nonlocal entanglement structure of anyonic systems. Using this formalism, we present a general method of deriving the universal topological contributions to the entanglement entropy for general system configurations of a topological phase, including surfaces of arbitrary genus, punctures, and quasiparticle content. We analyze a number of examples in detail. Our results recover and extend prior results for anyonic entanglement and the topological entanglement entropy.	0,1,0,0,0,0
Nonlinear fractal meaning of the Hubble constant	According to astrophysical observations value of recession velocity in a certain point is proportional to a distance to this point. The proportionality coefficient is the Hubble constant measured with 5% accuracy. It is used in many cosmological theories describing dark energy, dark matter, baryons, and their relation with the cosmological constant introduced by Einstein. In the present work we have determined a limit value of the global Hubble constant (in a big distance from a point of observations) theoretically without using any empirical constants on the base of our own fractal model used for the description a relation between distance to an observed galaxy and coordinate of its center. The distance has been defined as a nonlinear fractal measure with scale of measurement corresponding to a deviation of the measure from its fixed value (zero-gravity radius). We have suggested a model of specific anisotropic fractal for simulation a radial Universe expansion. Our theoretical results have shown existence of an inverse proportionality between accuracy of determination the Hubble constant and accuracy of calculation a coordinates of galaxies leading to ambiguity results obtained at cosmological observations.	0,1,0,0,0,0
Lower bounds for several online variants of bin packing	We consider several previously studied online variants of bin packing and prove new and improved lower bounds on the asymptotic competitive ratios for them. For that, we use a method of fully adaptive constructions. In particular, we improve the lower bound for the asymptotic competitive ratio of online square packing significantly, raising it from roughly 1.68 to above 1.75.	1,0,0,0,0,0
Investigating early-type galaxy evolution with a multiwavelength approach. II. The UV structure of 11 galaxies with Swift-UVOT	GALEX detected a significant fraction of early-type galaxies showing Far-UV bright structures. These features suggest the occurrence of recent star formation episodes. We aim at understanding their evolutionary path[s] and the mechanisms at the origin of their UV-bright structures. We investigate with a multi-lambda approach 11 early-types selected because of their nearly passive stage of evolution in the nuclear region. The paper, second of a series, focuses on the comparison between UV features detected by Swift-UVOT, tracing recent star formation, and the galaxy optical structure mapping older stellar populations. We performed their UV surface photometry and used BVRI photometry from other sources. Our integrated magnitudes have been analyzed and compared with corresponding values in the literature. We characterize the overall galaxy structure best fitting the UV and optical luminosity profiles using a single Sersic law. NGC 1366, NGC 1426, NGC 3818, NGC 3962 and NGC 7192 show featureless luminosity profiles. Excluding NGC 1366 which has a clear edge-on disk , n~1-2, and NGC 3818, the remaining three have Sersic's indices n~3-4 in optical and a lower index in the UV. Bright ring/arm-like structures are revealed by UV images and luminosity profiles of NGC 1415, NGC 1533, NGC 1543, NGC 2685, NGC 2974 and IC 2006. The ring/arm-like structures are different from galaxy to galaxy. Sersic indices of UV profiles for those galaxies are in the range n=1.5-3 both in S0s and in Es. In our sample optical Sersic indices are usually larger than the UV ones. (M2-V) color profiles are bluer in ring/arm-like structures with respect to the galaxy body. The lower values of Sersic's indices in the UV bands with respect to optical ones, suggesting the presence of a disk, point out that the role of the dissipation cannot be neglected in recent evolutionary phases of these early-type galaxies.	0,1,0,0,0,0
Test results of a prototype device to calibrate the Large Size Telescope camera proposed for the Cherenkov Telescope Array	A Large Size air Cherenkov Telescope (LST) prototype, proposed for the Cherenkov Telescope Array (CTA), is under construction at the Canary Island of La Palma (Spain) this year. The LST camera, which comprises an array of about 500 photomultipliers (PMTs), requires a precise and regular calibration over a large dynamic range, up to $10^3$ photo-electrons (pe's), for each PMT. We present a system built to provide the optical calibration of the camera consisting of a pulsed laser (355 nm wavelength, 400 ps pulse width), a set of filters to guarantee a large dynamic range of photons on the sensors, and a diffusing sphere to uniformly spread the laser light, with flat fielding within 3%, over the camera focal plane 28 m away. The prototype of the system developed at INFN is hermetically closed and filled with dry air to make the system completely isolated from the external environment. In the paper we present the results of the tests for the evaluation of the photon density at the camera plane, the system isolation from the environment, and the shape of the signal as detected by the PMTs. The description of the communication of the system with the rest of detector is also given.	0,1,0,0,0,0
Stacked transfer learning for tropical cyclone intensity prediction	Tropical cyclone wind-intensity prediction is a challenging task considering drastic changes climate patterns over the last few decades. In order to develop robust prediction models, one needs to consider different characteristics of cyclones in terms of spatial and temporal characteristics. Transfer learning incorporates knowledge from a related source dataset to compliment a target datasets especially in cases where there is lack or data. Stacking is a form of ensemble learning focused for improving generalization that has been recently used for transfer learning problems which is referred to as transfer stacking. In this paper, we employ transfer stacking as a means of studying the effects of cyclones whereby we evaluate if cyclones in different geographic locations can be helpful for improving generalization performs. Moreover, we use conventional neural networks for evaluating the effects of duration on cyclones in prediction performance. Therefore, we develop an effective strategy that evaluates the relationships between different types of cyclones through transfer learning and conventional learning methods via neural networks.	1,0,0,1,0,0
Turbulence, cascade and singularity in a generalization of the Constantin-Lax-Majda equation	We study numerically a Constantin-Lax-Majda-De Gregorio model generalized by Okamoto, Sakajo and Wunsch, which is a model of fluid turbulence in one dimension with an inviscid conservation law. In the presence of the viscosity and two types of the large-scale forcings, we show that turbulent cascade of the inviscid invariant, which is not limited to quadratic quantity, occurs and that properties of this model's turbulent state are related to singularity of the inviscid case by adopting standard tools of analyzing fluid turbulence.	0,1,0,0,0,0
Symmetry breaking in linear multipole traps	Radiofrequency multipole traps have been used for some decades in cold collision experiments, and are gaining interest for precision spectroscopy due to their low mi-cromotion contribution, and the predicted unusual cold-ion structures. However, the experimental realisation is not yet fully controlled, and open questions in the operation of these devices remain. We present experimental observations of symmetry breaking of the trapping potential in a macroscopic octupole trap with laser-cooled ions. Numerical simulations have been performed in order to explain the appearance of additional local potential minima, and be able to control them in a next step. We characterize these additional potential minima, in particular with respect to their position, their potential depth and their probability of population as a function of the radial and angular displacement of the trapping rods.	0,1,0,0,0,0
Which Stars are Ionizing the Orion Nebula ?	The common assumption that Theta-1-Ori C is the dominant ionizing source for the Orion Nebula is critically examined. This assumption underlies much of the existing analysis of the nebula. In this paper we establish through comparison of the relative strengths of emission lines with expectations from Cloudy models and through the direction of the bright edges of proplyds that Theta-2-Ori-A, which lies beyond the Bright Bar, also plays an important role. Theta-1-Ori-C does dominate ionization in the inner part of the Orion Nebula, but outside of the Bright Bar as far as the southeast boundary of the Extended Orion Nebula, Theta-2-Ori-A is the dominant source. In addition to identifying the ionizing star in sample regions, we were able to locate those portions of the nebula in 3-D. This analysis illustrates the power of MUSE spectral imaging observations in identifying sources of ionization in extended regions.	0,1,0,0,0,0
Large-Scale Online Semantic Indexing of Biomedical Articles via an Ensemble of Multi-Label Classification Models	Background: In this paper we present the approaches and methods employed in order to deal with a large scale multi-label semantic indexing task of biomedical papers. This work was mainly implemented within the context of the BioASQ challenge of 2014. Methods: The main contribution of this work is a multi-label ensemble method that incorporates a McNemar statistical significance test in order to validate the combination of the constituent machine learning algorithms. Some secondary contributions include a study on the temporal aspects of the BioASQ corpus (observations apply also to the BioASQ's super-set, the PubMed articles collection) and the proper adaptation of the algorithms used to deal with this challenging classification task. Results: The ensemble method we developed is compared to other approaches in experimental scenarios with subsets of the BioASQ corpus giving positive results. During the BioASQ 2014 challenge we obtained the first place during the first batch and the third in the two following batches. Our success in the BioASQ challenge proved that a fully automated machine-learning approach, which does not implement any heuristics and rule-based approaches, can be highly competitive and outperform other approaches in similar challenging contexts.	0,0,0,1,0,0
Alternating Double Euler Sums, Hypergeometric Identities and a Theorem of Zagier	In this work, we derive relations between generating functions of double stuffle relations and double shuffle relations to express the alternating double Euler sums $\zeta\left(\overline{r}, s\right)$, $\zeta\left(r, \overline{s}\right)$ and $\zeta\left(\overline{r}, \overline{s}\right)$ with $r+s$ odd in terms of zeta values. We also give a direct proof of a hypergeometric identity which is a limiting case of a basic hypergeometric identity of Andrews. Finally, we gave another proof for the formula of Zagier on the multiple zeta values $\zeta(2,\ldots,2,3,2,\ldots,2)$.	0,0,1,0,0,0
Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile	Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality - a property which we call coherence. We first show that ordinary, "vanilla" MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an "extra-gradient" step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. (2018) for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for establishing convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, as well as the CelebA and CIFAR-10 datasets).	0,0,0,1,0,0
Neural Episodic Control	Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.	1,0,0,1,0,0
A National Research Agenda for Intelligent Infrastructure	Our infrastructure touches the day-to-day life of each of our fellow citizens, and its capabilities, integrity and sustainability are crucial to the overall competitiveness and prosperity of our country. Unfortunately, the current state of U.S. infrastructure is not good: the American Society of Civil Engineers' latest report on America's infrastructure ranked it at a D+ -- in need of $3.9 trillion in new investments. This dire situation constrains the growth of our economy, threatens our quality of life, and puts our global leadership at risk. The ASCE report called out three actions that need to be taken to address our infrastructure problem: 1) investment and planning in the system; 2) bold leadership by elected officials at the local and federal state; and 3) planning sustainability and resiliency in our infrastructure. While our immediate infrastructure needs are critical, it would be shortsighted to simply replicate more of what we have today. By doing so, we miss the opportunity to create Intelligent Infrastructure that will provide the foundation for increased safety and resilience, improved efficiencies and civic services, and broader economic opportunities and job growth. Indeed, our challenge is to proactively engage the declining, incumbent national infrastructure system and not merely repair it, but to enhance it; to create an internationally competitive cyber-physical system that provides an immediate opportunity for better services for citizens and that acts as a platform for a 21st century, high-tech economy and beyond.	1,0,0,0,0,0
Stability and Robust Regulation of Passive Linear Systems	We study the stability of coupled impedance passive regular linear systems under power-preserving interconnections. We present new conditions for strong, exponential, and non-uniform stability of the closed-loop system. We apply the stability results to the construction of passive error feedback controllers for robust output tracking and disturbance rejection for strongly stabilizable passive systems. In the case of nonsmooth reference and disturbance signals we present conditions for non-uniform rational and logarithmic rates of convergence of the output. The results are illustrated with examples on designing controllers for linear wave and heat equations, and on studying the stability of a system of coupled partial differential equations.	0,0,1,0,0,0
Semi-Global Weighted Least Squares in Image Filtering	Solving the global method of Weighted Least Squares (WLS) model in image filtering is both time- and memory-consuming. In this paper, we present an alternative approximation in a time- and memory- efficient manner which is denoted as Semi-Global Weighed Least Squares (SG-WLS). Instead of solving a large linear system, we propose to iteratively solve a sequence of subsystems which are one-dimensional WLS models. Although each subsystem is one-dimensional, it can take two-dimensional neighborhood information into account due to the proposed special neighborhood construction. We show such a desirable property makes our SG-WLS achieve close performance to the original two-dimensional WLS model but with much less time and memory cost. While previous related methods mainly focus on the 4-connected/8-connected neighborhood system, our SG-WLS can handle a more general and larger neighborhood system thanks to the proposed fast solution. We show such a generalization can achieve better performance than the 4-connected/8-connected neighborhood system in some applications. Our SG-WLS is $\sim20$ times faster than the WLS model. For an image of $M\times N$, the memory cost of SG-WLS is at most at the magnitude of $max\{\frac{1}{M}, \frac{1}{N}\}$ of that of the WLS model. We show the effectiveness and efficiency of our SG-WLS in a range of applications.	1,0,0,0,0,0
Multi-Entity Dependence Learning with Rich Context via Conditional Variational Auto-encoder	Multi-Entity Dependence Learning (MEDL) explores conditional correlations among multiple entities. The availability of rich contextual information requires a nimble learning scheme that tightly integrates with deep neural networks and has the ability to capture correlation structures among exponentially many outcomes. We propose MEDL_CVAE, which encodes a conditional multivariate distribution as a generating process. As a result, the variational lower bound of the joint likelihood can be optimized via a conditional variational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE was motivated by two real-world applications in computational sustainability: one studies the spatial correlation among multiple bird species using the eBird data and the other models multi-dimensional landscape composition and human footprint in the Amazon rainforest with satellite images. We show that MEDL_CVAE captures rich dependency structures, scales better than previous methods, and further improves on the joint likelihood taking advantage of very large datasets that are beyond the capacity of previous methods.	1,0,0,1,0,0
Static vs Adaptive Strategies for Optimal Execution with Signals	We consider an optimal execution problem in which a trader is looking at a short-term price predictive signal while trading. In the case where the trader is creating an instantaneous market impact, we show that transactions costs resulting from the optimal adaptive strategy are substantially lower than the corresponding costs of the optimal static strategy. Later, we investigate the case where the trader is creating transient market impact. We show that strategies in which the trader is observing the signal a number of times during the trading period, can dramatically reduce the transaction costs and improve the performance of the optimal static strategy. These results answer a question which was raised by Brigo and Piat [6], by analyzing two cases where adaptive strategies can improve the performance of the execution.	0,0,0,0,0,1
Characterization and Photometric Performance of the Hyper Suprime-Cam Software Pipeline	The Subaru Strategic Program (SSP) is an ambitious multi-band survey using the Hyper Suprime-Cam (HSC) on the Subaru telescope. The Wide layer of the SSP is both wide and deep, reaching a detection limit of i~26.0 mag. At these depths, it is challenging to achieve accurate, unbiased, and consistent photometry across all five bands. The HSC data are reduced using a pipeline that builds on the prototype pipeline for the Large Synoptic Survey Telescope. We have developed a Python-based, flexible framework to inject synthetic galaxies into real HSC images called SynPipe. Here we explain the design and implementation of SynPipe and generate a sample of synthetic galaxies to examine the photometric performance of the HSC pipeline. For stars, we achieve 1% photometric precision at i~19.0 mag and 6% precision at i~25.0 in the i-band. For synthetic galaxies with single-Sersic profiles, forced CModel photometry achieves 13% photometric precision at i~20.0 mag and 18% precision at i~25.0 in the i-band. We show that both forced PSF and CModel photometry yield unbiased color estimates that are robust to seeing conditions. We identify several caveats that apply to the version of HSC pipeline used for the first public HSC data release (DR1) that need to be taking into consideration. First, the degree to which an object is blended with other objects impacts the overall photometric performance. This is especially true for point sources. Highly blended objects tend to have larger photometric uncertainties, systematically underestimated fluxes and slightly biased colors. Second, >20% of stars at 22.5< i < 25.0 mag can be misclassified as extended objects. Third, the current CModel algorithm tends to strongly underestimate the half-light radius and ellipticity of galaxy with i>21.5 mag.	0,1,0,0,0,0
Haptic Assembly and Prototyping: An Expository Review	An important application of haptic technology to digital product development is in virtual prototyping (VP), part of which deals with interactive planning, simulation, and verification of assembly-related activities, collectively called virtual assembly (VA). In spite of numerous research and development efforts over the last two decades, the industrial adoption of haptic-assisted VP/VA has been slower than expected. Putting hardware limitations aside, the main roadblocks faced in software development can be traced to the lack of effective and efficient computational models of haptic feedback. Such models must 1) accommodate the inherent geometric complexities faced when assembling objects of arbitrary shape; and 2) conform to the computation time limitation imposed by the notorious frame rate requirements---namely, 1 kHz for haptic feedback compared to the more manageable 30-60 Hz for graphic rendering. The simultaneous fulfillment of these competing objectives is far from trivial. This survey presents some of the conceptual and computational challenges and opportunities as well as promising future directions in haptic-assisted VP/VA, with a focus on haptic assembly from a geometric modeling and spatial reasoning perspective. The main focus is on revisiting definitions and classifications of different methods used to handle the constrained multibody simulation in real-time, ranging from physics-based and geometry-based to hybrid and unified approaches using a variety of auxiliary computational devices to specify, impose, and solve assembly constraints. Particular attention is given to the newly developed 'analytic methods' inherited from motion planning and protein docking that have shown great promise as an alternative paradigm to the more popular combinatorial methods.	1,0,0,0,0,0
Parabolic induction in characteristic p	Let G be the group of rational points of a reductive connected group over a finite field (resp. nonarchimedean local field of characteristic p) and R a commutative ring. The unipotent (resp. pro-p Iwahori) invariant functor takes a smooth representation of G to a module over the unipotent (resp. pro-p Iwahori) Hecke R-algebra H of G. We prove that these functors for G and for a Levi subgroup of G commute with the parabolic induction functors, as well as with the right adjoints of the parabolic induction functors. However, they do not commute with the left adjoints of the parabolic induction functors in general; they do if p is invertible in R. When R is an algebraically closed field of characteristic p, we show in the local case that an irreducible admissible R-representation V of G is supercuspidal (or equivalently supersingular) if and only if the H-module V^I of its invariants by the pro-p Iwahori I admits a supersingular subquotient, if and only if V^I is supersingular.	0,0,1,0,0,0
Transfer Learning for Performance Modeling of Configurable Systems: An Exploratory Analysis	Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.	1,0,0,1,0,0
On a Novel Speech Representation Using Multitapered Modified Group Delay Function	In this paper, a novel multitaper modified group delay function-based representation for speech signals is proposed. With a set of phoneme-based experiments, it is shown that the proposed method performs better that an existing multitaper magnitude (MT-MAG) estimation technique, in terms of variance and MSE, both in spectral- and cepstral-domains. In particular, the performance of MT-MOGDF is found to be the best with the Thomson tapers. Additionally, the utility of the MT-MOGDF technique is highlighted in a speaker recognition experimental setup, where an improvement of around $20\%$ compared to the next-best technique is obtained. Moreover, the computational requirements of the proposed technique is comparable to that of MT-MAG. The proposed feature can be used in for many speech-related applications; in particular, it is best suited among those that require information of speaker and speech.	1,0,0,0,0,0
Qualitative Measurements of Policy Discrepancy for Return-based Deep Q-Network	The deep Q-network (DQN) and return-based reinforcement learning are two promising algorithms proposed in recent years. DQN brings advances to complex sequential decision problems, while return-based algorithms have advantages in making use of sample trajectories. In this paper, we propose a general framework to combine DQN and most of the return-based reinforcement learning algorithms, named R-DQN. We show the performance of traditional DQN can be improved effectively by introducing return-based reinforcement learning. In order to further improve the R-DQN, we design a strategy with two measurements which can qualitatively measure the policy discrepancy. Moreover, we give the two measurements' bounds in the proposed R-DQN framework. We show that algorithms with our strategy can accurately express the trace coefficient and achieve a better approximation to return. The experiments, conducted on several representative tasks from the OpenAI Gym library, validate the effectiveness of the proposed measurements. The results also show that the algorithms with our strategy outperform the state-of-the-art methods.	0,0,0,1,0,0
Smoothing Properties of Bilinear Operators and Leibniz-Type Rules in Lebesgue and Mixed Lebesgue Spaces	We prove that bilinear fractional integral operators and similar multipliers are smoothing in the sense that they improve the regularity of functions. We also treat bilinear singular multiplier operators which preserve regularity and obtain several Leibniz-type rules in the contexts of Lebesgue and mixed Lebesgue spaces.	0,0,1,0,0,0
Bayesian Methods for Exoplanet Science	Exoplanet research is carried out at the limits of the capabilities of current telescopes and instruments. The studied signals are weak, and often embedded in complex systematics from instrumental, telluric, and astrophysical sources. Combining repeated observations of periodic events, simultaneous observations with multiple telescopes, different observation techniques, and existing information from theory and prior research can help to disentangle the systematics from the planetary signals, and offers synergistic advantages over analysing observations separately. Bayesian inference provides a self-consistent statistical framework that addresses both the necessity for complex systematics models, and the need to combine prior information and heterogeneous observations. This chapter offers a brief introduction to Bayesian inference in the context of exoplanet research, with focus on time series analysis, and finishes with an overview of a set of freely available programming libraries.	0,1,0,0,0,0
QAOA for Max-Cut requires hundreds of qubits for quantum speed-up	Computational quantum technologies are entering a new phase in which noisy intermediate-scale quantum computers are available, but are still too small to benefit from active error correction. Even with a finite coherence budget to invest in quantum information processing, noisy devices with about 50 qubits are expected to experimentally demonstrate quantum supremacy in the next few years. Defined in terms of artificial tasks, current proposals for quantum supremacy, even if successful, will not help to provide solutions to practical problems. Instead, we believe that future users of quantum computers are interested in actual applications and that noisy quantum devices may still provide value by approximately solving hard combinatorial problems via hybrid classical-quantum algorithms. To lower bound the size of quantum computers with practical utility, we perform realistic simulations of the Quantum Approximate Optimization Algorithm and conclude that quantum speedup will not be attainable, at least for a representative combinatorial problem, until several hundreds of qubits are available.	1,0,0,0,0,0
Dual quadratic differentials and entire minimal graphs in Heisenberg space	We define holomorphic quadratic differentials for spacelike surfaces with constant mean curvature in the Lorentzian homogeneous spaces $\mathbb{L}(\kappa,\tau)$ with isometry group of dimension 4, which are dual to the Abresch-Rosenberg differentials in the Riemannian counterparts $\mathbb{E}(\kappa,\tau)$, and obtain some consequences. On the one hand, we give a very short proof of the Bernstein problem in Heisenberg space, and provide a geometric description of the family of entire graphs sharing the same differential in terms of a 2-parameter conformal deformation. On the other hand, we prove that entire minimal graphs in Heisenberg space have negative Gauss curvature.	0,0,1,0,0,0
Comparison of Parallelisation Approaches, Languages, and Compilers for Unstructured Mesh Algorithms on GPUs	Efficiently exploiting GPUs is increasingly essential in scientific computing, as many current and upcoming supercomputers are built using them. To facilitate this, there are a number of programming approaches, such as CUDA, OpenACC and OpenMP 4, supporting different programming languages (mainly C/C++ and Fortran). There are also several compiler suites (clang, nvcc, PGI, XL) each supporting different combinations of languages. In this study, we take a detailed look at some of the currently available options, and carry out a comprehensive analysis and comparison using computational loops and applications from the domain of unstructured mesh computations. Beyond runtimes and performance metrics (GB/s), we explore factors that influence performance such as register counts, occupancy, usage of different memory types, instruction counts, and algorithmic differences. Results of this work show how clang's CUDA compiler frequently outperform NVIDIA's nvcc, performance issues with directive-based approaches on complex kernels, and OpenMP 4 support maturing in clang and XL; currently around 10% slower than CUDA.	1,0,0,0,0,0
Characterization of minimizers of an anisotropic variant of the Rudin-Osher-Fatemi functional with $L^1$ fidelity term	In this paper we study an anisotropic variant of the Rudin-Osher-Fatemi functional with $L^1$ fidelity term of the form \[ E(u) = \int_{\mathbb{R}^n} \phi(\nabla u) + \lambda \| u -f \|_{L^1(\mathbb{R}^n)}. \] We will characterize the minimizers of $E$ in terms of the Wulff shape of $\phi$ and the dual anisotropy. In particular we will calculate the subdifferential of $E$. We will apply this characterization to the special case $\phi = |\cdot|_1$ and $n=2$, which has been used in the denoising of 2D bar codes. In this case, we determine the shape of a minimizer $u$ when $f$ is the characteristic function of a circle.	0,0,1,0,0,0
Connectivity Properties of Factorization Posets in Generated Groups	We consider three notions of connectivity and their interactions in partially ordered sets coming from reduced factorizations of an element in a generated group. While one form of connectivity essentially reflects the connectivity of the poset diagram, the other two are a bit more involved: Hurwitz-connectivity has its origins in algebraic geometry, and shellability in topology. We propose a framework to study these connectivity properties in a uniform way. Our main tool is a certain total order of the generators that is compatible with the chosen element.	0,0,1,0,0,0
Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs	We address the problem of learning vector representations for entities and relations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This problem has received significant attention in the past few years and multiple methods have been proposed. Most of the existing methods in the literature use a predefined characteristic scoring function for evaluating the correctness of KG triples. These scoring functions distinguish correct triples (high score) from incorrect ones (low score). However, their performance vary across different datasets. In this work, we demonstrate that a simple neural network based score function can consistently achieve near start-of-the-art performance on multiple datasets. We also quantitatively demonstrate biases in standard benchmark datasets, and highlight the need to perform evaluation spanning various datasets.	1,0,0,1,0,0
Safe Execution of Concurrent Programs by Enforcement of Scheduling Constraints	Automated software verification of concurrent programs is challenging because of exponentially growing state spaces. Verification techniques such as model checking need to explore a large number of possible executions that are possible under a non-deterministic scheduler. State space reduction techniques such as partial order reduction simplify the verification problem, however, the reduced state space may still be exponentially large and intractable. This paper discusses Iteratively Relaxed Scheduling, a framework that uses scheduling constraints in order to simplify the verification problem and enable automated verification of programs which could not be handled with fully non-deterministic scheduling. Program executions are safe as long as the same scheduling constraints are enforced under which the program has been verified, e.g., by instrumenting a program with additional synchronization. As strict enforcement of scheduling constraints may induce a high execution time overhead, we present optimizations over a naive solution that reduce this overhead. Our evaluation of a prototype implementation on well-known benchmark programs shows the effect of scheduling constraints on the execution time overhead and how this overhead can be reduced by relaxing and choosing constraints.	1,0,0,0,0,0
Universality of group embeddability	Working in the framework of Borel reducibility, we study various notions of embeddability between groups. We prove that the embeddability between countable groups, the topological embeddability between (discrete) Polish groups, and the isometric embeddability between separable groups with a bounded bi-invariant complete metric are all invariantly universal analytic quasi-orders. This strengthens some results from [Wil14] and [FLR09].	0,0,1,0,0,0
Evolution Strategies as a Scalable Alternative to Reinforcement Learning	We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.	1,0,0,1,0,0
Enstrophy Cascade in Decaying Two-Dimensional Quantum Turbulence	We report evidence for an enstrophy cascade in large-scale point-vortex simulations of decaying two-dimensional quantum turbulence. Devising a method to generate quantum vortex configurations with kinetic energy narrowly localized near a single length scale, the dynamics are found to be well-characterised by a superfluid Reynolds number, $\mathrm{Re_s}$, that depends only on the number of vortices and the initial kinetic energy scale. Under free evolution the vortices exhibit features of a classical enstrophy cascade, including a $k^{-3}$ power-law kinetic energy spectrum, and steady enstrophy flux associated with inertial transport to small scales. Clear signatures of the cascade emerge for $N\gtrsim 500$ vortices. Simulating up to very large Reynolds numbers ($N = 32, 768$ vortices), additional features of the classical theory are observed: the Kraichnan-Batchelor constant is found to converge to $C' \approx 1.6$, and the width of the $k^{-3}$ range scales as $\mathrm{Re_s}^{1/2}$. The results support a universal phenomenology underpinning classical and quantum fluid turbulence.	0,1,0,0,0,0
Characterization of Near-Earth Asteroids using KMTNet-SAAO	We present here VRI spectrophotometry of 39 near-Earth asteroids (NEAs) observed with the Sutherland, South Africa, node of the Korea Microlensing Telescope Network (KMTNet). Of the 39 NEAs, 19 were targeted, but because of KMTNet's large 2 deg by 2 deg field of view, 20 serendipitous NEAs were also captured in the observing fields. Targeted observations were performed within 44 days (median: 16 days, min: 4 days) of each NEA's discovery date. Our broadband spectrophotometry is reliable enough to distinguish among four asteroid taxonomies and we were able to confidently categorize 31 of the 39 observed targets as either a S-, C-, X- or D-type asteroid by means of a Machine Learning (ML) algorithm approach. Our data suggest that the ratio between "stony" S-type NEAs and "not-stony" (C+X+D)-type NEAs, with H magnitudes between 15 and 25, is roughly 1:1. Additionally, we report ~1-hour light curve data for each NEA and of the 39 targets we were able to resolve the complete rotation period and amplitude for six targets and report lower limits for the remaining targets.	0,1,0,0,0,0
A Supervised STDP-based Training Algorithm for Living Neural Networks	Neural networks have shown great potential in many applications like speech recognition, drug discovery, image classification, and object detection. Neural network models are inspired by biological neural networks, but they are optimized to perform machine learning tasks on digital computers. The proposed work explores the possibilities of using living neural networks in vitro as basic computational elements for machine learning applications. A new supervised STDP-based learning algorithm is proposed in this work, which considers neuron engineering constrains. A 74.7% accuracy is achieved on the MNIST benchmark for handwritten digit recognition.	1,0,0,1,0,0
Full Workspace Generation of Serial-link Manipulators by Deep Learning based Jacobian Estimation	Apart from solving complicated problems that require a certain level of intelligence, fine-tuned deep neural networks can also create fast algorithms for slow, numerical tasks. In this paper, we introduce an improved version of [1]'s work, a fast, deep-learning framework capable of generating the full workspace of serial-link manipulators. The architecture consists of two neural networks: an estimation net that approximates the manipulator Jacobian, and a confidence net that measures the confidence of the approximation. We also introduce M3 (Manipulability Maps of Manipulators), a MATLAB robotics library based on [2](RTB), the datasets generated by which are used by this work. Results have shown that not only are the neural networks significantly faster than numerical inverse kinematics, it also offers superior accuracy when compared to other machine learning alternatives. Implementations of the algorithm (based on Keras[3]), including benchmark evaluation script, are available at this https URL . The M3 Library APIs and datasets are also available at this https URL .	1,0,0,0,0,0
3D Human Pose Estimation on a Configurable Bed from a Pressure Image	Robots have the potential to assist people in bed, such as in healthcare settings, yet bedding materials like sheets and blankets can make observation of the human body difficult for robots. A pressure-sensing mat on a bed can provide pressure images that are relatively insensitive to bedding materials. However, prior work on estimating human pose from pressure images has been restricted to 2D pose estimates and flat beds. In this work, we present two convolutional neural networks to estimate the 3D joint positions of a person in a configurable bed from a single pressure image. The first network directly outputs 3D joint positions, while the second outputs a kinematic model that includes estimated joint angles and limb lengths. We evaluated our networks on data from 17 human participants with two bed configurations: supine and seated. Our networks achieved a mean joint position error of 77 mm when tested with data from people outside the training set, outperforming several baselines. We also present a simple mechanical model that provides insight into ambiguity associated with limbs raised off of the pressure mat, and demonstrate that Monte Carlo dropout can be used to estimate pose confidence in these situations. Finally, we provide a demonstration in which a mobile manipulator uses our network's estimated kinematic model to reach a location on a person's body in spite of the person being seated in a bed and covered by a blanket.	1,0,0,0,0,0
Tradeoff Between Delay and High SNR Capacity in Quantized MIMO Systems	Analog-to-digital converters (ADCs) are a major contributor to the power consumption of multiple-input multiple-output (MIMO) communication systems with large number of antennas. Use of low resolution ADCs has been proposed as a means to decrease power consumption in MIMO receivers. However, reducing the ADC resolution leads to performance loss in terms of achievable transmission rates. In order to mitigate the rate-loss, the receiver can perform analog processing of the received signals before quantization. Prior works consider one-shot analog processing where at each channel-use, analog linear combinations of the received signals are fed to a set of one-bit threshold ADCs. In this paper, a receiver architecture is proposed which uses a sequence of delay elements to allow for blockwise linear combining of the received analog signals. In the high signal to noise ratio regime, it is shown that the proposed architecture achieves the maximum achievable transmission rate given a fixed number of one-bit ADCs. Furthermore, a tradeoff between transmission rate and the number of delay elements is identified which quantifies the increase in maximum achievable rate as the number of delay elements is increased.	1,0,0,0,0,0
Generalization of Effective Conductance Centrality for Egonetworks	We study the popular centrality measure known as effective conductance or in some circles as information centrality. This is an important notion of centrality for undirected networks, with many applications, e.g., for random walks, electrical resistor networks, epidemic spreading, etc. In this paper, we first reinterpret this measure in terms of modulus (energy) of families of walks on the network. This modulus centrality measure coincides with the effective conductance measure on simple undirected networks, and extends it to much more general situations, e.g., directed networks as well. Secondly, we study a variation of this modulus approach in the egocentric network paradigm. Egonetworks are networks formed around a focal node (ego) with a specific order of neighborhoods. We propose efficient analytical and approximate methods for computing these measures on both undirected and directed networks. Finally, we describe a simple method inspired by the modulus point-of-view, called shell degree, which proved to be a useful tool for network science.	1,1,0,0,0,0
Novel approaches to spectral properties of correlated electron materials: From generalized Kohn-Sham theory to screened exchange dynamical mean field theory	The most intriguing properties of emergent materials are typically consequences of highly correlated quantum states of their electronic degrees of freedom. Describing those materials from first principles remains a challenge for modern condensed matter theory. Here, we review, apply and discuss novel approaches to spectral properties of correlated electron materials, assessing current day predictive capabilities of electronic structure calculations. In particular, we focus on the recent Screened Exchange Dynamical Mean-Field Theory scheme and its relation to generalized Kohn-Sham theory. These concepts are illustrated on the transition metal pnictide BaCo$_2$As$_2$ and elemental zinc and cadmium.	0,1,0,0,0,0
How a small quantum bath can thermalize long localized chains	We investigate the stability of the many-body localized (MBL) phase for a system in contact with a single ergodic grain, modelling a Griffiths region with low disorder. Our numerical analysis provides evidence that even a small ergodic grain consisting of only 3 qubits can delocalize a localized chain, as soon as the localization length exceeds a critical value separating localized and extended regimes of the whole system. We present a simple theory, consistent with the arguments in [Phys. Rev. B 95, 155129 (2017)], that assumes a system to be locally ergodic unless the local relaxation time, determined by Fermi's Golden Rule, is larger than the inverse level spacing. This theory predicts a critical value for the localization length that is perfectly consistent with our numerical calculations. We analyze in detail the behavior of local operators inside and outside the ergodic grain, and find excellent agreement of numerics and theory.	0,1,0,0,0,0
Truth-Telling Mechanism for Secure Two-Way Relay Communications with Energy-Harvesting Revenue	This paper brings the novel idea of paying the utility to the winning agents in terms of some physical entity in cooperative communications. Our setting is a secret two-way communication channel where two transmitters exchange information in the presence of an eavesdropper. The relays are selected from a set of interested parties such that the secrecy sum rate is maximized. In return, the selected relay nodes' energy harvesting requirements will be fulfilled up to a certain threshold through their own payoff so that they have the natural incentive to be selected and involved in the communication. However, relays may exaggerate their private information in order to improve their chance to be selected. Our objective is to develop a mechanism for relay selection that enforces them to reveal the truth since otherwise they may be penalized. We also propose a joint cooperative relay beamforming and transmit power optimization scheme based on an alternating optimization approach. Note that the problem is highly non-convex since the objective function appears as a product of three correlated Rayleigh quotients. While a common practice in the existing literature is to optimize the relay beamforming vector for given transmit power via rank relaxation, we propose a second-order cone programming (SOCP)-based approach in this paper which requires a significantly lower computational task. The performance of the incentive control mechanism and the optimization algorithm has been evaluated through numerical simulations.	1,0,0,0,0,0
Heisenberg Modules over Quantum 2-tori are metrized quantum vector bundles	The modular Gromov-Hausdorff propinquity is a distance on classes of modules endowed with quantum metric information, in the form of a metric form of a connection and a left Hilbert module structure. This paper proves that the family of Heisenberg modules over quantum two tori, when endowed with their canonical connections, form a family of metrized quantum vector bundles, as a first step in proving that Heisenberg modules form a continuous family for the modular Gromov-Hausdorff propinquity.	0,0,1,0,0,0
Fast and Lightweight Rate Control for Onboard Predictive Coding of Hyperspectral Images	Predictive coding is attractive for compression of hyperspecral images onboard of spacecrafts in light of the excellent rate-distortion performance and low complexity of recent schemes. In this letter we propose a rate control algorithm and integrate it in a lossy extension to the CCSDS-123 lossless compression recommendation. The proposed rate algorithm overhauls our previous scheme by being orders of magnitude faster and simpler to implement, while still providing the same accuracy in terms of output rate and comparable or better image quality.	1,0,0,0,0,0
Stochastic Non-convex Ordinal Embedding with Stabilized Barzilai-Borwein Step Size	Learning representation from relative similarity comparisons, often called ordinal embedding, gains rising attention in recent years. Most of the existing methods are batch methods designed mainly based on the convex optimization, say, the projected gradient descent method. However, they are generally time-consuming due to that the singular value decomposition (SVD) is commonly adopted during the update, especially when the data size is very large. To overcome this challenge, we propose a stochastic algorithm called SVRG-SBB, which has the following features: (a) SVD-free via dropping convexity, with good scalability by the use of stochastic algorithm, i.e., stochastic variance reduced gradient (SVRG), and (b) adaptive step size choice via introducing a new stabilized Barzilai-Borwein (SBB) method as the original version for convex problems might fail for the considered stochastic \textit{non-convex} optimization problem. Moreover, we show that the proposed algorithm converges to a stationary point at a rate $\mathcal{O}(\frac{1}{T})$ in our setting, where $T$ is the number of total iterations. Numerous simulations and real-world data experiments are conducted to show the effectiveness of the proposed algorithm via comparing with the state-of-the-art methods, particularly, much lower computational cost with good prediction performance.	1,0,0,1,0,0
Photonic Band Structure of Two-dimensional Atomic Lattices	Two-dimensional atomic arrays exhibit a number of intriguing quantum optical phenomena, including subradiance, nearly perfect reflection of radiation and long-lived topological edge states. Studies of emission and scattering of photons in such lattices require complete treatment of the radiation pattern from individual atoms, including long-range interactions. We describe a systematic approach to perform the calculations of collective energy shifts and decay rates in the presence of such long-range interactions for arbitrary two-dimensional atomic lattices. As applications of our method, we investigate the topological properties of atomic lattices both in free-space and near plasmonic surfaces.	0,1,0,0,0,0
New nanostructures of carbon: Quasifullerenes Cn-q (n=20,42,48,60)	Based on the third allotropic form of carbon (Fullerenes) through theoretical study have been predicted structures described as non-classical fullerenes. We have studied novel allotropic carbon structures with a closed cage configuration that have been predicted for the first time, by using DFT at the B3LYP level. Such carbon Cn-q structures (where, n=20, 42, 48 and 60), combine states of hybridization sp1 and sp2, for the formation of bonds. A comparative analysis of quasi-fullerenes with respect to their isomers of greater stability was also performed. Chemical stability was evaluated with the criteria of aromaticity through the different rings that build the systems. The results show new isomerism of carbon nanostructures with interesting chemical properties such as hardness, chemical potential and HOMO-LUMO gaps. We also studied thermal stability with Lagrangian molecular dynamics method using Atom- Center Density propagation (ADMP) method.	0,1,0,0,0,0
A Survey of Riccati Equation Results in Negative Imaginary Systems Theory and Quantum Control Theory	This paper presents a survey of some new applications of algebraic Riccati equations. In particular, the paper surveys some recent results on the use of algebraic Riccati equations in testing whether a system is negative imaginary and in synthesizing state feedback controllers which make the closed loop system negative imaginary. The paper also surveys the use of Riccati equation methods in the control of quantum linear systems including coherent $H^\infty$ control.	1,0,1,0,0,0
Proceedings of the Fifth Workshop on Proof eXchange for Theorem Proving	This volume of EPTCS contains the proceedings of the Fifth Workshop on Proof Exchange for Theorem Proving (PxTP 2017), held on September 23-24, 2017 as part of the Tableaux, FroCoS and ITP conferences in Brasilia, Brazil. The PxTP workshop series brings together researchers working on various aspects of communication, integration, and cooperation between reasoning systems and formalisms, with a special focus on proofs. The progress in computer-aided reasoning, both automated and interactive, during the past decades, made it possible to build deduction tools that are increasingly more applicable to a wider range of problems and are able to tackle larger problems progressively faster. In recent years, cooperation between such tools in larger systems has demonstrated the potential to reduce the amount of manual intervention. Cooperation between reasoning systems relies on availability of theoretical formalisms and practical tools to exchange problems, proofs, and models. The PxTP workshop series strives to encourage such cooperation by inviting contributions on all aspects of cooperation between reasoning tools, whether automatic or interactive.	1,0,0,0,0,0
Space-efficient classical and quantum algorithms for the shortest vector problem	A lattice is the integer span of some linearly independent vectors. Lattice problems have many significant applications in coding theory and cryptographic systems for their conjectured hardness. The Shortest Vector Problem (SVP), which is to find the shortest non-zero vector in a lattice, is one of the well-known problems that are believed to be hard to solve, even with a quantum computer. In this paper we propose space-efficient classical and quantum algorithms for solving SVP. Currently the best time-efficient algorithm for solving SVP takes $2^{n+o(n)}$ time and $2^{n+o(n)}$ space. Our classical algorithm takes $2^{2.05n+o(n)}$ time to solve SVP with only $2^{0.5n+o(n)}$ space. We then modify our classical algorithm to a quantum version, which can solve SVP in time $2^{1.2553n+o(n)}$ with $2^{0.5n+o(n)}$ classical space and only poly(n) qubits.	1,0,0,0,0,0
Angular momentum evolution of galaxies over the past 10-Gyr: A MUSE and KMOS dynamical survey of 400 star-forming galaxies from z=0.3-1.7	We present a MUSE and KMOS dynamical study 405 star-forming galaxies at redshift z=0.28-1.65 (median redshift z=0.84). Our sample are representative of star-forming, main-sequence galaxies, with star-formation rates of SFR=0.1-30Mo/yr and stellar masses M=10^8-10^11Mo. For 49+/-4% of our sample, the dynamics suggest rotational support, 24+/-3% are unresolved systems and 5+/-2% appear to be early-stage major mergers with components on 8-30kpc scales. The remaining 22+/-5% appear to be dynamically complex, irregular (or face-on systems). For galaxies whose dynamics suggest rotational support, we derive inclination corrected rotational velocities and show these systems lie on a similar scaling between stellar mass and specific angular momentum as local spirals with j*=J/M*\propto M^(2/3) but with a redshift evolution that scales as j*\propto M^{2/3}(1+z)^(-1). We identify a correlation between specific angular momentum and disk stability such that galaxies with the highest specific angular momentum, log(j*/M^(2/3))>2.5, are the most stable, with Toomre Q=1.10+/-0.18, compared to Q=0.53+/-0.22 for galaxies with log(j*/M^(2/3))<2.5. At a fixed mass, the HST morphologies of galaxies with the highest specific angular momentum resemble spiral galaxies, whilst those with low specific angular momentum are morphologically complex and dominated by several bright star-forming regions. This suggests that angular momentum plays a major role in defining the stability of gas disks: at z~1, massive galaxies that have disks with low specific angular momentum, appear to be globally unstable, clumpy and turbulent systems. In contrast, galaxies with high specific angular have evolved in to stable disks with spiral structures.	0,1,0,0,0,0
Comparing Computing Platforms for Deep Learning on a Humanoid Robot	The goal of this study is to test two different computing platforms with respect to their suitability for running deep networks as part of a humanoid robot software system. One of the platforms is the CPU-centered Intel NUC7i7BNH and the other is a NVIDIA Jetson TX2 system that puts more emphasis on GPU processing. The experiments addressed a number of benchmarking tasks including pedestrian detection using deep neural networks. Some of the results were unexpected but demonstrate that platforms exhibit both advantages and disadvantages when taking computational performance and electrical power requirements of such a system into account.	1,0,0,0,0,0
Model Spaces of Regularity Structures for Space-Fractional SPDEs	We study model spaces, in the sense of Hairer, for stochastic partial differential equations involving the fractional Laplacian. We prove that the fractional Laplacian is a singular kernel suitable to apply the theory of regularity structures. Our main contribution is to study the dependence of the model space for a regularity structure on the three-parameter problem involving the spatial dimension, the polynomial order of the nonlinearity, and the exponent of the fractional Laplacian. The goal is to investigate the growth of the model space under parameter variation. In particular, we prove several results in the approaching subcriticality limit leading to universal growth exponents of the regularity structure. A key role is played by the viewpoint that model spaces can be identified with families of rooted trees. Our proofs are based upon a geometrical construction similar to Newton polygons for classical Taylor series and various combinatorial arguments. We also present several explicit examples listing all elements with negative homogeneity by implementing a new symbolic software package to work with regularity structures. We use this package to illustrate our analytical results and to obtain new conjectures regarding coarse-grained network measures for model spaces.	0,0,1,0,0,0
HyperMinHash: MinHash in LogLog space	In this extended abstract, we describe and analyze a lossy compression of MinHash from buckets of size $O(\log n)$ to buckets of size $O(\log\log n)$ by encoding using floating-point notation. This new compressed sketch, which we call HyperMinHash, as we build off a HyperLogLog scaffold, can be used as a drop-in replacement of MinHash. Unlike comparable Jaccard index fingerprinting algorithms in sub-logarithmic space (such as b-bit MinHash), HyperMinHash retains MinHash's features of streaming updates, unions, and cardinality estimation. For a multiplicative approximation error $1+ \epsilon$ on a Jaccard index $ t $, given a random oracle, HyperMinHash needs $O\left(\epsilon^{-2} \left( \log\log n + \log \frac{1}{ t \epsilon} \right)\right)$ space. HyperMinHash allows estimating Jaccard indices of 0.01 for set cardinalities on the order of $10^{19}$ with relative error of around 10\% using 64KiB of memory; MinHash can only estimate Jaccard indices for cardinalities of $10^{10}$ with the same memory consumption.	1,0,0,0,0,0
Coexistence of pressure-induced structural phases in bulk black phosphorus: a combined x-ray diffraction and Raman study up to 18 GPa	We report a study of the structural phase transitions induced by pressure in bulk black phosphorus by using both synchrotron x-ray diffraction for pressures up to 12.2 GPa and Raman spectroscopy up to 18.2 GPa. Very recently black phosphorus attracted large attention because of the unique properties of fewlayers samples (phosphorene), but some basic questions are still open in the case of the bulk system. As concerning the presence of a Raman spectrum above 10 GPa, which should not be observed in an elemental simple cubic system, we propose a new explanation by attributing a key role to the non-hydrostatic conditions occurring in Raman experiments. Finally, a combined analysis of Raman and XRD data allowed us to obtain quantitative information on presence and extent of coexistences between different structural phases from ~5 up to ~15 GPa. This information can have an important role in theoretical studies on pressure-induced structural and electronic phase transitions in black phosphorus.	0,1,0,0,0,0
Casimir-Polder size consistency -- a constraint violated by some dispersion theories	A key goal in quantum chemistry methods, whether ab initio or otherwise, is to achieve size consistency. In this manuscript we formulate the related idea of "Casimir-Polder size consistency" that manifests in long-range dispersion energetics. We show that local approximations in time-dependent density functional theory dispersion energy calculations violate the consistency condition because of incorrect treatment of highly non-local "xc kernel" physics, by up to 10% in our tests on closed-shell atoms.	0,1,0,0,0,0
Wasserstein Introspective Neural Networks	We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of the INN algorithm and that of Wasserstein generative adversarial networks (WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing nearly a 20 times reduction in model size over INN for unsupervised generative modeling. (3) When applied to supervised classification, WINN also gives rise to improved robustness against adversarial examples in terms of the error reduction. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attacks.	1,0,0,0,0,0
Error-Correcting Neural Sequence Prediction	In this paper we propose a novel neural language modelling (NLM) method based on \textit{error-correcting output codes} (ECOC), abbreviated as ECOC-NLM. This latent variable based approach provides a principled way to choose a varying amount of latent output codes and avoids exact softmax normalization. Instead of minimizing measures between the predicted probability distribution and true distribution, we use error-correcting codes to represent both predictions and outputs. Secondly, we propose multiple ways to improve accuracy and convergence rates by maximizing the separability between codes that correspond to classes proportional to word embedding similarities. Lastly, we introduce a novel method called \textit{Latent Mixture Sampling}, a technique that is used to mitigate exposure bias and can be integrated into training latent-based neural language models. This involves mixing the latent codes (i.e variables) of past predictions and past targets in one of two ways: (1) according to a predefined sampling schedule or (2) a differentiable sampling procedure whereby the mixing probability is learned throughout training by replacing the greedy argmax operation with a smooth approximation. In evaluating Codeword Mixture Sampling for ECOC-NLM, we also baseline it against CWMS in a closely related Hierarhical Softmax-based NLM.	1,0,0,1,0,0
Nonparametric Inference via Bootstrapping the Debiased Estimator	In this paper, we propose to construct confidence bands by bootstrapping the debiased kernel density estimator (for density estimation) and the debiased local polynomial regression estimator (for regression analysis). The idea of using a debiased estimator was first introduced in Calonico et al. (2015), where they construct a confidence interval of the density function (and regression function) at a given point by explicitly estimating stochastic variations. We extend their ideas and propose a bootstrap approach for constructing confidence bands that is uniform for every point in the support. We prove that the resulting bootstrap confidence band is asymptotically valid and is compatible with most tuning parameter selection approaches, such as the rule of thumb and cross-validation. We further generalize our method to confidence sets of density level sets and inverse regression problems. Simulation studies confirm the validity of the proposed confidence bands/sets.	0,0,1,1,0,0
Extended periodic links and HOMFLYPT polynomial	Extended strongly periodic links have been introduced by Przytycki and Sokolov as a symmetric surgery presentation of three-manifolds on which the finite cyclic group acts without fixed points. The purpose of this paper is to prove that the symmetry of these links is reflected by the first coefficients of the HOMFLYPT polynomial.	0,0,1,0,0,0
Continuum Limit of Posteriors in Graph Bayesian Inverse Problems	We consider the problem of recovering a function input of a differential equation formulated on an unknown domain $M$. We assume to have access to a discrete domain $M_n=\{x_1, \dots, x_n\} \subset M$, and to noisy measurements of the output solution at $p\le n$ of those points. We introduce a graph-based Bayesian inverse problem, and show that the graph-posterior measures over functions in $M_n$ converge, in the large $n$ limit, to a posterior over functions in $M$ that solves a Bayesian inverse problem with known domain. The proofs rely on the variational formulation of the Bayesian update, and on a new topology for the study of convergence of measures over functions on point clouds to a measure over functions on the continuum. Our framework, techniques, and results may serve to lay the foundations of robust uncertainty quantification of graph-based tasks in machine learning. The ideas are presented in the concrete setting of recovering the initial condition of the heat equation on an unknown manifold.	0,0,1,1,0,0
A General Probabilistic Approach for Quantitative Assessment of LES Combustion Models	The Wasserstein metric is introduced as a probabilistic method to enable quantitative evaluations of LES combustion models. The Wasserstein metric can directly be evaluated from scatter data or statistical results using probabilistic reconstruction against experimental data. The method is derived and generalized for turbulent reacting flows, and applied to validation tests involving the Sydney piloted jet flame. It is shown that the Wasserstein metric is an effective validation tool that extends to multiple scalar quantities, providing an objective and quantitative evaluation of model deficiencies and boundary conditions on the simulation accuracy. Several test cases are considered, beginning with a comparison of mixture-fraction results, and the subsequent extension to reactive scalars, including temperature and species mass fractions of \ce{CO} and \ce{CO2}. To demonstrate the versatility of the proposed method in application to multiple datasets, the Wasserstein metric is applied to a series of different simulations that were contributed to the TNF-workshop. Analysis of the results allowed to identify competing contributions to model deviations, arising from uncertainties in the boundary conditions and model deficiencies. These applications demonstrate that the Wasserstein metric constitutes an easily applicable mathematical tool that reduce multiscalar combustion data and large datasets into a scalar-valued quantitative measure.	0,1,0,0,0,0
Ce 3$p$ hard x-ray photoelectron spectroscopy study of the topological Kondo insulator CeRu$_4$Sn$_6$	Bulk sensitive hard x-ray photoelectron spectroscopy data of the Ce 3$p$ core level of CeRu$_4$Sn$_6$ are presented. Using a combination of full multiplet and configuration iteration model we were able to obtain an accurate lineshape analysis of the data, thereby taking into account correlations for the strong plasmon intensities. We conclude that CeRu$_4$Sn$_6$ is a moderately mixed valence compound with a weight of 8% for the Ce $f^0$ configuration in the ground state.	0,1,0,0,0,0
Counting intersecting and pairs of cross-intersecting families	A family of subsets of $\{1,\ldots,n\}$ is called {\it intersecting} if any two of its sets intersect. A classical result in extremal combinatorics due to Erdős, Ko, and Rado determines the maximum size of an intersecting family of $k$-subsets of $\{1,\ldots, n\}$. In this paper we study the following problem: how many intersecting families of $k$-subsets of $\{1,\ldots, n\}$ are there? Improving a result of Balogh, Das, Delcourt, Liu, and Sharifzadeh, we determine this quantity asymptotically for $n\ge 2k+2+2\sqrt{k\log k}$ and $k\to \infty$. Moreover, under the same assumptions we also determine asymptotically the number of {\it non-trivial} intersecting families, that is, intersecting families for which the intersection of all sets is empty. We obtain analogous results for pairs of cross-intersecting families.	1,0,1,0,0,0
CERES in Propositional Proof Schemata	Cut-elimination is one of the most famous problems in proof theory, and it was defined and solved for first-order sequent calculus by Gentzen in his celebrated Hauptsatz. Ceres is a different cut-elimination algorithm for first- and higher-order classical logic. Ceres was extended to proof schemata, which are templates for usual first-order proofs, with parameters for natural numbers. However, while Ceres is known to be a complete cut-elimination algorithm for first-order logic, it is not clear whether this holds for first-order schemata too: given in input a proof schema with cuts, does Ceres always produce a schema for a cut-free proof? The difficult step is finding and representing an appropriate refutation schema for the characteristic term schema of a proof schema. In this thesis, we progress in solving this problem by restricting Ceres to propositional schemata, which are templates for propositional proofs. By limiting adequately the expressivity of propositional schemata and proof schemata, we aim at providing a version of schematic Ceres which is a complete cut-elimination algorithm for propositional schemata. We focus on one particular step of Ceres: resolution refutation schemata. First, we prove that by naively adapting Ceres for first-order schemata to our case, we end up with an incomplete algorithm. Then, we modify slightly the concept of resolution refutation schema: to refute a clause set, first we bring it to a generic form, and then we use a fixed refutation of that generic clause set. Our variation of schematic Ceres is the first step towards completeness with respect to propositional schemata.	1,0,1,0,0,0
Buildup of Speaking Skills in an Online Learning Community: A Network-Analytic Exploration	In this study, we explore peer-interaction effects in online networks on speaking skill development. In particular, we present an evidence for gradual buildup of skills in a small-group setting that has not been reported in the literature. We introduce a novel dataset of six online communities consisting of 158 participants focusing on improving their speaking skills. They video-record speeches for 5 prompts in 10 days and exchange comments and performance-ratings with their peers. We ask (i) whether the participants' ratings are affected by their interaction patterns with peers, and (ii) whether there is any gradual buildup of speaking skills in the communities towards homogeneity. To analyze the data, we employ tools from the emerging field of Graph Signal Processing (GSP). GSP enjoys a distinction from Social Network Analysis in that the latter is concerned primarily with the connection structures of graphs, while the former studies signals on top of graphs. We study the performance ratings of the participants as graph signals atop underlying interaction topologies. Total variation analysis of the graph signals show that the participants' rating differences decrease with time (slope=-0.04, p<0.01), while average ratings increase (slope=0.07, p<0.05)--thereby gradually building up the ratings towards community-wide homogeneity. We provide evidence for peer-influence through a prediction formulation. Our consensus-based prediction model outperforms baseline network-agnostic regression models by about 23% in predicting performance ratings. This, in turn, shows that participants' ratings are affected by their peers' ratings and the associated interaction patterns, corroborating previous findings. Then, we formulate a consensus-based diffusion model that captures these observations of peer-influence from our analyses.	1,0,0,0,0,0
The transition matrix between the Specht and web bases is unipotent with additional vanishing entries	We compare two important bases of an irreducible representation of the symmetric group: the web basis and the Specht basis. The web basis has its roots in the Temperley-Lieb algebra and knot-theoretic considerations. The Specht basis is a classic algebraic and combinatorial construction of symmetric group representations which arises in this context through the geometry of varieties called Springer fibers. We describe a graph that encapsulates combinatorial relations between each of these bases, prove that there is a unique way (up to scaling) to map the Specht basis into the web representation, and use this to recover a result of Garsia-McLarnan that the transition matrix between the Specht and web bases is upper-triangular with ones along the diagonal. We then strengthen their result to prove vanishing of certain additional entries unless a nesting condition on webs is satisfied. In fact we conjecture that the entries of the transition matrix are nonnegative and are nonzero precisely when certain directed paths exist in the web graph.	0,0,1,0,0,0
Segment Parameter Labelling in MCMC Mean-Shift Change Detection	This work addresses the problem of segmentation in time series data with respect to a statistical parameter of interest in Bayesian models. It is common to assume that the parameters are distinct within each segment. As such, many Bayesian change point detection models do not exploit the segment parameter patterns, which can improve performance. This work proposes a Bayesian mean-shift change point detection algorithm that makes use of repetition in segment parameters, by introducing segment class labels that utilise a Dirichlet process prior. The performance of the proposed approach was assessed on both synthetic and real world data, highlighting the enhanced performance when using parameter labelling.	1,0,0,1,0,0
The Amplitude-Phase Decomposition for the Magnetotelluric Impedance Tensor	The Phase Tensor (PT) marked a breakthrough in understanding and analysis of electric galvanic distortion but does not contain any impedance amplitude information and therefore cannot quantify resistivity without complementary data. We formulate a complete impedance tensor decomposition into the PT and a new Amplitude Tensor (AT) that is shown to be complementary and mathematically independent to the PT. We show that for the special cases of 1D and 2D models, the geometric AT parameters (strike and skew angles) converge to PT parameters and the singular values of the AT correspond to the impedance amplitudes of the transverse electric and transverse magnetic modes. In all cases, we show that the AT contains both galvanic and inductive amplitudes, the latter of which is argued to be physically related to the inductive information of the PT. The geometric parameters of the inductive AT and the PT represent the same geometry of the subsurface conductivity distribution that is affected by induction processes, and therefore we hypothesise that geometric PT parameters can be used to approximate the inductive AT. Then, this hypothesis leads to the estimation of the galvanic AT which is equal to the galvanic electric distortion tensor at the lowest measured period. This estimation of the galvanic distortion departs from the common assumption to consider 1D or 2D regional structures and can be applied for general 3D subsurfaces. We demonstrate exemplarily with an explicit formulation how our hypothesis can be used to recover the galvanic electric anisotropic distortion for 2D subsurfaces, which was, until now, believed to be indeterminable for 2D data. Moreover, we illustrate the AT as a mapping tool and we compare it to the PT with both synthetic and real data examples. Lastly, we argue that the AT can provide important non-redundant amplitude information to PT inversions.	0,1,0,0,0,0
Learning Multimodal Transition Dynamics for Model-Based Reinforcement Learning	In this paper we study how to learn stochastic, multimodal transition dynamics in reinforcement learning (RL) tasks. We focus on evaluating transition function estimation, while we defer planning over this model to future work. Stochasticity is a fundamental property of many task environments. However, discriminative function approximators have difficulty estimating multimodal stochasticity. In contrast, deep generative models do capture complex high-dimensional outcome distributions. First we discuss why, amongst such models, conditional variational inference (VI) is theoretically most appealing for model-based RL. Subsequently, we compare different VI models on their ability to learn complex stochasticity on simulated functions, as well as on a typical RL gridworld with multimodal dynamics. Results show VI successfully predicts multimodal outcomes, but also robustly ignores these for deterministic parts of the transition dynamics. In summary, we show a robust method to learn multimodal transitions using function approximation, which is a key preliminary for model-based RL in stochastic domains.	1,0,0,1,0,0
The spin-Brauer diagram algebra	We investigate the spin-Brauer diagram algebra, denoted ${\bf SB}_n(\delta)$, that arises from studying an analogous form of Schur-Weyl duality for the action of the pin group on ${\bf V}^{\otimes n} \otimes \Delta$. Here ${\bf V}$ is the standard $N$-dimensional complex representation of ${\bf Pin}(N)$ and $\Delta$ is the spin representation. When $\delta = N$ is a positive integer, we define a surjective map ${\bf SB}_n(N) \twoheadrightarrow {\rm End}_{{\bf Pin}(N)}({\bf V}^{\otimes n} \otimes \Delta)$ and show it is an isomorphism for $N \geq 2n$. We show ${\bf SB}_n(\delta)$ is a cellular algebra and use cellularity to characterize its irreducible representations.	0,0,1,0,0,0
Latent Geometry and Memorization in Generative Models	It can be difficult to tell whether a trained generative model has learned to generate novel examples or has simply memorized a specific set of outputs. In published work, it is common to attempt to address this visually, for example by displaying a generated example and its nearest neighbor(s) in the training set (in, for example, the L2 metric). As any generative model induces a probability density on its output domain, we propose studying this density directly. We first study the geometry of the latent representation and generator, relate this to the output density, and then develop techniques to compute and inspect the output density. As an application, we demonstrate that "memorization" tends to a density made of delta functions concentrated on the memorized examples. We note that without first understanding the geometry, the measurement would be essentially impossible to make.	1,0,0,1,0,0
Deep Multi-view Learning to Rank	We study the problem of learning to rank from multiple sources. Though multi-view learning and learning to rank have been studied extensively leading to a wide range of applications, multi-view learning to rank as a synergy of both topics has received little attention. The aim of the paper is to propose a composite ranking method while keeping a close correlation with the individual rankings simultaneously. We propose a multi-objective solution to ranking by capturing the information of the feature mapping from both within each view as well as across views using autoencoder-like networks. Moreover, a novel end-to-end solution is introduced to enhance the joint ranking with minimum view-specific ranking loss, so that we can achieve the maximum global view agreements within a single optimization process. The proposed method is validated on a wide variety of ranking problems, including university ranking, multi-view lingual text ranking and image data ranking, providing superior results.	0,0,0,1,0,0
Dehn functions of subgroups of right-angled Artin groups	We show that for each positive integer $k$ there exist right-angled Artin groups containing free-by-cyclic subgroups whose monodromy automorphisms grow as $n^k$. As a consequence we produce examples of right-angled Artin groups containing finitely presented subgroups whose Dehn functions grow as $n^{k+2}$.	0,0,1,0,0,0
Scalable Graph Learning for Anti-Money Laundering: A First Look	Organized crime inflicts human suffering on a genocidal scale: the Mexican drug cartels have murdered 150,000 people since 2006, upwards of 700,000 people per year are "exported" in a human trafficking industry enslaving an estimated 40 million people. These nefarious industries rely on sophisticated money laundering schemes to operate. Despite tremendous resources dedicated to anti-money laundering (AML) only a tiny fraction of illicit activity is prevented. The research community can help. In this brief paper, we map the structural and behavioral dynamics driving the technical challenge. We review AML methods, current and emergent. We provide a first look at scalable graph convolutional neural networks for forensic analysis of financial data, which is massive, dense, and dynamic. We report preliminary experimental results using a large synthetic graph (1M nodes, 9M edges) generated by a data simulator we created called AMLSim. We consider opportunities for high performance efficiency, in terms of computation and memory, and we share results from a simple graph compression experiment. Our results support our working hypothesis that graph deep learning for AML bears great promise in the fight against criminal financial activity.	1,0,0,0,0,0
Universal fitness dynamics through an adaptive resource utilization model	The fitness of a species determines its abundance and survival in an ecosystem. At the same time, species take up resources for growth, so their abundance affects the availability of resources in an ecosystem. We show here that such species-resource coupling can be used to assign a quantitative metric for fitness to each species. This fitness metric also allows for the modeling of drift in species composition, and hence ecosystem evolution through speciation and adaptation. Our results provide a foundation for an entirely computational exploration of evolutionary ecosystem dynamics on any length or time scale. For example, we can evolve ecosystem dynamics even by initiating dynamics out of a single primordial ancestor and show that there exists a well defined ecosystem-averaged fitness dynamics that is resilient against resource shocks.	0,0,0,0,1,0
The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes	In the work of Peng et al. in 2012, a new measure was proposed for fault diagnosis of systems: namely, g-good-neighbor conditional diagnosability, which requires that any fault-free vertex has at least g fault-free neighbors in the system. In this paper, we establish the g-good-neighbor conditional diagnosability of locally twisted cubes under the PMC model and the MM^* model.	1,0,1,0,0,0
A Novel Algorithm for Optimal Electricity Pricing in a Smart Microgrid Network	The evolution of smart microgrid and its demand-response characteristics not only will change the paradigms of the century-old electric grid but also will shape the electricity market. In this new market scenario, once always energy consumers, now may act as sellers due to the excess of energy generated from newly deployed distributed generators (DG). The smart microgrid will use the existing electrical transmission network and a pay per use transportation cost without implementing new transmission lines which involve a massive capital investment. In this paper, we propose a novel algorithm to minimize the electricity price with the optimal trading of energy between sellers and buyers of the smart microgrid network. The algorithm is capable of solving the optimal power allocation problem (with optimal transmission cost) for a microgrid network in a polynomial time without modifying the actual marginal costs of power generation. We mathematically formulate the problem as a nonlinear non-convex and decompose the problem to separate the optimal marginal cost model from the electricity allocation model. Then, we develop a divide-and-conquer method to minimize the electricity price by jointly solving the optimal marginal cost model and electricity allocation problems. To evaluate the performance of the solution method, we develop and simulate the model with different marginal cost functions and compare it with a first come first serve electricity allocation method.	1,0,0,0,0,0
User Experience of the CoSTAR System for Instruction of Collaborative Robots	How can we enable novice users to create effective task plans for collaborative robots? Must there be a tradeoff between generalizability and ease of use? To answer these questions, we conducted a user study with the CoSTAR system, which integrates perception and reasoning into a Behavior Tree-based task plan editor. In our study, we ask novice users to perform simple pick-and-place assembly tasks under varying perception and planning capabilities. Our study shows that users found Behavior Trees to be an effective way of specifying task plans. Furthermore, users were also able to more quickly, effectively, and generally author task plans with the addition of CoSTAR's planning, perception, and reasoning capabilities. Despite these improvements, concepts associated with these capabilities were rated by users as less usable, and our results suggest a direction for further refinement.	1,0,0,0,0,0
On low for speed oracles	Relativizing computations of Turing machines to an oracle is a central concept in the theory of computation, both in complexity theory and in computability theory(!). Inspired by lowness notions from computability theory, Allender introduced the concept of "low for speed" oracles. An oracle A is low for speed if relativizing to A has essentially no effect on computational complexity, meaning that if a decidable language can be decided in time $f(n)$ with access to oracle A, then it can be decided in time poly(f(n)) without any oracle. The existence of non-computable such A's was later proven by Bayer and Slaman, who even constructed a computably enumerable one, and exhibited a number of properties of these oracles as well as interesting connections with computability theory. In this paper, we pursue this line of research, answering the questions left by Bayer and Slaman and give further evidence that the structure of the class of low for speed oracles is a very rich one.	1,0,1,0,0,0
Delivery Latency Trade-Offs of Heterogeneous Contents in Fog Radio Access Networks	A Fog Radio Access Network (F-RAN) is a cellular wireless system that enables content delivery via the caching of popular content at edge nodes (ENs) and cloud processing. The existing information-theoretic analyses of F-RAN systems, and special cases thereof, make the assumption that all requests should be guaranteed the same delivery latency, which results in identical latency for all files in the content library. In practice, however, contents may have heterogeneous timeliness requirements depending on the applications that operate on them. Given per-EN cache capacity constraint, there exists a fundamental trade-off among the delivery latencies of different users' requests, since contents that are allocated more cache space generally enjoy lower delivery latencies. For the case with two ENs and two users, the optimal latency trade-off is characterized in the high-SNR regime in terms of the Normalized Delivery Time (NDT) metric. The main results are illustrated by numerical examples.	1,0,0,0,0,0
The cosmic spiderweb: equivalence of cosmic, architectural, and origami tessellations	For over twenty years, the term 'cosmic web' has guided our understanding of the large-scale arrangement of matter in the cosmos, accurately evoking the concept of a network of galaxies linked by filaments. But the physical correspondence between the cosmic web and structural-engineering or textile 'spiderwebs' is even deeper than previously known, and extends to origami tessellations as well. Here we explain that in a good structure-formation approximation known as the adhesion model, threads of the cosmic web form a spiderweb, i.e. can be strung up to be entirely in tension. The correspondence is exact if nodes sampling voids are included, and if structure is excluded within collapsed regions (walls, filaments and haloes), where dark-matter multistreaming and baryonic physics affect the structure. We also suggest how concepts arising from this link might be used to test cosmological models: for example, to test for large-scale anisotropy and rotational flows in the cosmos.	0,1,0,0,0,0
Urban Scene Segmentation with Laser-Constrained CRFs	Robots typically possess sensors of different modalities, such as colour cameras, inertial measurement units, and 3D laser scanners. Often, solving a particular problem becomes easier when more than one modality is used. However, while there are undeniable benefits to combine sensors of different modalities the process tends to be complicated. Segmenting scenes observed by the robot into a discrete set of classes is a central requirement for autonomy as understanding the scene is the first step to reason about future situations. Scene segmentation is commonly performed using either image data or 3D point cloud data. In computer vision many successful methods for scene segmentation are based on conditional random fields (CRF) where the maximum a posteriori (MAP) solution to the segmentation can be obtained by inference. In this paper we devise a new CRF inference method for scene segmentation that incorporates global constraints, enforcing the sets of nodes are assigned the same class label. To do this efficiently, the CRF is formulated as a relaxed quadratic program whose MAP solution is found using a gradient-based optimisation approach. The proposed method is evaluated on images and 3D point cloud data gathered in urban environments where image data provides the appearance features needed by the CRF, while the 3D point cloud data provides global spatial constraints over sets of nodes. Comparisons with belief propagation, conventional quadratic programming relaxation, and higher order potential CRF show the benefits of the proposed method.	1,0,0,0,0,0
Steady Galactic Dynamos and Observational Consequences I: Halo Magnetic Fields	We study the global consequences in the halos of spiral galaxies of the steady, axially symmetric, mean field dynamo. We use the classical theory but add the possibility of using the velocity field components as parameters in addition to the helicity and diffusivity. The analysis is based on the simplest version of the theory and uses scale-invariant solutions. The velocity field (subject to restrictions) is a scale invariant field in a `pattern' frame, in place of a full dynamical theory. The `pattern frame' of reference may either be the systemic frame or some rigidly rotating spiral pattern frame. One type of solution for the magnetic field yields off-axis, spirally wound, magnetic field lines. These predict sign changes in the Faraday screen rotation measure in every quadrant of the halo of an edge-on galaxy. Such rotation measure oscillations have been observed in the CHANG-ES survey.	0,1,0,0,0,0
On the number of cyclic subgroups of a finite group	Let $G$ be a finite group and let $c(G)$ be the number of cyclic subgroups of $G$. We study the function $\alpha(G) = c(G)/|G|$. We explore its basic properties and we point out a connection with the probability of commutation. For many families $\mathscr{F}$ of groups we characterize the groups $G \in \mathscr{F}$ for which $\alpha(G)$ is maximal and we classify the groups $G$ for which $\alpha(G) > 3/4$. We also study the number of cyclic subgroups of a direct power of a given group deducing an asymptotic result and we characterize the equality $\alpha(G) = \alpha(G/N)$ when $G/N$ is a symmetric group.	0,0,1,0,0,0
Parameters for Generalized Hecke Algebras in Type B	The irreducible representations of full support in the rational Cherednik category $\mathcal{O}_c(W)$ attached to a Coxeter group $W$ are in bijection with the irreducible representations of an associated Iwahori-Hecke algebra. Recent work has shown that the irreducible representations in $\mathcal{O}_c(W)$ of arbitrary given support are similarly governed by certain generalized Hecke algebras. In this paper we compute the parameters for these generalized Hecke algebras in the remaining previously unknown cases, corresponding to the parabolic subgroup $B_n \times S_k$ in $B_{n+k}$ for $k \geq 2$ and $n \geq 0$.	0,0,1,0,0,0
Spurious Vanishing Problem in Approximate Vanishing Ideal	Approximate vanishing ideal, which is a new concept from computer algebra, is a set of polynomials that almost takes a zero value for a set of given data points. The introduction of approximation to exact vanishing ideal has played a critical role in capturing the nonlinear structures of noisy data by computing the approximate vanishing polynomials. However, approximate vanishing has a theoretical problem, which is giving rise to the spurious vanishing problem that any polynomial turns into an approximate vanishing polynomial by coefficient scaling. In the present paper, we propose a general method that enables many basis construction methods to overcome this problem. Furthermore, a coefficient truncation method is proposed that balances the theoretical soundness and computational cost. The experiments show that the proposed method overcomes the spurious vanishing problem and significantly increases the accuracy of classification.	1,0,0,1,0,0
Quantum X Waves with Orbital Angular Momentum in Nonlinear Dispersive Media	We present a complete and consistent quantum theory of generalised X waves with orbital angular momentum (OAM) in dispersive media. We show that the resulting quantised light pulses are affected by neither dispersion nor diffraction and are therefore resilient against external perturbations. The nonlinear interaction of quantised X waves in quadratic and Kerr nonlinear media is also presented and studied in detail.	0,1,0,0,0,0
A spectral approach to transit timing variations	The high planetary multiplicity revealed by Kepler implies that Transit Time Variations (TTVs) are intrinsically common. The usual procedure for detecting these TTVs is biased to long-period, deep transit planets whereas most transiting planets have short periods and shallow transits. Here we introduce the Spectral Approach to TTVs technique that allows expanding the TTVs catalog towards lower TTV amplitude, shorter orbital period, and shallower transit depth. In the Spectral Approach we assume that a sinusoidal TTV exists in the data and then calculate the improvement to $\chi^2$ this model allows over that of linear ephemeris model. This enables detection of TTVs even in cases where the transits are too shallow so individual transits cannot be timed. The Spectral Approach is more sensitive due to the reduced number of free parameters in its model. Using the Spectral Approach, we: (a) detect 131 new periodic TTVs in Kepler data (an increase of ~2/3 over a previous TTV catalog); (b) Constrain the TTV periods of 34 long-period TTVs and reduce amplitude errors of known TTVs; (c) Identify cases of multi-periodic TTVs, for which absolute planetary mass determination may be possible. We further extend our analysis by using perturbation theory assuming small TTV amplitude at the detection stage, which greatly speeds up our detection (to a level of few seconds per star). Our extended TTVs sample shows no deficit of short period or low amplitude transits, in contrast to previous surveys in which the detection schemes were significantly biased against such systems.	0,1,0,0,0,0
A data driven trimming procedure for robust classification	Classification rules can be severely affected by the presence of disturbing observations in the training sample. Looking for an optimal classifier with such data may lead to unnecessarily complex rules. So, simpler effective classification rules could be achieved if we relax the goal of fitting a good rule for the whole training sample but only consider a fraction of the data. In this paper we introduce a new method based on trimming to produce classification rules with guaranteed performance on a significant fraction of the data. In particular, we provide an automatic way of determining the right trimming proportion and obtain in this setting oracle bounds for the classification error on the new data set.	0,0,1,1,0,0
Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task	End-to-end control for robot manipulation and grasping is emerging as an attractive alternative to traditional pipelined approaches. However, end-to-end methods tend to either be slow to train, exhibit little or no generalisability, or lack the ability to accomplish long-horizon or multi-stage tasks. In this paper, we show how two simple techniques can lead to end-to-end (image to velocity) execution of a multi-stage task, which is analogous to a simple tidying routine, without having seen a single real image. This involves locating, reaching for, and grasping a cube, then locating a basket and dropping the cube inside. To achieve this, robot trajectories are computed in a simulator, to collect a series of control velocities which accomplish the task. Then, a CNN is trained to map observed images to velocities, using domain randomisation to enable generalisation to real world images. Results show that we are able to successfully accomplish the task in the real world with the ability to generalise to novel environments, including those with dynamic lighting conditions, distractor objects, and moving objects, including the basket itself. We believe our approach to be simple, highly scalable, and capable of learning long-horizon tasks that have until now not been shown with the state-of-the-art in end-to-end robot control.	1,0,0,0,0,0
Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization	Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice. As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form. It remains open to explore duality theory and algorithms in such a non-convex and NP-hard problem setting. In this paper, we bridge this gap by establishing a duality theory for sparsity-constrained minimization with $\ell_2$-regularized loss function and proposing an IHT-style algorithm for dual maximization. Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be equivalently solved in a dual formulation. The proposed dual IHT algorithm is a super-gradient method for maximizing the non-smooth dual objective. An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by virtually all the existing primal IHT algorithms without sparsity relaxation. Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization. Numerical results demonstrate the superiority of dual IHT algorithms to the state-of-the-art primal IHT-style algorithms in model estimation accuracy and computational efficiency.	1,0,0,1,0,0
Inference for Multiple Change-points in Linear and Non-linear Time Series Models	In this paper we develop a generalized likelihood ratio scan method (GLRSM) for multiple change-points inference in piecewise stationary time series, which estimates the number and positions of change-points and provides a confidence interval for each change-point. The computational complexity of using GLRSM for multiple change-points detection is as low as $O(n(\log n)^3)$ for a series of length $n$. Consistency of the estimated numbers and positions of the change-points is established. Extensive simulation studies are provided to demonstrate the effectiveness of the proposed methodology under different scenarios.	0,0,1,1,0,0
Now Playing: Continuous low-power music recognition	Existing music recognition applications require a connection to a server that performs the actual recognition. In this paper we present a low-power music recognizer that runs entirely on a mobile device and automatically recognizes music without user interaction. To reduce battery consumption, a small music detector runs continuously on the mobile device's DSP chip and wakes up the main application processor only when it is confident that music is present. Once woken, the recognizer on the application processor is provided with a few seconds of audio which is fingerprinted and compared to the stored fingerprints in the on-device fingerprint database of tens of thousands of songs. Our presented system, Now Playing, has a daily battery usage of less than 1% on average, respects user privacy by running entirely on-device and can passively recognize a wide range of music.	1,0,0,0,0,0
Transpiling Programmable Computable Functions to Answer Set Programs	Programming Computable Functions (PCF) is a simplified programming language which provides the theoretical basis of modern functional programming languages. Answer set programming (ASP) is a programming paradigm focused on solving search problems. In this paper we provide a translation from PCF to ASP. Using this translation it becomes possible to specify search problems using PCF.	1,0,0,0,0,0
On utility maximization without passing by the dual problem	We treat utility maximization from terminal wealth for an agent with utility function $U:\mathbb{R}\to\mathbb{R}$ who dynamically invests in a continuous-time financial market and receives a possibly unbounded random endowment. We prove the existence of an optimal investment without introducing the associated dual problem. We rely on a recent result of Orlicz space theory, due to Delbaen and Owari which leads to a simple and transparent proof. Our results apply to non-smooth utilities and even strict concavity can be relaxed. We can handle certain random endowments with non-hedgeable risks, complementing earlier papers. Constraints on the terminal wealth can also be incorporated. As examples, we treat frictionless markets with finitely many assets and large financial markets.	0,0,1,0,0,0
Some Aspects of Uniqueness Theory of Entire and Meromorphic Functions (Ph.D. thesis)	The subject of our thesis is the uniqueness theory of meromorphic functions and it is devoted to problems concerning Bruck conjecture, set sharing and related topics. The tool, we used in our discussions is classical Nevanlinna theory of meromorphic functions. In 1996, in order to find the relation between an entire function with its derivative, counterpart sharing one value CM, a famous conjecture was proposed by R. Bruck. Since then the conjecture and its analogous results have been investigated by many researchers and continuous efforts have been put on by them. In our thesis, we have obtained similar types of conclusions as that of Bruck for two differential polynomials which in turn improve several existing results under different sharing environment. A number of examples have been exhibited to justify the necessity or sharpness of some conditions, hypothesis used in the thesis. As a variation of value sharing, F. Gross first introduced the idea of set sharing, by proposing a problem, which has later became popular as Gross Problem. Inspired by the Gross' Problem, the set sharing problems were started which was later shifted towards the characterization of the polynomial backbone of different unique range sets. In our study, we introduced some new type of unique range sets and at the same time, we further explored the anatomy of these unique range sets generating polynomials as well as connected Bruck conjecture with Gross' Problem.	0,0,1,0,0,0
A formula for the nonsymmetric Opdam's hypergeometric function of type $A_2$	The aim of this paper is to give an explicit formula for the nonsymmetric Heckman-Opdam's hypergeometric function of type $A_2$. This is obtained by differentiating the corresponding symmetric hypergeometric function.	0,0,1,0,0,0
Towards Reverse-Engineering Black-Box Neural Networks	Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks -- we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models.	1,0,0,1,0,0
Monads on higher monoidal categories	We study the action of monads on categories equipped with several monoidal structures. We identify the structure and conditions that guarantee that the higher monoidal structure is inherited by the category of algebras over the monad. Monoidal monads and comonoidal monads appear as the base cases in this hierarchy. Monads acting on duoidal categories constitute the next case. We cover the general case of $n$-monoidal categories and discuss several naturally occurring examples in which $n\leq 3$.	0,0,1,0,0,0
Noncommutative Knörrer type equivalences via noncommutative resolutions of singularities	We construct Knörrer type equivalences outside of the hypersurface case, namely, between singularity categories of cyclic quotient surface singularities and certain finite dimensional local algebras. This generalises Knörrer's equivalence for singularities of Dynkin type A (between Krull dimensions $2$ and $0$) and yields many new equivalences between singularity categories of finite dimensional algebras. Our construction uses noncommutative resolutions of singularities, relative singularity categories, and an idea of Hille & Ploog yielding strongly quasi-hereditary algebras which we describe explicitly by building on Wemyss's work on reconstruction algebras. Moreover, K-theory gives obstructions to generalisations of our main result.	0,0,1,0,0,0
A Weakly Supervised Approach to Train Temporal Relation Classifiers and Acquire Regular Event Pairs Simultaneously	Capabilities of detecting temporal relations between two events can benefit many applications. Most of existing temporal relation classifiers were trained in a supervised manner. Instead, we explore the observation that regular event pairs show a consistent temporal relation despite of their various contexts, and these rich contexts can be used to train a contextual temporal relation classifier, which can further recognize new temporal relation contexts and identify new regular event pairs. We focus on detecting after and before temporal relations and design a weakly supervised learning approach that extracts thousands of regular event pairs and learns a contextual temporal relation classifier simultaneously. Evaluation shows that the acquired regular event pairs are of high quality and contain rich commonsense knowledge and domain specific knowledge. In addition, the weakly supervised trained temporal relation classifier achieves comparable performance with the state-of-the-art supervised systems.	1,0,0,0,0,0
Selective Inference for Change Point Detection in Multi-dimensional Sequences	We study the problem of detecting change points (CPs) that are characterized by a subset of dimensions in a multi-dimensional sequence. A method for detecting those CPs can be formulated as a two-stage method: one for selecting relevant dimensions, and another for selecting CPs. It has been difficult to properly control the false detection probability of these CP detection methods because selection bias in each stage must be properly corrected. Our main contribution in this paper is to formulate a CP detection problem as a selective inference problem, and show that exact (non-asymptotic) inference is possible for a class of CP detection methods. We demonstrate the performances of the proposed selective inference framework through numerical simulations and its application to our motivating medical data analysis problem.	0,0,0,1,0,0
State Representation Learning for Control: An Overview	Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent's actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.	0,0,0,1,0,0
Theoretical Foundation of Co-Training and Disagreement-Based Algorithms	Disagreement-based approaches generate multiple classifiers and exploit the disagreement among them with unlabeled data to improve learning performance. Co-training is a representative paradigm of them, which trains two classifiers separately on two sufficient and redundant views; while for the applications where there is only one view, several successful variants of co-training with two different classifiers on single-view data instead of two views have been proposed. For these disagreement-based approaches, there are several important issues which still are unsolved, in this article we present theoretical analyses to address these issues, which provides a theoretical foundation of co-training and disagreement-based approaches.	1,0,0,1,0,0
Kneser ranks of random graphs and minimum difference representations	Every graph $G=(V,E)$ is an induced subgraph of some Kneser graph of rank $k$, i.e., there is an assignment of (distinct) $k$-sets $v \mapsto A_v$ to the vertices $v\in V$ such that $A_u$ and $A_v$ are disjoint if and only if $uv\in E$. The smallest such $k$ is called the Kneser rank of $G$ and denoted by $f_{\rm Kneser}(G)$. As an application of a result of Frieze and Reed concerning the clique cover number of random graphs we show that for constant $0< p< 1$ there exist constants $c_i=c_i(p)>0$, $i=1,2$ such that with high probability \[ c_1 n/(\log n)< f_{\rm Kneser}(G) < c_2 n/(\log n). \] We apply this for other graph representations defined by Boros, Gurvich and Meshulam. A {\em $k$-min-difference representation} of a graph $G$ is an assignment of a set $A_i$ to each vertex $i\in V(G)$ such that \[ ij\in E(G) \,\, \Leftrightarrow \, \, \min \{|A_i\setminus A_j|,|A_j\setminus A_i| \}\geq k. \] The smallest $k$ such that there exists a $k$-min-difference representation of $G$ is denoted by $f_{\min}(G)$. Balogh and Prince proved in 2009 that for every $k$ there is a graph $G$ with $f_{\min}(G)\geq k$. We prove that there are constants $c''_1, c''_2>0$ such that $c''_1 n/(\log n)< f_{\min}(G) < c''_2n/(\log n)$ holds for almost all bipartite graphs $G$ on $n+n$ vertices.	0,0,1,0,0,0
Crystal and Magnetic Structures in Layered, Transition Metal Dihalides and Trihalides	Materials composed of two dimensional layers bonded to one another through weak van der Waals interactions often exhibit strongly anisotropic behaviors and can be cleaved into very thin specimens and sometimes into monolayer crystals. Interest in such materials is driven by the study of low dimensional physics and the design of functional heterostructures. Binary compounds with the compositions MX2 and MX3 where M is a metal cation and X is a halogen anion often form such structures. Magnetism can be incorporated by choosing a transition metal with a partially filled d-shell for M, enabling ferroic responses for enhanced functionality. Here a brief overview of binary transition metal dihalides and trihalides is given, summarizing their crystallographic properties and long-range-ordered magnetic structures, focusing on those materials with layered crystal structures and partially filled d-shells required for combining low dimensionality and cleavability with magnetism.	0,1,0,0,0,0
When Do Birds of a Feather Flock Together? k-Means, Proximity, and Conic Programming	Given a set of data, one central goal is to group them into clusters based on some notion of similarity between the individual objects. One of the most popular and widely-used approaches is k-means despite the computational hardness to find its global minimum. We study and compare the properties of different convex relaxations by relating them to corresponding proximity conditions, an idea originally introduced by Kumar and Kannan. Using conic duality theory, we present an improved proximity condition under which the Peng-Wei relaxation of k-means recovers the underlying clusters exactly. Our proximity condition improves upon Kumar and Kannan, and is comparable to that of Awashti and Sheffet where proximity conditions are established for projective k-means. In addition, we provide a necessary proximity condition for the exactness of the Peng-Wei relaxation. For the special case of equal cluster sizes, we establish a different and completely localized proximity condition under which the Amini-Levina relaxation yields exact clustering, thereby having addressed an open problem by Awasthi and Sheffet in the balanced case. Our framework is not only deterministic and model-free but also comes with a clear geometric meaning which allows for further analysis and generalization. Moreover, it can be conveniently applied to analyzing various data generative models such as the stochastic ball models and Gaussian mixture models. With this method, we improve the current minimum separation bound for the stochastic ball models and achieve the state-of-the-art results of learning Gaussian mixture models.	0,0,1,0,0,0
Non Relativistic Limit of Integrable QFT with fermionic excitations	The aim of this paper is to investigate the non-relativistic limit of integrable quantum field theories with fermionic fields, such as the O(N) Gross-Neveu model, the supersymmetric Sinh-Gordon and non-linear sigma models. The non-relativistic limit of these theories is implemented by a double scaling limit which consists of sending the speed of light c to infinity and rescaling at the same time the relevant coupling constant of the model in such a way to have finite energy excitations. For the general purpose of mapping the space of continuous non-relativistic integrable models, this paper completes and integrates the analysis done in Ref.[1] on the non-relativistic limit of purely bosonic theories.	0,1,0,0,0,0
Collective spin excitations of helices and magnetic skyrmions: review and perspectives of magnonics in non-centrosymmetric magnets	Magnetic materials hosting correlated electrons play an important role for information technology and signal processing. The currently used ferro-, ferri- and antiferromagnetic materials provide microscopic moments (spins) that are mainly collinear. Recently more complex spin structures such as spin helices and cycloids have regained a lot of interest. The interest has been initiated by the discovery of the skyrmion lattice phase in non-centrosymmetric helical magnets. In this review we address how spin helices and skyrmion lattices enrich the microwave characteristics of magnetic materials. When discussing perspectives for microwave electronics and magnonics we focus particularly on insulating materials as they avoid eddy current losses, offer low spin-wave damping, and might allow for electric field control of collective spin excitations. Thereby, they further fuel the vision of magnonics operated at low energy consumption.	0,1,0,0,0,0
Diversity of Abundance Patterns of Light Neutron-capture Elements in Very-metal-poor Stars	We determine the abundances of neutron-capture elements from Sr to Eu for five very-metal-poor stars (-3<[Fe/H]<-2) in the Milky Way halo to reveal the origin of light neutron-capture elements. Previous spectroscopic studies have shown evidence of at least two components in the r-process; one referred to as the "main r-process" and the other as the "weak r-process," which is mainly responsible for producing heavy and light neutron-capture elements, respectively. Observational studies of metal-poor stars suggest that there is a universal pattern in the main r-process, similar to the abundance pattern of the r-process component of solar-system material. Still, it is uncertain whether the abundance pattern of the weak r-process shows universality or diversity, due to the sparseness of measured light neutron-capture elements. We have detected the key elements, Mo, Ru, and Pd, in five target stars to give an answer to this question. The abundance patterns of light neutron-capture elements from Sr to Pd suggest a diversity in the weak r-process. In particular, scatter in the abundance ratio between Ru and Pd is significant when the abundance patterns are normalized at Zr. Our results are compared with the elemental abundances predicted by nucleosynthesis models of supernovae with parameters such as electron fraction or proto-neutron-star mass, to investigate sources of such diversity in the abundance patterns of light neutron-capture elements. This paper presents that the variation in the abundances of observed stars can be explained with a small range of parameters, which can serve as constraints on future modeling of supernova models.	0,1,0,0,0,0
Confidence Interval Estimators for MOS Values	For the quantification of QoE, subjects often provide individual rating scores on certain rating scales which are then aggregated into Mean Opinion Scores (MOS). From the observed sample data, the expected value is to be estimated. While the sample average only provides a point estimator, confidence intervals (CI) are an interval estimate which contains the desired expected value with a given confidence level. In subjective studies, the number of subjects performing the test is typically small, especially in lab environments. The used rating scales are bounded and often discrete like the 5-point ACR rating scale. Therefore, we review statistical approaches in the literature for their applicability in the QoE domain for MOS interval estimation (instead of having only a point estimator, which is the MOS). We provide a conservative estimator based on the SOS hypothesis and binomial distributions and compare its performance (CI width, outlier ratio of CI violating the rating scale bounds) and coverage probability with well known CI estimators. We show that the provided CI estimator works very well in practice for MOS interval estimators, while the commonly used studentized CIs suffer from a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale. As an alternative, bootstrapping, i.e., random sampling of the subjective ratings with replacement, is an efficient CI estimator leading to typically smaller CIs, but lower coverage than the proposed estimator.	1,0,0,0,0,0
Fast and Accurate 3D Medical Image Segmentation with Data-swapping Method	Deep neural network models used for medical image segmentation are large because they are trained with high-resolution three-dimensional (3D) images. Graphics processing units (GPUs) are widely used to accelerate the trainings. However, the memory on a GPU is not large enough to train the models. A popular approach to tackling this problem is patch-based method, which divides a large image into small patches and trains the models with these small patches. However, this method would degrade the segmentation quality if a target object spans multiple patches. In this paper, we propose a novel approach for 3D medical image segmentation that utilizes the data-swapping, which swaps out intermediate data from GPU memory to CPU memory to enlarge the effective GPU memory size, for training high-resolution 3D medical images without patching. We carefully tuned parameters in the data-swapping method to obtain the best training performance for 3D U-Net, a widely used deep neural network model for medical image segmentation. We applied our tuning to train 3D U-Net with full-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result, communication overhead, which is the most important issue, was reduced by 17.1%. Compared with the patch-based method for patches of 128 x 128 x 128 voxels, our training for full-size images achieved improvement on the mean Dice score by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core sub-region, respectively. The total training time was reduced from 164 hours to 47 hours, resulting in 3.53 times of acceleration.	1,0,0,0,0,0
On the Mechanism of Large Amplitude Flapping of Inverted Foil in a Uniform Flow	An elastic foil interacting with a uniform flow with its trailing edge clamped, also known as the inverted foil, exhibits a wide range of complex self-induced flapping regimes such as large amplitude flapping (LAF), deformed and flipped flapping. Here, we perform three-dimensional numerical experiments to examine the role of vortex shedding and the vortex-vortex interaction on the LAF response at Reynolds number Re=30,000. Here we investigate the dynamics of the inverted foil for a novel configuration wherein we introduce a fixed splitter plate at the trailing edge to suppress the vortex shedding from trailing edge and inhibit the interaction between the counter-rotating vortices. We find that the inhibition of the interaction has an insignificant effect on the transverse flapping amplitudes, due to a relatively weaker coupling between the counter-rotating vortices emanating from the leading edge and trailing edge. However, the inhibition of the trailing edge vortex reduces the streamwise flapping amplitude, the flapping frequency and the net strain energy of foil. To further generalize our understanding of the LAF, we next perform low-Reynolds number (Re$\in[0.1,50]$) simulations for the identical foil properties to realize the impact of vortex shedding on the large amplitude flapping. Due to the absence of vortex shedding process in the low-$Re$ regime, the inverted foil no longer exhibits the periodic flapping. However, the flexible foil still loses its stability through divergence instability to undergo a large static deformation. Finally, we introduce an analogous analytical model for the LAF based on the dynamics of an elastically mounted flat plate undergoing flow-induced pitching oscillations in a uniform stream.	0,1,0,0,0,0
Detecting Changes in Hidden Markov Models	We consider the problem of sequential detection of a change in the statistical behavior of a hidden Markov model. By adopting a worst-case analysis with respect to the time of change and by taking into account the data that can be accessed by the change-imposing mechanism we offer alternative formulations of the problem. For each formulation we derive the optimum Shewhart test that maximizes the worst-case detection probability while guaranteeing infrequent false alarms.	0,0,1,1,0,0
Beam Based RF Voltage Measurements and Longitudinal Beam Tomography at the Fermilab Booster	Increasing proton beam power on neutrino production targets is one of the major goals of the Fermilab long term accelerator programs. In this effort, the Fermilab 8 GeV Booster synchrotron plays a critical role for at least the next two decades. Therefore, understanding the Booster in great detail is important as we continue to improve its performance. For example, it is important to know accurately the available RF power in the Booster by carrying out beam-based measurements in order to specify the needed upgrades to the Booster RF system. Since the Booster magnetic field is changing continuously measuring/calibrating the RF voltage is not a trivial task. Here, we present a beam based method for the RF voltage measurements. Data analysis is carried out using computer programs developed in Python and MATLAB. The method presented here is applicable to any RCS which do not have flat-bottom and flat-top in the acceleration magnetic ramps. We have also carried out longitudinal beam tomography at injection and extraction energies with the data used for RF voltage measurements. Beam based RF voltage measurements and beam tomography were never done before for the Fermilab Booster. The results from these investigations will be very useful in future intensity upgrades.	0,1,0,0,0,0
Non-Ergodic Delocalization in the Rosenzweig-Porter Model	We consider the Rosenzweig-Porter model $H = V + \sqrt{T}\, \Phi$, where $V$ is a $N \times N$ diagonal matrix, $\Phi$ is drawn from the $N \times N$ Gaussian Orthogonal Ensemble, and $N^{-1} \ll T \ll 1$. We prove that the eigenfunctions of $H$ are typically supported in a set of approximately $NT$ sites, thereby confirming the existence of a previously conjectured non-ergodic delocalized phase. Our proof is based on martingale estimates along the characteristic curves of the stochastic advection equation satisfied by the local resolvent of the Brownian motion representation of $H$.	0,1,0,0,0,0
Quantum repeaters with individual rare-earth ions at telecommunication wavelengths	We present a quantum repeater scheme that is based on individual erbium and europium ions. Erbium ions are attractive because they emit photons at telecommunication wavelength, while europium ions offer exceptional spin coherence for long-term storage. Entanglement between distant erbium ions is created by photon detection. The photon emission rate of each erbium ion is enhanced by a microcavity with high Purcell factor, as has recently been demonstrated. Entanglement is then transferred to nearby europium ions for storage. Gate operations between nearby ions are performed using dynamically controlled electric-dipole coupling. These gate operations allow entanglement swapping to be employed in order to extend the distance over which entanglement is distributed. The deterministic character of the gate operations allows improved entanglement distribution rates in comparison to atomic ensemble-based protocols. We also propose an approach that utilizes multiplexing in order to enhance the entanglement distribution rate.	0,1,0,0,0,0
Polishness of some topologies related to word or tree automata	We prove that the Büchi topology and the automatic topology are Polish. We also show that this cannot be fully extended to the case of a space of infinite labelled binary trees; in particular the Büchi and the Muller topologies are not Polish in this case.	1,0,1,0,0,0
The Junk News Aggregator: Examining junk news posted on Facebook, starting with the 2018 US Midterm Elections	In recent years, the phenomenon of online misinformation and junk news circulating on social media has come to constitute an important and widespread problem affecting public life online across the globe, particularly around important political events such as elections. At the same time, there have been calls for more transparency around misinformation on social media platforms, as many of the most popular social media platforms function as "walled gardens," where it is impossible for researchers and the public to readily examine the scale and nature of misinformation activity as it is unfolding on the platforms. In order to help address this, this paper, we present the Junk News Aggregator, an interactive web tool made publicly available, which allows the public to examine, in near real-time, all of the public content posted to Facebook by important junk news sources in the US. It allows the public to gain access to and examine the latest articles posted on Facebook (the most popular social media platform in the US and one where content is not readily accessible at scale from the open Web), as well as organise them by time, news publisher, and keywords of interest, and sort them based on all eight engagement metrics available on Facebook. Therefore, the Aggregator allows the public to gain insights on the volume, content, key themes, and types and volumes of engagement received by content posted by junk news publishers, in near real time, hence opening up and offering transparency in these activities, at scale across the top most popular junk news publishers and in near real time. In this way, the Aggregator can help increase transparency around the nature, volume, and engagement with junk news on social media, and serve as a media literacy tool for the public.	1,0,0,0,0,0
Volatile memory forensics for the Robot Operating System	The increasing impact of robotics on industry and on society will unavoidably lead to the involvement of robots in incidents and mishaps. In such cases, forensic analyses are key techniques to provide useful evidence on what happened, and try to prevent future incidents. This article discusses volatile memory forensics for the Robot Operating System (ROS). The authors start by providing a general overview of forensic techniques in robotics and then present a robotics-specific Volatility plugin named linux_rosnode, packaged within the ros_volatility project and aimed to extract evidence from robot's volatile memory. They demonstrate how this plugin can be used to detect a specific attack pattern on ROS, where a publisher node is unregistered externally, leading to denial of service and disruption of robotic behaviors. Step-by-step, common practices are introduced for performing forensic analysis and several techniques to capture memory are described. The authors finalize by introducing some future remarks while providing references to reproduce their work.	1,0,0,0,0,0
Experimental and Theoretical Study of Magnetohydrodynamic Ship Models	Magnetohydrodynamic (MHD) ships represent a clear demonstration of the Lorentz force in fluids, which explains the number of students practicals or exercises described on the web. However, the related literature is rather specific and no complete comparison between theory and typical small scale experiments is currently available. This work provides, in a self-consistent framework, a detailed presentation of the relevant theoretical equations for small MHD ships and experimental measurements for future benchmarks. Theoretical results of the literature are adapted to these simple battery/magnets powered ships moving on salt water. Comparison between theory and experiments are performed to validate each theoretical step such as the Tafel and the Kohlrausch laws, or the predicted ship speed. A successful agreement is obtained without any adjustable parameter. Finally, based on these results, an optimal design is then deduced from the theory. Therefore this work provides a solid theoretical and experimental ground for small scale MHD ships, by presenting in detail several approximations and how they affect the boat efficiency. Moreover, the theory is general enough to be adapted to other contexts, such as large scale ships or industrial flow measurement techniques.	0,1,0,0,0,0
Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms	This paper investigates asymptotic behaviors of gradient descent algorithms (particularly accelerated gradient descent and stochastic gradient descent) in the context of stochastic optimization arose in statistics and machine learning where objective functions are estimated from available data. We show that these algorithms can be modeled by continuous-time ordinary or stochastic differential equations, and their asymptotic dynamic evolutions and distributions are governed by some linear ordinary or stochastic differential equations, as the data size goes to infinity. We illustrate that our study can provide a novel unified framework for a joint computational and statistical asymptotic analysis on dynamic behaviors of these algorithms with the time (or the number of iterations in the algorithms) and large sample behaviors of the statistical decision rules (like estimators and classifiers) that the algorithms are applied to compute, where the statistical decision rules are the limits of the random sequences generated from these iterative algorithms as the number of iterations goes to infinity. The analysis results may shed light on the empirically observed phenomenon of escaping from saddle points, avoiding bad local minimizers, and converging to good local minimizers, which depends on local geometry, learning rate and batch size, when stochastic gradient descent algorithms are applied to solve non-convex optimization problems.	0,0,0,1,0,0
Poincaré profiles of groups and spaces	We introduce a spectrum of monotone coarse invariants for metric measure spaces called Poincaré profiles. The two extremes of this spectrum determine the growth of the space, and the separation profile as defined by Benjamini--Schramm--Timár. In this paper we focus on properties of the Poincaré profiles of groups with polynomial growth, and of hyperbolic spaces, where we deduce a striking connection between these profiles and conformal dimension. One application of our results is that there is a collection of hyperbolic Coxeter groups, indexed by a countable dense subset of $(1,\infty)$, such that $G_s$ does not coarsely embed into $G_t$ whenever $s<t$.	0,0,1,0,0,0
Local Hardy spaces with variable exponents associated to non-negative self-adjoint operators satisfying Gaussian estimates	In this paper we introduce variable exponent local Hardy spaces associated with a non-negative self-adjoint operator L. We define them by using an area square integral involving the heat semigroup associated to L. A molecular characterization is established and as an aplication of the molecular characterization we prove that our local Hardy space coincides with the (global) variable exponent Hardy space associated to L, provided that 0 does not belong to the spectrum of L. Also, we show that it coincides with the global variable exponent Hardy space associated to L+I.	0,0,1,0,0,0
Possible spin excitation structure in monolayer FeSe grown on SrTiO$_{3}$	Based on recent high-resolution angle-resolved photoemission spectroscopy measurement in monolayer FeSe grown on SrTiO$_{3}$, we constructed a tight-binding model and proposed a superconducting (SC) pairing function which can well fit the observed band structure and SC gap anisotropy. Then we investigated the spin excitation spectra in order to determine the possible sign structure of the SC order parameter. We found that a resonance-like spin excitation may occur if the SC order parameter changes sign along the Fermi surfaces. However, this resonance is located at different locations in momentum space compared to other FeSe-based superconductors, suggesting that the Fermi surface shape and pairing symmetry in monolayer FeSe grown on SrTiO$_{3}$ may be different from other FeSe-based superconductors.	0,1,0,0,0,0
Cross-Sectional Variation of Intraday Liquidity, Cross-Impact, and their Effect on Portfolio Execution	The composition of natural liquidity has been changing over time. An analysis of intraday volumes for the S&P500 constituent stocks illustrates that (i) volume surprises, i.e., deviations from their respective forecasts, are correlated across stocks, and (ii) this correlation increases during the last few hours of the trading session. These observations could be attributed, in part, to the prevalence of portfolio trading activity that is implicit in the growth of ETF, passive and systematic investment strategies; and, to the increased trading intensity of such strategies towards the end of the trading session, e.g., due to execution of mutual fund inflows/outflows that are benchmarked to the closing price on each day. In this paper, we investigate the consequences of such portfolio liquidity on price impact and portfolio execution. We derive a linear cross-asset market impact from a stylized model that explicitly captures the fact that a certain fraction of natural liquidity providers only trade portfolios of stocks whenever they choose to execute. We find that due to cross-impact and its intraday variation, it is optimal for a risk-neutral, cost minimizing liquidator to execute a portfolio of orders in a coupled manner, as opposed to a separable VWAP-like execution that is often assumed. The optimal schedule couples the execution of the various orders so as to be able to take advantage of increased portfolio liquidity towards the end of the day. A worst case analysis shows that the potential cost reduction from this optimized execution schedule over the separable approach can be as high as 6% for plausible model parameters. Finally, we discuss how to estimate cross-sectional price impact if one had a dataset of realized portfolio transaction records that exploits the low-rank structure of its coefficient matrix suggested by our analysis.	0,0,0,0,0,1
Learning latent representations for style control and transfer in end-to-end speech synthesis	In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.	1,0,0,0,0,0
Ramsey properties and extending partial automorphisms for classes of finite structures	We show that every free amalgamation class of finite structures with relations and (symmetric) partial functions is a Ramsey class when enriched by a free linear ordering of vertices. This is a common strengthening of the Nešetřil-Rödl Theorem and the second and third authors' Ramsey theorem for finite models (that is, structures with both relations and functions). We also find subclasses with the ordering property. For languages with relational symbols and unary functions we also show the extension property for partial automorphisms (EPPA) of free amalgamation classes. These general results solve several conjectures and provide an easy Ramseyness test for many classes of structures.	1,0,1,0,0,0
Fast Amortized Inference and Learning in Log-linear Models with Randomly Perturbed Nearest Neighbor Search	Inference in log-linear models scales linearly in the size of output space in the worst-case. This is often a bottleneck in natural language processing and computer vision tasks when the output space is feasibly enumerable but very large. We propose a method to perform inference in log-linear models with sublinear amortized cost. Our idea hinges on using Gumbel random variable perturbations and a pre-computed Maximum Inner Product Search data structure to access the most-likely elements in sublinear amortized time. Our method yields provable runtime and accuracy guarantees. Further, we present empirical experiments on ImageNet and Word Embeddings showing significant speedups for sampling, inference, and learning in log-linear models.	1,0,0,1,0,0
Privacy-Preserving Deep Inference for Rich User Data on The Cloud	Deep neural networks are increasingly being used in a variety of machine learning applications applied to rich user data on the cloud. However, this approach introduces a number of privacy and efficiency challenges, as the cloud operator can perform secondary inferences on the available data. Recently, advances in edge processing have paved the way for more efficient, and private, data processing at the source for simple tasks and lighter models, though they remain a challenge for larger, and more complicated models. In this paper, we present a hybrid approach for breaking down large, complex deep models for cooperative, privacy-preserving analytics. We do this by breaking down the popular deep architectures and fine-tune them in a particular way. We then evaluate the privacy benefits of this approach based on the information exposed to the cloud service. We also asses the local inference cost of different layers on a modern handset for mobile applications. Our evaluations show that by using certain kind of fine-tuning and embedding techniques and at a small processing costs, we can greatly reduce the level of information available to unintended tasks applied to the data feature on the cloud, and hence achieving the desired tradeoff between privacy and performance.	1,0,0,0,0,0
Increased stability of CuZrAl metallic glasses prepared by physical vapor deposition	We carried out molecular dynamics simulations (MD) using realistic empirical potentials for the vapor deposition (VD) of CuZrAl glasses. VD glasses have higher densities and lower potential and inherent structure energies than the melt-quenched glasses for the same alloys. The optimal substrate temperature for the deposition process is 0.625$\times T_\mathrm{g}$. In VD metallic glasses (MGs), the total number of icosahedral like clusters is higher than in the melt-quenched MGs. Surprisingly, the VD glasses have a lower degree of chemical mixing than the melt-quenched glasses. The reason for it is that the melt-quenched MGs can be viewed as frozen liquids, which means that their chemical order is the same as in the liquid state. In contrast, during the formation of the VD MGs, the absence of the liquid state results in the creation of a different chemical order with more Zr-Zr homonuclear bonds compared with the melt-quenched MGs. In order to obtain MGs from melt-quench technique with similarly low energies as in the VD process, the cooling rate during quenching would have to be many orders of magnitude lower than currently accessible to MD simulations. The method proposed in this manuscript is a more efficient way to create MGs by using MD simulations.	0,1,0,0,0,0
Complete algebraic solution of multidimensional optimization problems in tropical semifield	We consider multidimensional optimization problems that are formulated in the framework of tropical mathematics to minimize functions defined on vectors over a tropical semifield (a semiring with idempotent addition and invertible multiplication). The functions, given by a matrix and calculated through multiplicative conjugate transposition, are nonlinear in the tropical mathematics sense. We start with known results on the solution of the problems with irreducible matrices. To solve the problems in the case of arbitrary (reducible) matrices, we first derive the minimum value of the objective function, and find a set of solutions. We show that all solutions of the problem satisfy a system of vector inequalities, and then use these inequalities to establish characteristic properties of the solution set. Furthermore, all solutions of the problem are represented as a family of subsets, each defined by a matrix that is obtained by using a matrix sparsification technique. We describe a backtracking procedure that allows one to reduce the brute-force generation of sparsified matrices by skipping those, which cannot provide solutions, and thus offers an economical way to obtain all subsets in the family. Finally, the characteristic properties of the solution set are used to provide complete solutions in a closed form. We illustrate the results obtained with simple numerical examples.	1,0,1,0,0,0
Estimating Graphlet Statistics via Lifting	Exploratory analysis over network data is often limited by our ability to efficiently calculate graph statistics, which can provide a model-free understanding of macroscopic properties of a network. This work introduces a framework for estimating the graphlet count - the number of occurrences of a small subgraph motif (e.g. a wedge or a triangle) in the network. For massive graphs, where accessing the whole graph is not possible, the only viable algorithms are those which act locally by making a limited number of vertex neighborhood queries. We introduce a Monte Carlo sampling technique for graphlet counts, called lifting, which can simultaneously sample all graphlets of size up to $k$ vertices. We outline three variants of lifted graphlet counts: the ordered, unordered, and shotgun estimators. We prove that our graphlet count updates are unbiased for the true graphlet count, have low correlation between samples, and have a controlled variance. We compare the experimental performance of lifted graphlet counts to the state-of-the art graphlet sampling procedures: Waddling and the pairwise subgraph random walk.	0,0,0,1,0,0
Cellular function given parametric variation: excitability in the Hodgkin-Huxley model	How is reliable physiological function maintained in cells despite considerable variability in the values of key parameters of multiple interacting processes that govern that function? Here we use the classic Hodgkin-Huxley formulation of the squid giant axon action potential to propose a possible approach to this problem. Although the full Hodgkin-Huxley model is very sensitive to fluctuations that independently occur in its many parameters, the outcome is in fact determined by simple combinations of these parameters along two physiological dimensions: Structural and Kinetic (denoted $S$ and $K$). Structural parameters describe the properties of the cell, including its capacitance and the densities of its ion channels. Kinetic parameters are those that describe the opening and closing of the voltage-dependent conductances. The impacts of parametric fluctuations on the dynamics of the system, seemingly complex in the high dimensional representation of the Hodgkin-Huxley model, are tractable when examined within the $S-K$ plane. We demonstrate that slow inactivation, a ubiquitous activity-dependent feature of ionic channels, is a powerful local homeostatic control mechanism that stabilizes excitability amid changes in structural and kinetic parameters.	0,0,0,0,1,0
Comparing anticyclotomic Selmer groups of positive coranks for congruent modular forms	We study the variation of Iwasawa invariants of the anticyclotomic Selmer groups of congruent modular forms under the Heegner hypothesis. In particular, we show that even if the Selmer groups we study may have positive coranks, the mu-invariant vanishes for one modular form if and only if it vanishes for the other, and that their lambda-invariants are related by an explicit formula. This generalizes results of Greenberg-Vatsal for the cyclotomic extension, as well as results of Pollack-Weston and Castella-Kim-Longo for the anticyclotomic extension when the Selmer groups in question are cotorsion.	0,0,1,0,0,0
Exploring nucleon spin structure through neutrino neutral-current interactions in MicroBooNE	The net contribution of the strange quark spins to the proton spin, $\Delta s$, can be determined from neutral current elastic neutrino-proton interactions at low momentum transfer combined with data from electron-proton scattering. The probability of neutrino-proton interactions depends in part on the axial form factor, which represents the spin structure of the proton and can be separated into its quark flavor contributions. Low momentum transfer neutrino neutral current interactions can be measured in MicroBooNE, a high-resolution liquid argon time projection chamber (LArTPC) in its first year of running in the Booster Neutrino Beamline at Fermilab. The signal for these interactions in MicroBooNE is a single short proton track. We present our work on the automated reconstruction and classification of proton tracks in LArTPCs, an important step in the determination of neutrino- nucleon cross sections and the measurement of $\Delta s$.	0,1,0,0,0,0
Variance-Reduced Stochastic Learning by Networked Agents under Random Reshuffling	A new amortized variance-reduced gradient (AVRG) algorithm was developed in \cite{ying2017convergence}, which has constant storage requirement in comparison to SAGA and balanced gradient computations in comparison to SVRG. One key advantage of the AVRG strategy is its amenability to decentralized implementations. In this work, we show how AVRG can be extended to the network case where multiple learning agents are assumed to be connected by a graph topology. In this scenario, each agent observes data that is spatially distributed and all agents are only allowed to communicate with direct neighbors. Moreover, the amount of data observed by the individual agents may differ drastically. For such situations, the balanced gradient computation property of AVRG becomes a real advantage in reducing idle time caused by unbalanced local data storage requirements, which is characteristic of other reduced-variance gradient algorithms. The resulting diffusion-AVRG algorithm is shown to have linear convergence to the exact solution, and is much more memory efficient than other alternative algorithms. In addition, we propose a mini-batch strategy to balance the communication and computation efficiency for diffusion-AVRG. When a proper batch size is employed, it is observed in simulations that diffusion-AVRG is more computationally efficient than exact diffusion or EXTRA while maintaining almost the same communication efficiency.	1,0,0,1,0,0
The search for neutron-antineutron oscillations at the Sudbury Neutrino Observatory	Tests on $B-L$ symmetry breaking models are important probes to search for new physics. One proposed model with $\Delta(B-L)=2$ involves the oscillations of a neutron to an antineutron. In this paper a new limit on this process is derived for the data acquired from all three operational phases of the Sudbury Neutrino Observatory experiment. The search was concentrated in oscillations occurring within the deuteron, and 23 events are observed against a background expectation of 30.5 events. These translate to a lower limit on the nuclear lifetime of $1.48\times 10^{31}$ years at 90% confidence level (CL) when no restriction is placed on the signal likelihood space (unbounded). Alternatively, a lower limit on the nuclear lifetime was found to be $1.18\times 10^{31}$ years at 90% CL when the signal was forced into a positive likelihood space (bounded). Values for the free oscillation time derived from various models are also provided in this article. This is the first search for neutron-antineutron oscillation with the deuteron as a target.	0,1,0,0,0,0
Fractional Brownian markets with time-varying volatility and high-frequency data	Diffusion processes driven by Fractional Brownian motion (FBM) have often been considered in modeling stock price dynamics in order to capture the long range dependence of stock price observed in reality. Option prices for such models had been obtained by Necula (2002) under constant drift and volatility. We obtain option prices under time varying volatility model. The expression depends on volatility and the Hurst parameter in a complicated manner. We derive a central limit theorem for the quadratic variation as an estimator for volatility for both the cases, constant as well as time varying volatility. That will help us to find estimators of the option prices and to find their asymptotic distributions.	0,0,1,1,0,0
Probabilistic risk bounds for the characterization of radiological contamination	The radiological characterization of contaminated elements (walls, grounds, objects) from nuclear facilities often suffers from a too small number of measurements. In order to determine risk prediction bounds on the level of contamination, some classic statistical methods may then reveal unsuited as they rely upon strong assumptions (e.g. that the underlying distribution is Gaussian) which cannot be checked. Considering that a set of measurements or their average value arise from a Gaussian distribution can sometimes lead to erroneous conclusion, possibly underconservative. This paper presents several alternative statistical approaches which are based on much weaker hypotheses than Gaussianity. They result from general probabilistic inequalities and order-statistics based formula. Given a data sample, these inequalities make it possible to derive prediction intervals for a random variable, which can be directly interpreted as probabilistic risk bounds. For the sake of validation, they are first applied to synthetic data samples generated from several known theoretical distributions. In a second time, the proposed methods are applied to two data sets obtained from real radiological contamination measurements.	0,0,1,1,0,0
A self-consistent cloud model for brown dwarfs and young giant exoplanets: comparison with photometric and spectroscopic observations	We developed a simple, physical and self-consistent cloud model for brown dwarfs and young giant exoplanets. We compared different parametrisations for the cloud particle size, by either fixing particle radii, or fixing the mixing efficiency (parameter fsed) or estimating particle radii from simple microphysics. The cloud scheme with simple microphysics appears as the best parametrisation by successfully reproducing the observed photometry and spectra of brown dwarfs and young giant exoplanets. In particular, it reproduces the L-T transition, due to the condensation of silicate and iron clouds below the visible/near-IR photosphere. It also reproduces the reddening observed for low-gravity objects, due to an increase of cloud optical depth for low gravity. In addition, we found that the cloud greenhouse effect shifts chemical equilibriums, increasing the abundances of species stable at high temperature. This effect should significantly contribute to the strong variation of methane abundance at the L-T transition and to the methane depletion observed on young exoplanets. Finally, we predict the existence of a continuum of brown dwarfs and exoplanets for absolute J magnitude=15-18 and J-K color=0-3, due to the evolution of the L-T transition with gravity. This self-consistent model therefore provides a general framework to understand the effects of clouds and appears well-suited for atmospheric retrievals.	0,1,0,0,0,0
Anomalous dynamical phase in quantum spin chains with long-range interactions	The existence or absence of non-analytic cusps in the Loschmidt-echo return rate is traditionally employed to distinguish between a regular dynamical phase (regular cusps) and a trivial phase (no cusps) in quantum spin chains after a global quench. However, numerical evidence in a recent study [J. C. Halimeh and V. Zauner-Stauber, arXiv:1610.02019] suggests that instead of the trivial phase a distinct anomalous dynamical phase characterized by a novel type of non-analytic cusps occurs in the one-dimensional transverse-field Ising model when interactions are sufficiently long-range. Using an analytic semiclassical approach and exact diagonalization, we show that this anomalous phase also arises in the fully-connected case of infinite-range interactions, and we discuss its defining signature. Our results show that the transition from the regular to the anomalous dynamical phase coincides with Z2-symmetry breaking in the infinite-time limit, thereby showing a connection between two different concepts of dynamical criticality. Our work further expands the dynamical phase diagram of long-range interacting quantum spin chains, and can be tested experimentally in ion-trap setups and ultracold atoms in optical cavities, where interactions are inherently long-range.	0,1,0,0,0,0
RAIL: Risk-Averse Imitation Learning	Imitation learning algorithms learn viable policies by imitating an expert's behavior when reward signals are not available. Generative Adversarial Imitation Learning (GAIL) is a state-of-the-art algorithm for learning policies when the expert's behavior is available as a fixed set of trajectories. We evaluate in terms of the expert's cost function and observe that the distribution of trajectory-costs is often more heavy-tailed for GAIL-agents than the expert at a number of benchmark continuous-control tasks. Thus, high-cost trajectories, corresponding to tail-end events of catastrophic failure, are more likely to be encountered by the GAIL-agents than the expert. This makes the reliability of GAIL-agents questionable when it comes to deployment in risk-sensitive applications like robotic surgery and autonomous driving. In this work, we aim to minimize the occurrence of tail-end events by minimizing tail risk within the GAIL framework. We quantify tail risk by the Conditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse Imitation Learning (RAIL) algorithm. We observe that the policies learned with RAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed RAIL algorithm appears as a potent alternative to GAIL for improved reliability in risk-sensitive applications.	1,0,0,0,0,0
IoT Data Analytics Using Deep Learning	Deep learning is a popular machine learning approach which has achieved a lot of progress in all traditional machine learning areas. Internet of thing (IoT) and Smart City deployments are generating large amounts of time-series sensor data in need of analysis. Applying deep learning to these domains has been an important topic of research. The Long-Short Term Memory (LSTM) network has been proven to be well suited for dealing with and predicting important events with long intervals and delays in the time series. LTSM networks have the ability to maintain long-term memory. In an LTSM network, a stacked LSTM hidden layer also makes it possible to learn a high level temporal feature without the need of any fine tuning and preprocessing which would be required by other techniques. In this paper, we construct a long-short term memory (LSTM) recurrent neural network structure, use the normal time series training set to build the prediction model. And then we use the predicted error from the prediction model to construct a Gaussian naive Bayes model to detect whether the original sample is abnormal. This method is called LSTM-Gauss-NBayes for short. We use three real-world data sets, each of which involve long-term time-dependence or short-term time-dependence, even very weak time dependence. The experimental results show that LSTM-Gauss-NBayes is an effective and robust model.	1,0,0,0,0,0
Accurate Single Stage Detector Using Recurrent Rolling Convolution	Most of the recent successful methods in accurate object detection and localization used some variants of R-CNN style two stage Convolutional Neural Networks (CNN) where plausible regions were proposed in the first stage then followed by a second stage for decision refinement. Despite the simplicity of training and the efficiency in deployment, the single stage detection methods have not been as competitive when evaluated in benchmarks consider mAP for high IoU thresholds. In this paper, we proposed a novel single stage end-to-end trainable object detection network to overcome this limitation. We achieved this by introducing Recurrent Rolling Convolution (RRC) architecture over multi-scale feature maps to construct object classifiers and bounding box regressors which are "deep in context". We evaluated our method in the challenging KITTI dataset which measures methods under IoU threshold of 0.7. We showed that with RRC, a single reduced VGG-16 based model already significantly outperformed all the previously published results. At the time this paper was written our models ranked the first in KITTI car detection (the hard level), the first in cyclist detection and the second in pedestrian detection. These results were not reached by the previous single stage methods. The code is publicly available.	1,0,0,0,0,0
Incorporation of prior knowledge of the signal behavior into the reconstruction to accelerate the acquisition of MR diffusion data	Diffusion MRI measurements using hyperpolarized gases are generally acquired during patient breath hold, which yields a compromise between achievable image resolution, lung coverage and number of b-values. In this work, we propose a novel method that accelerates the acquisition of MR diffusion data by undersampling in both spatial and b-value dimensions, thanks to incorporating knowledge about the signal decay into the reconstruction (SIDER). SIDER is compared to total variation (TV) reconstruction by assessing their effect on both the recovery of ventilation images and estimated mean alveolar dimensions (MAD). Both methods are assessed by retrospectively undersampling diffusion datasets of normal volunteers and COPD patients (n=8) for acceleration factors between x2 and x10. TV led to large errors and artefacts for acceleration factors equal or larger than x5. SIDER improved TV, presenting lower errors and histograms of MAD closer to those obtained from fully sampled data for accelerations factors up to x10. SIDER preserved image quality at all acceleration factors but images were slightly smoothed and some details were lost at x10. In conclusion, we have developed and validated a novel compressed sensing method for lung MRI imaging and achieved high acceleration factors, which can be used to increase the amount of data acquired during a breath-hold. This methodology is expected to improve the accuracy of estimated lung microstructure dimensions and widen the possibilities of studying lung diseases with MRI.	1,1,0,0,0,0
Thermalization in simple metals: The role of electron-phonon and phonon-phonon scatterings	We study the electron and phonon thermalization in simple metals excited by a laser pulse. The thermalization is investigated numerically by solving the Boltzmann transport equation taking into account all the relevant scattering mechanism: the electron-electron, electron-phonon (e-ph), phonon-electron (ph-e), and phonon-phonon (ph-ph) scatterings. In the initial stage of the relaxation, most of the excitation energy is transferred from the electrons to phonons through the e-ph scattering. This creates hot high-frequency phonons due to the ph-e scatterings, followed by an energy redistribution between phonon subsystems through the ph-ph scatterings. This yields an overshoot of the total longitudinal-acoustic phonon energy at a time, across which a crossover occurs from a nonequilibrium state, where the e-ph and ph-e scatterings frequently occur, to a state, where the ph-ph scattering occurs to reach a thermal equilibrium. This picture is quite different from the scenario of the well-known two-temperature model (2TM). The behavior of the relaxation dynamics is compared with those calculated by several models, including the 2TM, the four-temperature model, and nonequilibrium electron or phonon models. The relationship between the relaxation time and the initial distribution function is also discussed.	0,1,0,0,0,0
Active set algorithms for estimating shape-constrained density ratios	We review and modify the active set algorithm by Duembgen et al. (2011) for nonparametric maximum-likelihood estimation of a log-concave density. This particular estimation problem is embedded into a more general framework including also the estimation of a log-convex tail inflation function as proposed by McCullagh and Polson (2012).	0,0,0,1,0,0
Gas around galaxy haloes - III: hydrogen absorption signatures around galaxies and QSOs in the Sherwood simulation suite	Modern theories of galaxy formation predict that galaxies impact on their gaseous surroundings, playing the fundamental role of regulating the amount of gas converted into stars. While star-forming galaxies are believed to provide feedback through galactic winds, Quasi-Stellar Objects (QSOs) are believed instead to provide feedback through the heat generated by accretion onto a central supermassive black hole. A quantitative difference in the impact of feedback on the gaseous environments of star-forming galaxies and QSOs has not been established through direct observations. Using the Sherwood cosmological simulations, we demonstrate that measurements of neutral hydrogen in the vicinity of star-forming galaxies and QSOs during the era of peak galaxy formation show excess LyA absorption extending up to comoving radii of about 150 kpc for star-forming galaxies and 300 - 700 kpc for QSOs. Simulations including supernovae-driven winds with the wind velocity scaling like the escape velocity of the halo account for the absorption around star-forming galaxies but not QSOs.	0,1,0,0,0,0
Generalized two-dimensional linear discriminant analysis with regularization	Recent advances show that two-dimensional linear discriminant analysis (2DLDA) is a successful matrix based dimensionality reduction method. However, 2DLDA may encounter the singularity issue theoretically and the sensitivity to outliers. In this paper, a generalized Lp-norm 2DLDA framework with regularization for an arbitrary $p>0$ is proposed, named G2DLDA. There are mainly two contributions of G2DLDA: one is G2DLDA model uses an arbitrary Lp-norm to measure the between-class and within-class scatter, and hence a proper $p$ can be selected to achieve the robustness. The other one is that by introducing an extra regularization term, G2DLDA achieves better generalization performance, and solves the singularity problem. In addition, G2DLDA can be solved through a series of convex problems with equality constraint, and it has closed solution for each single problem. Its convergence can be guaranteed theoretically when $1\leq p\leq2$. Preliminary experimental results on three contaminated human face databases show the effectiveness of the proposed G2DLDA.	0,0,0,1,0,0
A brain signature highly predictive of future progression to Alzheimer's dementia	Early prognosis of Alzheimer's dementia is hard. Mild cognitive impairment (MCI) typically precedes Alzheimer's dementia, yet only a fraction of MCI individuals will progress to dementia, even when screened using biomarkers. We propose here to identify a subset of individuals who share a common brain signature highly predictive of oncoming dementia. This signature was composed of brain atrophy and functional dysconnectivity and discovered using a machine learning model in patients suffering from dementia. The model recognized the same brain signature in MCI individuals, 90% of which progressed to dementia within three years. This result is a marked improvement on the state-of-the-art in prognostic precision, while the brain signature still identified 47% of all MCI progressors. We thus discovered a sizable MCI subpopulation which represents an excellent recruitment target for clinical trials at the prodromal stage of Alzheimer's disease.	0,0,0,1,0,0
Joint Atlas-Mapping of Multiple Histological Series combined with Multimodal MRI of Whole Marmoset Brains	Development of a mesoscale neural circuitry map of the common marmoset is an essential task due to the ideal characteristics of the marmoset as a model organism for neuroscience research. To facilitate this development there is a need for new computational tools to cross-register multi-modal data sets containing MRI volumes as well as multiple histological series, and to register the combined data set to a common reference atlas. We present a fully automatic pipeline for same-subject-MRI guided reconstruction of image volumes from a series of histological sections of different modalities, followed by diffeomorphic mapping to a reference atlas. We show registration results for Nissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo MRI as our reference and show that our method achieves accurate registration and eliminates artifactual warping that may be result from the absence of a reference MRI data set. Examination of the determinant of the local metric tensor of the diffeomorphic mapping between each subject's ex-vivo MRI and resultant Nissl reconstruction allows an unprecedented local quantification of geometrical distortions resulting from the histological processing, showing a slight shrinkage, a median linear scale change of ~-1% in going from the ex-vivo MRI to the tape-transfer generated histological image data.	0,0,0,0,1,0
Simultaneous shot inversion for nonuniform geometries using fast data interpolation	Stochastic optimization is key to efficient inversion in PDE-constrained optimization. Using 'simultaneous shots', or random superposition of source terms, works very well in simple acquisition geometries where all sources see all receivers, but this rarely occurs in practice. We develop an approach that interpolates data to an ideal acquisition geometry while solving the inverse problem using simultaneous shots. The approach is formulated as a joint inverse problem, combining ideas from low-rank interpolation with full-waveform inversion. Results using synthetic experiments illustrate the flexibility and efficiency of the approach.	0,0,0,1,0,0
The Burst Failure Influence on the $H_\infty$ Norm	In this work, we present an analysis of the Burst failure effect in the $H_\infty$ norm. We present a procedure to perform an analysis between different Markov Chain models and a numerical example. In the numerical example the results obtained pointed out that the burst failure effect in the performance does not exceed 6.3%. However, this work is an introduction for a wider and more extensive analysis in this subject.	1,0,0,0,0,0
Dirichlet-to-Neumann or Poincaré-Steklov operator on fractals described by d -sets	In the framework of the Laplacian transport, described by a Robin boundary value problem in an exterior domain in $\mathbb{R}^n$, we generalize the definition of the Poincaré-Steklov operator to $d$-set boundaries, $n-2< d<n$, and give its spectral properties to compare to the spectra of the interior domain and also of a truncated domain, considered as an approximation of the exterior case. The well-posedness of the Robin boundary value problems for the truncated and exterior domains is given in the general framework of $n$-sets. The results are obtained thanks to a generalization of the continuity and compactness properties of the trace and extension operators in Sobolev, Lebesgue and Besov spaces, in particular, by a generalization of the classical Rellich-Kondrachov Theorem of compact embeddings for $n$ and $d$-sets.	0,0,1,0,0,0
Exclusion of GNSS NLOS Receptions Caused by Dynamic Objects in Heavy Traffic Urban Scenarios Using Real-Time 3D Point Cloud: An Approach without 3D Maps	Absolute positioning is an essential factor for the arrival of autonomous driving. Global Navigation Satellites System (GNSS) receiver provides absolute localization for it. GNSS solution can provide satisfactory positioning in open or sub-urban areas, however, its performance suffered in super-urbanized area due to the phenomenon which are well-known as multipath effects and NLOS receptions. The effects dominate GNSS positioning performance in the area. The recent proposed 3D map aided (3DMA) GNSS can mitigate most of the multipath effects and NLOS receptions caused by buildings based on 3D city models. However, the same phenomenon caused by moving objects in urban area is currently not modelled in the 3D geographic information system (GIS). Moving objects with tall height, such as the double-decker bus, can also cause NLOS receptions because of the blockage of GNSS signals by surface of objects. Therefore, we present a novel method to exclude the NLOS receptions caused by double-decker bus in highly urbanized area, Hong Kong. To estimate the geometry dimension and orientation relative to GPS receiver, a Euclidean cluster algorithm and a classification method are used to detect the double-decker buses and calculate their relative locations. To increase the accuracy and reliability of the proposed NLOS exclusion method, an NLOS exclusion criterion is proposed to exclude the blocked satellites considering the elevation, signal noise ratio (SNR) and horizontal dilution of precision (HDOP). Finally, GNSS positioning is estimated by weighted least square (WLS) method using the remaining satellites after the NLOS exclusion. A static experiment was performed near a double-decker bus stop in Hong Kong, which verified the effectiveness of the proposed method.	1,0,0,0,0,0
A general framework for solving convex optimization problems involving the sum of three convex functions	In this paper, we consider solving a class of convex optimization problem which minimizes the sum of three convex functions $f(x)+g(x)+h(Bx)$, where $f(x)$ is differentiable with a Lipschitz continuous gradient, $g(x)$ and $h(x)$ have a closed-form expression of their proximity operators and $B$ is a bounded linear operator. This type of optimization problem has wide application in signal recovery and image processing. To make full use of the differentiability function in the optimization problem, we take advantage of two operator splitting methods: the forward-backward splitting method and the three operator splitting method. In the iteration scheme derived from the two operator splitting methods, we need to compute the proximity operator of $g+h \circ B$ and $h \circ B$, respectively. Although these proximity operators do not have a closed-form solution in general, they can be solved very efficiently. We mainly employ two different approaches to solve these proximity operators: one is dual and the other is primal-dual. Following this way, we fortunately find that three existing iterative algorithms including Condat and Vu algorithm, primal-dual fixed point (PDFP) algorithm and primal-dual three operator (PD3O) algorithm are a special case of our proposed iterative algorithms. Moreover, we discover a new kind of iterative algorithm to solve the considered optimization problem, which is not covered by the existing ones. Under mild conditions, we prove the convergence of the proposed iterative algorithms. Numerical experiments applied on fused Lasso problem, constrained total variation regularization in computed tomography (CT) image reconstruction and low-rank total variation image super-resolution problem demonstrate the effectiveness and efficiency of the proposed iterative algorithms.	0,0,1,0,0,0
Post-hoc labeling of arbitrary EEG recordings for data-efficient evaluation of neural decoding methods	Many cognitive, sensory and motor processes have correlates in oscillatory neural sources, which are embedded as a subspace into the recorded brain signals. Decoding such processes from noisy magnetoencephalogram/electroencephalogram (M/EEG) signals usually requires the use of data-driven analysis methods. The objective evaluation of such decoding algorithms on experimental raw signals, however, is a challenge: the amount of available M/EEG data typically is limited, labels can be unreliable, and raw signals often are contaminated with artifacts. The latter is specifically problematic, if the artifacts stem from behavioral confounds of the oscillatory neural processes of interest. To overcome some of these problems, simulation frameworks have been introduced for benchmarking decoding methods. Generating artificial brain signals, however, most simulation frameworks make strong and partially unrealistic assumptions about brain activity, which limits the generalization of obtained results to real-world conditions. In the present contribution, we thrive to remove many shortcomings of current simulation frameworks and propose a versatile alternative, that allows for objective evaluation and benchmarking of novel data-driven decoding methods for neural signals. Its central idea is to utilize post-hoc labelings of arbitrary M/EEG recordings. This strategy makes it paradigm-agnostic and allows to generate comparatively large datasets with noiseless labels. Source code and data of the novel simulation approach are made available for facilitating its adoption.	1,0,0,1,0,0
A study of existing Ontologies in the IoT-domain	Several domains have adopted the increasing use of IoT-based devices to collect sensor data for generating abstractions and perceptions of the real world. This sensor data is multi-modal and heterogeneous in nature. This heterogeneity induces interoperability issues while developing cross-domain applications, thereby restricting the possibility of reusing sensor data to develop new applications. As a solution to this, semantic approaches have been proposed in the literature to tackle problems related to interoperability of sensor data. Several ontologies have been proposed to handle different aspects of IoT-based sensor data collection, ranging from discovering the IoT sensors for data collection to applying reasoning on the collected sensor data for drawing inferences. In this paper, we survey these existing semantic ontologies to provide an overview of the recent developments in this field. We highlight the fundamental ontological concepts (e.g., sensor-capabilities and context-awareness) required for an IoT-based application, and survey the existing ontologies which include these concepts. Based on our study, we also identify the shortcomings of currently available ontologies, which serves as a stepping stone to state the need for a common unified ontology for the IoT domain.	1,0,0,0,0,0
Automated Synthesis of Secure Platform Mappings	System development often involves decisions about how a high-level design is to be implemented using primitives from a low-level platform. Certain decisions, however, may introduce undesirable behavior into the resulting implementation, possibly leading to a violation of a desired property that has already been established at the design level. In this paper, we introduce the problem of synthesizing a property-preserving platform mapping: A set of implementation decisions ensuring that a desired property is preserved from a high-level design into a low-level platform implementation. We provide a formalization of the synthesis problem and propose a technique for synthesizing a mapping based on symbolic constraint search. We describe our prototype implementation, and a real-world case study demonstrating the application of our technique to synthesizing secure mappings for the popular web authorization protocols OAuth 1.0 and 2.0.	1,0,0,0,0,0
Robust, Deep and Inductive Anomaly Detection	PCA is a classical statistical technique whose simplicity and maturity has seen it find widespread use as an anomaly detection technique. However, it is limited in this regard by being sensitive to gross perturbations of the input, and by seeking a linear subspace that captures normal behaviour. The first issue has been dealt with by robust PCA, a variant of PCA that explicitly allows for some data points to be arbitrarily corrupted, however, this does not resolve the second issue, and indeed introduces the new issue that one can no longer inductively find anomalies on a test set. This paper addresses both issues in a single model, the robust autoencoder. This method learns a nonlinear subspace that captures the majority of data points, while allowing for some data to have arbitrary corruption. The model is simple to train and leverages recent advances in the optimisation of deep neural networks. Experiments on a range of real-world datasets highlight the model's effectiveness.	1,0,0,1,0,0
Foolbox: A Python toolbox to benchmark the robustness of machine learning models	Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at this https URL . The most up-to-date documentation can be found at this http URL .	1,0,0,1,0,0
Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo	Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.	0,0,0,1,0,0
Structured Peer Learning Program - An Innovative Approach to Computer Science Education	Structured Peer Learning (SPL) is a form of peer-based supplemental instruction that focuses on mentoring, guidance, and development of technical, communication, and social skills in both the students receiving assistance and the students in teaching roles. This paper explores the methodology, efficacy, and reasoning behind the practical realization of a SPL program designed to increase student knowledge and success in undergraduate Computer Science courses. Students expressed an increased level of comfort when asking for help from student teachers versus traditional educational resources, historically showed an increased average grade in lower-level courses, and felt that the program positively impacted their desire to continue in or switch to a Computer major. Additionally, results indicated that advances in programming, analytical thinking, and abstract analysis skills were evident in not only the students but also the student teachers, suggesting a strong bidirectional flow of knowledge.	1,0,0,0,0,0
T* : A Heuristic Search Based Algorithm for Motion Planning with Temporal Goals	Motion planning is the core problem to solve for developing any application involving an autonomous mobile robot. The fundamental motion planning problem involves generating a trajectory for a robot for point-to-point navigation while avoiding obstacles. Heuristic-based search algorithms like A* have been shown to be extremely efficient in solving such planning problems. Recently, there has been an increased interest in specifying complex motion plans using temporal logic. In the state-of-the-art algorithm, the temporal logic motion planning problem is reduced to a graph search problem and Dijkstra's shortest path algorithm is used to compute the optimal trajectory satisfying the specification. The A* algorithm when used with a proper heuristic for the distance from the destination can generate an optimal path in a graph efficiently. The primary challenge for using A* algorithm in temporal logic path planning is that there is no notion of a single destination state for the robot. In this thesis, we present a novel motion planning algorithm T* that uses the A* search procedure in temporal logic path planning \emph{opportunistically} to generate an optimal trajectory satisfying a temporal logic query. Our experimental results demonstrate that T* achieves an order of magnitude improvement over the state-of-the-art algorithm to solve many temporal logic motion planning problems.	1,0,0,0,0,0
Analytic continuation with Padé decomposition	The ill-posed analytic continuation problem for Green's functions or self-energies can be done using the Padé rational polynomial approximation. However, to extract accurate results from this approximation, high precision input data of the Matsubara Green's function are needed. The calculation of the Matsubara Green's function generally involves a Matsubara frequency summation which cannot be evaluated analytically. Numerical summation is requisite but it converges slowly with the increase of the Matsubara frequency. Here we show that this slow convergence problem can be significantly improved by utilizing the Padé decomposition approach to replace the Matsubara frequency summation by a Padé frequency summation, and high precision input data can be obtained to successfully perform the Padé analytic continuation.	0,1,0,0,0,0
Synergies between Asteroseismology and Three-dimensional Simulations of Stellar Turbulence	Turbulent mixing of chemical elements by convection has fundamental effects on the evolution of stars. The standard algorithm at present, mixing-length theory (MLT), is intrinsically local, and must be supplemented by extensions with adjustable parameters. As a step toward reducing this arbitrariness, we compare asteroseismically inferred internal structures of two Kepler slowly pulsating B stars (SPB's; $M\sim 3.25 M_\odot$) to predictions of 321D turbulence theory, based upon well-resolved, truly turbulent three-dimensional simulations (Arnett , et al. 2015, Christini, et al. 2016) which include boundary physics absent from MLT. We find promising agreement between the steepness and shapes of the theoretically-predicted composition profile outside the convective region in 3D simulations and in asteroseismically constrained composition profiles in the best 1D models of the two SPBs. The structure and motion of the boundary layer, and the generation of waves, are discussed.	0,1,0,0,0,0
A New Family of Asymmetric Distributions for Modeling Light-Tailed and Right-Skewed Data	A new three-parameter cumulative distribution function defined on $(\alpha,\infty)$, for some $\alpha\geq0$, with asymmetric probability density function and showing exponential decays at its both tails, is introduced. The new distribution is near to familiar distributions like the gamma and log-normal distributions, but this new one shows own elements and thus does not generalize neither of these distributions. Hence, the new distribution constitutes a new alternative to fit values showing light-tailed behaviors. Further, this new distribution shows great flexibility to fit the bulk of data by tuning some parameters. We refer to this new distribution as the generalized exponential log-squared distribution (GEL-S). Statistical properties of the GEL-S distribution are discussed. The maximum likelihood method is proposed for estimating the model parameters, but incorporating adaptations in computational procedures due to difficulties in the manipulation of the parameters. The perfomance of the new distribution is studied using simulations. Applications to real data sets coming from different domains are showed.	0,0,1,1,0,0
A projection pursuit framework for testing general high-dimensional hypothesis	This article develops a framework for testing general hypothesis in high-dimensional models where the number of variables may far exceed the number of observations. Existing literature has considered less than a handful of hypotheses, such as testing individual coordinates of the model parameter. However, the problem of testing general and complex hypotheses remains widely open. We propose a new inference method developed around the hypothesis adaptive projection pursuit framework, which solves the testing problems in the most general case. The proposed inference is centered around a new class of estimators defined as $l_1$ projection of the initial guess of the unknown onto the space defined by the null. This projection automatically takes into account the structure of the null hypothesis and allows us to study formal inference for a number of long-standing problems. For example, we can directly conduct inference on the sparsity level of the model parameters and the minimum signal strength. This is especially significant given the fact that the former is a fundamental condition underlying most of the theoretical development in high-dimensional statistics, while the latter is a key condition used to establish variable selection properties. Moreover, the proposed method is asymptotically exact and has satisfactory power properties for testing very general functionals of the high-dimensional parameters. The simulation studies lend further support to our theoretical claims and additionally show excellent finite-sample size and power properties of the proposed test.	0,0,1,1,0,0
Reverse iterative volume sampling for linear regression	We study the following basic machine learning task: Given a fixed set of $d$-dimensional input points for a linear regression problem, we wish to predict a hidden response value for each of the points. We can only afford to attain the responses for a small subset of the points that are then used to construct linear predictions for all points in the dataset. The performance of the predictions is evaluated by the total square loss on all responses (the attained as well as the hidden ones). We show that a good approximate solution to this least squares problem can be obtained from just dimension $d$ many responses by using a joint sampling technique called volume sampling. Moreover, the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all n responses. This unbiasedness is a desirable property that is not shared by other common subset selection techniques. Motivated by these basic properties, we develop a theoretical framework for studying volume sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which are of importance not only to least squares regression but also to numerical linear algebra in general. Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient algorithms for volume sampling which make this technique a practical tool in the machine learning toolbox. Finally, we provide experimental evidence which confirms our theoretical findings.	0,0,0,1,0,0
Relativistic wide-angle galaxy bispectrum on the light-cone	Given the important role that the galaxy bispectrum has recently acquired in cosmology and the scale and precision of forthcoming galaxy clustering observations, it is timely to derive the full expression of the large-scale bispectrum going beyond approximated treatments which neglect integrated terms or higher-order bias terms or use the Limber approximation. On cosmological scales, relativistic effects that arise from observing on the past light-cone alter the observed galaxy number counts, therefore leaving their imprints on N-point correlators at all orders. In this paper we compute for the first time the bispectrum including all general relativistic, local and integrated, effects at second order, the tracers' bias at second order, geometric effects as well as the primordial non-Gaussianity contribution. This is timely considering that future surveys will probe scales comparable to the horizon where approximations widely used currently may not hold; neglecting these effects may introduce biases in estimation of cosmological parameters as well as primordial non-Gaussianity.	0,1,0,0,0,0
Coble's group and the integrability of the Gosset-Elte polytopes and tessellations	This paper considers the planar figure of a combinatorial polytope or tessellation identified by the Coxeter symbol $k_{i,j}$ , inscribed in a conic, satisfying the geometric constraint that each octahedral cell has a centre. This realisation exists, and is movable, on account of some constraints being satisfied as a consequence of the others. A close connection to the birational group found originally by Coble in the different context of invariants for sets of points in projective space, allows to specify precisely a determining subset of vertices that may be freely chosen. This gives a unified geometric view of certain integrable discrete systems in one, two and three dimensions. Making contact with previous geometric accounts in the case of three dimensions, it is shown how the figure also manifests as a configuration of circles generalising the Clifford lattices, and how it can be applied to construct the spatial point-line configurations called the Desargues maps.	0,1,1,0,0,0
Towards Evolutional Compression	Compressing convolutional neural networks (CNNs) is essential for transferring the success of CNNs to a wide variety of applications to mobile devices. In contrast to directly recognizing subtle weights or filters as redundant in a given CNN, this paper presents an evolutionary method to automatically eliminate redundant convolution filters. We represent each compressed network as a binary individual of specific fitness. Then, the population is upgraded at each evolutionary iteration using genetic operations. As a result, an extremely compact CNN is generated using the fittest individual. In this approach, either large or small convolution filters can be redundant, and filters in the compressed network are more distinct. In addition, since the number of filters in each convolutional layer is reduced, the number of filter channels and the size of feature maps are also decreased, naturally improving both the compression and speed-up ratios. Experiments on benchmark deep CNN models suggest the superiority of the proposed algorithm over the state-of-the-art compression methods.	1,0,0,1,0,0
Replay spoofing detection system for automatic speaker verification using multi-task learning of noise classes	In this paper, we propose a replay attack spoofing detection system for automatic speaker verification using multitask learning of noise classes. We define the noise that is caused by the replay attack as replay noise. We explore the effectiveness of training a deep neural network simultaneously for replay attack spoofing detection and replay noise classification. The multi-task learning includes classifying the noise of playback devices, recording environments, and recording devices as well as the spoofing detection. Each of the three types of the noise classes also includes a genuine class. The experiment results on the ASVspoof2017 datasets demonstrate that the performance of our proposed system is improved by 30% relatively on the evaluation set.	1,0,0,1,0,0
Simultaneous 183 GHz H2O Maser and SiO Observations Towards Evolved Stars Using APEX SEPIA Band 5	We investigate the use of 183 GHz H2O masers for characterization of the physical conditions and mass loss process in the circumstellar envelopes of evolved stars. We used APEX SEPIA Band 5 to observe the 183 GHz H2O line towards 2 Red Supergiant and 3 Asymptotic Giant Branch stars. Simultaneously, we observed lines in 28SiO v0, 1, 2 and 3, and for 29SiO v0 and 1. We detected the 183 GHz H2O line towards all the stars with peak flux densities greater than 100 Jy, including a new detection from VY CMa. Towards all 5 targets, the water line had indications of being due to maser emission and had higher peak flux densities than for the SiO lines. The SiO lines appear to originate from both thermal and maser processes. Comparison with simulations and models indicate that 183 GHz maser emission is likely to extend to greater radii in the circumstellar envelopes than SiO maser emission and to similar or greater radii than water masers at 22, 321 and 325 GHz. We speculate that a prominent blue-shifted feature in the W Hya 183 GHz spectrum is amplifying the stellar continuum, and is located at a similar distance from the star as mainline OH maser emission. From a comparison of the individual polarizations, we find that the SiO maser linear polarization fraction of several features exceeds the maximum fraction allowed under standard maser assumptions and requires strong anisotropic pumping of the maser transition and strongly saturated maser emission. The low polarization fraction of the H2O maser however, fits with the expectation for a non-saturated maser. 183 GHz H2O masers can provide strong probes of the mass loss process of evolved stars. Higher angular resolution observations of this line using ALMA Band 5 will enable detailed investigation of the emission location in circumstellar envelopes and can also provide information on magnetic field strength and structure.	0,1,0,0,0,0
Action Robust Reinforcement Learning and Applications in Continuous Control	A policy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. Specifically, we consider two scenarios in which the agent attempts to perform an action $\mathbf{a}$, and (i) with probability $\alpha$, an alternative adversarial action $\bar{\mathbf{a}}$ is taken, or (ii) an adversary adds a perturbation to the selected action in the case of continuous action space. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrupt forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our approach to deep reinforcement learning (DRL) and provide extensive experiments in the various MuJoCo domains. Our experiments show that not only does our approach produce robust policies, but it also improves the performance in the absence of perturbations. This generalization indicates that action-robustness can be thought of as implicit regularization in RL problems.	1,0,0,1,0,0
Existence of Evolutionarily Stable Strategies Remains Hard to Decide for a Wide Range of Payoff Values	The concept of an evolutionarily stable strategy (ESS), introduced by Smith and Price, is a refinement of Nash equilibrium in 2-player symmetric games in order to explain counter-intuitive natural phenomena, whose existence is not guaranteed in every game. The problem of deciding whether a game possesses an ESS has been shown to be $\Sigma_{2}^{P}$-complete by Conitzer using the preceding important work by Etessami and Lochbihler. The latter, among other results, proved that deciding the existence of ESS is both NP-hard and coNP-hard. In this paper we introduce a "reduction robustness" notion and we show that deciding the existence of an ESS remains coNP-hard for a wide range of games even if we arbitrarily perturb within some intervals the payoff values of the game under consideration. In contrast, ESS exist almost surely for large games with random and independent payoffs chosen from the same distribution.	1,0,0,0,0,0
FLaapLUC: a pipeline for the generation of prompt alerts on transient Fermi-LAT $γ$-ray sources	The large majority of high energy sources detected with Fermi-LAT are blazars, which are known to be very variable sources. High cadence long-term monitoring simultaneously at different wavelengths being prohibitive, the study of their transient activities can help shedding light on our understanding of these objects. The early detection of such potentially fast transient events is the key for triggering follow-up observations at other wavelengths. A Python tool, FLaapLUC, built on top of the Science Tools provided by the Fermi Science Support Center and the Fermi-LAT collaboration, has been developed using a simple aperture photometry approach. This tool can effectively detect relative flux variations in a set of predefined sources and alert potential users. Such alerts can then be used to trigger target of opportunity observations with other facilities. It is shown that FLaapLUC is an efficient tool to reveal transient events in Fermi-LAT data, providing quick results which can be used to promptly organise follow-up observations. Results from this simple aperture photometry method are also compared to full likelihood analyses. The FLaapLUC package is made available on GitHub and is open to contributions by the community.	0,1,0,0,0,0
An oscillation criterion for delay differential equations with several non-monotone arguments	The oscillatory behavior of the solutions to a differential equation with several non-monotone delay arguments and non-negative coefficients is studied. A new sufficient oscillation condition, involving lim sup, is obtained. An example illustrating the significance of the result is also given.	0,0,1,0,0,0
Interactions between Health Searchers and Search Engines	The Web is an important resource for understanding and diagnosing medical conditions. Based on exposure to online content, people may develop undue health concerns, believing that common and benign symptoms are explained by serious illnesses. In this paper, we investigate potential strategies to mine queries and searcher histories for clues that could help search engines choose the most appropriate information to present in response to exploratory medical queries. To do this, we performed a longitudinal study of health search behavior using the logs of a popular search engine. We found that query variations which might appear innocuous (e.g. "bad headache" vs "severe headache") may hold valuable information about the searcher which could be used by search engines to improve performance. Furthermore, we investigated how medically concerned users respond differently to search engine result pages (SERPs) and find that their disposition for clicking on concerning pages is pronounced, potentially leading to a self-reinforcement of concern. Finally, we studied to which degree variations in the SERP impact future search and real-world health-seeking behavior and obtained some surprising results (e.g., viewing concerning pages may lead to a short-term reduction of real-world health seeking).	1,0,0,0,0,0
Continued fractions and conformal mappings for domains with angle points	Here we construct the conformal mappings with the help of continuous fractions approximations. These approximations converge to the algebraic roots $\sqrt[N]{z}$ for $N \in \mathbb{N}$ and $z$ from the right half-plane of the complex plane. We estimate both the convergence rate and the compact set of convergence. Also we give the examples that illustrate the introduced technique of a conformal mapping construction.	0,0,1,0,0,0
Devam vs. Tamam: 2018 Turkish Elections	On June 24, 2018, Turkey held a historical election, transforming its parliamentary system to a presidential one. One of the main questions for Turkish voters was whether to start this new political era with reelecting its long-time political leader Recep Tayyip Erdogan or not. In this paper, we analyzed 108M tweets posted in the two months leading to the election to understand the groups that supported or opposed Erdogan's reelection. We examined the most distinguishing hashtags and retweeted accounts for both groups. Our findings indicate strong polarization between both groups as they differ in terms of ideology, news sources they follow, and preferred TV entertainment.	1,0,0,0,0,0
A new precision measurement of the α-decay half-life of 190Pt	A laboratory measurement of the $\alpha$-decay half-life of $^{190}$Pt has been performed using a low background Frisch grid ionisation chamber. A total amount of 216.60(17) mg of natural platinum has been measured for 75.9 days. The resulting half-life is $(4.97\pm0.16)\times 10^{11}$ years, with a total uncertainty of 3.2%. This number is in good agreement with the half-life obtained using the geological comparison method.	0,1,0,0,0,0
Permutation Tests for Infection Graphs	We formulate and analyze a novel hypothesis testing problem for inferring the edge structure of an infection graph. In our model, a disease spreads over a network via contagion or random infection, where the random variables governing the rates of contracting the disease from neighbors or random infection are independent exponential random variables with unknown rate parameters. A subset of nodes is also censored uniformly at random. Given the statuses of nodes in the network, the goal is to determine the underlying graph. We present a procedure based on permutation testing, and we derive sufficient conditions for the validity of our test in terms of automorphism groups of the graphs corresponding to the null and alternative hypotheses. Further, the test is valid more generally for infection processes satisfying a basic symmetry condition. Our test is easy to compute and does not involve estimating unknown parameters governing the process. We also derive risk bounds for our permutation test in a variety of settings, and motivate our test statistic in terms of approximate equivalence to likelihood ratio testing and maximin tests. We conclude with an application to real data from an HIV infection network.	1,0,1,1,0,0
Unpredictable sequences and Poincaré chaos	To make research of chaos more friendly with discrete equations, we introduce the concept of an unpredictable sequence as a specific unpredictable function on the set of integers. It is convenient to be verified as a solution of a discrete equation. This is rigorously proved in this paper for quasilinear systems, and we demonstrate the result numerically for linear systems in the critical case with respect to the stability of the origin. The completed research contributes to the theory of chaos as well as to the theory of discrete equations, considering unpredictable solutions.	0,1,0,0,0,0
A Passivity-Based Approach to Nash Equilibrium Seeking over Networks	In this paper we consider the problem of distributed Nash equilibrium (NE) seeking over networks, a setting in which players have limited local information. We start from a continuous-time gradient-play dynamics that converges to an NE under strict monotonicity of the pseudo-gradient and assumes perfect information, i.e., instantaneous all-to-all player communication. We consider how to modify this gradient-play dynamics in the case of partial, or networked information between players. We propose an augmented gradient-play dynamics with correction in which players communicate locally only with their neighbours to compute an estimate of the other players' actions. We derive the new dynamics based on the reformulation as a multi-agent coordination problem over an undirected graph. We exploit incremental passivity properties and show that a synchronizing, distributed Laplacian feedback can be designed using relative estimates of the neighbours. Under a strict monotonicity property of the pseudo-gradient, we show that the augmented gradient-play dynamics converges to consensus on the NE of the game. We further discuss two cases that highlight the tradeoff between properties of the game and the communication graph.	0,0,1,0,0,0
Modeling Spatial Overdispersion with the Generalized Waring Process	Modeling spatial overdispersion requires point processes models with finite dimensional distributions that are overdisperse relative to the Poisson. Fitting such models usually heavily relies on the properties of stationarity, ergodicity, and orderliness. And, though processes based on negative binomial finite dimensional distributions have been widely considered, they typically fail to simultaneously satisfy the three required properties for fitting. Indeed, it has been conjectured by Diggle and Milne that no negative binomial model can satisfy all three properties. In light of this, we change perspective, and construct a new process based on a different overdisperse count model, the Generalized Waring Distribution. While comparably tractable and flexible to negative binomial processes, the Generalized Waring process is shown to possess all required properties, and additionally span the negative binomial and Poisson processes as limiting cases. In this sense, the GW process provides an approximate resolution to the conundrum highlighted by Diggle and Milne.	0,0,1,1,0,0
Relative Error Tensor Low Rank Approximation	We consider relative error low rank approximation of $tensors$ with respect to the Frobenius norm: given an order-$q$ tensor $A \in \mathbb{R}^{\prod_{i=1}^q n_i}$, output a rank-$k$ tensor $B$ for which $\|A-B\|_F^2 \leq (1+\epsilon)$OPT, where OPT $= \inf_{\textrm{rank-}k~A'} \|A-A'\|_F^2$. Despite the success on obtaining relative error low rank approximations for matrices, no such results were known for tensors. One structural issue is that there may be no rank-$k$ tensor $A_k$ achieving the above infinum. Another, computational issue, is that an efficient relative error low rank approximation algorithm for tensors would allow one to compute the rank of a tensor, which is NP-hard. We bypass these issues via (1) bicriteria and (2) parameterized complexity solutions: (1) We give an algorithm which outputs a rank $k' = O((k/\epsilon)^{q-1})$ tensor $B$ for which $\|A-B\|_F^2 \leq (1+\epsilon)$OPT in $nnz(A) + n \cdot \textrm{poly}(k/\epsilon)$ time in the real RAM model. Here $nnz(A)$ is the number of non-zero entries in $A$. (2) We give an algorithm for any $\delta >0$ which outputs a rank $k$ tensor $B$ for which $\|A-B\|_F^2 \leq (1+\epsilon)$OPT and runs in $ ( nnz(A) + n \cdot \textrm{poly}(k/\epsilon) + \exp(k^2/\epsilon) ) \cdot n^\delta$ time in the unit cost RAM model. For outputting a rank-$k$ tensor, or even a bicriteria solution with rank-$Ck$ for a certain constant $C > 1$, we show a $2^{\Omega(k^{1-o(1)})}$ time lower bound under the Exponential Time Hypothesis. Our results give the first relative error low rank approximations for tensors for a large number of robust error measures for which nothing was known, as well as column row and tube subset selection. We also obtain new results for matrices, such as $nnz(A)$-time CUR decompositions, improving previous $nnz(A)\log n$-time algorithms, which may be of independent interest.	1,0,0,0,0,0
Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference	We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.	1,0,0,1,0,0
Lurking Variable Detection via Dimensional Analysis	Lurking variables represent hidden information, and preclude a full understanding of phenomena of interest. Detection is usually based on serendipity -- visual detection of unexplained, systematic variation. However, these approaches are doomed to fail if the lurking variables do not vary. In this article, we address these challenges by introducing formal hypothesis tests for the presence of lurking variables, based on Dimensional Analysis. These procedures utilize a modified form of the Buckingham Pi theorem to provide structure for a suitable null hypothesis. We present analytic tools for reasoning about lurking variables in physical phenomena, construct procedures to handle cases of increasing complexity, and present examples of their application to engineering problems. The results of this work enable algorithm-driven lurking variable detection, complementing a traditionally inspection-based approach.	0,0,0,1,0,0
Modeling non-stationary extreme dependence with stationary max-stable processes and multidimensional scaling	Modeling the joint distribution of extreme weather events in multiple locations is a challenging task with important applications. In this study, we use max-stable models to study extreme daily precipitation events in Switzerland. The non-stationarity of the spatial process at hand involves important challenges, which are often dealt with by using a stationary model in a so-called climate space, with well-chosen covariates. Here, we instead chose to warp the weather stations under study in a latent space of higher dimension using multidimensional scaling (MDS). The advantage of this approach is its improved flexibility to reproduce highly non-stationary phenomena, while keeping a tractable stationary spatial model in the latent space. Two model fitting approaches, which both use MDS, are presented and compared to a classical approach that relies on composite likelihood maximization in a climate space. Results suggest that the proposed methods better reproduce the observed extremal coefficients and their complex spatial dependence.	0,0,0,1,0,0
Dixmier traces and residues on weak operator ideals	We develop the theory of modulated operators in general principal ideals of compact operators. For Laplacian modulated operators we establish Connes' trace formula in its local Euclidean model and a global version thereof. It expresses Dixmier traces in terms of the vector-valued Wodzicki residue. We demonstrate the applicability of our main results in the context of log-classical pseudo-differential operators, studied by Lesch, and a class of operators naturally appearing in noncommutative geometry.	0,0,1,0,0,0
Nonparametric Neural Networks	Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce *nonparametric neural networks*, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty. We employ a novel optimization algorithm, which we term *adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising results.	1,0,0,0,0,0
On the Glitch Phenomenon	The Principle of the Glitch states that for any device which makes a discrete decision based upon a continuous range of possible inputs, there are inputs for which it will take arbitrarily long to reach a decision. The appropriate mathematical setting for studying this principle is described. This involves defining the concept of continuity for mappings on sets of functions. It can then be shown that the glitch principle follows from the continuous behavior of the device.	1,0,1,0,0,0
MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment	Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at this https URL .	1,0,0,0,0,0
The MISRA C Coding Standard and its Role in the Development and Analysis of Safety- and Security-Critical Embedded Software	The MISRA project started in 1990 with the mission of providing world-leading best practice guidelines for the safe and secure application of both embedded control systems and standalone software. MISRA C is a coding standard defining a subset of the C language, initially targeted at the automotive sector, but now adopted across all industry sectors that develop C software in safety- and/or security-critical contexts. In this paper, we introduce MISRA C, its role in the development of critical software, especially in embedded systems, its relevance to industry safety standards, as well as the challenges of working with a general-purpose programming language standard that is written in natural language with a slow evolution over the last 40+ years. We also outline the role of static analysis in the automatic checking of compliance with respect to MISRA C, and the role of the MISRA C language subset in enabling a wider application of formal methods to industrial software written in C.	1,0,0,0,0,0
Resilience of Core-Periphery Networks in the Case of Rich-Club	Core-periphery networks are structures that present a set of central and densely connected nodes, namely the core, and a set of non-central and sparsely connected nodes, namely the periphery. The rich-club refers to a set in which the highest degree nodes show a high density of connections. Thus, a network that displays a rich-club can be interpreted as a core-periphery network in which the core is made up by a number of hubs. In this paper, we test the resilience of networks showing a progressively denser rich-club and we observe how this structure is able to affect the network measures in terms of both cohesion and efficiency in information flow. Additionally, we consider the case in which, instead of making the core denser, we add links to the periphery. These two procedures of core and periphery thickening delineate a decision process in the placement of new links and allow us to conduct a scenario analysis that can be helpful in the comprehension and supervision of complex networks under the resilience perspective. The advantages of the two procedures, as well as their implications, are discussed in relation to both network effciency and node heterogeneity.	1,0,0,0,0,0
Electron Paramagnetic Resonance Spectroscopy of Er$^{3+}$:Y$_2$SiO$_5$ Using Josephson Bifurcation Amplifier: Observation of Hyperfine and Quadrupole Structures	We performed magnetic field and frequency tunable electron paramagnetic resonance spectroscopy of an Er$^{3+}$ doped Y$_2$SiO$_5$ crystal by observing the change in flux induced on a direct current-superconducting quantum interference device (dc-SQUID) loop of a tunable Josephson bifurcation amplifer. The observed spectra show multiple transitions which agree well with the simulated energy levels, taking into account the hyperfine and quadrupole interactions of $^{167}$Er. The sensing volume is about 0.15 pl, and our inferred measurement sensitivity (limited by external flux noise) is approximately $1.5\times10^4$ electron spins for a 1 s measurement. The sensitivity value is two orders of magnitude better than similar schemes using dc-SQUID switching readout.	0,1,0,0,0,0
Ro-vibrational states of H$_2^+$. Variational calculations	The nonrelativistic variational calculation of a complete set of ro-vibrational states in the H$_2^+$ molecular ion supported by the ground $1s\sigma$ adiabatic potential is presented. It includes both bound states and resonances located above the $n=1$ threshold. In the latter case we also evaluate a predissociation width of a state wherever it is significant. Relativistic and radiative corrections are discussed and effective adiabatic potentials of these corrections are included as supplementary files.	0,1,0,0,0,0
Tikhonov Regularization for Long Short-Term Memory Networks	It is a well-known fact that adding noise to the input data often improves network performance. While the dropout technique may be a cause of memory loss, when it is applied to recurrent connections, Tikhonov regularization, which can be regarded as the training with additive noise, avoids this issue naturally, though it implies regularizer derivation for different architectures. In case of feedforward neural networks this is straightforward, while for networks with recurrent connections and complicated layers it leads to some difficulties. In this paper, a Tikhonov regularizer is derived for Long-Short Term Memory (LSTM) networks. Although it is independent of time for simplicity, it considers interaction between weights of the LSTM unit, which in theory makes it possible to regularize the unit with complicated dependences by using only one parameter that measures the input data perturbation. The regularizer that is proposed in this paper has three parameters: one to control the regularization process, and other two to maintain computation stability while the network is being trained. The theory developed in this paper can be applied to get such regularizers for different recurrent neural networks with Hadamard products and Lipschitz continuous functions.	1,0,0,1,0,0
The Importance of System-Level Information in Multiagent Systems Design: Cardinality and Covering Problems	A fundamental challenge in multiagent systems is to design local control algorithms to ensure a desirable collective behaviour. The information available to the agents, gathered either through communication or sensing, naturally restricts the achievable performance. Hence, it is fundamental to identify what piece of information is valuable and can be exploited to design control laws with enhanced performance guarantees. This paper studies the case when such information is uncertain or inaccessible for a class of submodular resource allocation problems termed covering problems. In the first part of this work we pinpoint a fundamental risk-reward tradeoff faced by the system operator when conditioning the control design on a valuable but uncertain piece of information, which we refer to as the cardinality, that represents the maximum number of agents that can simultaneously select any given resource. Building on this analysis, we propose a distributed algorithm that allows agents to learn the cardinality while adjusting their behaviour over time. This algorithm is proved to perform on par or better to the optimal design obtained when the exact cardinality is known a priori.	1,0,0,0,0,0
Static non-reciprocity in mechanical metamaterials	Reciprocity is a fundamental principle governing various physical systems, which ensures that the transfer function between any two points in space is identical, regardless of geometrical or material asymmetries. Breaking this transmission symmetry offers enhanced control over signal transport, isolation and source protection. So far, devices that break reciprocity have been mostly considered in dynamic systems, for electromagnetic, acoustic and mechanical wave propagation associated with spatio-temporal variations. Here we show that it is possible to strongly break reciprocity in static systems, realizing mechanical metamaterials that, by combining large nonlinearities with suitable geometrical asymmetries, and possibly topological features, exhibit vastly different output displacements under excitation from different sides, as well as one-way displacement amplification. In addition to extending non-reciprocity and isolation to statics, our work sheds new light on the understanding of energy propagation in non-linear materials with asymmetric crystalline structures and topological properties, opening avenues for energy absorption, conversion and harvesting, soft robotics, prosthetics and optomechanics.	0,1,0,0,0,0
Contextual Regression: An Accurate and Conveniently Interpretable Nonlinear Model for Mining Discovery from Scientific Data	Machine learning algorithms such as linear regression, SVM and neural network have played an increasingly important role in the process of scientific discovery. However, none of them is both interpretable and accurate on nonlinear datasets. Here we present contextual regression, a method that joins these two desirable properties together using a hybrid architecture of neural network embedding and dot product layer. We demonstrate its high prediction accuracy and sensitivity through the task of predictive feature selection on a simulated dataset and the application of predicting open chromatin sites in the human genome. On the simulated data, our method achieved high fidelity recovery of feature contributions under random noise levels up to 200%. On the open chromatin dataset, the application of our method not only outperformed the state of the art method in terms of accuracy, but also unveiled two previously unfound open chromatin related histone marks. Our method can fill the blank of accurate and interpretable nonlinear modeling in scientific data mining tasks.	1,0,0,1,0,0
Linear and bilinear restriction to certain rotationally symmetric hypersurfaces	Conditional on Fourier restriction estimates for elliptic hypersurfaces, we prove optimal restriction estimates for polynomial hypersurfaces of revolution for which the defining polynomial has non-negative coefficients. In particular, we obtain uniform--depending only on the dimension and polynomial degree--estimates for restriction with affine surface measure, slightly beyond the bilinear range. The main step in the proof of our linear result is an (unconditional) bilinear adjoint restriction estimate for pieces at different scales.	0,0,1,0,0,0
Detection via simultaneous trajectory estimation and long time integration	In this work, we consider the detection of manoeuvring small objects with radars. Such objects induce low signal to noise ratio (SNR) reflections in the received signal. We consider both co-located and separated transmitter/receiver pairs, i.e., mono-static and bi-static configurations, respectively, as well as multi-static settings involving both types. We propose a detection approach which is capable of coherently integrating these reflections within a coherent processing interval (CPI) in all these configurations and continuing integration for an arbitrarily long time across consecutive CPIs. We estimate the complex value of the reflection coefficients for integration while simultaneously estimating the object trajectory. Compounded with this is the estimation of the unknown time reference shift of the separated transmitters necessary for coherent processing. Detection is made by using the resulting integration value in a Neyman-Pearson test against a constant false alarm rate threshold. We demonstrate the efficacy of our approach in a simulation example with a very low SNR object which cannot be detected with conventional techniques.	1,0,0,1,0,0
Generalization of two Bonnet's Theorems to the relative Differential Geometry of the 3-dimensional Euclidean space	This paper is devoted to the 3-dimensional relative differential geometry of surfaces. In the Euclidean space $\R{E} ^3 $ we consider a surface $\varPhi %\colon \vect{x} = \vect{x}(u^1,u^2) $ with position vector field $\vect{x}$, which is relatively normalized by a relative normalization $\vect{y}% (u^1,u^2) $. A surface $\varPhi^*% \colon \vect{x}^* = \vect{x}^*(u^1,u^2) $ with position vector field $\vect{x}^* = \vect{x} + \mu \, \vect{y}$, where $\mu$ is a real constant, is called a relatively parallel surface to $\varPhi$. Then $\vect{y}$ is also a relative normalization of $\varPhi^*$. The aim of this paper is to formulate and prove the relative analogues of two well known theorems of O.~Bonnet which concern the parallel surfaces (see~\cite{oB1853}).	0,0,1,0,0,0
Feeble fish in time-dependent waters and homogenization of the G-equation	We study the following control problem. A fish with bounded aquatic locomotion speed swims in fast waters. Can this fish, under reasonable assumptions, get to a desired destination? It can, even if the flow is time-dependent. Moreover, given a prescribed sufficiently large time $t$, it can be there at exactly the time $t$. The major difference from our previous work is the time-dependence of the flow. We also give an application to homogenization of the G-equation.	0,0,1,0,0,0
A FEL Based on a Superlattice	The motion and photon emission of electrons in a superlattice may be described as in an undulator. Therefore, there is a close analogy between ballistic electrons in a superlattice and electrons in a free electron laser (FEL). Touching upon this analogy the intensity of photon emission in the IR region and the gain are calculated. It is shown that the amplification can be significant, reaching tens of percent.	0,1,0,0,0,0
Recurrent Scene Parsing with Perspective Understanding in the Loop	Objects may appear at arbitrary scales in perspective images of a scene, posing a challenge for recognition systems that process images at a fixed resolution. We propose a depth-aware gating module that adaptively selects the pooling field size in a convolutional network architecture according to the object scale (inversely proportional to the depth) so that small details are preserved for distant objects while larger receptive fields are used for those nearby. The depth gating signal is provided by stereo disparity or estimated directly from monocular input. We integrate this depth-aware gating into a recurrent convolutional neural network to perform semantic segmentation. Our recurrent module iteratively refines the segmentation results, leveraging the depth and semantic predictions from the previous iterations. Through extensive experiments on four popular large-scale RGB-D datasets, we demonstrate this approach achieves competitive semantic segmentation performance with a model which is substantially more compact. We carry out extensive analysis of this architecture including variants that operate on monocular RGB but use depth as side-information during training, unsupervised gating as a generic attentional mechanism, and multi-resolution gating. We find that gated pooling for joint semantic segmentation and depth yields state-of-the-art results for quantitative monocular depth estimation.	1,0,0,0,0,0
Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics	Kernel-based methods exhibit well-documented performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior information. Especially when the latter is not available, multi-kernel learning has gained popularity thanks to its flexibility in choosing kernels from a prescribed kernel dictionary. Leveraging the random feature approximation and its recent orthogonality-promoting variant, the present contribution develops a scalable multi-kernel learning scheme (termed Raker) to obtain the sought nonlinear learning function `on the fly,' first for static environments. To further boost performance in dynamic environments, an adaptive multi-kernel learning scheme (termed AdaRaker) is developed. AdaRaker accounts not only for data-driven learning of kernel combination, but also for the unknown dynamics. Performance is analyzed in terms of both static and dynamic regrets. AdaRaker is uniquely capable of tracking nonlinear learning functions in environments with unknown dynamics, and with with analytic performance guarantees. Tests with synthetic and real datasets are carried out to showcase the effectiveness of the novel algorithms.	1,0,0,1,0,0
Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern Discovery in Large Images and Signals	Convolutional dictionary learning (CDL) estimates shift invariant basis adapted to multidimensional data. CDL has proven useful for image denoising or inpainting, as well as for pattern discovery on multivariate signals. As estimated patterns can be positioned anywhere in signals or images, optimization techniques face the difficulty of working in extremely high dimensions with millions of pixels or time samples, contrarily to standard patch-based dictionary learning. To address this optimization problem, this work proposes a distributed and asynchronous algorithm, employing locally greedy coordinate descent and an asynchronous locking mechanism that does not require a central server. This algorithm can be used to distribute the computation on a number of workers which scales linearly with the encoded signal's size. Experiments confirm the scaling properties which allows us to learn patterns on large scales images from the Hubble Space Telescope.	1,0,0,1,0,0
Efficient Structured Surrogate Loss and Regularization in Structured Prediction	In this dissertation, we focus on several important problems in structured prediction. In structured prediction, the label has a rich intrinsic substructure, and the loss varies with respect to the predicted label and the true label pair. Structured SVM is an extension of binary SVM to adapt to such structured tasks. In the first part of the dissertation, we study the surrogate losses and its efficient methods. To minimize the empirical risk, a surrogate loss which upper bounds the loss, is used as a proxy to minimize the actual loss. Since the objective function is written in terms of the surrogate loss, the choice of the surrogate loss is important, and the performance depends on it. Another issue regarding the surrogate loss is the efficiency of the argmax label inference for the surrogate loss. Efficient inference is necessary for the optimization since it is often the most time-consuming step. We present a new class of surrogate losses named bi-criteria surrogate loss, which is a generalization of the popular surrogate losses. We first investigate an efficient method for a slack rescaling formulation as a starting point utilizing decomposability of the model. Then, we extend the algorithm to the bi-criteria surrogate loss, which is very efficient and also shows performance improvements. In the second part of the dissertation, another important issue of regularization is studied. Specifically, we investigate a problem of regularization in hierarchical classification when a structural imbalance exists in the label structure. We present a method to normalize the structure, as well as a new norm, namely shared Frobenius norm. It is suitable for hierarchical classification that adapts to the data in addition to the label structure.	0,0,0,1,0,0
Characterization of Calabi--Yau variations of Hodge structure over tube domains by characteristic forms	Sheng and Zuo's characteristic forms are invariants of a variation of Hodge structure. We show that they characterize Gross's canonical variations of Hodge structure of Calabi-Yau type over (Hermitian symmetric) tube domains.	0,0,1,0,0,0
Reynolds number dependence of the structure functions in homogeneous turbulence	We compare the predictions of stochastic closure theory (SCT) with experimental measurements of homogeneous turbulence made in the Variable Density Turbulence Tunnel (VDTT) at the Max Planck Institute for Dynamics and Self-Organization in Gottingen. While the general form of SCT contains infinitely many free parameters, the data permit us to reduce the number to seven, only three of which are active over the entire inertial range. Of these three, one parameter characterizes the variance of the mean field noise in SCT and another characterizes the rate in the large deviations of the mean. The third parameter is the decay exponent of the Fourier variables in the Fourier expansion of the noise, which characterizes the smoothness of the turbulent velocity. SCT compares favorably with velocity structure functions measured in the experiment. We considered even-order structure functions ranging in order from two to eight as well as the third-order structure functions at five Taylor-Reynolds numbers (Rl) between 110 and 1450. The comparisons highlight several advantages of the SCT, which include explicit predictions for the structure functions at any scale and for any Reynolds number. We observed that finite-Rl corrections, for instance, are important even at the highest Reynolds numbers produced in the experiments. SCT gives us the correct basis function to express all the moments of the velocity differences in turbulence in Fourier space. The SCT produces the coefficients of the series and so determines the statistical quantities that characterize the small scales in turbulence. It also characterizes the random force acting on the fluid in the stochastic Navier-Stokes equation, as described in the paper.	0,1,0,0,0,0
Identification of Near-Infrared [Se III] and [Kr VI] Emission Lines in Planetary Nebulae	We identify [Se III] 1.0994 micron in the planetary nebula (PN) NGC 5315 and [Kr VI] 1.2330 micron in three PNe, from spectra obtained with the FIRE spectrometer on the 6.5-m Baade Telescope. Se and Kr are the two most widely-detected neutron-capture elements in astrophysical nebulae, and can be enriched by s-process nucleosynthesis in PN progenitor stars. The detection of [Se III] 1.0994 micron is particularly valuable when paired with observations of [Se IV] 2.2858 micron, as it can be used to improve the accuracy of nebular Se abundance determinations, and allows Se ionization correction factor (ICF) schemes to be empirically tested for the first time. We present new effective collision strength calculations for Se^{2+} and Kr^{5+}, which we use to compute ionic abundances. In NGC 5315, we find that the Se abundance computed from Se^{3+}/H^+ is lower than that determined with ICFs that incorporate Se^{2+}/H^+. We compute new Kr ICFs that take Kr^{5+}/H^+ into account, by fitting correlations found in grids of Cloudy models between Kr ionic fractions and those of more abundant elements, and use these to derive Kr abundances in four PNe. Observations of [Se III] and [Kr VI] in a larger sample of PNe, with a range of excitation levels, are needed to rigorously test the ICF prescriptions for Se and our new Kr ICFs.	0,1,0,0,0,0
Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience	We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at this https URL	1,0,0,0,0,0
Coded Caching Schemes with Low Rate and Subpacketizations	Coded caching scheme, which is an effective technique to increase the transmission efficiency during peak traffic times, has recently become quite popular among the coding community. Generally rate can be measured to the transmission in the peak traffic times, i.e., this efficiency increases with the decreasing of rate. In order to implement a coded caching scheme, each file in the library must be split in a certain number of packets. And this number directly reflects the complexity of a coded caching scheme, i.e., the complexity increases with the increasing of the packet number. However there exists a tradeoff between the rate and packet number. So it is meaningful to characterize this tradeoff and design the related Pareto-optimal coded caching schemes with respect to both parameters. Recently, a new concept called placement delivery array (PDA) was proposed to characterize the coded caching scheme. However as far as we know no one has yet proved that one of the previously known PDAs is Pareto-optimal. In this paper, we first derive two lower bounds on the rate under the framework of PDA. Consequently, the PDA proposed by Maddah-Ali and Niesen is Pareto-optimal, and a tradeoff between rate and packet number is obtained for some parameters. Then, from the above observations and the view point of combinatorial design, two new classes of Pareto-optimal PDAs are obtained. Based on these PDAs, the schemes with low rate and packet number are obtained. Finally the performance of some previously known PDAs are estimated by comparing with these two classes of schemes.	1,0,0,0,0,0
Improving Massive MIMO Belief Propagation Detector with Deep Neural Network	In this paper, deep neural network (DNN) is utilized to improve the belief propagation (BP) detection for massive multiple-input multiple-output (MIMO) systems. A neural network architecture suitable for detection task is firstly introduced by unfolding BP algorithms. DNN MIMO detectors are then proposed based on two modified BP detectors, damped BP and max-sum BP. The correction factors in these algorithms are optimized through deep learning techniques, aiming at improved detection performance. Numerical results are presented to demonstrate the performance of the DNN detectors in comparison with various BP modifications. The neural network is trained once and can be used for multiple online detections. The results show that, compared to other state-of-the-art detectors, the DNN detectors can achieve lower bit error rate (BER) with improved robustness against various antenna configurations and channel conditions at the same level of complexity.	0,0,0,1,0,0
A combinatorial proof of Bass's determinant formula for the zeta function of regular graphs	We give an elementary combinatorial proof of Bass's determinant formula for the zeta function of a finite regular graph. This is done by expressing the number of non-backtracking cycles of a given length in terms of Chebychev polynomials in the eigenvalues of the adjacency operator of the graph.	1,0,0,0,0,0
Identifying Product Order with Restricted Boltzmann Machines	Unsupervised machine learning via a restricted Boltzmann machine is an useful tool in distinguishing an ordered phase from a disordered phase. Here we study its application on the two-dimensional Ashkin-Teller model, which features a partially ordered product phase. We train the neural network with spin configuration data generated by Monte Carlo simulations and show that distinct features of the product phase can be learned from non-ergodic samples resulting from symmetry breaking. Careful analysis of the weight matrices inspires us to define a nontrivial machine-learning motivated quantity of the product form, which resembles the conventional product order parameter.	0,1,0,0,0,0
Gaia Data Release 1: The archive visualisation service	Context: The first Gaia data release (DR1) delivered a catalogue of astrometry and photometry for over a billion astronomical sources. Within the panoply of methods used for data exploration, visualisation is often the starting point and even the guiding reference for scientific thought. However, this is a volume of data that cannot be efficiently explored using traditional tools, techniques, and habits. Aims: We aim to provide a global visual exploration service for the Gaia archive, something that is not possible out of the box for most people. The service has two main goals. The first is to provide a software platform for interactive visual exploration of the archive contents, using common personal computers and mobile devices available to most users. The second aim is to produce intelligible and appealing visual representations of the enormous information content of the archive. Methods: The interactive exploration service follows a client-server design. The server runs close to the data, at the archive, and is responsible for hiding as far as possible the complexity and volume of the Gaia data from the client. This is achieved by serving visual detail on demand. Levels of detail are pre-computed using data aggregation and subsampling techniques. For DR1, the client is a web application that provides an interactive multi-panel visualisation workspace as well as a graphical user interface. Results: The Gaia archive Visualisation Service offers a web-based multi-panel interactive visualisation desktop in a browser tab. It currently provides highly configurable 1D histograms and 2D scatter plots of Gaia DR1 and the Tycho-Gaia Astrometric Solution (TGAS) with linked views. An innovative feature is the creation of ADQL queries from visually defined regions in plots. [abridged]	0,1,0,0,0,0
A Markov decision process approach to optimizing cancer therapy using multiple modalities	There are several different modalities, e.g., surgery, chemotherapy, and radiotherapy, that are currently used to treat cancer. It is common practice to use a combination of these modalities to maximize clinical outcomes, which are often measured by a balance between maximizing tumor damage and minimizing normal tissue side effects due to treatment. However, multi-modality treatment policies are mostly empirical in current practice, and are therefore subject to individual clinicians' experiences and intuition. We present a novel formulation of optimal multi-modality cancer management using a finite-horizon Markov decision process approach. Specifically, at each decision epoch, the clinician chooses an optimal treatment modality based on the patient's observed state, which we define as a combination of tumor progression and normal tissue side effect. Treatment modalities are categorized as (1) Type 1, which has a high risk and high reward, but is restricted in the frequency of administration during a treatment course, (2) Type 2, which has a lower risk and lower reward than Type 1, but may be repeated without restriction, and (3) Type 3, no treatment (surveillance), which has the possibility of reducing normal tissue side effect at the risk of worsening tumor progression. Numerical simulations using various intuitive, concave reward functions show the structural insights of optimal policies and demonstrate the potential applications of using a rigorous approach to optimizing multi-modality cancer management.	0,1,1,0,0,0
Interaction between magnetic moments and itinerant carriers in d0 ferromagnetic SiC	Elucidating the interaction between magnetic moments and itinerant carriers is an important step to spintronic applications. Here, we investigate magnetic and transport properties in d0 ferromagnetic SiC single crystals prepared by postimplantation pulsed laser annealing. Magnetic moments are contributed by the p states of carbon atoms, but their magnetic circular dichroism is different from that in semi-insulating SiC samples. The anomalous Hall effect and negative magnetoresistance indicate the influence of d0 spin order on free carriers. The ferromagnetism is relatively weak in N-implanted SiC compared with that in Al-implanted SiC after annealing. The results suggest that d0 magnetic moments and itinerant carriers can interact with each other, which will facilitate the development of SiC spintronic devices with d0 ferromagnetism.	0,1,0,0,0,0
Buildings-to-Grid Integration Framework	This paper puts forth a mathematical framework for Buildings-to-Grid (BtG) integration in smart cities. The framework explicitly couples power grid and building's control actions and operational decisions, and can be utilized by buildings and power grids operators to simultaneously optimize their performance. Simplified dynamics of building clusters and building-integrated power networks with algebraic equations are presented---both operating at different time-scales. A model predictive control (MPC)-based algorithm that formulates the BtG integration and accounts for the time-scale discrepancy is developed. The formulation captures dynamic and algebraic power flow constraints of power networks and is shown to be numerically advantageous. The paper analytically establishes that the BtG integration yields a reduced total system cost in comparison with decoupled designs where grid and building operators determine their controls separately. The developed framework is tested on standard power networks that include thousands of buildings modeled using industrial data. Case studies demonstrate building energy savings and significant frequency regulation, while these findings carry over in network simulations with nonlinear power flows and mismatch in building model parameters. Finally, simulations indicate that the performance does not significantly worsen when there is uncertainty in the forecasted weather and base load conditions.	1,0,1,0,0,0
Value Asymptotics in Dynamic Games on Large Horizons	This paper is concerned with two-person dynamic zero-sum games. Let games for some family have common dynamics, running costs and capabilities of players, and let these games differ in densities only. We show that the Dynamic Programming Principle directly leads to the General Tauberian Theorem---that the existence of a uniform limit of the value functions for uniform distribution or for exponential distribution implies that the value functions uniformly converge to the same limit for arbitrary distribution from large class. No assumptions on strategies are necessary. Applications to differential games and stochastic statement are considered.	0,0,1,0,0,0
Exceeding the Shockley-Queisser limit within the detailed balance framework	The Shockley-Queisser limit is one of the most fundamental results in the field of photovoltaics. Based on the principle of detailed balance, it defines an upper limit for a single junction solar cell that uses an absorber material with a specific band gap. Although methods exist that allow a solar cell to exceed the Shockley-Queisser limit, here we show that it is possible to exceed the Shockley-Queisser limit without considering any of these additions. Merely by introducing an absorptivity that does not assume that every photon with an energy above the band gap is absorbed, efficiencies above the Shockley-Queisser limit are obtained. This is related to the fact that assuming optimal absorption properties also maximizes the recombination current within the detailed balance approach. We conclude that considering a finite thickness for the absorber layer allows the efficiency to exceed the Shockley-Queisser limit, and that this is more likely to occur for materials with small band gaps.	0,1,0,0,0,0
Correlation plots of the Siberian radioheliograph	The Siberian Solar Radio Telescope is now being upgraded. The upgrading is aimed at providing the aperture synthesis imaging in the 4-8 GHz frequency range, instead of the single-frequency direct imaging due to the Earth rotation. The first phase of the upgrading is a 48-antenna array - the Siberian Radioheliograph. One type of radioheliograph data represents correlation plots. In evaluating the covariance of two-level signals, these plots are sums of complex correlations, obtained for different antenna pairs. Bearing in mind that correlation of signals from an antenna pair is related to a spatial frequency, we can say that each value of the plot is an integral over a spatial spectrum. Limits of the integration are defined by the task. Only high spatial frequencies are integrated to obtain dynamics of compact sources. The whole spectrum is integrated to reach maximum sensitivity. We show that the covariance of two-level variables up to Van Vleck correction is a correlation coefficient of these variables.	0,1,0,0,0,0
Stopping Active Learning based on Predicted Change of F Measure for Text Classification	During active learning, an effective stopping method allows users to limit the number of annotations, which is cost effective. In this paper, a new stopping method called Predicted Change of F Measure will be introduced that attempts to provide the users an estimate of how much performance of the model is changing at each iteration. This stopping method can be applied with any base learner. This method is useful for reducing the data annotation bottleneck encountered when building text classification systems.	1,0,0,1,0,0
Matrix KP: tropical limit and Yang-Baxter maps	We study soliton solutions of matrix Kadomtsev-Petviashvili (KP) equations in a tropical limit, in which their support at fixed time is a planar graph and polarizations are attached to its constituting lines. There is a subclass of "pure line soliton solutions" for which we find that, in this limit, the distribution of polarizations is fully determined by a Yang-Baxter map. For a vector KP equation, this map is given by an R-matrix, whereas it is a non-linear map in case of a more general matrix KP equation. We also consider the corresponding Korteweg-deVries (KdV) reduction. Furthermore, exploiting the fine structure of soliton interactions in the tropical limit, we obtain a new solution of the tetrahedron (or Zamolodchikov) equation. Moreover, a solution of the functional tetrahedron equation arises from the parameter-dependence of the vector KP R-matrix.	0,1,0,0,0,0
Discrete Cycloids from Convex Symmetric Polygons	Cycloids, hipocycloids and epicycloids have an often forgotten common property: they are homothetic to their evolutes. But what if use convex symmetric polygons as unit balls, can we define evolutes and cycloids which are genuinely discrete? Indeed, we can! We define discrete cycloids as eigenvectors of a discrete double evolute transform which can be seen as a linear operator on a vector space we call curvature radius space. We are also able to classify such cycloids according to the eigenvalues of that transform, and show that the number of cusps of each cycloid is well determined by the ordering of those eigenvalues. As an elegant application, we easily establish a version of the four-vertex theorem for closed convex polygons. The whole theory is developed using only linear algebra, and concrete examples are given.	0,0,1,0,0,0
Social Network based Short-Term Stock Trading System	This paper proposes a novel adaptive algorithm for the automated short-term trading of financial instrument. The algorithm adopts a semantic sentiment analysis technique to inspect the Twitter posts and to use them to predict the behaviour of the stock market. Indeed, the algorithm is specifically developed to take advantage of both the sentiment and the past values of a certain financial instrument in order to choose the best investment decision. This allows the algorithm to ensure the maximization of the obtainable profits by trading on the stock market. We have conducted an investment simulation and compared the performance of our proposed with a well-known benchmark (DJTATO index) and the optimal results, in which an investor knows in advance the future price of a product. The result shows that our approach outperforms the benchmark and achieves the performance score close to the optimal result.	1,0,0,0,0,1
A Kronecker-type identity and the representations of a number as a sum of three squares	By considering a limiting case of a Kronecker-type identity, we obtain an identity found by both Andrews and Crandall. We then use the Andrews-Crandall identity to give a new proof of a formula of Gauss for the representations of a number as a sum of three squares. From the Kronecker-type identity, we also deduce Gauss's theorem that every positive integer is representable as a sum of three triangular numbers.	0,0,1,0,0,0
The quest for H$_3^+$ at Neptune: deep burn observations with NASA IRTF iSHELL	Emission from the molecular ion H$_3^+$ is a powerful diagnostic of the upper atmosphere of Jupiter, Saturn, and Uranus, but it remains undetected at Neptune. In search of this emission, we present near-infrared spectral observations of Neptune between 3.93 and 4.00 $\mu$m taken with the newly commissioned iSHELL instrument on the NASA Infrared Telescope Facility in Hawaii, obtained 17-20 August 2017. We spent 15.4 h integrating across the disk of the planet, yet were unable to unambiguously identify any H$_3^+$ line emissions. Assuming a temperature of 550 K, we derive an upper limit on the column integrated density of $1.0^{+1.2}_{-0.8}\times10^{13}$ m$^{-2}$, which is an improvement of 30\% on the best previous observational constraint. This result means that models are over-estimating the density by at least a factor of 5, highlighting the need for renewed modelling efforts. A potential solution is strong vertical mixing of polyatomic neutral species from Neptune's upper stratosphere to the thermosphere, reacting with H$_3^+$, thus greatly reducing the column integrated H$_3^+$ densities. This upper limit also provide constraints on future attempts at detecting H$_3^+$ using the James Webb Space Telescope.	0,1,0,0,0,0
BAMBI: An R package for Fitting Bivariate Angular Mixture Models	Statistical analyses of directional or angular data have applications in a variety of fields, such as geology, meteorology and bioinformatics. There is substantial literature on descriptive and inferential techniques for univariate angular data, with the bivariate (or more generally, multivariate) cases receiving more attention in recent years. However, there is a lack of software implementing inferential techniques in practice, especially in the bivariate situation. In this article, we introduce *BAMBI*, an R package for analyzing bivariate (and univariate) angular data. We implement random generation, density evaluation, and computation of summary measures for three bivariate (viz., bivariate wrapped normal, von Mises sine and von Mises cosine) and two univariate (viz., univariate wrapped normal and von Mises) angular distributions. The major contribution of BAMBI to statistical computing is in providing Bayesian methods for modeling angular data using finite mixtures of these distributions. We also provide functions for visual and numerical diagnostics and Bayesian inference for the fitted models. In this article, we first provide a brief review of the distributions and techniques used in BAMBI, then describe the capabilities of the package, and finally conclude with demonstrations of mixture model fitting using BAMBI on the two real datasets included in the package, one univariate and one bivariate.	0,0,0,1,0,0
The Hubble Catalog of Variables	The Hubble Catalog of Variables (HCV) is a 3 year ESA funded project that aims to develop a set of algorithms to identify variables among the sources included in the Hubble Source Catalog (HSC) and produce the HCV. We will process all HSC sources with more than a predefined number of measurements in a single filter/instrument combination and compute a range of lightcurve features to determine the variability status of each source. At the end of the project, the first release of the Hubble Catalog of Variables will be made available at the Mikulski Archive for Space Telescopes (MAST) and the ESA Science Archives. The variability detection pipeline will be implemented at the Space Telescope Science Institute (STScI) so that updated versions of the HCV may be created following the future releases of the HSC.	0,1,0,0,0,0
Controlled dynamic screening of excitonic complexes in 2D semiconductors	We report a combined theoretical/experimental study of dynamic screening of excitons in media with frequency-dependent dielectric functions. We develop an analytical model showing that interparticle interactions in an exciton are screened in the range of frequencies from zero to the characteristic binding energy depending on the symmetries and transition energies of that exciton. The problem of the dynamic screening is then reduced to simply solving the Schrodinger equation with an effectively frequency-independent potential. Quantitative predictions of the model are experimentally verified using a test system: neutral, charged and defect-bound excitons in two-dimensional monolayer WS2, screened by metallic, liquid, and semiconducting environments. The screening-induced shifts of the excitonic peaks in photoluminescence spectra are in good agreement with our model.	0,1,0,0,0,0
Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel Ground Truth using Stochastic Grammars	We propose a systematic learning-based approach to the generation of massive quantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D images thereof, with associated ground truth information, for the purposes of training, benchmarking, and diagnosing learning-based computer vision and robotics algorithms. In particular, we devise a learning-based pipeline of algorithms capable of automatically generating and rendering a potentially infinite variety of indoor scenes by using a stochastic grammar, represented as an attributed Spatial And-Or Graph, in conjunction with state-of-the-art physics-based rendering. Our pipeline is capable of synthesizing scene layouts with high diversity, and it is configurable inasmuch as it enables the precise customization and control of important attributes of the generated scenes. It renders photorealistic RGB images of the generated scenes while automatically synthesizing detailed, per-pixel ground truth data, including visible surface depth and normal, object identity, and material information (detailed to object parts), as well as environments (e.g., illuminations and camera viewpoints). We demonstrate the value of our synthesized dataset, by improving performance in certain machine-learning-based scene understanding tasks--depth and surface normal prediction, semantic segmentation, reconstruction, etc.--and by providing benchmarks for and diagnostics of trained models by modifying object attributes and scene properties in a controllable manner.	1,0,0,1,0,0
Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations	Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.	1,0,0,1,0,0
The common patterns of abundance: the log series and Zipf's law	In a language corpus, the probability that a word occurs $n$ times is often proportional to $1/n^2$. Assigning rank, $s$, to words according to their abundance, $\log s$ vs $\log n$ typically has a slope of minus one. That simple Zipf's law pattern also arises in the population sizes of cities, the sizes of corporations, and other patterns of abundance. By contrast, for the abundances of different biological species, the probability of a population of size $n$ is typically proportional to $1/n$, declining exponentially for larger $n$, the log series pattern. This article shows that the differing patterns of Zipf's law and the log series arise as the opposing endpoints of a more general theory. The general theory follows from the generic form of all probability patterns as a consequence of conserved average values and the associated invariances of scale. To understand the common patterns of abundance, the generic form of probability distributions plus the conserved average abundance is sufficient. The general theory includes cases that are between the Zipf and log series endpoints, providing a broad framework for analyzing widely observed abundance patterns.	0,0,0,0,1,0
On Compression of Unsupervised Neural Nets by Pruning Weak Connections	Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep Belif Networks(DBNs), are powerful in automatic feature extraction,unsupervised weight initialization and density estimation. In this paper,we demonstrate that the parameters of these neural nets can be dramatically reduced without affecting their performance. We describe a method to reduce the parameters required by RBM which is the basic building block for deep architectures. Further we propose an unsupervised sparse deep architectures selection algorithm to form sparse deep neural networks.Experimental results show that there is virtually no loss in either generative or discriminative performance.	1,0,0,0,0,0
Modeling of networks and globules of charged domain walls observed in pump and pulse induced states	Experiments on optical and STM injection of carriers in layered $\mathrm{MX_2}$ materials revealed the formation of nanoscale patterns with networks and globules of domain walls. This is thought to be responsible for the metallization transition of the Mott insulator and for stabilization of a "hidden" state. In response, here we present studies of the classical charged lattice gas model emulating the superlattice of polarons ubiquitous to the material of choice $1T-\mathrm{TaS_2}$. The injection pulse was simulated by introducing a small random concentration of voids which subsequent evolution was followed by means of Monte Carlo cooling. Below the detected phase transition, the voids gradually coalesce into domain walls forming locally connected globules and then the global network leading to a mosaic fragmentation into domains with different degenerate ground states. The obtained patterns closely resemble the experimental STM visualizations. The surprising aggregation of charged voids is understood by fractionalization of their charges across the walls' lines.	0,1,0,0,0,0
Abstract Interpretation with Unfoldings	We present and evaluate a technique for computing path-sensitive interference conditions during abstract interpretation of concurrent programs. In lieu of fixed point computation, we use prime event structures to compactly represent causal dependence and interference between sequences of transformers. Our main contribution is an unfolding algorithm that uses a new notion of independence to avoid redundant transformer application, thread-local fixed points to reduce the size of the unfolding, and a novel cutoff criterion based on subsumption to guarantee termination of the analysis. Our experiments show that the abstract unfolding produces an order of magnitude fewer false alarms than a mature abstract interpreter, while being several orders of magnitude faster than solver-based tools that have the same precision.	1,0,0,0,0,0
Time-dependent focusing Mean-Field Games: the sub-critical case	We consider time-dependent viscous Mean-Field Games systems in the case of local, decreasing and unbounded coupling. These systems arise in mean-field game theory, and describe Nash equilibria of games with a large number of agents aiming at aggregation. We prove the existence of weak solutions that are minimisers of an associated non-convex functional, by rephrasing the problem in a convex framework. Under additional assumptions involving the growth at infinity of the coupling, the Hamiltonian, and the space dimension, we show that such minimisers are indeed classical solutions by a blow-up argument and additional Sobolev regularity for the Fokker-Planck equation. We exhibit an example of non-uniqueness of solutions. Finally, by means of a contraction principle, we observe that classical solutions exist just by local regularity of the coupling if the time horizon is short.	0,0,1,0,0,0
Multi-resolution polymer Brownian dynamics with hydrodynamic interactions	A polymer model given in terms of beads, interacting through Hookean springs and hydrodynamic forces, is studied. Brownian dynamics description of this bead-spring polymer model is extended to multiple resolutions. Using this multiscale approach, a modeller can efficiently look at different regions of the polymer in different spatial and temporal resolutions with scalings given for the number of beads, statistical segment length and bead radius in order to maintain macro-scale properties of the polymer filament. The Boltzmann distribution of a Gaussian chain for differing statistical segment lengths gives a Langevin equation for the multi-resolution model with a mobility tensor for different bead sizes. Using the pre-averaging approximation, the translational diffusion coefficient is obtained as a function of the inverse of a matrix and then in closed form in the long-chain limit. This is then confirmed with numerical experiments.	0,1,0,0,0,0
Online Multi-Label Classification: A Label Compression Method	Many modern applications deal with multi-label data, such as functional categorizations of genes, image labeling and text categorization. Classification of such data with a large number of labels and latent dependencies among them is a challenging task, and it becomes even more challenging when the data is received online and in chunks. Many of the current multi-label classification methods require a lot of time and memory, which make them infeasible for practical real-world applications. In this paper, we propose a fast linear label space dimension reduction method that transforms the labels into a reduced encoded space and trains models on the obtained pseudo labels. Additionally, it provides an analytical method to update the decoding matrix which maps the labels into the original space and is used during the test phase. Experimental results show the effectiveness of this approach in terms of running times and the prediction performance over different measures.	0,0,0,1,0,0
Riemann-Theta Boltzmann Machine	A general Boltzmann machine with continuous visible and discrete integer valued hidden states is introduced. Under mild assumptions about the connection matrices, the probability density function of the visible units can be solved for analytically, yielding a novel parametric density function involving a ratio of Riemann-Theta functions. The conditional expectation of a hidden state for given visible states can also be calculated analytically, yielding a derivative of the logarithmic Riemann-Theta function. The conditional expectation can be used as activation function in a feedforward neural network, thereby increasing the modelling capacity of the network. Both the Boltzmann machine and the derived feedforward neural network can be successfully trained via standard gradient- and non-gradient-based optimization techniques.	1,0,0,1,0,0
Estimation of a noisy subordinated Brownian Motion via two-scales power variations	High frequency based estimation methods for a semiparametric pure-jump subordinated Brownian motion exposed to a small additive microstructure noise are developed building on the two-scales realized variations approach originally developed by Zhang et. al. (2005) for the estimation of the integrated variance of a continuous Ito process. The proposed estimators are shown to be robust against the noise and, surprisingly, to attain better rates of convergence than their precursors, method of moment estimators, even in the absence of microstructure noise. Our main results give approximate optimal values for the number K of regular sparse subsamples to be used, which is an important tune-up parameter of the method. Finally, a data-driven plug-in procedure is devised to implement the proposed estimators with the optimal K-value. The developed estimators exhibit superior performance as illustrated by Monte Carlo simulations and a real high-frequency data application.	0,0,1,1,0,0
Generalised Seiberg-Witten equations and almost-Hermitian geometry	In this article, we study a generalisation of the Seiberg-Witten equations, replacing the spinor representation with a hyperKahler manifold equipped with certain symmetries. Central to this is the construction of a (non-linear) Dirac operator acting on the sections of the non-linear fibre-bundle. For hyperKahler manifolds admitting a hyperKahler potential, we derive a transformation formula for the Dirac operator under the conformal change of metric on the base manifold. As an application, we show that when the hyperKahler manifold is of dimension four, then away from a singular set, the equations can be expressed as a second order PDE in terms of almost-complex structure on the base manifold and a conformal factor. This extends a result of Donaldson to generalised Seiberg-Witten equations.	0,0,1,0,0,0
The K-Nearest Neighbour UCB algorithm for multi-armed bandits with covariates	In this paper we propose and explore the k-Nearest Neighbour UCB algorithm for multi-armed bandits with covariates. We focus on a setting where the covariates are supported on a metric space of low intrinsic dimension, such as a manifold embedded within a high dimensional ambient feature space. The algorithm is conceptually simple and straightforward to implement. The k-Nearest Neighbour UCB algorithm does not require prior knowledge of the either the intrinsic dimension of the marginal distribution or the time horizon. We prove a regret bound for the k-Nearest Neighbour UCB algorithm which is minimax optimal up to logarithmic factors. In particular, the algorithm automatically takes advantage of both low intrinsic dimensionality of the marginal distribution over the covariates and low noise in the data, expressed as a margin condition. In addition, focusing on the case of bounded rewards, we give corresponding regret bounds for the k-Nearest Neighbour KL-UCB algorithm, which is an analogue of the KL-UCB algorithm adapted to the setting of multi-armed bandits with covariates. Finally, we present empirical results which demonstrate the ability of both the k-Nearest Neighbour UCB and k-Nearest Neighbour KL-UCB to take advantage of situations where the data is supported on an unknown sub-manifold of a high-dimensional feature space.	0,0,0,1,0,0
Relative Singularity Categories	We study the following generalization of singularity categories. Let X be a quasi-projective Gorenstein scheme with isolated singularities and A a non-commutative resolution of singularities of X in the sense of Van den Bergh. We introduce the relative singularity category as the Verdier quotient of the bounded derived category of coherent sheaves on A modulo the category of perfect complexes on X. We view it as a measure for the difference between X and A. The main results of this thesis are the following. (i) We prove an analogue of Orlov's localization result in our setup. If X has isolated singularities, then this reduces the study of the relative singularity categories to the affine case. (ii) We prove Hom-finiteness and idempotent completeness of the relative singularity categories in the complete local situation and determine its Grothendieck group. (iii) We give a complete and explicit description of the relative singularity categories when X has only nodal singularities and the resolution is given by a sheaf of Auslander algebras. (iv) We study relations between relative singularity categories and classical singularity categories. For a simple hypersurface singularity and its Auslander resolution, we show that these categories determine each other. (v) The developed technique leads to the following `purely commutative' application: a description of Iyama & Wemyss triangulated category for rational surface singularities in terms of the singularity category of the rational double point resolution. (vi) We give a description of singularity categories of gentle algebras.	0,0,1,0,0,0
Asymptotic behaviour of the fifth Painlevé transcendents in the space of initial values	We study the asymptotic behaviour of the solutions of the fifth Painlevé equation as the independent variable approaches zero and infinity in the space of initial values. We show that the limit set of each solution is compact and connected and, moreover, that any solution with the essential singularity at zero has an infinite number of poles and zeroes, and any solution with the essential singularity at infinity has infinite number of poles and takes value $1$ infinitely many times.	0,1,1,0,0,0
Deep Multi-User Reinforcement Learning for Distributed Dynamic Spectrum Access	We consider the problem of dynamic spectrum access for network utility maximization in multichannel wireless networks. The shared bandwidth is divided into K orthogonal channels. In the beginning of each time slot, each user selects a channel and transmits a packet with a certain transmission probability. After each time slot, each user that has transmitted a packet receives a local observation indicating whether its packet was successfully delivered or not (i.e., ACK signal). The objective is a multi-user strategy for accessing the spectrum that maximizes a certain network utility in a distributed manner without online coordination or message exchanges between users. Obtaining an optimal solution for the spectrum access problem is computationally expensive in general due to the large state space and partial observability of the states. To tackle this problem, we develop a novel distributed dynamic spectrum access algorithm based on deep multi-user reinforcement leaning. Specifically, at each time slot, each user maps its current state to spectrum access actions based on a trained deep-Q network used to maximize the objective function. Game theoretic analysis of the system dynamics is developed for establishing design principles for the implementation of the algorithm. Experimental results demonstrate strong performance of the algorithm.	1,0,0,0,0,0
Cavitation near the oscillating piezoelectric plate in water	It is known that gas bubbles on the surface bounding a fluid flow can change the coefficient of friction and affect the parameters of the boundary layer. In this paper, we propose a method that allows us to create, in the near-wall region, a thin layer of liquid filled with bubbles. It will be shown that if there is an oscillating piezoelectric plate on the surface bounding a liquid, then, under certain conditions, cavitation develops in the boundary layer. The relationship between the parameters of cavitation and the characteristics of the piezoelectric plate oscillations is obtained. Possible applications are discussed.	0,1,0,0,0,0
Superradiance with local phase-breaking effects	We study the superradiant evolution of a set of $N$ two-level systems spontaneously radiating under the effect of phase-breaking mechanisms. We investigate the dynamics generated by non-radiative losses and pure dephasing, and their interplay with spontaneous emission. Our results show that in the parameter region relevant to many solid-state cavity quantum electrodynamics experiments, even with a dephasing rate much faster than the radiative lifetime of a single two-level system, a sub-optimal collective superfluorescent burst is still observable. We also apply our theory to the dilute excitation regime, often used to describe optical excitations in solid-state systems. In this regime, excitations can be described in terms of bright and dark bosonic quasiparticles. We show how the effect of dephasing and losses in this regime translates into inter-mode scattering rates and quasiparticle lifetimes.	0,1,0,0,0,0
GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium	Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fréchet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.	1,0,0,1,0,0
Information Directed Sampling for Stochastic Bandits with Graph Feedback	We consider stochastic multi-armed bandit problems with graph feedback, where the decision maker is allowed to observe the neighboring actions of the chosen action. We allow the graph structure to vary with time and consider both deterministic and Erdős-Rényi random graph models. For such a graph feedback model, we first present a novel analysis of Thompson sampling that leads to tighter performance bound than existing work. Next, we propose new Information Directed Sampling based policies that are graph-aware in their decision making. Under the deterministic graph case, we establish a Bayesian regret bound for the proposed policies that scales with the clique cover number of the graph instead of the number of actions. Under the random graph case, we provide a Bayesian regret bound for the proposed policies that scales with the ratio of the number of actions over the expected number of observations per iteration. To the best of our knowledge, this is the first analytical result for stochastic bandits with random graph feedback. Finally, using numerical evaluations, we demonstrate that our proposed IDS policies outperform existing approaches, including adaptions of upper confidence bound, $\epsilon$-greedy and Exp3 algorithms.	1,0,0,1,0,0
Edge-Based Recognition of Novel Objects for Robotic Grasping	In this paper, we investigate the problem of grasping novel objects in unstructured environments. To address this problem, consideration of the object geometry, reachability and force closure analysis are required. We propose a framework for grasping unknown objects by localizing contact regions on the contours formed by a set of depth edges in a single view 2D depth image. According to the edge geometric features obtained from analyzing the data of the depth map, the contact regions are determined. Finally,We validate the performance of the approach by applying it to the scenes with both single and multiple objects, using Baxter manipulator.	1,0,0,0,0,0
A note on integrating products of linear forms over the unit simplex	Integrating a product of linear forms over the unit simplex can be done in polynomial time if the number of variables n is fixed (V. Baldoni et al., 2011). In this note, we highlight that this problem is equivalent to obtaining the normalizing constant of state probabilities for a popular class of Markov processes used in queueing network theory. In light of this equivalence, we survey existing computational algorithms developed in queueing theory that can be used for exact integration. For example, under some regularity conditions, queueing theory algorithms can exactly integrate a product of linear forms of total degree N by solving N systems of linear equations.	1,0,1,0,0,0
Detection of planet candidates around K giants, HD 40956, HD 111591, and HD 113996	Aims. The purpose of this paper is to detect and investigate the nature of long-term radial velocity (RV) variations of K-type giants and to confirm planetary companions around the stars. Methods. We have conducted two planet search programs by precise RV measurement using the 1.8 m telescope at Bohyunsan Optical Astronomy Observatory (BOAO) and the 1.88 m telescope at Okayama Astrophysical Observatory (OAO). The BOAO program searches for planets around 55 early K giants. The OAO program is looking for 190 G-K type giants. Results. In this paper, we report the detection of long-period RV variations of three K giant stars, HD 40956, HD 111591, and HD 113996. We investigated the cause of the observed RV variations and conclude the substellar companions are most likely the cause of the RV variations. The orbital analyses yield P = 578.6 $\pm$ 3.3 d, $m$ sin $i$ = 2.7 $\pm$ 0.6 $M_{\rm{J}}$, $a$ = 1.4 $\pm$ 0.1 AU for HD 40956; P = 1056.4 $\pm$ 14.3 d, $m$ sin $i$ = 4.4 $\pm$ 0.4 $M_{\rm{J}}$, $a$ = 2.5 $\pm$ 0.1 AU for HD 111591; P = 610.2 $\pm$ 3.8 d, $m$ sin $i$ = 6.3 $\pm$ 1.0 $M_{\rm{J}}$, $a$ = 1.6 $\pm$ 0.1 AU for HD 113996.	0,1,0,0,0,0
Sobolev GAN	We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure $\mu$. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure $\mu$ plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramér statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.	1,0,0,1,0,0
A sparse linear algebra algorithm for fast computation of prediction variances with Gaussian Markov random fields	Gaussian Markov random fields are used in a large number of disciplines in machine vision and spatial statistics. The models take advantage of sparsity in matrices introduced through the Markov assumptions, and all operations in inference and prediction use sparse linear algebra operations that scale well with dimensionality. Yet, for very high-dimensional models, exact computation of predictive variances of linear combinations of variables is generally computationally prohibitive, and approximate methods (generally interpolation or conditional simulation) are typically used instead. A set of conditions are established under which the variances of linear combinations of random variables can be computed exactly using the Takahashi recursions. The ensuing computational simplification has wide applicability and may be used to enhance several software packages where model fitting is seated in a maximum-likelihood framework. The resulting algorithm is ideal for use in a variety of spatial statistical applications, including \emph{LatticeKrig} modelling, statistical downscaling, and fixed rank kriging. It can compute hundreds of thousands exact predictive variances of linear combinations on a standard desktop with ease, even when large spatial GMRF models are used.	0,0,0,1,0,0
Step evolution in two-dimensional diblock copolymer films	The formation and dynamics of free-surface structures, such as steps or terraces and their interplay with the phase separation in the bulk are key features of diblock copolymer films. We present a phase-field model with an obstacle potential which follows naturally from derivations of the Ohta-Kawasaki energy functional via self-consistent field theory. The free surface of the film is incorporated into the phase-field model by including a third phase for the void. The resulting model and its sharp interface limit are shown to capture the energetics of films with steps in two dimensions. For this model, we then develop a numerical approach that is capable of resolving the long-time complex free-surface structures that arise in diblock copolymer films.	0,1,0,0,0,0
Dynamic Task Allocation for Crowdsourcing Settings	We consider the problem of optimal budget allocation for crowdsourcing problems, allocating users to tasks to maximize our final confidence in the crowdsourced answers. Such an optimized worker assignment method allows us to boost the efficacy of any popular crowdsourcing estimation algorithm. We consider a mutual information interpretation of the crowdsourcing problem, which leads to a stochastic subset selection problem with a submodular objective function. We present experimental simulation results which demonstrate the effectiveness of our dynamic task allocation method for achieving higher accuracy, possibly requiring fewer labels, as well as improving upon a previous method which is sensitive to the proportion of users to questions.	1,0,0,1,0,0
Shape optimization in laminar flow with a label-guided variational autoencoder	Computational design optimization in fluid dynamics usually requires to solve non-linear partial differential equations numerically. In this work, we explore a Bayesian optimization approach to minimize an object's drag coefficient in laminar flow based on predicting drag directly from the object shape. Jointly training an architecture combining a variational autoencoder mapping shapes to latent representations and Gaussian process regression allows us to generate improved shapes in the two dimensional case we consider.	1,0,0,0,0,0
Geometry and Arithmetic of Crystallographic Sphere Packings	We introduce the notion of a "crystallographic sphere packing," defined to be one whose limit set is that of a geometrically finite hyperbolic reflection group in one higher dimension. We exhibit for the first time an infinite family of conformally-inequivalent such with all radii being reciprocals of integers. We then prove a result in the opposite direction: the "superintegral" ones exist only in finitely many "commensurability classes," all in dimensions below 30.	0,0,1,0,0,0
Training GANs with Optimism	We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.	1,0,0,1,0,0
Gamma-ray bursts and their relation to astroparticle physics and cosmology	This article gives an overview of gamma-ray bursts (GRBs) and their relation to astroparticle physics and cosmology. GRBs are the most powerful explosions in the universe that occur roughly once per day and are characterized by flashes of gamma-rays typically lasting from a fraction of a second to thousands of seconds. Even after more than four decades since their discovery they still remain not fully understood. Two types of GRBs are observed: spectrally harder short duration bursts and softer long duration bursts. The long GRBs originate from the collapse of massive stars whereas the preferred model for the short GRBs is coalescence of compact objects such as two neutron stars or a neutron star and a black hole. There were suggestions that GRBs can produce ultra-high energy cosmic rays and neutrinos. Also a certain sub-type of GRBs may serve as a new standard candle that can help constrain and measure the cosmological parameters to much higher redshift than what was possible so far. I will review the recent experimental observations.	0,1,0,0,0,0
Unsupervised Body Part Regression via Spatially Self-ordering Convolutional Neural Networks	Automatic body part recognition for CT slices can benefit various medical image applications. Recent deep learning methods demonstrate promising performance, with the requirement of large amounts of labeled images for training. The intrinsic structural or superior-inferior slice ordering information in CT volumes is not fully exploited. In this paper, we propose a convolutional neural network (CNN) based Unsupervised Body part Regression (UBR) algorithm to address this problem. A novel unsupervised learning method and two inter-sample CNN loss functions are presented. Distinct from previous work, UBR builds a coordinate system for the human body and outputs a continuous score for each axial slice, representing the normalized position of the body part in the slice. The training process of UBR resembles a self-organization process: slice scores are learned from inter-slice relationships. The training samples are unlabeled CT volumes that are abundant, thus no extra annotation effort is needed. UBR is simple, fast, and accurate. Quantitative and qualitative experiments validate its effectiveness. In addition, we show two applications of UBR in network initialization and anomaly detection.	1,0,0,0,0,0
Generalizations of the 'Linear Chain Trick': Incorporating more flexible dwell time distributions into mean field ODE models	Mathematical modelers have long known of a "rule of thumb" referred to as the Linear Chain Trick (LCT; aka the Gamma Chain Trick): a technique used to construct mean field ODE models from continuous-time stochastic state transition models where the time an individual spends in a given state (i.e., the dwell time) is Erlang distributed (i.e., gamma distributed with integer shape parameter). Despite the LCT's widespread use, we lack general theory to facilitate the easy application of this technique, especially for complex models. This has forced modelers to choose between constructing ODE models using heuristics with oversimplified dwell time assumptions, using time consuming derivations from first principles, or to instead use non-ODE models (like integro-differential equations or delay differential equations) which can be cumbersome to derive and analyze. Here, we provide analytical results that enable modelers to more efficiently construct ODE models using the LCT or related extensions. Specifically, we 1) provide novel extensions of the LCT to various scenarios found in applications; 2) provide formulations of the LCT and it's extensions that bypass the need to derive ODEs from integral or stochastic model equations; and 3) introduce a novel Generalized Linear Chain Trick (GLCT) framework that extends the LCT to a much broader family of distributions, including the flexible phase-type distributions which can approximate distributions on $\mathbb{R}^+$ and be fit to data. These results give modelers more flexibility to incorporate appropriate dwell time assumptions into mean field ODEs, including conditional dwell time distributions, and these results help clarify connections between individual-level stochastic model assumptions and the structure of corresponding mean field ODEs.	0,0,0,0,1,0
Stable absorbing boundary conditions for molecular dynamics in general domains	A new type of absorbing boundary conditions for molecular dynamics simulations are presented. The exact boundary conditions for crystalline solids with harmonic approximation are expressed as a dynamic Dirichlet- to-Neumann (DtN) map. It connects the displacement of the atoms at the boundary to the traction on these atoms. The DtN map is valid for a domain with general geometry. To avoid evaluating the time convo- lution of the dynamic DtN map, we approximate the associated kernel function by rational functions in the Laplace domain. The parameters in the approximations are determined by interpolations. The explicit forms of the zeroth, first, and second order approximations will be presented. The stability of the molecular dynamics model, supplemented with these absorbing boundary conditions is established. Two numerical simulations are performed to demonstrate the effectiveness of the methods.	0,1,0,0,0,0
Simulating Dirac models with ultracold atoms in optical lattices	We present a general model allowing "quantum simulation" of one-dimensional Dirac models with 2- and 4-component spinors using ultracold atoms in driven 1D tilted optical latices. The resulting Dirac physics is illustrated by one of its well-known manifestations, Zitterbewegung. This general model can be extended and applied with great flexibility to more complex situations.	0,1,0,0,0,0
Diversity of preferences can increase collective welfare in sequential exploration problems	In search engines, online marketplaces and other human-computer interfaces large collectives of individuals sequentially interact with numerous alternatives of varying quality. In these contexts, trial and error (exploration) is crucial for uncovering novel high-quality items or solutions, but entails a high cost for individual users. Self-interested decision makers, are often better off imitating the choices of individuals who have already incurred the costs of exploration. Although imitation makes sense at the individual level, it deprives the group of additional information that could have been gleaned by individual explorers. In this paper we show that in such problems, preference diversity can function as a welfare enhancing mechanism. It leads to a consistent increase in the quality of the consumed alternatives that outweighs the increased cost of search for the users.	1,0,0,0,0,0
Fine cophasing of segmented aperture telescopes with ZELDA, a Zernike wavefront sensor in the diffraction-limited regime	Segmented aperture telescopes require an alignment procedure with successive steps from coarse alignment to monitoring process in order to provide very high optical quality images for stringent science operations such as exoplanet imaging. The final step, referred to as fine phasing, calls for a high sensitivity wavefront sensing and control system in a diffraction-limited regime to achieve segment alignment with nanometric accuracy. In this context, Zernike wavefront sensors represent promising options for such a calibration. A concept called the Zernike unit for segment phasing (ZEUS) was previously developed for ground-based applications to operate under seeing-limited images. Such a concept is, however, not suitable for fine cophasing with diffraction-limited images. We revisit ZELDA, a Zernike sensor that was developed for the measurement of residual aberrations in exoplanet direct imagers, to measure segment piston, tip, and tilt in the diffraction-limited regime. We introduce a novel analysis scheme of the sensor signal that relies on piston, tip, and tilt estimators for each segment, and provide probabilistic insights to predict the success of a closed-loop correction as a function of the initial wavefront error. The sensor unambiguously and simultaneously retrieves segment piston and tip-tilt misalignment. Our scheme allows for correction of these errors in closed-loop operation down to nearly zero residuals in a few iterations. This sensor also shows low sensitivity to misalignment of its parts and high ability for operation with a relatively bright natural guide star. Our cophasing sensor relies on existing mask technologies that make the concept already available for segmented apertures in future space missions.	0,1,0,0,0,0
Phase diagram of hydrogen and a hydrogen-helium mixture at planetary conditions by Quantum Monte Carlo simulations	Understanding planetary interiors is directly linked to our ability of simulating exotic quantum mechanical systems such as hydrogen (H) and hydrogen-helium (H-He) mixtures at high pressures and temperatures. Equations of State (EOSs) tables based on Density Functional Theory (DFT), are commonly used by planetary scientists, although this method allows only for a qualitative description of the phase diagram, due to an incomplete treatment of electronic interactions. Here we report Quantum Monte Carlo (QMC) molecular dynamics simulations of pure H and H-He mixture. We calculate the first QMC EOS at 6000 K for an H-He mixture of a proto-solar composition, and show the crucial influence of He on the H metallization pressure. Our results can be used to calibrate other EOS calculations and are very timely given the accurate determination of Jupiter's gravitational field from the NASA Juno mission and the effort to determine its structure.	0,1,0,0,0,0
Parallel implementation of a vehicle rail dynamical model for multi-core systems	This research presents a model of a complex dynamic object running on a multi-core system. Discretization and numerical integration for multibody models of vehicle rail elements in the vertical longitudinal plane fluctuations is considered. The implemented model and solution of the motion differential equations allow estimating the basic processes occurring in the system with various external influences. Hence the developed programming model can be used for performing analysis and comparing new vehicle designs. Keywords-dynamic model; multi-core system; SMP system; rolling stock.	1,0,0,0,0,0
Physics-Informed Regularization of Deep Neural Networks	This paper presents a novel physics-informed regularization method for training of deep neural networks (DNNs). In particular, we focus on the DNN representation for the response of a physical or biological system, for which a set of governing laws are known. These laws often appear in the form of differential equations, derived from first principles, empirically-validated laws, and/or domain expertise. We propose a DNN training approach that utilizes these known differential equations in addition to the measurement data, by introducing a penalty term to the training loss function to penalize divergence form the governing laws. Through three numerical examples, we will show that the proposed regularization produces surrogates that are physically interpretable with smaller generalization errors, when compared to other common regularization methods.	1,0,0,0,0,0
Strong Functional Representation Lemma and Applications to Coding Theorems	This paper shows that for any random variables $X$ and $Y$, it is possible to represent $Y$ as a function of $(X,Z)$ such that $Z$ is independent of $X$ and $I(X;Z|Y)\le\log(I(X;Y)+1)+4$ bits. We use this strong functional representation lemma (SFRL) to establish a bound on the rate needed for one-shot exact channel simulation for general (discrete or continuous) random variables, strengthening the results by Harsha et al. and Braverman and Garg, and to establish new and simple achievability results for one-shot variable-length lossy source coding, multiple description coding and Gray-Wyner system. We also show that the SFRL can be used to reduce the channel with state noncausally known at the encoder to a point-to-point channel, which provides a simple achievability proof of the Gelfand-Pinsker theorem.	1,0,1,0,0,0
Quantifying Performance of Bipedal Standing with Multi-channel EMG	Spinal cord stimulation has enabled humans with motor complete spinal cord injury (SCI) to independently stand and recover some lost autonomic function. Quantifying the quality of bipedal standing under spinal stimulation is important for spinal rehabilitation therapies and for new strategies that seek to combine spinal stimulation and rehabilitative robots (such as exoskeletons) in real time feedback. To study the potential for automated electromyography (EMG) analysis in SCI, we evaluated the standing quality of paralyzed patients undergoing electrical spinal cord stimulation using both video and multi-channel surface EMG recordings during spinal stimulation therapy sessions. The quality of standing under different stimulation settings was quantified manually by experienced clinicians. By correlating features of the recorded EMG activity with the expert evaluations, we show that multi-channel EMG recording can provide accurate, fast, and robust estimation for the quality of bipedal standing in spinally stimulated SCI patients. Moreover, our analysis shows that the total number of EMG channels needed to effectively predict standing quality can be reduced while maintaining high estimation accuracy, which provides more flexibility for rehabilitation robotic systems to incorporate EMG recordings.	1,0,0,1,0,0
Case Studies on Plasma Wakefield Accelerator Design	The field of plasma-based particle accelerators has seen tremendous progress over the past decade and experienced significant growth in the number of activities. During this process, the involved scientific community has expanded from traditional university-based research and is now encompassing many large research laboratories worldwide, such as BNL, CERN, DESY, KEK, LBNL and SLAC. As a consequence, there is a strong demand for a consolidated effort in education at the intersection of accelerator, laser and plasma physics. The CERN Accelerator School on Plasma Wake Acceleration has been organized as a result of this development. In this paper, we describe the interactive component of this one-week school, which consisted of three case studies to be solved in 11 working groups by the participants of the CERN Accelerator School.	0,1,0,0,0,0
Adiabatic Quantum Computing for Binary Clustering	Quantum computing for machine learning attracts increasing attention and recent technological developments suggest that especially adiabatic quantum computing may soon be of practical interest. In this paper, we therefore consider this paradigm and discuss how to adopt it to the problem of binary clustering. Numerical simulations demonstrate the feasibility of our approach and illustrate how systems of qubits adiabatically evolve towards a solution.	0,0,0,1,0,0
On the conformal duality between constant mean curvature surfaces in $\mathbb{E}(κ,τ)$ and $\mathbb{L}(κ,τ)$	The main aim of this survey paper is to gather together some results concerning the Calabi type duality discovered by Hojoo Lee between certain families of (spacelike) graphs with constant mean curvature in Riemannian and Lorentzian homogeneous 3-manifolds with isometry group of dimension 4. The duality is conformal and swaps mean curvature and bundle curvature, and we will revisit it by giving a more general statement in terms of conformal immersions. This will show that some features in the theory of surfaces with mean curvature $\frac{1}{2}$ in $\mathbb{H}^2\times\mathbb{R}$ or minimal surfaces in the Heisenberg space have nice geometric interpretations in terms of their dual Lorentzian counterparts. We will briefly discuss some applications such as gradient estimates for entire minimal graphs in Heisenberg space or the existence of complete spacelike surfaces, and we will also give an uniform treatment to the behavior of the duality with respect to ambient isometries. Finally, some open questions are posed in the last section.	0,0,1,0,0,0
Accelerating Prototype-Based Drug Discovery using Conditional Diversity Networks	Designing a new drug is a lengthy and expensive process. As the space of potential molecules is very large (10^23-10^60), a common technique during drug discovery is to start from a molecule which already has some of the desired properties. An interdisciplinary team of scientists generates hypothesis about the required changes to the prototype. In this work, we develop an algorithmic unsupervised-approach that automatically generates potential drug molecules given a prototype drug. We show that the molecules generated by the system are valid molecules and significantly different from the prototype drug. Out of the compounds generated by the system, we identified 35 FDA-approved drugs. As an example, our system generated Isoniazid - one of the main drugs for Tuberculosis. The system is currently being deployed for use in collaboration with pharmaceutical companies to further analyze the additional generated molecules.	0,0,0,1,0,0
An Executable Specification of Typing Rules for Extensible Records based on Row Polymorphism	Type inference is an application domain that is a natural fit for logic programming (LP). LP systems natively support unification, which serves as a basic building block of typical type inference algorithms. In particular, polymorphic type inference in the Hindley--Milner type system (HM) can be succinctly specified and executed in Prolog. In our previous work, we have demonstrated that more advanced features of parametric polymorphism beyond HM, such as type-constructor polymorphism and kind polymorphism, can be similarly specified in Prolog. Here, we demonstrate a specification for records, which is one of the most widely supported compound data structures in real-world programming languages, and discuss the advantages and limitations of Prolog as a specification language for type systems. Record types are specified as order-irrelevant collections of named fields mapped to their corresponding types. In addition, an open-ended collection is used to support row polymorphism for record types to be extensible.	1,0,0,0,0,0
A Hilbert Space of Stationary Ergodic Processes	Identifying meaningful signal buried in noise is a problem of interest arising in diverse scenarios of data-driven modeling. We present here a theoretical framework for exploiting intrinsic geometry in data that resists noise corruption, and might be identifiable under severe obfuscation. Our approach is based on uncovering a valid complete inner product on the space of ergodic stationary finite valued processes, providing the latter with the structure of a Hilbert space on the real field. This rigorous construction, based on non-standard generalizations of the notions of sum and scalar multiplication of finite dimensional probability vectors, allows us to meaningfully talk about "angles" between data streams and data sources, and, make precise the notion of orthogonal stochastic processes. In particular, the relative angles appear to be preserved, and identifiable, under severe noise, and will be developed in future as the underlying principle for robust classification, clustering and unsupervised featurization algorithms.	0,0,0,1,0,1
Opinion diversity and community formation in adaptive networks	It is interesting and of significant importance to investigate how network structures co-evolve with opinions. The existing models of such co-evolution typically lead to the final states where network nodes either reach a global consensus or break into separated communities, each of which holding its own community consensus. Such results, however, can hardly explain the richness of real-life observations that opinions are always diversified with no global or even community consensus, and people seldom, if not never, totally cut off themselves from dissenters. In this article, we show that, a simple model integrating consensus formation, link rewiring and opinion change allows complex system dynamics to emerge, driving the system into a dynamic equilibrium with co-existence of diversified opinions. Specifically, similar opinion holders may form into communities yet with no strict community consensus; and rather than being separated into disconnected communities, different communities remain to be interconnected by non-trivial proportion of inter-community links. More importantly, we show that the complex dynamics may lead to different numbers of communities at steady state with a given tolerance between different opinion holders. We construct a framework for theoretically analyzing the co-evolution process. Theoretical analysis and extensive simulation results reveal some useful insights into the complex co-evolution process, including the formation of dynamic equilibrium, the phase transition between different steady states with different numbers of communities, and the dynamics between opinion distribution and network modularity, etc.	1,1,0,0,0,0
Transverse-spin correlations of the random transverse-field Ising model	The critical behavior of the random transverse-field Ising model in finite dimensional lattices is governed by infinite disorder fixed points, several properties of which have already been calculated by the use of the strong disorder renormalization group (SDRG) method. Here we extend these studies and calculate the connected transverse-spin correlation function by a numerical implementation of the SDRG method in $d=1,2$ and $3$ dimensions. At the critical point an algebraic decay of the form $\sim r^{-\eta_t}$ is found, with a decay exponent being approximately $\eta_t \approx 2+2d$. In $d=1$ the results are related to dimer-dimer correlations in the random AF XX-chain and have been tested by numerical calculations using free-fermionic techniques.	0,1,0,0,0,0
ALMA Observations of Starless Core Substructure in Ophiuchus	Compact substructure is expected to arise in a starless core as mass becomes concentrated in the central region likely to form a protostar. Additionally, multiple peaks may form if fragmentation occurs. We present ALMA Cycle 2 observations of 60 starless and protostellar cores in the Ophiuchus molecular cloud. We detect eight compact substructures which are >15 arcsec from the nearest Spitzer YSO. Only one of these has strong evidence for being truly starless after considering ancillary data, e.g., from Herschel and X-ray telescopes. An additional extended emission structure has tentative evidence for starlessness. The number of our detections is consistent with estimates from a combination of synthetic observations of numerical simulations and analytical arguments. This result suggests that a similar ALMA study in the Chamaeleon I cloud, which detected no compact substructure in starless cores, may be due to the peculiar evolutionary state of cores in that cloud.	0,1,0,0,0,0
Real eigenvalues of a non-self-adjoint perturbation of the self-adjoint Zakharov-Shabat operator	We study the eigenvalues of the self-adjoint Zakharov-Shabat operator corresponding to the defocusing nonlinear Schrodinger equation in the inverse scattering method. Real eigenvalues exist when the square of the potential has a simple well. We derive two types of quantization condition for the eigenvalues by using the exact WKB method, and show that the eigenvalues stay real for a sufficiently small non-self-adjoint perturbation when the potential has some PT-like symmetry.	0,0,1,0,0,0
Combinatorial formulas for Kazhdan-Lusztig polynomials with respect to W-graph ideals	In \cite{y1} Yin generalized the definition of $W$-graph ideal $E_J$ in weighted Coxeter groups and introduced the weighted Kazhdan-Lusztig polynomials $ \left \{ P_{x,y} \mid x,y\in E_J\right \}$, where $J$ is a subset of simple generators $S$. In this paper, we study the combinatorial formulas for those polynomials, which extend the results of Deodhar \cite{v3} and Tagawa \cite{h1}.	0,0,1,0,0,0
Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Network	Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus on addressing audio information only. In this work, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model. In the proposed AVDCNN SE model, audio and visual data are first processed using individual CNNs, and then, fused into a joint network to generate enhanced speech at the output layer. The AVDCNN model is trained in an end-to-end manner, and parameters are jointly learned through back-propagation. We evaluate enhanced speech using five objective criteria. Results show that the AVDCNN yields notably better performance, compared with an audio-only CNN-based SE model and two conventional SE approaches, confirming the effectiveness of integrating visual information into the SE process.	1,0,0,1,0,0
Ground state sign-changing solutions for a class of nonlinear fractional Schrödinger-Poisson system in $\mathbb{R}^{3}$	In this paper, we are concerned with the existence of the least energy sign-changing solutions for the following fractional Schrödinger-Poisson system: \begin{align*} \left\{ \begin{aligned} &(-\Delta)^{s} u+V(x)u+\lambda\phi(x)u=f(x, u),\quad &\text{in}\, \ \mathbb{R}^{3},\\ &(-\Delta)^{t}\phi=u^{2},& \text{in}\,\ \mathbb{R}^{3}, \end{aligned} \right. \end{align*} where $\lambda\in \mathbb{R}^{+}$ is a parameter, $s, t\in (0, 1)$ and $4s+2t>3$, $(-\Delta)^{s}$ stands for the fractional Laplacian. By constraint variational method and quantitative deformation lemma, we prove that the above problem has one least energy sign-changing solution. Moreover, for any $\lambda>0$, we show that the energy of the least energy sign-changing solutions is strictly larger than two times the ground state energy. Finally, we consider $\lambda$ as a parameter and study the convergence property of the least energy sign-changing solutions as $\lambda\searrow 0$.	0,0,1,0,0,0
Decentralized Connectivity-Preserving Deployment of Large-Scale Robot Swarms	We present a decentralized and scalable approach for deployment of a robot swarm. Our approach tackles scenarios in which the swarm must reach multiple spatially distributed targets, and enforce the constraint that the robot network cannot be split. The basic idea behind our work is to construct a logical tree topology over the physical network formed by the robots. The logical tree acts as a backbone used by robots to enforce connectivity constraints. We study and compare two algorithms to form the logical tree: outwards and inwards. These algorithms differ in the order in which the robots join the tree: the outwards algorithm starts at the tree root and grows towards the targets, while the inwards algorithm proceeds in the opposite manner. Both algorithms perform periodic reconfiguration, to prevent suboptimal topologies from halting the growth of the tree. Our contributions are (i) The formulation of the two algorithms; (ii) A comparison of the algorithms in extensive physics-based simulations; (iii) A validation of our findings through real-robot experiments.	1,0,0,0,0,0
Service Providers of the Sharing Economy: Who Joins and Who Benefits?	Many "sharing economy" platforms, such as Uber and Airbnb, have become increasingly popular, providing consumers with more choices and suppliers a chance to make profit. They, however, have also brought about emerging issues regarding regulation, tax obligation, and impact on urban environment, and have generated heated debates from various interest groups. Empirical studies regarding these issues are limited, partly due to the unavailability of relevant data. Here we aim to understand service providers of the sharing economy, investigating who joins and who benefits, using the Airbnb market in the United States as a case study. We link more than 211 thousand Airbnb listings owned by 188 thousand hosts with demographic, socio-economic status (SES), housing, and tourism characteristics. We show that income and education are consistently the two most influential factors that are linked to the joining of Airbnb, regardless of the form of participation or year. Areas with lower median household income, or higher fraction of residents who have Bachelor's and higher degrees, tend to have more hosts. However, when considering the performance of listings, as measured by number of newly received reviews, we find that income has a positive effect for entire-home listings; listings located in areas with higher median household income tend to have more new reviews. Our findings demonstrate empirically that the disadvantage of SES-disadvantaged areas and the advantage of SES-advantaged areas may be present in the sharing economy.	1,0,0,0,0,0
Weak quadrupole moments	Collective effects in deformed atomic nuclei present possible avenues of study on the non-spherical distribution of neutrons and the violation of the local Lorentz invariance. We introduce the weak quadrupole moment of nuclei, related to the quadrupole distribution of the weak charge in the nucleus. The weak quadrupole moment produces tensor weak interaction between the nucleus and electrons and can be observed in atomic and molecular experiments measuring parity nonconservation. The dominating contribution to the weak quadrupole is given by the quadrupole moment of the neutron distribution, therefore, corresponding experiments should allow one to measure the neutron quadrupoles. Using the deformed oscillator model and the Schmidt model we calculate the quadrupole distributions of neutrons, $Q_{n}$, the weak quadrupole moments ,$Q_{W}^{(2)}$, and the Lorentz Innvariance violating energy shifts in $^{9}$Be, $^{21}$Ne , $^{27}$Al, $^{131}$Xe, $^{133}$Cs, $^{151}$Eu, $^{153}$Eu, $^{163}$Dy, $^{167}$Er, $^{173}$Yb, $^{177}$Hf, $^{179}$Hf, $^{181}$Ta, $^{201}$Hg and $^{229}$Th.	0,1,0,0,0,0
Optimization of Wireless Power Transfer Systems Enhanced by Passive Elements and Metasurfaces	This paper presents a rigorous optimization technique for wireless power transfer (WPT) systems enhanced by passive elements, ranging from simple reflectors and intermedi- ate relays all the way to general electromagnetic guiding and focusing structures, such as metasurfaces and metamaterials. At its core is a convex semidefinite relaxation formulation of the otherwise nonconvex optimization problem, of which tightness and optimality can be confirmed by a simple test of its solutions. The resulting method is rigorous, versatile, and general -- it does not rely on any assumptions. As shown in various examples, it is able to efficiently and reliably optimize such WPT systems in order to find their physical limitations on performance, optimal operating parameters and inspect their working principles, even for a large number of active transmitters and passive elements.	1,0,1,0,0,0
A unified thermostat scheme for efficient configurational sampling for classical/quantum canonical ensembles via molecular dynamics	We show a unified second-order scheme for constructing simple, robust and accurate algorithms for typical thermostats for configurational sampling for the canonical ensemble. When Langevin dynamics is used, the scheme leads to the BAOAB algorithm that has been recently investigated. We show that the scheme is also useful for other types of thermostat, such as the Andersen thermostat and Nosé-Hoover chain. Two 1-dimensional models and three typical realistic molecular systems that range from the gas phase, clusters, to the condensed phase are used in numerical examples for demonstration. Accuracy may be increased by an order of magnitude for estimating coordinate-dependent properties in molecular dynamics (when the same time interval is used), irrespective of which type of thermostat is applied. The scheme is especially useful for path integral molecular dynamics, because it consistently improves the efficiency for evaluating all thermodynamic properties for any type of thermostat.	0,1,0,0,0,0
On methods to determine bounds on the Q-factor for a given directivity	This paper revisit and extend the interesting case of bounds on the Q-factor for a given directivity for a small antenna of arbitrary shape. A higher directivity in a small antenna is closely connected with a narrow impedance bandwidth. The relation between bandwidth and a desired directivity is still not fully understood, not even for small antennas. Initial investigations in this direction has related the radius of a circumscribing sphere to the directivity, and bounds on the Q-factor has also been derived for a partial directivity in a given direction. In this paper we derive lower bounds on the Q-factor for a total desired directivity for an arbitrarily shaped antenna in a given direction as a convex problem using semi-definite relaxation techniques (SDR). We also show that the relaxed solution is also a solution of the original problem of determining the lower Q-factor bound for a total desired directivity. SDR can also be used to relax a class of other interesting non-convex constraints in antenna optimization such as tuning, losses, front-to-back ratio. We compare two different new methods to determine the lowest Q-factor for arbitrary shaped antennas for a given total directivity. We also compare our results with full EM-simulations of a parasitic element antenna with high directivity.	0,1,1,0,0,0
Compression of Deep Neural Networks for Image Instance Retrieval	Image instance retrieval is the problem of retrieving images from a database which contain the same object. Convolutional Neural Network (CNN) based descriptors are becoming the dominant approach for generating {\it global image descriptors} for the instance retrieval problem. One major drawback of CNN-based {\it global descriptors} is that uncompressed deep neural network models require hundreds of megabytes of storage making them inconvenient to deploy in mobile applications or in custom hardware. In this work, we study the problem of neural network model compression focusing on the image instance retrieval task. We study quantization, coding, pruning and weight sharing techniques for reducing model size for the instance retrieval problem. We provide extensive experimental results on the trade-off between retrieval performance and model size for different types of networks on several data sets providing the most comprehensive study on this topic. We compress models to the order of a few MBs: two orders of magnitude smaller than the uncompressed models while achieving negligible loss in retrieval performance.	1,0,0,0,0,0
Mask R-CNN	We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL	1,0,0,0,0,0
Tetramer Bound States in Heteronuclear Systems	We calculate the universal spectrum of trimer and tetramer states in heteronuclear mixtures of ultracold atoms with different masses in the vicinity of the heavy-light dimer threshold. To extract the energies, we solve the three- and four-body problem for simple two- and three-body potentials tuned to the universal region using the Gaussian expansion method. We focus on the case of one light particle of mass $m$ and two or three heavy bosons of mass $M$ with resonant heavy-light interactions. We find that trimer and tetramer cross into the heavy-light dimer threshold at almost the same point and that as the mass ratio $M/m$ decreases, the distance between the thresholds for trimer and tetramer states becomes smaller. We also comment on the possibility of observing exotic three-body states consisting of a dimer and two atoms in this region and compare with previous work.	0,1,0,0,0,0
K-theory of group Banach algebras and Banach property RD	We investigate Banach algebras of convolution operators on the $L^p$ spaces of a locally compact group, and their K-theory. We show that for a discrete group, the corresponding K-theory groups depend continuously on $p$ in an inductive sense. Via a Banach version of property RD, we show that for a large class of groups, the K-theory groups of the Banach algebras are independent of $p$.	0,0,1,0,0,0
Homotopy Parametric Simplex Method for Sparse Learning	High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper, we are interested in a broad class of sparse learning approaches formulated as linear programs parametrized by a {\em regularization factor}, and solve them by the parametric simplex method (PSM). Our parametric simplex method offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations, and the solution sparsity significantly reduces the computational cost per iteration. Particularly, we demonstrate the superiority of PSM over various sparse learning approaches, including Dantzig selector for sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME for sparse precision matrix estimation, sparse differential network estimation, and sparse Linear Programming Discriminant (LPD) analysis. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.	1,0,1,1,0,0
Dual Based DSP Bidding Strategy and its Application	In recent years, RTB(Real Time Bidding) becomes a popular online advertisement trading method. During the auction, each DSP(Demand Side Platform) is supposed to evaluate current opportunity and respond with an ad and corresponding bid price. It's essential for DSP to find an optimal ad selection and bid price determination strategy which maximizes revenue or performance under budget and ROI(Return On Investment) constraints in P4P(Pay For Performance) or P4U(Pay For Usage) mode. We solve this problem by 1) formalizing the DSP problem as a constrained optimization problem, 2) proposing the augmented MMKP(Multi-choice Multi-dimensional Knapsack Problem) with general solution, 3) and demonstrating the DSP problem is a special case of the augmented MMKP and deriving specialized strategy. Our strategy is verified through simulation and outperforms state-of-the-art strategies in real application. To the best of our knowledge, our solution is the first dual based DSP bidding framework that is derived from strict second price auction assumption and generally applicable to the multiple ads scenario with various objectives and constraints.	1,0,0,1,0,0
Hyers-Ulam stability of elliptic Möbius difference equation	The linear fractional map $ f(z) = \frac{az+ b}{cz + d} $ on the Riemann sphere with complex coefficients $ ad-bc \neq 0 $ is called Möbius map. If $ f $ satisfies $ ad-bc=1 $ and $ -2<a+d<2 $, then $ f $ is called $\textit{elliptic}$ Möbius map. Let $ \{ b_n \}_{n \in \mathbb{N}_0} $ be the solution of the elliptic Möbius difference equation $ b_{n+1} = f(b_n) $ for every $ n \in \mathbb{N}_0 $. Then the sequence $ \{ b_n \}_{n \in \mathbb{N}_0} $ has no Hyers-Ulam stability.	0,0,1,0,0,0
Phase Transitions in Approximate Ranking	We study the problem of approximate ranking from observations of pairwise interactions. The goal is to estimate the underlying ranks of $n$ objects from data through interactions of comparison or collaboration. Under a general framework of approximate ranking models, we characterize the exact optimal statistical error rates of estimating the underlying ranks. We discover important phase transition boundaries of the optimal error rates. Depending on the value of the signal-to-noise ratio (SNR) parameter, the optimal rate, as a function of SNR, is either trivial, polynomial, exponential or zero. The four corresponding regimes thus have completely different error behaviors. To the best of our knowledge, this phenomenon, especially the phase transition between the polynomial and the exponential rates, has not been discovered before.	0,0,1,1,0,0
Improving TSP tours using dynamic programming over tree decomposition	Given a traveling salesman problem (TSP) tour $H$ in graph $G$ a $k$-move is an operation which removes $k$ edges from $H$, and adds $k$ edges of $G$ so that a new tour $H'$ is formed. The popular $k$-OPT heuristics for TSP finds a local optimum by starting from an arbitrary tour $H$ and then improving it by a sequence of $k$-moves. Until 2016, the only known algorithm to find an improving $k$-move for a given tour was the naive solution in time $O(n^k)$. At ICALP'16 de Berg, Buchin, Jansen and Woeginger showed an $O(n^{\lfloor 2/3k \rfloor+1})$-time algorithm. We show an algorithm which runs in $O(n^{(1/4+\epsilon_k)k})$ time, where $\lim \epsilon_k = 0$. We are able to show that it improves over the state of the art for every $k=5,\ldots,10$. For the most practically relevant case $k=5$ we provide a slightly refined algorithm running in $O(n^{3.4})$ time. We also show that for the $k=4$ case, improving over the $O(n^3)$-time algorithm of de Berg et al. would be a major breakthrough: an $O(n^{3-\epsilon})$-time algorithm for any $\epsilon>0$ would imply an $O(n^{3-\delta})$-time algorithm for the ALL PAIRS SHORTEST PATHS problem, for some $\delta>0$.	1,0,0,0,0,0
Free fermions on a piecewise linear four-manifold. II: Pachner moves	This is the second in a series of papers where we construct an invariant of a four-dimensional piecewise linear manifold $M$ with a given middle cohomology class $h\in H^2(M,\mathbb C)$. This invariant is the square root of the torsion of unusual chain complex introduced in Part I (arXiv:1605.06498) of our work, multiplied by a correcting factor. Here we find this factor by studying the behavior of our construction under all four-dimensional Pachner moves, and show that it can be represented in a multiplicative form: a product of same-type multipliers over all 2-faces, multiplied by a product of same-type multipliers over all pentachora.	0,0,1,0,0,0
Subexponentially growing Hilbert space and nonconcentrating distributions in a constrained spin model	Motivated by recent experiments with two-component Bose-Einstein condensates, we study fully-connected spin models subject to an additional constraint. The constraint is responsible for the Hilbert space dimension to scale only linearly with the system size. We discuss the unconventional statistical physical and thermodynamic properties of such a system, in particular the absence of concentration of the underlying probability distributions. As a consequence, expectation values are less suitable to characterize such systems, and full distribution functions are required instead. Sharp signatures of phase transitions do not occur in such a setting, but transitions from singly peaked to doubly peaked distribution functions of an "order parameter" may be present.	0,1,0,0,0,0
Deep Structured Learning for Facial Action Unit Intensity Estimation	We consider the task of automated estimation of facial expression intensity. This involves estimation of multiple output variables (facial action units --- AUs) that are structurally dependent. Their structure arises from statistically induced co-occurrence patterns of AU intensity levels. Modeling this structure is critical for improving the estimation performance; however, this performance is bounded by the quality of the input features extracted from face images. The goal of this paper is to model these structures and estimate complex feature representations simultaneously by combining conditional random field (CRF) encoded AU dependencies with deep learning. To this end, we propose a novel Copula CNN deep learning approach for modeling multivariate ordinal variables. Our model accounts for $ordinal$ structure in output variables and their $non$-$linear$ dependencies via copula functions modeled as cliques of a CRF. These are jointly optimized with deep CNN feature encoding layers using a newly introduced balanced batch iterative training algorithm. We demonstrate the effectiveness of our approach on the task of AU intensity estimation on two benchmark datasets. We show that joint learning of the deep features and the target output structure results in significant performance gains compared to existing deep structured models for analysis of facial expressions.	1,0,0,0,0,0
Detecting hip fractures with radiologist-level performance using deep neural networks	We developed an automated deep learning system to detect hip fractures from frontal pelvic x-rays, an important and common radiological task. Our system was trained on a decade of clinical x-rays (~53,000 studies) and can be applied to clinical data, automatically excluding inappropriate and technically unsatisfactory studies. We demonstrate diagnostic performance equivalent to a human radiologist and an area under the ROC curve of 0.994. Translated to clinical practice, such a system has the potential to increase the efficiency of diagnosis, reduce the need for expensive additional testing, expand access to expert level medical image interpretation, and improve overall patient outcomes.	0,0,0,1,0,0
Flat $F$-manifolds, Miura invariants and integrable systems of conservation laws	We extend some of the results proved for scalar equations in [3,4], to the case of systems of integrable conservation laws. In particular, for such systems we prove that the eigenvalues of a matrix obtained from the quasilinear part of the system are invariants under Miura transformations and we show how these invariants are related to dispersion relations. Furthermore, focusing on one-parameter families of dispersionless systems of integrable conservation laws associated to the Coxeter groups of rank $2$ found in [1], we study the corresponding integrable deformations up to order $2$ in the deformation parameter $\epsilon$. Each family contains both bi-Hamiltonian and non-Hamiltonian systems of conservation laws and therefore we use it to probe to which extent the properties of the dispersionless limit impact the nature and the existence of integrable deformations. It turns out that a part two values of the parameter all deformations of order one in $\epsilon$ are Miura-trivial, while all those of order two in $\epsilon$ are essentially parameterized by two arbitrary functions of single variables (the Riemann invariants) both in the bi-Hamiltonian and in the non-Hamiltonian case. In the two remaining cases, due to the existence of non-trivial first order deformations, there is an additional functional parameter.	0,1,0,0,0,0
Assessing the state of e-Readiness for Small and Medium Companies in Mexico: a Proposed Taxonomy and Adoption Model	Emerging economies frequently show a large component of their Gross Domestic Product to be dependant on the economic activity of small and medium enterprises. Nevertheless, e-business solutions are more likely designed for large companies. SMEs seem to follow a classical family-based management, used to traditional activities, rather than seeking new ways of adding value to their business strategy. Thus, a large portion of a nations economy may be at disadvantage for competition. This paper aims at assessing the state of e-business readiness of Mexican SMEs based on already published e-business evolution models and by means of a survey research design. Data is being collected in three cities with differing sizes and infrastructure conditions. Statistical results are expected to be presented. A second part of this research aims at applying classical adoption models to suggest potential causal relationships, as well as more suitable recommendations for development.	0,0,0,0,0,1
On the convergence properties of a $K$-step averaging stochastic gradient descent algorithm for nonconvex optimization	Despite their popularity, the practical performance of asynchronous stochastic gradient descent methods (ASGD) for solving large scale machine learning problems are not as good as theoretical results indicate. We adopt and analyze a synchronous K-step averaging stochastic gradient descent algorithm which we call K-AVG. We establish the convergence results of K-AVG for nonconvex objectives and explain why the K-step delay is necessary and leads to better performance than traditional parallel stochastic gradient descent which is a special case of K-AVG with $K=1$. We also show that K-AVG scales better than ASGD. Another advantage of K-AVG over ASGD is that it allows larger stepsizes. On a cluster of $128$ GPUs, K-AVG is faster than ASGD implementations and achieves better accuracies and faster convergence for \cifar dataset.	1,0,0,1,0,0
Simple closed curves, finite covers of surfaces, and power subgroups of Out(F_n)	We construct examples of finite covers of punctured surfaces where the first rational homology is not spanned by lifts of simple closed curves. More generally, for any set $\mathcal{O} \subset F_n$ which is contained in the union of finitely many $Aut(F_n)$-orbits, we construct finite-index normal subgroups of $F_n$ whose first rational homology is not spanned by powers of elements of $\mathcal{O}$. These examples answer questions of Farb-Hensel, Looijenga, and Marche. We also show that the quotient of $Out(F_n)$ by the subgroup generated by kth powers of transvections often contains infinite order elements, strengthening a result of Bridson-Vogtmann saying that it is often infinite. Finally, for any set $\mathcal{O} \subset F_n$ which is contained in the union of finitely many $Aut(F_n)$-orbits, we construct integral linear representations of free groups that have infinite image and map all elements of $\mathcal{O}$ to torsion elements.	0,0,1,0,0,0
Correlating Cell Shape and Cellular Stress in Motile Confluent Tissues	Collective cell migration is a highly regulated process involved in wound healing, cancer metastasis and morphogenesis. Mechanical interactions among cells provide an important regulatory mechanism to coordinate such collective motion. Using a Self-Propelled Voronoi (SPV) model that links cell mechanics to cell shape and cell motility, we formulate a generalized mechanical inference method to obtain the spatio-temporal distribution of cellular stresses from measured traction forces in motile tissues and show that such traction-based stresses match those calculated from instantaneous cell shapes. We additionally use stress information to characterize the rheological properties of the tissue. We identify a motility-induced swim stress that adds to the interaction stress to determine the global contractility or extensibility of epithelia. We further show that the temporal correlation of the interaction shear stress determines an effective viscosity of the tissue that diverges at the liquid-solid transition, suggesting the possibility of extracting rheological information directly from traction data.	0,1,0,0,0,0
Spectral and Energy Efficiency of Uplink D2D Underlaid Massive MIMO Cellular Networks	One of key 5G scenarios is that device-to-device (D2D) and massive multiple-input multiple-output (MIMO) will be co-existed. However, interference in the uplink D2D underlaid massive MIMO cellular networks needs to be coordinated, due to the vast cellular and D2D transmissions. To this end, this paper introduces a spatially dynamic power control solution for mitigating the cellular-to-D2D and D2D-to-cellular interference. In particular, the proposed D2D power control policy is rather flexible including the special cases of no D2D links or using maximum transmit power. Under the considered power control, an analytical approach is developed to evaluate the spectral efficiency (SE) and energy efficiency (EE) in such networks. Thus, the exact expressions of SE for a cellular user or D2D transmitter are derived, which quantify the impacts of key system parameters such as massive MIMO antennas and D2D density. Moreover, the D2D scale properties are obtained, which provide the sufficient conditions for achieving the anticipated SE. Numerical results corroborate our analysis and show that the proposed power control solution can efficiently mitigate interference between the cellular and D2D tier. The results demonstrate that there exists the optimal D2D density for maximizing the area SE of D2D tier. In addition, the achievable EE of a cellular user can be comparable to that of a D2D user.	1,0,1,0,0,0
Neural Models for Documents with Metadata	Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.	1,0,0,1,0,0
Rayleigh-Brillouin light scattering spectroscopy of nitrous oxide (N$_2$O)	High signal-to-noise and high-resolution light scattering spectra are measured for nitrous oxide (N$_2$O) gas at an incident wavelength of 403.00 nm, at 90$^\circ$ scattering, at room temperature and at gas pressures in the range $0.5-4$ bar. The resulting Rayleigh-Brillouin light scattering spectra are compared to a number of models describing in an approximate manner the collisional dynamics and energy transfer in this gaseous medium of this polyatomic molecular species. The Tenti-S6 model, based on macroscopic gas transport coefficients, reproduces the scattering profiles in the entire pressure range at less than 2\% deviation at a similar level as does the alternative kinetic Grad's 6-moment model, which is based on the internal collisional relaxation as a decisive parameter. A hydrodynamic model fails to reproduce experimental spectra for the low pressures of 0.5-1 bar, but yields very good agreement ($< 1$\%) in the pressure range $2-4$ bar. While these three models have a different physical basis the internal molecular relaxation derived can for all three be described in terms of a bulk viscosity of $\eta_b \sim (6 \pm 2) \times 10^{-5}$ Pa$\cdot$s. A 'rough-sphere' model, previously shown to be effective to describe light scattering in SF$_6$ gas, is not found to be suitable, likely in view of the non-sphericity and asymmetry of the N-N-O structured linear polyatomic molecule.	0,1,0,0,0,0
On the Complexity of Detecting Convexity over a Box	It has recently been shown that the problem of testing global convexity of polynomials of degree four is {strongly} NP-hard, answering an open question of N.Z. Shor. This result is minimal in the degree of the polynomial when global convexity is of concern. In a number of applications however, one is interested in testing convexity only over a compact region, most commonly a box (i.e., hyper-rectangle). In this paper, we show that this problem is also strongly NP-hard, in fact for polynomials of degree as low as three. This result is minimal in the degree of the polynomial and in some sense justifies why convexity detection in nonlinear optimization solvers is limited to quadratic functions or functions with special structure. As a byproduct, our proof shows that the problem of testing whether all matrices in an interval family are positive semidefinite is strongly NP-hard. This problem, which was previously shown to be (weakly) NP-hard by Nemirovski, is of independent interest in the theory of robust control.	0,0,0,1,0,0
The ALMA Early Science View of FUor/EXor objects. III. The Slow and Wide Outflow of V883 Ori	We present Atacama Large Millimeter/ sub-millimeter Array (ALMA) observations of V883 Ori, an FU Ori object. We describe the molecular outflow and envelope of the system based on the $^{12}$CO and $^{13}$CO emissions, which together trace a bipolar molecular outflow. The C$^{18}$O emission traces the rotational motion of the circumstellar disk. From the $^{12}$CO blue-shifted emission, we estimate a wide opening angle of $\sim$ 150$^{^{\circ}}$ for the outflow cavities. Also, we find that the outflow is very slow (characteristic velocity of only 0.65 km~s$^{-1}$), which is unique for an FU Ori object. We calculate the kinematic properties of the outflow in the standard manner using the $^{12}$CO and $^{13}$CO emissions. In addition, we present a P Cygni profile observed in the high-resolution optical spectrum, evidence of a wind driven by the accretion and being the cause for the particular morphology of the outflows. We discuss the implications of our findings and the rise of these slow outflows during and/or after the formation of a rotationally supported disk.	0,1,0,0,0,0
Random Manifolds have no Totally Geodesic Submanifolds	For $n\geq 4$ we show that generic closed Riemannian $n$-manifolds have no nontrivial totally geodesic submanifolds, answering a question of Spivak. An immediate consequence is a severe restriction on the isometry group of a generic Riemannian metric. Both results are widely believed to be true, but we are not aware of any proofs in the literature.	0,0,1,0,0,0
Sign reversal of magnetoresistance and p to n transition in Ni doped ZnO thin film	We report the magnetoresistance and nonlinear Hall effect studies over a wide temperature range in pulsed laser deposited Ni0.07Zn0.93O thin film. Negative and positive contributions to magnetoresistance at high and low temperatures have been successfully modeled by the localized magnetic moment and two band conduction process involving heavy and light hole subbands, respectively. Nonlinearity in the Hall resistance also agrees well with the two channel conduction model. A negative Hall voltage has been found for T $\gte 50 K$, implying a dominant conduction mainly by electrons whereas positive Hall voltage for T less than 50 K shows hole dominated conduction in this material. Crossover in the sign of magnetoresistance from negative to positive reveals the spin polarization of the charge carriers and hence the applicability of Ni doped ZnO thin film for spintronic applications.	0,1,0,0,0,0
Adaptive Bayesian Sampling with Monte Carlo EM	We present a novel technique for learning the mass matrices in samplers obtained from discretized dynamics that preserve some energy function. Existing adaptive samplers use Riemannian preconditioning techniques, where the mass matrices are functions of the parameters being sampled. This leads to significant complexities in the energy reformulations and resultant dynamics, often leading to implicit systems of equations and requiring inversion of high-dimensional matrices in the leapfrog steps. Our approach provides a simpler alternative, by using existing dynamics in the sampling step of a Monte Carlo EM framework, and learning the mass matrices in the M step with a novel online technique. We also propose a way to adaptively set the number of samples gathered in the E step, using sampling error estimates from the leapfrog dynamics. Along with a novel stochastic sampler based on Nosé-Poincaré dynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as well as newer stochastic algorithms such as SGHMC and SGNHT, and show strong performance on synthetic and real high-dimensional sampling scenarios; we achieve sampling accuracies comparable to Riemannian samplers while being significantly faster.	1,0,0,1,0,0
Geometry of Factored Nuclear Norm Regularization	This work investigates the geometry of a nonconvex reformulation of minimizing a general convex loss function $f(X)$ regularized by the matrix nuclear norm $\|X\|_*$. Nuclear-norm regularized matrix inverse problems are at the heart of many applications in machine learning, signal processing, and control. The statistical performance of nuclear norm regularization has been studied extensively in literature using convex analysis techniques. Despite its optimal performance, the resulting optimization has high computational complexity when solved using standard or even tailored fast convex solvers. To develop faster and more scalable algorithms, we follow the proposal of Burer-Monteiro to factor the matrix variable $X$ into the product of two smaller rectangular matrices $X=UV^T$ and also replace the nuclear norm $\|X\|_*$ with $(\|U\|_F^2+\|V\|_F^2)/2$. In spite of the nonconvexity of the factored formulation, we prove that when the convex loss function $f(X)$ is $(2r,4r)$-restricted well-conditioned, each critical point of the factored problem either corresponds to the optimal solution $X^\star$ of the original convex optimization or is a strict saddle point where the Hessian matrix has a strictly negative eigenvalue. Such a geometric structure of the factored formulation allows many local search algorithms to converge to the global optimum with random initializations.	1,0,1,0,0,0
Convergence of row sequences of simultaneous Padé-Faber approximants	We consider row sequences of vector valued Padé-Faber approximants (simultaneous Padé-Faber approximants) and prove a Montessus de Ballore type theorem.	0,0,1,0,0,0
A Cluster Elastic Net for Multivariate Regression	We propose a method for estimating coefficients in multivariate regression when there is a clustering structure to the response variables. The proposed method includes a fusion penalty, to shrink the difference in fitted values from responses in the same cluster, and an L1 penalty for simultaneous variable selection and estimation. The method can be used when the grouping structure of the response variables is known or unknown. When the clustering structure is unknown the method will simultaneously estimate the clusters of the response and the regression coefficients. Theoretical results are presented for the penalized least squares case, including asymptotic results allowing for p >> n. We extend our method to the setting where the responses are binomial variables. We propose a coordinate descent algorithm for both the normal and binomial likelihood, which can easily be extended to other generalized linear model (GLM) settings. Simulations and data examples from business operations and genomics are presented to show the merits of both the least squares and binomial methods.	0,0,0,1,0,0
A critical nonlinear elliptic equation with non local regional diffusion	In this article we are interested in the nonlocal regional Schrödinger equation with critical exponent \begin{eqnarray*} &\epsilon^{2\alpha} (-\Delta)_{\rho}^{\alpha}u + u = \lambda u^q + u^{2_{\alpha}^{*}-1} \mbox{ in } \mathbb{R}^{N}, \\ & u \in H^{\alpha}(\mathbb{R}^{N}), \end{eqnarray*} where $\epsilon$ is a small positive parameter, $\alpha \in (0,1)$, $q\in (1,2_{\alpha}^{*}-1)$, $2_{\alpha}^{*} = \frac{2N}{N-2\alpha}$ is the critical Sobolev exponent, $\lambda >0$ is a parameter and $(-\Delta)_{\rho}^{\alpha}$ is a variational version of the regional laplacian, whose range of scope is a ball with radius $\rho(x)>0$. We study the existence of a ground state and we analyze the behavior of semi-classical solutions as $\varepsilon\to 0$.	0,0,1,0,0,0
Balancing Selection Pressures, Multiple Objectives, and Neural Modularity to Coevolve Cooperative Agent Behavior	Previous research using evolutionary computation in Multi-Agent Systems indicates that assigning fitness based on team vs.\ individual behavior has a strong impact on the ability of evolved teams of artificial agents to exhibit teamwork in challenging tasks. However, such research only made use of single-objective evolution. In contrast, when a multiobjective evolutionary algorithm is used, populations can be subject to individual-level objectives, team-level objectives, or combinations of the two. This paper explores the performance of cooperatively coevolved teams of agents controlled by artificial neural networks subject to these types of objectives. Specifically, predator agents are evolved to capture scripted prey agents in a torus-shaped grid world. Because of the tension between individual and team behaviors, multiple modes of behavior can be useful, and thus the effect of modular neural networks is also explored. Results demonstrate that fitness rewarding individual behavior is superior to fitness rewarding team behavior, despite being applied to a cooperative task. However, the use of networks with multiple modules allows predators to discover intelligent behavior, regardless of which type of objectives are used.	1,0,0,0,0,0
Photometric Redshifts with the LSST: Evaluating Survey Observing Strategies	In this paper we present and characterize a nearest-neighbors color-matching photometric redshift estimator that features a direct relationship between the precision and accuracy of the input magnitudes and the output photometric redshifts. This aspect makes our estimator an ideal tool for evaluating the impact of changes to LSST survey parameters that affect the measurement errors of the photometry, which is the main motivation of our work (i.e., it is not intended to provide the "best" photometric redshifts for LSST data). We show how the photometric redshifts will improve with time over the 10-year LSST survey and confirm that the nominal distribution of visits per filter provides the most accurate photo-$z$ results. The LSST survey strategy naturally produces observations over a range of airmass, which offers the opportunity of using an SED- and $z$-dependent atmospheric affect on the observed photometry as a color-independent redshift indicator. We show that measuring this airmass effect and including it as a prior has the potential to improve the photometric redshifts and can ameliorate extreme outliers, but that it will only be adequately measured for the brightest galaxies, which limits its overall impact on LSST photometric redshifts. We furthermore demonstrate how this airmass effect can induce a bias in the photo-$z$ results, and caution against survey strategies that prioritize high-airmass observations for the purpose of improving this prior. Ultimately, we intend for this work to serve as a guide for the expectations and preparations of the LSST science community with regards to the minimum quality of photo-$z$ as the survey progresses.	0,1,0,0,0,0
Optimal Input Placement in Lattice Graphs	The control of dynamical, networked systems continues to receive much attention across the engineering and scientific research fields. Of particular interest is the proper way to determine which nodes of the network should receive external control inputs in order to effectively and efficiently control portions of the network. Published methods to accomplish this task either find a minimal set of driver nodes to guarantee controllability or a larger set of driver nodes which optimizes some control metric. Here, we investigate the control of lattice systems which provides analytical insight into the relationship between network structure and controllability. First we derive a closed form expression for the individual elements of the controllability Gramian of infinite lattice systems. Second, we focus on nearest neighbor lattices for which the distance between nodes appears in the expression for the controllability Gramian. We show that common control energy metrics scale exponentially with respect to the maximum distance between a driver node and a target node.	1,0,0,0,0,0
Unbiased Multi-index Monte Carlo	We introduce a new class of Monte Carlo based approximations of expectations of random variables such that their laws are only available via certain discretizations. Sampling from the discretized versions of these laws can typically introduce a bias. In this paper, we show how to remove that bias, by introducing a new version of multi-index Monte Carlo (MIMC) that has the added advantage of reducing the computational effort, relative to i.i.d. sampling from the most precise discretization, for a given level of error. We cover extensions of results regarding variance and optimality criteria for the new approach. We apply the methodology to the problem of computing an unbiased mollified version of the solution of a partial differential equation with random coefficients. A second application concerns the Bayesian inference (the smoothing problem) of an infinite dimensional signal modelled by the solution of a stochastic partial differential equation that is observed on a discrete space grid and at discrete times. Both applications are complemented by numerical simulations.	0,0,0,1,0,0
The Theta Number of Simplicial Complexes	We introduce a generalization of the celebrated Lovász theta number of a graph to simplicial complexes of arbitrary dimension. Our generalization takes advantage of real simplicial cohomology theory, in particular combinatorial Laplacians, and provides a semidefinite programming upper bound of the independence number of a simplicial complex. We consider properties of the graph theta number such as the relationship to Hoffman's ratio bound and to the chromatic number and study how they extend to higher dimensions. Like in the case of graphs, the higher dimensional theta number can be extended to a hierarchy of semidefinite programming upper bounds reaching the independence number. We analyze the value of the theta number and of the hierarchy for dense random simplicial complexes.	1,0,1,0,0,0
Smart Contract SLAs for Dense Small-Cell-as-a-Service	The disruptive power of blockchain technologies represents a great opportunity to re-imagine standard practices of telecommunication networks and to identify critical areas that can benefit from brand new approaches. As a starting point for this debate, we look at the current limits of infrastructure sharing, and specifically at the Small-Cell-as-a-Service trend, asking ourselves how we could push it to its natural extreme: a scenario in which any individual home or business user can become a service provider for mobile network operators, freed from all the scalability and legal constraints that are inherent to the current modus operandi. We propose the adoption of smart contracts to implement simple but effective Service Level Agreements (SLAs) between small cell providers and mobile operators, and present an example contract template based on the Ethereum blockchain.	1,0,0,0,0,0
Simultaneous determination of the drift and diffusion coefficients in stochastic differential equations	In this work, we consider a one-dimensional It{ô} diffusion process X t with possibly nonlinear drift and diffusion coefficients. We show that, when the diffusion coefficient is known, the drift coefficient is uniquely determined by an observation of the expectation of the process during a small time interval, and starting from values X 0 in a given subset of R. With the same type of observation, and given the drift coefficient, we also show that the diffusion coefficient is uniquely determined. When both coefficients are unknown, we show that they are simultaneously uniquely determined by the observation of the expectation and variance of the process, during a small time interval, and starting again from values X 0 in a given subset of R. To derive these results, we apply the Feynman-Kac theorem which leads to a linear parabolic equation with unknown coefficients in front of the first and second order terms. We then solve the corresponding inverse problem with PDE technics which are mainly based on the strong parabolic maximum principle.	0,0,1,0,0,0
Vanishing of Littlewood-Richardson polynomials is in P	J. DeLoera-T. McAllister and K. D. Mulmuley-H. Narayanan-M. Sohoni independently proved that determining the vanishing of Littlewood-Richardson coefficients has strongly polynomial time computational complexity. Viewing these as Schubert calculus numbers, we prove the generalization to the Littlewood-Richardson polynomials that control equivariant cohomology of Grassmannians. We construct a polytope using the edge-labeled tableau rule of H. Thomas-A. Yong. Our proof then combines a saturation theorem of D. Anderson-E. Richmond-A. Yong, a reading order independence property, and E. Tardos' algorithm for combinatorial linear programming.	1,0,1,0,0,0
Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ	Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics.	1,0,0,0,0,0
Practical Processing of Mobile Sensor Data for Continual Deep Learning Predictions	We present a practical approach for processing mobile sensor time series data for continual deep learning predictions. The approach comprises data cleaning, normalization, capping, time-based compression, and finally classification with a recurrent neural network. We demonstrate the effectiveness of the approach in a case study with 279 participants. On the basis of sparse sensor events, the network continually predicts whether the participants would attend to a notification within 10 minutes. Compared to a random baseline, the classifier achieves a 40% performance increase (AUC of 0.702) on a withheld test set. This approach allows to forgo resource-intensive, domain-specific, error-prone feature engineering, which may drastically increase the applicability of machine learning to mobile phone sensor data.	1,0,0,0,0,0
An overview of the marine food web in Icelandic waters using Ecopath with Ecosim	Fishing activities have broad impacts that affect, although not exclusively, the targeted stocks. These impacts affect predators and prey of the harvested species, as well as the whole ecosystem it inhabits. Ecosystem models can be used to study the interactions that occur within a system, including those between different organisms and those between fisheries and targeted species. Trophic web models like Ecopath with Ecosim (EwE) can handle fishing fleets as a top predator, with top-down impact on harvested organisms. The aim of this study was to better understand the Icelandic marine ecosystem and the interactions within. This was done by constructing an EwE model of Icelandic waters. The model was run from 1984 to 2013 and was fitted to time series of biomass estimates, landings data and mean annual temperature. The final model was chosen by selecting the model with the lowest Akaike information criterion. A skill assessment was performed using the Pearson's correlation coefficient, the coefficient of determination, the modelling efficiency and the reliability index to evaluate the model performance. The model performed satisfactorily when simulating previously estimated biomass and known landings. Most of the groups with time series were estimated to have top-down control over their prey. These are harvested species with direct and/or indirect links to lower trophic levels and future fishing policies should take this into account. This model could be used as a tool to investigate how such policies could impact the marine ecosystem in Icelandic waters.	0,0,0,0,1,0
Angular and Temporal Correlation of V2X Channels Across Sub-6 GHz and mmWave Bands	5G millimeter wave (mmWave) technology is envisioned to be an integral part of next-generation vehicle-to-everything (V2X) networks and autonomous vehicles due to its broad bandwidth, wide field of view sensing, and precise localization capabilities. The reliability of mmWave links may be compromised due to difficulties in beam alignment for mobile channels and due to blocking effects between a mmWave transmitter and a receiver. To address such challenges, out-of-band information from sub-6 GHz channels can be utilized for predicting the temporal and angular channel characteristics in mmWave bands, which necessitates a good understanding of how propagation characteristics are coupled across different bands. In this paper, we use ray tracing simulations to characterize the angular and temporal correlation across a wide range of propagation frequencies for V2X channels ranging from 900 MHz up to 73 GHz, for a vehicle maintaining line-of-sight (LOS) and non-LOS (NLOS) beams with a transmitter in an urban environment. Our results shed light on increasing sparsity behavior of propagation channels with increasing frequency and highlight the strong temporal/angular correlation among 5.9 GHz and 28 GHz bands especially for LOS channels.	1,0,0,0,0,0
Neutral Carbon Emission in luminous infrared galaxies The \CI\ Lines as Total Molecular Gas Tracers	We present a statistical study on the [C I]($^{3} \rm P_{1} \rightarrow {\rm ^3 P}_{0}$), [C I] ($^{3} \rm P_{2} \rightarrow {\rm ^3 P}_{1}$) lines (hereafter [C I] (1$-$0) and [C I] (2$-$1), respectively) and the CO (1$-$0) line for a sample of (ultra)luminous infrared galaxies [(U)LIRGs]. We explore the correlations between the luminosities of CO (1$-$0) and [C I] lines, and find that $L'_\mathrm{CO(1-0)}$ correlates almost linearly with both $L'_ \mathrm{[CI](1-0)}$ and $L'_\mathrm{[CI](2-1)}$, suggesting that [C I] lines can trace total molecular gas mass at least for (U)LIRGs. We also investigate the dependence of $L'_\mathrm{[CI](1-0)}$/$L'_\mathrm{CO(1-0)}$, $L'_\mathrm{[CI](2-1)}$/$L'_\mathrm{CO(1-0)}$ and $L'_\mathrm{[CI](2-1)}$/$L'_\mathrm{[CI](1-0)}$ on the far-infrared color of 60-to-100 $\mu$m, and find non-correlation, a weak correlation and a modest correlation, respectively. Under the assumption that these two carbon transitions are optically thin, we further calculate the [C I] line excitation temperatures, atomic carbon masses, and the mean [C I] line flux-to-H$_2$ mass conversion factors for our sample. The resulting $\mathrm{H_2}$ masses using these [C I]-based conversion factors roughly agree with those derived from $L'_\mathrm{CO(1-0)}$ and CO-to-H$_2$ conversion factor.	0,1,0,0,0,0
A unitary "quantization commutes with reduction" map for the adjoint action of a compact Lie group	Let $K$ be a simply connected compact Lie group and $T^{\ast}(K)$ its cotangent bundle. We consider the problem of "quantization commutes with reduction" for the adjoint action of $K$ on $T^{\ast}(K).$ We quantize both $T^{\ast}(K)$ and the reduced phase space using geometric quantization with half-forms. We then construct a geometrically natural map from the space of invariant elements in the quantization of $T^{\ast}(K)$ to the quantization of the reduced phase space. We show that this map is a constant multiple of a unitary map.	0,0,1,0,0,0
Inconsistency of Template Estimation by Minimizing of the Variance/Pre-Variance in the Quotient Space	We tackle the problem of template estimation when data have been randomly deformed under a group action in the presence of noise. In order to estimate the template, one often minimizes the variance when the influence of the transformations have been removed (computation of the Fr{é}chet mean in the quotient space). The consistency bias is defined as the distance (possibly zero) between the orbit of the template and the orbit of one element which minimizes the variance. In the first part, we restrict ourselves to isometric group action, in this case the Hilbertian distance is invariant under the group action. We establish an asymptotic behavior of the consistency bias which is linear with respect to the noise level. As a result the inconsistency is unavoidable as soon as the noise is enough. In practice, template estimation with a finite sample is often done with an algorithm called "max-max". In the second part, also in the case of isometric group finite, we show the convergence of this algorithm to an empirical Karcher mean. Our numerical experiments show that the bias observed in practice can not be attributed to the small sample size or to a convergence problem but is indeed due to the previously studied inconsistency. In a third part, we also present some insights of the case of a non invariant distance with respect to the group action. We will see that the inconsistency still holds as soon as the noise level is large enough. Moreover we prove the inconsistency even when a regularization term is added.	0,0,1,1,0,0
Population of collective modes in light scattering by many atoms	The interaction of light with an atomic sample containing a large number of particles gives rise to many collective (or cooperative) effects, such as multiple scattering, superradiance and subradiance, even if the atomic density is low and the incident optical intensity weak (linear optics regime). Tracing over the degrees of freedom of the light field, the system can be well described by an effective atomic Hamiltonian, which contains the light-mediated dipole-dipole interaction between atoms. This long-range interaction is at the origin of the various collective effects, or of collective excitation modes of the system. Even though an analysis of the eigenvalues and eigenfunctions of these collective modes does allow distinguishing superradiant modes, for instance, from other collective modes, this is not sufficient to understand the dynamics of a driven system, as not all collective modes are significantly populated. Here, we study how the excitation parameters, i.e. the driving field, determines the population of the collective modes. We investigate in particular the role of the laser detuning from the atomic transition, and demonstrate a simple relation between the detuning and the steady-state population of the modes. This relation allows understanding several properties of cooperative scattering, such as why superradiance and subradiance become independent of the detuning at large enough detuning without vanishing, and why superradiance, but not subradiance, is suppressed near resonance.	0,1,0,0,0,0
The application of Monte Carlo methods for learning generalized linear model	Monte Carlo method is a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other mathematical methods. Basically, many statisticians have been increasingly drawn to Monte Carlo method in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution. In this paper, we will introduce the Monte Carlo method for calculating coefficients in Generalized Linear Model(GLM), especially for Logistic Regression. Our main methods are Metropolis Hastings(MH) Algorithms and Stochastic Approximation in Monte Carlo Computation(SAMC). For comparison, we also get results automatically using MLE method in R software.	0,0,0,1,0,0
GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks	The Fisher information metric is an important foundation of information geometry, wherein it allows us to approximate the local geometry of a probability distribution. Recurrent neural networks such as the Sequence-to-Sequence (Seq2Seq) networks that have lately been used to yield state-of-the-art performance on speech translation or image captioning have so far ignored the geometry of the latent embedding, that they iteratively learn. We propose the information geometric Seq2Seq (GeoSeq2Seq) network which abridges the gap between deep recurrent neural networks and information geometry. Specifically, the latent embedding offered by a recurrent network is encoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism common in computer vision. We utilise such a network to predict the shortest routes between two nodes of a graph by learning the adjacency matrix using the GeoSeq2Seq formalism; our results show that for such a problem the probabilistic representation of the latent embedding supersedes the non-probabilistic embedding by 10-15\%.	1,0,0,1,0,0
Nodal domains, spectral minimal partitions, and their relation to Aharonov-Bohm operators	This survey is a short version of a chapter written by the first two authors in the book [A. Henrot, editor. Shape optimization and spectral theory. Berlin: De Gruyter, 2017] (where more details and references are given) but we have decided here to put more emphasis on the role of the Aharonov-Bohm operators which appear to be a useful tool coming from physics for understanding a problem motivated either by spectral geometry or dynamics of population. Similar questions appear also in Bose-Einstein theory. Finally some open problems which might be of interest are mentioned.	0,0,1,0,0,0
On The Robustness of a Neural Network	With the development of neural networks based machine learning and their usage in mission critical applications, voices are rising against the \textit{black box} aspect of neural networks as it becomes crucial to understand their limits and capabilities. With the rise of neuromorphic hardware, it is even more critical to understand how a neural network, as a distributed system, tolerates the failures of its computing nodes, neurons, and its communication channels, synapses. Experimentally assessing the robustness of neural networks involves the quixotic venture of testing all the possible failures, on all the possible inputs, which ultimately hits a combinatorial explosion for the first, and the impossibility to gather all the possible inputs for the second. In this paper, we prove an upper bound on the expected error of the output when a subset of neurons crashes. This bound involves dependencies on the network parameters that can be seen as being too pessimistic in the average case. It involves a polynomial dependency on the Lipschitz coefficient of the neurons activation function, and an exponential dependency on the depth of the layer where a failure occurs. We back up our theoretical results with experiments illustrating the extent to which our prediction matches the dependencies between the network parameters and robustness. Our results show that the robustness of neural networks to the average crash can be estimated without the need to neither test the network on all failure configurations, nor access the training set used to train the network, both of which are practically impossible requirements.	1,0,0,1,0,0
Persistence Codebooks for Topological Data Analysis	Topological data analysis, such as persistent homology has shown beneficial properties for machine learning in many tasks. Topological representations, such as the persistence diagram (PD), however, have a complex structure (multiset of intervals) which makes it difficult to combine with typical machine learning workflows. We present novel compact fixed-size vectorial representations of PDs based on clustering and bag of words encodings that cope well with the inherent sparsity of PDs. Our novel representations outperform state-of-the-art approaches from topological data analysis and are computationally more efficient.	0,0,0,1,0,0
Semi-Supervised Deep Learning for Monocular Depth Map Prediction	Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-the-art methods.	1,0,0,0,0,0
Document Retrieval for Large Scale Content Analysis using Contextualized Dictionaries	This paper presents a procedure to retrieve subsets of relevant documents from large text collections for Content Analysis, e.g. in social sciences. Document retrieval for this purpose needs to take account of the fact that analysts often cannot describe their research objective with a small set of key terms, especially when dealing with theoretical or rather abstract research interests. Instead, it is much easier to define a set of paradigmatic documents which reflect topics of interest as well as targeted manner of speech. Thus, in contrast to classic information retrieval tasks we employ manually compiled collections of reference documents to compose large queries of several hundred key terms, called dictionaries. We extract dictionaries via Topic Models and also use co-occurrence data from reference collections. Evaluations show that the procedure improves retrieval results for this purpose compared to alternative methods of key term extraction as well as neglecting co-occurrence data.	1,0,0,0,0,0
Accurate Computation of Marginal Data Densities Using Variational Bayes	Bayesian model selection and model averaging rely on estimates of marginal data densities (MDDs) also known as marginal likelihoods. Estimation of MDDs is often nontrivial and requires elaborate numerical integration methods. We propose using the variational Bayes posterior density as a weighting density within the class of reciprocal importance sampling MDD estimators. This proposal is computationally convenient, is based on variational Bayes posterior densities that are available for many models, only requires simulated draws from the posterior distribution, and provides accurate estimates with a moderate number of posterior draws. We show that this estimator is theoretically well-justified, has finite variance, provides a minimum variance candidate for the class of reciprocal importance sampling MDD estimators, and that its reciprocal is consistent, asymptotically normally distributed and unbiased. We also investigate the performance of the variational Bayes approximate density as a weighting density within the class of bridge sampling estimators. Using several examples, we show that our proposed estimators are at least as good as the best existing estimators and outperform many MDD estimators in terms of bias and numerical standard errors.	0,0,0,1,0,0
Distribution-Based Categorization of Classifier Transfer Learning	Transfer Learning (TL) aims to transfer knowledge acquired in one problem, the source problem, onto another problem, the target problem, dispensing with the bottom-up construction of the target model. Due to its relevance, TL has gained significant interest in the Machine Learning community since it paves the way to devise intelligent learning models that can easily be tailored to many different applications. As it is natural in a fast evolving area, a wide variety of TL methods, settings and nomenclature have been proposed so far. However, a wide range of works have been reporting different names for the same concepts. This concept and terminology mixture contribute however to obscure the TL field, hindering its proper consideration. In this paper we present a review of the literature on the majority of classification TL methods, and also a distribution-based categorization of TL with a common nomenclature suitable to classification problems. Under this perspective three main TL categories are presented, discussed and illustrated with examples.	1,0,0,0,0,0
Airway segmentation from 3D chest CT volumes based on volume of interest using gradient vector flow	Some lung diseases are related to bronchial airway structures and morphology. Although airway segmentation from chest CT volumes is an important task in the computer-aided diagnosis and surgery assistance systems for the chest, complete 3-D airway structure segmentation is a quite challenging task due to its complex tree-like structure. In this paper, we propose a new airway segmentation method from 3D chest CT volumes based on volume of interests (VOI) using gradient vector flow (GVF). This method segments the bronchial regions by applying the cavity enhancement filter (CEF) to trace the bronchial tree structure from the trachea. It uses the CEF in the VOI to segment each branch. And a tube-likeness function based on GVF and the GVF magnitude map in each VOI are utilized to assist predicting the positions and directions of child branches. By calculating the tube-likeness function based on GVF and the GVF magnitude map, the airway-like candidate structures are identified and their centrelines are extracted. Based on the extracted centrelines, we can detect the branch points of the bifurcations and directions of the airway branches in the next level. At the same time, a leakage detection is performed to avoid the leakage by analysing the pixel information and the shape information of airway candidate regions extracted in the VOI. Finally, we unify all of the extracted bronchial regions to form an integrated airway tree. Preliminary experiments using four cases of chest CT volumes demonstrated that the proposed method can extract more bronchial branches in comparison with other methods.	1,0,0,0,0,0
Backlund transformations and divisor doubling	In classical mechanics well-known cryptographic algorithms and protocols can be very useful for construction canonical transformations preserving form of Hamiltonians. We consider application of a standard generic divisor doubling for construction of new auto Bäcklund transformations for the Lagrange top and Hénon-Heiles system separable in parabolic coordinates.	0,1,1,0,0,0
Better Text Understanding Through Image-To-Text Transfer	Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.	1,0,0,0,0,0
Design and characterization of the Large-Aperture Experiment to Detect the Dark Age (LEDA) radiometer systems	The Large-Aperture Experiment to Detect the Dark Age (LEDA) was designed to detect the predicted O(100)mK sky-averaged absorption of the Cosmic Microwave Background by Hydrogen in the neutral pre- and intergalactic medium just after the cosmological Dark Age. The spectral signature would be associated with emergence of a diffuse Ly$\alpha$ background from starlight during 'Cosmic Dawn'. Recently, Bowman et al. (2018) have reported detection of this predicted absorption feature, with an unexpectedly large amplitude of 530 mK, centered at 78 MHz. Verification of this result by an independent experiment, such as LEDA, is pressing. In this paper, we detail design and characterization of the LEDA radiometer systems, and a first-generation pipeline that instantiates a signal path model. Sited at the Owens Valley Radio Observatory Long Wavelength Array, LEDA systems include the station correlator, five well-separated redundant dual polarization radiometers and backend electronics. The radiometers deliver a 30-85MHz band (16<z<34) and operate as part of the larger interferometric array, for purposes ultimately of in situ calibration. Here, we report on the LEDA system design, calibration approach, and progress in characterization as of January 2016. The LEDA systems are currently being modified to improve performance near 78 MHz in order to verify the purported absorption feature.	0,1,0,0,0,0
Observing the Atmospheres of Known Temperate Earth-sized Planets with JWST	Nine transiting Earth-sized planets have recently been discovered around nearby late M dwarfs, including the TRAPPIST-1 planets and two planets discovered by the MEarth survey, GJ 1132b and LHS 1140b. These planets are the smallest known planets that may have atmospheres amenable to detection with JWST. We present model thermal emission and transmission spectra for each planet, varying composition and surface pressure of the atmosphere. We base elemental compositions on those of Earth, Titan, and Venus and calculate the molecular compositions assuming chemical equilibrium, which can strongly depend on temperature. Both thermal emission and transmission spectra are sensitive to the atmospheric composition; thermal emission spectra are sensitive to surface pressure and temperature. We predict the observability of each planet's atmosphere with JWST. GJ 1132b and TRAPPIST-1b are excellent targets for emission spectroscopy with JWST/MIRI, requiring fewer than 10 eclipse observations. Emission photometry for TRAPPIST-1c requires 5-15 eclipses; LHS 1140b and TRAPPIST-1d, TRAPPIST-1e, and TRAPPIST-1f, which could possibly have surface liquid water, may be accessible with photometry. Seven of the nine planets are strong candidates for transmission spectroscopy measurements with JWST, though the number of transits required depends strongly on the planets' actual masses. Using the measured masses, fewer than 20 transits are required for a 5 sigma detection of spectral features for GJ 1132b and six of the TRAPPIST-1 planets. Dedicated campaigns to measure the atmospheres of these nine planets will allow us, for the first time, to probe formation and evolution processes of terrestrial planetary atmospheres beyond our solar system.	0,1,0,0,0,0
Geometric clustering in normed planes	Given two sets of points $A$ and $B$ in a normed plane, we prove that there are two linearly separable sets $A'$ and $B'$ such that $\mathrm{diam}(A')\leq \mathrm{diam}(A)$, $\mathrm{diam}(B')\leq \mathrm{diam}(B)$, and $A'\cup B'=A\cup B.$ This extends a result for the Euclidean distance to symmetric convex distance functions. As a consequence, some Euclidean $k$-clustering algorithms are adapted to normed planes, for instance, those that minimize the maximum, the sum, or the sum of squares of the $k$ cluster diameters. The 2-clustering problem when two different bounds are imposed to the diameters is also solved. The Hershberger-Suri's data structure for managing ball hulls can be useful in this context.	0,0,1,0,0,0
Simultaneous Multiparty Communication Complexity of Composed Functions	In the Number On the Forehead (NOF) multiparty communication model, $k$ players want to evaluate a function $F : X_1 \times\cdots\times X_k\rightarrow Y$ on some input $(x_1,\dots,x_k)$ by broadcasting bits according to a predetermined protocol. The input is distributed in such a way that each player $i$ sees all of it except $x_i$. In the simultaneous setting, the players cannot speak to each other but instead send information to a referee. The referee does not know the players' input, and cannot give any information back. At the end, the referee must be able to recover $F(x_1,\dots,x_k)$ from what she obtained. A central open question, called the $\log n$ barrier, is to find a function which is hard to compute for $polylog(n)$ or more players (where the $x_i$'s have size $poly(n)$) in the simultaneous NOF model. This has important applications in circuit complexity, as it could help to separate $ACC^0$ from other complexity classes. One of the candidates belongs to the family of composed functions. The input to these functions is represented by a $k\times (t\cdot n)$ boolean matrix $M$, whose row $i$ is the input $x_i$ and $t$ is a block-width parameter. A symmetric composed function acting on $M$ is specified by two symmetric $n$- and $kt$-variate functions $f$ and $g$, that output $f\circ g(M)=f(g(B_1),\dots,g(B_n))$ where $B_j$ is the $j$-th block of width $t$ of $M$. As the majority function $MAJ$ is conjectured to be outside of $ACC^0$, Babai et. al. suggested to study $MAJ\circ MAJ_t$, with $t$ large enough. So far, it was only known that $t=1$ is not enough for $MAJ\circ MAJ_t$ to break the $\log n$ barrier in the simultaneous deterministic NOF model. In this paper, we extend this result to any constant block-width $t>1$, by giving a protocol of cost $2^{O(2^t)}\log^{2^{t+1}}(n)$ for any symmetric composed function when there are $2^{\Omega(2^t)}\log n$ players.	1,0,0,0,0,0
Criticality & Deep Learning II: Momentum Renormalisation Group	Guided by critical systems found in nature we develop a novel mechanism consisting of inhomogeneous polynomial regularisation via which we can induce scale invariance in deep learning systems. Technically, we map our deep learning (DL) setup to a genuine field theory, on which we act with the Renormalisation Group (RG) in momentum space and produce the flow equations of the couplings; those are translated to constraints and consequently interpreted as "critical regularisation" conditions in the optimiser; the resulting equations hence prove to be sufficient conditions for - and serve as an elegant and simple mechanism to induce scale invariance in any deep learning setup.	1,0,0,0,0,0
Learning to compress and search visual data in large-scale systems	The problem of high-dimensional and large-scale representation of visual data is addressed from an unsupervised learning perspective. The emphasis is put on discrete representations, where the description length can be measured in bits and hence the model capacity can be controlled. The algorithmic infrastructure is developed based on the synthesis and analysis prior models whose rate-distortion properties, as well as capacity vs. sample complexity trade-offs are carefully optimized. These models are then extended to multi-layers, namely the RRQ and the ML-STC frameworks, where the latter is further evolved as a powerful deep neural network architecture with fast and sample-efficient training and discrete representations. For the developed algorithms, three important applications are developed. First, the problem of large-scale similarity search in retrieval systems is addressed, where a double-stage solution is proposed leading to faster query times and shorter database storage. Second, the problem of learned image compression is targeted, where the proposed models can capture more redundancies from the training images than the conventional compression codecs. Finally, the proposed algorithms are used to solve ill-posed inverse problems. In particular, the problems of image denoising and compressive sensing are addressed with promising results.	1,0,0,1,0,0
Enhancing the significance of gravitational wave bursts through signal classification	The quest to observe gravitational waves challenges our ability to discriminate signals from detector noise. This issue is especially relevant for transient gravitational waves searches with a robust eyes wide open approach, the so called all- sky burst searches. Here we show how signal classification methods inspired by broad astrophysical characteristics can be implemented in all-sky burst searches preserving their generality. In our case study, we apply a multivariate analyses based on artificial neural networks to classify waves emitted in compact binary coalescences. We enhance by orders of magnitude the significance of signals belonging to this broad astrophysical class against the noise background. Alternatively, at a given level of mis-classification of noise events, we can detect about 1/4 more of the total signal population. We also show that a more general strategy of signal classification can actually be performed, by testing the ability of artificial neural networks in discriminating different signal classes. The possible impact on future observations by the LIGO-Virgo network of detectors is discussed by analysing recoloured noise from previous LIGO-Virgo data with coherent WaveBurst, one of the flagship pipelines dedicated to all-sky searches for transient gravitational waves.	0,1,0,0,0,0
AI4AI: Quantitative Methods for Classifying Host Species from Avian Influenza DNA Sequence	Avian Influenza breakouts cause millions of dollars in damage each year globally, especially in Asian countries such as China and South Korea. The impact magnitude of a breakout directly correlates to time required to fully understand the influenza virus, particularly the interspecies pathogenicity. The procedure requires laboratory tests that require resources typically lacking in a breakout emergency. In this study, we propose new quantitative methods utilizing machine learning and deep learning to correctly classify host species given raw DNA sequence data of the influenza virus, and provide probabilities for each classification. The best deep learning models achieve top-1 classification accuracy of 47%, and top-3 classification accuracy of 82%, on a dataset of 11 host species classes.	0,0,0,1,1,0
Efficient Correlated Topic Modeling with Topic Embedding	Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.	1,0,0,1,0,0
Frank-Wolfe with Subsampling Oracle	We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algorithm. While classical FW algorithms require solving a linear minimization problem over the domain at each iteration, the proposed method only requires to solve a linear minimization problem over a small \emph{subset} of the original domain. The first algorithm that we propose is a randomized variant of the original FW algorithm and achieves a $\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic counterpart. The second algorithm is a randomized variant of the Away-step FW algorithm, and again as its deterministic counterpart, reaches linear (i.e., exponential) convergence rate making it the first provably convergent randomized variant of Away-step FW. In both cases, while subsampling reduces the convergence rate by a constant factor, the linear minimization step can be a fraction of the cost of that of the deterministic versions, especially when the data is streamed. We illustrate computational gains of the algorithms on regression problems, involving both $\ell_1$ and latent group lasso penalties.	0,0,0,1,0,0
Analysis of Approximate Stochastic Gradient Using Quadratic Constraints and Sequential Semidefinite Programs	We present convergence rate analysis for the approximate stochastic gradient method, where individual gradient updates are corrupted by computation errors. We develop stochastic quadratic constraints to formulate a small linear matrix inequality (LMI) whose feasible set characterizes convergence properties of the approximate stochastic gradient. Based on this LMI condition, we develop a sequential minimization approach to analyze the intricate trade-offs that couple stepsize selection, convergence rate, optimization accuracy, and robustness to gradient inaccuracy. We also analytically solve this LMI condition and obtain theoretical formulas that quantify the convergence properties of the approximate stochastic gradient under various assumptions on the loss functions.	0,0,0,1,0,0
Orbital Graphs	We introduce orbital graphs and discuss some of their basic properties. Then we focus on their usefulness for search algorithms for permutation groups, including finding the intersection of groups and the stabilizer of sets in a group.	1,0,1,0,0,0
The Short Baseline Neutrino Oscillation Program at Fermilab	The Short-Baseline Neutrino (SBN) Program is a short-baseline neutrino oscillation experiment in the Booster Neutrino Beam-line (BNB) at Fermilab. It consists of three Liquid Argon Time Projection Chambers (LArTPCs) from the Short-Baseline Near Detector (SBND), Micro Booster Neutrino Experiment (MicroBooNE), and Imaging Cosmic And Rare Underground Signals (ICARUS) experiments. The SBN Program will definitively search for short-baseline neutrino oscillations in the 1 eV mass range, make precision neutrino-argon interaction measurements, and further develop the LArTPC technology. The physics program and current status of the program, and its constituent experiments, are presented.	0,1,0,0,0,0
NeuroNER: an easy-to-use program for named-entity recognition based on neural networks	Named-entity recognition (NER) aims at identifying entities of interest in a text. Artificial neural networks (ANNs) have recently been shown to outperform existing NER systems. However, ANNs remain challenging to use for non-expert users. In this paper, we present NeuroNER, an easy-to-use named-entity recognition tool based on ANNs. Users can annotate entities using a graphical web-based user interface (BRAT): the annotations are then used to train an ANN, which in turn predict entities' locations and categories in new texts. NeuroNER makes this annotation-training-prediction flow smooth and accessible to anyone.	1,0,0,1,0,0
On Conjugates and Adjoint Descent	In this note we present an $\infty$-categorical framework for descent along adjunctions and a general formula for counting conjugates up to equivalence which unifies several known formulae from different fields.	0,0,1,0,0,0
A Data Science Approach to Understanding Residential Water Contamination in Flint	When the residents of Flint learned that lead had contaminated their water system, the local government made water-testing kits available to them free of charge. The city government published the results of these tests, creating a valuable dataset that is key to understanding the causes and extent of the lead contamination event in Flint. This is the nation's largest dataset on lead in a municipal water system. In this paper, we predict the lead contamination for each household's water supply, and we study several related aspects of Flint's water troubles, many of which generalize well beyond this one city. For example, we show that elevated lead risks can be (weakly) predicted from observable home attributes. Then we explore the factors associated with elevated lead. These risk assessments were developed in part via a crowd sourced prediction challenge at the University of Michigan. To inform Flint residents of these assessments, they have been incorporated into a web and mobile application funded by \texttt{Google.org}. We also explore questions of self-selection in the residential testing program, examining which factors are linked to when and how frequently residents voluntarily sample their water.	1,0,0,1,0,0
Independently Controllable Factors	It has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation. However, it remains an open question what kind of training framework could potentially achieve that. Whereas most previous work focuses on the static setting (e.g., with images), we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment. The agent can experiment with different actions and observe their effects. More specifically, we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable, i.e., that there exists a policy and a learnable feature for each such aspect of the environment, such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data. We propose a specific objective function to find such factors and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal.	1,0,0,1,0,0
Cayley deformations of compact complex surfaces	In this article, we consider Cayley deformations of a compact complex surface in a Calabi--Yau four-fold. We will study complex deformations of compact complex submanifolds of Calabi--Yau manifolds with a view to explaining why complex and Cayley deformations of a compact complex surface are the same. We in fact prove that the moduli space of complex deformations of any compact complex embedded submanifold of a Calabi--Yau manifold is a smooth manifold.	0,0,1,0,0,0
Communication Modalities for Supervised Teleoperation in Highly Dexterous Tasks - Does one size fit all?	This study tries to explain the connection between communication modalities and levels of supervision in teleoperation during a dexterous task, like surgery. This concept is applied to two surgical related tasks: incision and peg transfer. It was found that as the complexity of the task escalates, the combination linking human supervision with a more expressive modality shows better performance than other combinations of modalities and control. More specifically, in the peg transfer task, the combination of speech modality and action level supervision achieves shorter task completion time (77.1 +- 3.4 s) with fewer mistakes (0.20 +- 0.17 pegs dropped).	1,0,0,0,0,0
Estimation of the covariance structure of heavy-tailed distributions	We propose and analyze a new estimator of the covariance matrix that admits strong theoretical guarantees under weak assumptions on the underlying distribution, such as existence of moments of only low order. While estimation of covariance matrices corresponding to sub-Gaussian distributions is well-understood, much less in known in the case of heavy-tailed data. As K. Balasubramanian and M. Yuan write, "data from real-world experiments oftentimes tend to be corrupted with outliers and/or exhibit heavy tails. In such cases, it is not clear that those covariance matrix estimators .. remain optimal" and "..what are the other possible strategies to deal with heavy tailed distributions warrant further studies." We make a step towards answering this question and prove tight deviation inequalities for the proposed estimator that depend only on the parameters controlling the "intrinsic dimension" associated to the covariance matrix (as opposed to the dimension of the ambient space); in particular, our results are applicable in the case of high-dimensional observations.	0,0,1,1,0,0
Exploring Latent Semantic Factors to Find Useful Product Reviews	Online reviews provided by consumers are a valuable asset for e-Commerce platforms, influencing potential consumers in making purchasing decisions. However, these reviews are of varying quality, with the useful ones buried deep within a heap of non-informative reviews. In this work, we attempt to automatically identify review quality in terms of its helpfulness to the end consumers. In contrast to previous works in this domain exploiting a variety of syntactic and community-level features, we delve deep into the semantics of reviews as to what makes them useful, providing interpretable explanation for the same. We identify a set of consistency and semantic factors, all from the text, ratings, and timestamps of user-generated reviews, making our approach generalizable across all communities and domains. We explore review semantics in terms of several latent factors like the expertise of its author, his judgment about the fine-grained facets of the underlying product, and his writing style. These are cast into a Hidden Markov Model -- Latent Dirichlet Allocation (HMM-LDA) based model to jointly infer: (i) reviewer expertise, (ii) item facets, and (iii) review helpfulness. Large-scale experiments on five real-world datasets from Amazon show significant improvement over state-of-the-art baselines in predicting and ranking useful reviews.	1,0,0,1,0,0
A Convex Parametrization of a New Class of Universal Kernel Functions for use in Kernel Learning	We propose a new class of universal kernel functions which admit a linear parametrization using positive semidefinite matrices. These kernels are generalizations of the Sobolev kernel and are defined by piecewise-polynomial functions. The class of kernels is termed "tessellated" as the resulting discriminant is defined piecewise with hyper-rectangular domains whose corners are determined by the training data. The kernels have scalable complexity, but each instance is universal in the sense that its hypothesis space is dense in $L_2$. Using numerical testing, we show that for the soft margin SVM, this class can eliminate the need for Gaussian kernels. Furthermore, we demonstrate that when the ratio of the number of training data to features is high, this method will significantly outperform other kernel learning algorithms. Finally, to reduce the complexity associated with SDP-based kernel learning methods, we use a randomized basis for the positive matrices to integrate with existing multiple kernel learning algorithms such as SimpleMKL.	1,0,0,1,0,0
Anomalous electron states	By the certain macroscopic perturbations in condensed matter anomalous electron wells can be formed due to a local reduction of electromagnetic zero point energy. These wells are narrow, of the width $\sim 10^{-11}cm$, and with the depth $\sim 1MeV$. Such anomalous states, from the formal standpoint of quantum mechanics, correspond to a singular solution of a wave equation produced by the non-physical $\delta(\vec R)$ source. The resolution, on the level of the Standard Model, of the tiny region around the formal singularity shows that the state is physical. The creation of those states in an atomic system is of the formal probability $\exp(-1000)$. The probability becomes not small under a perturbation which rapidly varies in space, on the scale $10^{-11}cm$. In condensed matter such perturbation may relate to acoustic shock waves. In this process the short scale is the length of the standing de Broglie wave of a reflected lattice atom. Under electron transitions in the anomalous well (anomalous atom) $keV$ X-rays are expected to be emitted. A macroscopic amount of anomalous atoms, of the size $10^{-11}cm$ each, can be formed in a solid resulting in ${\it collapsed}$ ${\it matter}$ with $10^9$ times enhanced density.	0,1,0,0,0,0
Towards Industry 4.0: Gap Analysis between Current Automotive MES and Industry Standards using Model-Based Requirement Engineering	The dawn of the fourth industrial revolution, Industry 4.0 has created great enthusiasm among companies and researchers by giving them an opportunity to pave the path towards the vision of a connected smart factory ecosystem. However, in context of automotive industry there is an evident gap between the requirements supported by the current automotive manufacturing execution systems (MES) and the requirements proposed by industrial standards from the International Society of Automation (ISA) such as, ISA-95, ISA-88 over which the Industry 4.0 is being built on. In this paper, we bridge this gap by following a model-based requirements engineering approach along with a gap analysis process. Our work is mainly divided into three phases, (i) automotive MES tool selection phase, (ii) requirements modeling phase, (iii) and gap analysis phase based on the modeled requirements. During the MES tool selection phase, we used known reliable sources such as, MES product survey reports, white papers that provide in-depth and comprehensive information about various comparison criteria and tool vendors list for the current MES landscape. During the requirement modeling phase, we specified requirements derived from the needs of ISA-95 and ISA-88 industrial standards using the general purpose Systems Modeling Language (SysML). During the gap analysis phase, we find the misalignment between standard requirements and the compliance of the existing software tools to those standards.	1,0,0,0,0,0
Smoothing of transport plans with fixed marginals and rigorous semiclassical limit of the Hohenberg-Kohn functional	We prove rigorously that the exact N-electron Hohenberg-Kohn density functional converges in the strongly interacting limit to the strictly correlated electrons (SCE) functional, and that the absolute value squared of the associated constrained-search wavefunction tends weakly in the sense of probability measures to a minimizer of the multi-marginal optimal transport problem with Coulomb cost associated to the SCE functional. This extends our previous work for N=2 [CFK11]. The correct limit problem has been derived in the physics literature by Seidl [Se99] and Seidl, Gori-Giorgi and Savin [SGS07]; in these papers the lack of a rigorous proof was pointed out. We also give a mathematical counterexample to this type of result, by replacing the constraint of given one-body density -- an infinite-dimensional quadratic expression in the wavefunction -- by an infinite-dimensional quadratic expression in the wavefunction and its gradient. Connections with the Lawrentiev phenomenon in the calculus of variations are indicated.	0,0,1,0,0,0
The Quest for Scalability and Accuracy in the Simulation of the Internet of Things: an Approach based on Multi-Level Simulation	This paper presents a methodology for simulating the Internet of Things (IoT) using multi-level simulation models. With respect to conventional simulators, this approach allows us to tune the level of detail of different parts of the model without compromising the scalability of the simulation. As a use case, we have developed a two-level simulator to study the deployment of smart services over rural territories. The higher level is base on a coarse grained, agent-based adaptive parallel and distributed simulator. When needed, this simulator spawns OMNeT++ model instances to evaluate in more detail the issues concerned with wireless communications in restricted areas of the simulated world. The performance evaluation confirms the viability of multi-level simulations for IoT environments.	1,0,0,0,0,0
Nb3Sn wire shape and cross sectional area inhomogeneity in Rutherford cables	During Rutherford cable production the wires are plastically deformed and their initially round shape is distorted. Using X-ray absorption tomography we have determined the 3D shape of an unreacted Nb3Sn 11 T dipole Rutherford cable, and of a reacted and impregnated Nb3Sn cable double stack. State-of-the-art image processing was applied to correct for tomographic artefacts caused by the large cable aspect ratio, for the segmentation of the individual wires and subelement bundles inside the wires, and for the calculation of the wire cross sectional area and shape variations. The 11 T dipole cable cross section oscillates by 2% with a frequency of 1.24 mm (1/80 of the transposition pitch length of the 40 wire cable). A comparatively stronger cross sectional area variation is observed in the individual wires at the thin edge of the keystoned cable where the wire aspect ratio is largest.	0,1,0,0,0,0
Inferring Narrative Causality between Event Pairs in Films	To understand narrative, humans draw inferences about the underlying relations between narrative events. Cognitive theories of narrative understanding define these inferences as four different types of causality, that include pairs of events A, B where A physically causes B (X drop, X break), to pairs of events where A causes emotional state B (Y saw X, Y felt fear). Previous work on learning narrative relations from text has either focused on "strict" physical causality, or has been vague about what relation is being learned. This paper learns pairs of causal events from a corpus of film scene descriptions which are action rich and tend to be told in chronological order. We show that event pairs induced using our methods are of high quality and are judged to have a stronger causal relation than event pairs from Rel-grams.	1,0,0,0,0,0
Scheduling Constraint Based Abstraction Refinement for Multi-Threaded Program Verification	Bounded model checking is among the most efficient techniques for the automatic verification of concurrent programs. However, encoding all possible interleavings often requires a huge and complex formula, which significantly limits the salability. This paper proposes a novel and efficient abstraction refinement method for multi-threaded program verification. Observing that the huge formula is usually dominated by the exact encoding of the scheduling constraint, this paper proposes a \tsc based abstraction refinement method, which avoids the huge and complex encoding of BMC. In addition, to obtain an effective refinement, we have devised two graph-based algorithms over event order graph for counterexample validation and refinement generation, which can always obtain a small yet effective refinement constraint. Enhanced by two constraint-based algorithms for counterexample validation and refinement generation, we have proved that our method is sound and complete w.r.t. the given loop unwinding depth. Experimental results on \svcompc benchmarks indicate that our method is promising and significantly outperforms the existing state-of-the-art tools.	1,0,0,0,0,0
Deep learning enhanced mobile-phone microscopy	Mobile-phones have facilitated the creation of field-portable, cost-effective imaging and sensing technologies that approach laboratory-grade instrument performance. However, the optical imaging interfaces of mobile-phones are not designed for microscopy and produce spatial and spectral distortions in imaging microscopic specimens. Here, we report on the use of deep learning to correct such distortions introduced by mobile-phone-based microscopes, facilitating the production of high-resolution, denoised and colour-corrected images, matching the performance of benchtop microscopes with high-end objective lenses, also extending their limited depth-of-field. After training a convolutional neural network, we successfully imaged various samples, including blood smears, histopathology tissue sections, and parasites, where the recorded images were highly compressed to ease storage and transmission for telemedicine applications. This method is applicable to other low-cost, aberrated imaging systems, and could offer alternatives for costly and bulky microscopes, while also providing a framework for standardization of optical images for clinical and biomedical applications.	1,1,0,0,0,0
Autonomous drone cinematographer: Using artistic principles to create smooth, safe, occlusion-free trajectories for aerial filming	Autonomous aerial cinematography has the potential to enable automatic capture of aesthetically pleasing videos without requiring human intervention, empowering individuals with the capability of high-end film studios. Current approaches either only handle off-line trajectory generation, or offer strategies that reason over short time horizons and simplistic representations for obstacles, which result in jerky movement and low real-life applicability. In this work we develop a method for aerial filming that is able to trade off shot smoothness, occlusion, and cinematography guidelines in a principled manner, even under noisy actor predictions. We present a novel algorithm for real-time covariant gradient descent that we use to efficiently find the desired trajectories by optimizing a set of cost functions. Experimental results show that our approach creates attractive shots, avoiding obstacles and occlusion 65 times over 1.25 hours of flight time, re-planning at 5 Hz with a 10 s time horizon. We robustly film human actors, cars and bicycles performing different motion among obstacles, using various shot types.	1,0,0,0,0,0
Nonlinear Unknown Input and State Estimation Algorithm in Mobile Robots	This technical report provides the description and the derivation of a novel nonlinear unknown input and state estimation algorithm (NUISE) for mobile robots. The algorithm is designed for real-world robots with nonlinear dynamic models and subject to stochastic noises on sensing and actuation. Leveraging sensor readings and planned control commands, the algorithm detects and quantifies anomalies on both sensors and actuators. Later, we elaborate the dynamic models of two distinctive mobile robots for the purpose of demonstrating the application of NUISE. This report serves as a supplementary document for [1].	1,0,0,0,0,0
DeSIGN: Design Inspiration from Generative Networks	Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost creativity in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) novel loss functions that encourage novelty, inspired from Sharma-Mittal divergence, a generalized mutual information measure for the widely used relative entropies such as Kullback-Leibler, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture components). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies that we hope will help ease future research. We show that our proposed creativity criterion yield better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.	0,0,0,1,0,0
Ranks of rational points of the Jacobian varieties of hyperelliptic curves	In this paper, we obtain bounds for the Mordell-Weil ranks over cyclotomic extensions of a wide range of abelian varieties defined over a number field $F$ whose primes above $p$ are totally ramified over $F/\mathbb{Q}$. We assume that the abelian varieties may have good non-ordinary reduction at those primes. Our work is a generalization of \cite{Kim}, in which the second author generalized Perrin-Riou's Iwasawa theory for elliptic curves over $\mathbb{Q}$ with supersingular reduction (\cite{Perrin-Riou}) to elliptic curves defined over the above-mentioned number field $F$. On top of non-ordinary reduction and the ramification of the field $F$, we deal with the additional difficulty that the dimensions of the abelian varieties can be any number bigger than 1 which causes a variety of issues. As a result, we obtain bounds for the ranks over cyclotomic extensions $\mathbb{Q}(\mu_{p^{\max(M,N)+n}})$ of the Jacobian varieties of {\it ramified} hyperelliptic curves $y^{2p^M}=x^{3p^N}+ax^{p^N}+b$ among others.	0,0,1,0,0,0
On the Distribution, Model Selection Properties and Uniqueness of the Lasso Estimator in Low and High Dimensions	We derive expressions for the finite-sample distribution of the Lasso estimator in the context of a linear regression model with normally distributed errors in low as well as in high dimensions by exploiting the structure of the optimization problem defining the estimator. In low dimensions we assume full rank of the regressor matrix and present expressions for the cumulative distribution function as well as the densities of the absolutely continuous parts of the estimator. Additionally, we establish an explicit formula for the correspondence between the Lasso and the least-squares estimator. We derive analogous results for the distribution in less explicit form in high dimensions where we make no assumptions on the regressor matrix at all. In this setting, we also investigate the model selection properties of the Lasso and show that possibly only a subset of models might be selected by the estimator, completely independently of the observed response vector. Finally, we present a condition for uniqueness of the estimator that is necessary as well as sufficient.	0,0,1,1,0,0
Spatial analysis of airborne laser scanning point clouds for predicting forest variables	With recent developments in remote sensing technologies, plot-level forest resources can be predicted utilizing airborne laser scanning (ALS). The prediction is often assisted by mostly vertical summaries of the ALS point clouds. We present a spatial analysis of the point cloud by studying the horizontal distribution of the pulse returns through canopy height models thresholded at different height levels. The resulting patterns of patches of vegetation and gabs on each layer are summarized to spatial ALS features. We propose new features based on the Euler number, which is the number of patches minus the number of gaps, and the empty-space function, which is a spatial summary function of the gab space. The empty-space function is also used to describe differences in the gab structure between two different layers. We illustrate usefulness of the proposed spatial features for predicting different forest variables that summarize the spatial structure of forests or their breast height diameter distribution. We employ the proposed spatial features, in addition to commonly used features from literature, in the well-known k-nn estimation method to predict the forest variables. We present the methodology on the example of a study site in Central Finland.	0,0,0,1,1,0
Constraints on a possible evolution of mass density power-law index in strong gravitational lensing from cosmological data	In this work, by using strong gravitational lensing (SGL) observations along with Type Ia Supernovae (Union2.1) and gamma ray burst data (GRBs), we propose a new method to study a possible redshift evolution of $\gamma(z)$, the mass density power-law index of strong gravitational lensing systems. In this analysis, we assume the validity of cosmic distance duality relation and the flat universe. In order to explore the $\gamma(z)$ behavior, three different parametrizations are considered, namely: (P1) $\gamma(z_l)=\gamma_0+\gamma_1 z_l$, (P2) $\gamma(z_l)=\gamma_0+\gamma_1 z_l/(1+z_l)$ and (P3) $\gamma(z_l)=\gamma_0+\gamma_1 \ln(1+z_l)$, where $z_l$ corresponds to lens redshift. If $\gamma_0=2$ and $\gamma_1=0$ the singular isothermal sphere model is recovered. Our method is performed on SGL sub-samples defined by different lens redshifts and velocity dispersions. For the former case, the results are in full agreement with each other, while a 1$\sigma$ tension between the sub-samples with low ($\leq 250$ km/s) and high ($>250$ km/s) velocity dispersions was obtained on the ($\gamma_0$-$\gamma_1$) plane. By considering the complete SGL sample, we obtain $\gamma_0 \approx 2$ and $ \gamma_1 \approx 0$ within 1$\sigma$ c.l. for all $\gamma(z)$ parametrizations. However, we find the following best fit values of $\gamma_1$: $-0.085$, $-0.16$ and $-0.12$ for P1, P2 and P3 parametrizations, respectively, suggesting a mild evolution for $\gamma(z)$. By repeating the analysis with Type Ia Supernovae from JLA compilation, GRBs and SGL systems this mild evolution is reinforced.	0,1,0,0,0,0
Design of a Robotic System for Diagnosis and Rehabilitation of Lower Limbs	Currently, lower limb robotic rehabilitation is widely developed, However, the devices used so far seem to not have a uniform criteria for their design, because, on the contrary, each developed mechanism is often presented as if it does not take into account the criteria used in previous designs. On the other hand, the diagnosis of lower limb from robotic devices has been little studied. This chapter presents a guide for the design of robotic devices in diagnosis of lower limbs, taking into account the mobility of the human leg and the techniques used by physiotherapists in the execution of exercises and the rehabilitation of rehabilitation and diagnosis tests, as well as the recommendations made by various authors, among other aspects. The proposed guide is illustrated through a case study based on a parallel robot RPU+3UPS able to make movements that are applied during the processes of rehabilitation and diagnosis. The proposal presents advantages over some existing devices such as its load capacity that can support, and also allows you to restrict the movement in directions required by the rehabilitation and the diagnosis movements.	1,1,0,0,0,0
A New Perspective on Robust $M$-Estimation: Finite Sample Theory and Applications to Dependence-Adjusted Multiple Testing	Heavy-tailed errors impair the accuracy of the least squares estimate, which can be spoiled by a single grossly outlying observation. As argued in the seminal work of Peter Huber in 1973 [{\it Ann. Statist.} {\bf 1} (1973) 799--821], robust alternatives to the method of least squares are sorely needed. To achieve robustness against heavy-tailed sampling distributions, we revisit the Huber estimator from a new perspective by letting the tuning parameter involved diverge with the sample size. In this paper, we develop nonasymptotic concentration results for such an adaptive Huber estimator, namely, the Huber estimator with the tuning parameter adapted to sample size, dimension, and the variance of the noise. Specifically, we obtain a sub-Gaussian-type deviation inequality and a nonasymptotic Bahadur representation when noise variables only have finite second moments. The nonasymptotic results further yield two conventional normal approximation results that are of independent interest, the Berry-Esseen inequality and Cramér-type moderate deviation. As an important application to large-scale simultaneous inference, we apply these robust normal approximation results to analyze a dependence-adjusted multiple testing procedure for moderately heavy-tailed data. It is shown that the robust dependence-adjusted procedure asymptotically controls the overall false discovery proportion at the nominal level under mild moment conditions. Thorough numerical results on both simulated and real datasets are also provided to back up our theory.	0,0,1,1,0,0
Extensile actomyosin?	Living cells move thanks to assemblies of actin filaments and myosin motors that range from very organized striated muscle tissue to disordered intracellular bundles. The mechanisms powering these disordered structures are debated, and all models studied so far predict that they are contractile. We reexamine this prediction through a theoretical treatment of the interplay of three well-characterized internal dynamical processes in actomyosin bundles: actin treadmilling, the attachement-detachment dynamics of myosin and that of crosslinking proteins. We show that these processes enable an extensive control of the bundle's active mechanics, including reversals of the filaments' apparent velocities and the possibility of generating extension instead of contraction. These effects offer a new perspective on well-studied in vivo systems, as well as a robust criterion to experimentally elucidate the underpinnings of actomyosin activity.	0,1,0,0,0,0
Infinite Matrix Product States vs Infinite Projected Entangled-Pair States on the Cylinder: a comparative study	In spite of their intrinsic one-dimensional nature matrix product states have been systematically used to obtain remarkably accurate results for two-dimensional systems. Motivated by basic entropic arguments favoring projected entangled-pair states as the method of choice, we assess the relative performance of infinite matrix product states and infinite projected entangled-pair states on cylindrical geometries. By considering the Heisenberg and half-filled Hubbard models on the square lattice as our benchmark cases, we evaluate their variational energies as a function of both bond dimension as well as cylinder width. In both examples we find crossovers at moderate cylinder widths, i.e. for the largest bond dimensions considered we find an improvement on the variational energies for the Heisenberg model by using projected entangled-pair states at a width of about 11 sites, whereas for the half-filled Hubbard model this crossover occurs at about 7 sites.	0,1,0,0,0,0
Holographic Neural Architectures	Representation learning is at the heart of what makes deep learning effective. In this work, we introduce a new framework for representation learning that we call "Holographic Neural Architectures" (HNAs). In the same way that an observer can experience the 3D structure of a holographed object by looking at its hologram from several angles, HNAs derive Holographic Representations from the training set. These representations can then be explored by moving along a continuous bounded single dimension. We show that HNAs can be used to make generative networks, state-of-the-art regression models and that they are inherently highly resistant to noise. Finally, we argue that because of their denoising abilities and their capacity to generalize well from very few examples, models based upon HNAs are particularly well suited for biological applications where training examples are rare or noisy.	0,0,0,1,1,0
Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks	The life of the modern world essentially depends on the work of the large artificial homogeneous networks, such as wired and wireless communication systems, networks of roads and pipelines. The support of their effective continuous functioning requires automatic screening and permanent optimization with processing of the huge amount of data by high-performance distributed systems. We propose new meta-algorithm of large homogeneous network analysis, its decomposition into alternative sets of loosely connected subnets, and parallel optimization of the most independent elements. This algorithm is based on a network-specific correlation function, Simulated Annealing technique, and is adapted to work in the computer cluster. On the example of large wireless network, we show that proposed algorithm essentially increases speed of parallel optimization. The elaborated general approach can be used for analysis and optimization of the wide range of networks, including such specific types as artificial neural networks or organized in networks physiological systems of living organisms.	1,0,1,0,0,0
Meromorphic Jacobi Forms of Half-Integral Index and Umbral Moonshine Modules	In this work we consider an association of meromorphic Jacobi forms of half-integral index to the pure D-type cases of umbral moonshine, and solve the module problem for four of these cases by constructing vertex operator superalgebras that realise the corresponding meromorphic Jacobi forms as graded traces. We also present a general discussion of meromorphic Jacobi forms with half-integral index and their relationship to mock modular forms.	0,0,1,0,0,0
Implementing focal-plane phase masks optimized for real telescope apertures with SLM-based digital adaptive coronagraphy	Direct imaging of exoplanets or circumstellar disk material requires extreme contrast at the 10-6 to 10-12 levels at < 100 mas angular separation from the star. Focal-plane mask (FPM) coronagraphic imaging has played a key role in this field, taking advantage of progress in Adaptive Optics on ground-based 8+m class telescopes. However, large telescope entrance pupils usually consist of complex, sometimes segmented, non-ideal apertures, which include a central obstruction for the secondary mirror and its support structure. In practice, this negatively impacts wavefront quality and coronagraphic performance, in terms of achievable contrast and inner working angle. Recent theoretical works on structured darkness have shown that solutions for FPM phase profiles, optimized for non-ideal apertures, can be numerically derived. Here we present and discuss a first experimental validation of this concept, using reflective liquid crystal spatial light modulators as adaptive FPM coronagraphs.	0,1,0,0,0,0
Well-posedness of the Two-dimensional Nonlinear Schrödinger Equation with Concentrated Nonlinearity	We consider a two-dimensional nonlinear Schrödinger equation with concentrated nonlinearity. In both the focusing and defocusing case we prove local well-posedness, i.e., existence and uniqueness of the solution for short times, as well as energy and mass conservation. In addition, we prove that this implies global existence in the defocusing case, irrespective of the power of the nonlinearity, while in the focusing case blowing-up solutions may arise.	0,0,1,0,0,0
The Wisdom of Polarized Crowds	As political polarization in the United States continues to rise, the question of whether polarized individuals can fruitfully cooperate becomes pressing. Although diversity of individual perspectives typically leads to superior team performance on complex tasks, strong political perspectives have been associated with conflict, misinformation and a reluctance to engage with people and perspectives beyond one's echo chamber. It is unclear whether self-selected teams of politically diverse individuals will create higher or lower quality outcomes. In this paper, we explore the effect of team political composition on performance through analysis of millions of edits to Wikipedia's Political, Social Issues, and Science articles. We measure editors' political alignments by their contributions to conservative versus liberal articles. A survey of editors validates that those who primarily edit liberal articles identify more strongly with the Democratic party and those who edit conservative ones with the Republican party. Our analysis then reveals that polarized teams---those consisting of a balanced set of politically diverse editors---create articles of higher quality than politically homogeneous teams. The effect appears most strongly in Wikipedia's Political articles, but is also observed in Social Issues and even Science articles. Analysis of article "talk pages" reveals that politically polarized teams engage in longer, more constructive, competitive, and substantively focused but linguistically diverse debates than political moderates. More intense use of Wikipedia policies by politically diverse teams suggests institutional design principles to help unleash the power of politically polarized teams.	1,0,0,1,0,0
Explicit expression for the stationary distribution of reflected brownian motion in a wedge	For Brownian motion in a (two-dimensional) wedge with negative drift and oblique reflection on the axes, we derive an explicit formula for the Laplace transform of its stationary distribution (when it exists), in terms of Cauchy integrals and generalized Chebyshev polyno-mials. To that purpose we solve a Carleman-type boundary value problem on a hyperbola, satisfied by the Laplace transforms of the boundary stationary distribution.	0,0,1,0,0,0
Precision Prediction for the Cosmological Density Distribution	The distribution of matter in the universe is, to first order, lognormal. Improving this approximation requires characterization of the third moment (skewness) of the log density field. Thus, using Millennium Simulation phenomenology and building on previous work, we present analytic fits for the mean, variance, and skewness of the log density field $A$. We further show that a Generalized Extreme Value (GEV) distribution accurately models $A$; we submit that this GEV behavior is the result of strong intrapixel correlations, without which the smoothed distribution would tend (by the Central Limit Theorem) toward a Gaussian. Our GEV model yields cumulative distribution functions accurate to within 1.7 per cent for near-concordance cosmologies, over a range of redshifts and smoothing scales.	0,1,0,0,0,0
Quantum key distribution protocol with pseudorandom bases	Quantum key distribution (QKD) offers a way for establishing information-theoretically secure communications. An important part of QKD technology is a high-quality random number generator (RNG) for quantum states preparation and for post-processing procedures. In the present work, we consider a novel class of prepare-and-measure QKD protocols, utilizing additional pseudorandomness in the preparation of quantum states. We study one of such protocols and analyze its security against the intercept-resend attack. We demonstrate that, for single-photon sources, the considered protocol gives better secret key rates than the BB84 and the asymmetric BB84 protocol. However, the protocol strongly requires single-photon sources.	1,0,0,0,0,0
Extended Gray-Wyner System with Complementary Causal Side Information	We establish the rate region of an extended Gray-Wyner system for 2-DMS $(X,Y)$ with two additional decoders having complementary causal side information. This extension is interesting because in addition to the operationally significant extreme points of the Gray-Wyner rate region, which include Wyner's common information, G{á}cs-K{ö}rner common information and information bottleneck, the rate region for the extended system also includes the K{ö}rner graph entropy, the privacy funnel and excess functional information, as well as three new quantities of potential interest, as extreme points. To simplify the investigation of the 5-dimensional rate region of the extended Gray-Wyner system, we establish an equivalence of this region to a 3-dimensional mutual information region that consists of the set of all triples of the form $(I(X;U),\,I(Y;U),\,I(X,Y;U))$ for some $p_{U|X,Y}$. We further show that projections of this mutual information region yield the rate regions for many settings involving a 2-DMS, including lossless source coding with causal side information, distributed channel synthesis, and lossless source coding with a helper.	1,0,1,0,0,0
Numerical modelling of surface water wave interaction with a moving wall	In the present manuscript, we consider the practical problem of wave interaction with a vertical wall. However, the novelty here consists in the fact that the wall can move horizontally due to a system of springs. The water wave evolution is described with the free surface potential flow model. Then, a semi-analytical numerical method is presented. It is based on a mapping technique and a finite difference scheme in the transformed domain. The idea is to pose the equations on a fixed domain. This method is thoroughly tested and validated in our study. By choosing specific values of spring parameters, this system can be used to damp (or in other words to extract the energy of) incident water waves.	1,1,0,0,0,0
The fundamental group of the complement of the singular locus of Lauricella's $F_C$	We study the fundamental group of the complement of the singular locus of Lauricella's hypergeometric function $F_C$ of $n$ variables. The singular locus consists of $n$ hyperplanes and a hypersurface of degree $2^{n-1}$ in the complex $n$-space. We derive some relations that holds for general $n\geq 3$. We give an explicit presentation of the fundamental groupin the three-dimensional case. We also consider a presentation of the fundamental group of $2^3$-covering of this space. In the version 2, we omit some of the calculations. For all the calculations, refer to the version 1 (arXiv:1710.09594v1) of this article.	0,0,1,0,0,0
A Study on Arbitrarily Varying Channels with Causal Side Information at the Encoder	In this work, we study two models of arbitrarily varying channels, when causal side information is available at the encoder in a causal manner. First, we study the arbitrarily varying channel (AVC) with input and state constraints, when the encoder has state information in a causal manner. Lower and upper bounds on the random code capacity are developed. A lower bound on the deterministic code capacity is established in the case of a message-averaged input constraint. In the setting where a state constraint is imposed on the jammer, while the user is under no constraints, the random code bounds coincide, and the random code capacity is determined. Furthermore, for this scenario, a generalized non-symmetrizability condition is stated, under which the deterministic code capacity coincides with the random code capacity. A second model considered in our work is the arbitrarily varying degraded broadcast channel with causal side information at the encoder (without constraints). We establish inner and outer bounds on both the random code capacity region and the deterministic code capacity region. The capacity region is then determined for a class of channels satisfying a condition on the mutual informations between the strategy variables and the channel outputs. As an example, we show that the condition holds for the arbitrarily varying binary symmetric broadcast channel, and we find the corresponding capacity region.	1,0,1,0,0,0
Weak Keys and Cryptanalysis of a Cold War Block Cipher	T-310 is a cipher that was used for encryption of governmental communications in East Germany during the final years of the Cold War. Due to its complexity and the encryption process,there was no published attack for a period of more than 40 years until 2018 by Nicolas T. Courtois et al. in [10]. In this thesis we study the so called 'long term keys' that were used in the cipher, in order to expose weaknesses which will assist the design of various attacks on T-310.	1,0,0,0,0,0
Poisson brackets with prescribed family of functions in involution	It is well known that functions in involution with respect to Poisson brackets have a privileged role in the theory of completely integrable systems. Finding functionally independent functions in involution with a given function $h$ on a Poisson manifold is a fundamental problem of this theory and is very useful for the explicit integration of the equations of motion defined by $h$. In this paper, we present our results on the study of the inverse, so to speak, problem. By developing a technique analogous to that presented in P. Damianou and F. Petalidou, Poisson brackets with prescribed Casimirs, Canad. J. Math., 2012, vol. 64, 991-1018, for the establishment of Poisson brackets with prescribed Casimir invariants, we construct an algorithm which yields Poisson brackets having a given family of functions in involution. Our approach allows us to deal with bi-Hamiltonian structures constructively and therefore allows us to also deal with the completely integrable systems that arise in such a framework.	0,0,1,0,0,0
Self-Imitation Learning	This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.	0,0,0,1,0,0
Achieving rental harmony with a secretive roommate	Given the subjective preferences of n roommates in an n-bedroom apartment, one can use Sperner's lemma to find a division of the rent such that each roommate is content with a distinct room. At the given price distribution, no roommate has a strictly stronger preference for a different room. We give a new elementary proof that the subjective preferences of only n-1 of the roommates actually suffice to achieve this envy-free rent division. Our proof, in particular, yields an algorithm to find such a fair division of rent. The techniques also give generalizations of Sperner's lemma including a new proof of a conjecture of the third author.	0,0,1,0,0,0
Strong Bayesian Evidence for the Normal Neutrino Hierarchy	The configuration of the three neutrino masses can take two forms, known as the normal and inverted hierarchies. We compute the Bayesian evidence associated with these two hierarchies. Previous studies found a mild preference for the normal hierarchy, and this was driven by the asymmetric manner in which cosmological data has confined the available parameter space. Here we identify the presence of a second asymmetry, which is imposed by data from neutrino oscillations. By combining constraints on the squared-mass splittings with the limit on the sum of neutrino masses of $\Sigma m_\nu < 0.13$ eV, and using a minimally informative prior on the masses, we infer odds of 42:1 in favour of the normal hierarchy, which is classified as "strong" in the Jeffreys' scale. We explore how these odds may evolve in light of higher precision cosmological data, and discuss the implications of this finding with regards to the nature of neutrinos. Finally the individual masses are inferred to be $m_1 = 3.80^{+26.2}_{-3.73} \, \text{meV}, m_2 = 8.8^{+18}_{-1.2} \, \text{meV}, m_3 = 50.4^{+5.8}_{-1.2} \, \text{meV}$ ($95\%$ credible intervals).	0,1,0,0,0,0
Ab initio design of drug carriers for zoledronate guest molecule using phosphonated and sulfonated calix[4]arene and calix[4]resorcinarene host molecules	Monomolecular drug carriers based on calix[n]-arenes and -resorcinarenes containing the interior cavity can enhance the affinity and specificity of the osteoporosis inhibitor drug zoledronate (ZOD). In this work we investigate the suitability of nine different calix[4]-arenes and -resorcinarenes based macrocycles as hosts for the ZOD guest molecule by conducting {\it ab initio} density functional theory calculations for structures and energetics of eighteen different host-guest complexes. For the optimized molecular structures of the free, phosphonated, sulfonated calix[4]-arenes and -resorcinarenes, the geometric sizes of their interior cavities are measured and compared with those of the host-guest complexes in order to check the appropriateness for host-guest complex formation. Our calculations of binding energies indicate that in gaseous states some of the complexes might be unstable but in aqueous states almost all of the complexes can be formed spontaneously. Of the two different docking ways, the insertion of ZOD with the \ce{P-C-P} branch into the cavity of host is easier than that with the nitrogen containing heterocycle of ZOD. The work will open a way for developing effective drug delivering systems for the ZOD drug and promote experimentalists to synthesize them.	0,1,0,0,0,0
End-to-end distance and contour length distribution functions of DNA helices	We present a computational method to evaluate the end-to-end and the contour length distribution functions of short DNA molecules described by a mesoscopic Hamiltonian. The method generates a large statistical ensemble of possible configurations for each dimer in the sequence, selects the global equilibrium twist conformation for the molecule and determines the average base pair distances along the molecule backbone. Integrating over the base pair radial and angular fluctuations, we derive the room temperature distribution functions as a function of the sequence length. The obtained values for the most probable end-to-end distance and contour length distance, providing a measure of the global molecule size, are used to examine the DNA flexibility at short length scales. It is found that, also in molecules with less than $\sim 60$ base pairs, coiled configurations maintain a large statistical weight and, consistently, the persistence lengths may be much smaller than in kilo-base DNA.	0,0,0,0,1,0
Range-efficient consistent sampling and locality-sensitive hashing for polygons	Locality-sensitive hashing (LSH) is a fundamental technique for similarity search and similarity estimation in high-dimensional spaces. The basic idea is that similar objects should produce hash collisions with probability significantly larger than objects with low similarity. We consider LSH for objects that can be represented as point sets in either one or two dimensions. To make the point sets finite size we consider the subset of points on a grid. Directly applying LSH (e.g. min-wise hashing) to these point sets would require time proportional to the number of points. We seek to achieve time that is much lower than direct approaches. Technically, we introduce new primitives for range-efficient consistent sampling (of independent interest), and show how to turn such samples into LSH values. Another application of our technique is a data structure for quickly estimating the size of the intersection or union of a set of preprocessed polygons. Curiously, our consistent sampling method uses transformation to a geometric problem.	1,0,0,0,0,0
High-resolution photoelectron-spectroscopic investigation of the H$_2$O$^+$ cation in its ${\mathrm {\tilde A^+}}$ electronic state	The photoelectron spectrum of water has been recorded in the vicinity of the ${\mathrm {\tilde A^+}}$ $\leftarrow$ $\tilde{\mathrm{X}}$ transition between 112 000 and 116 000 cm$^{-1}$ (13.89-14.38 eV). The high-resolution allowed the observation of the rotational structure of several bands. Rotational assignments of the transitions involving the $\Pi(080)$, $\Sigma(070)$ and $\Pi(060)$ vibronic states of the $\tilde{\mathrm{A}}^+$ electronic state are deduced from previous studies of the $\tilde{\mathrm{A}}^+ - \tilde{\mathrm{X}}^+$ band system of H$_2$O$^+$ (Lew, Can. J. Phys. 54, 2028 (1976) and Huet et al., J. Chem. Phys. 107, 5645 (1997)) and photoionization selection rules. The transition to the $\Sigma(030)$ vibronic state is tentatively assigned.	0,1,0,0,0,0
From Query-By-Keyword to Query-By-Example: LinkedIn Talent Search Approach	One key challenge in talent search is to translate complex criteria of a hiring position into a search query, while it is relatively easy for a searcher to list examples of suitable candidates for a given position. To improve search efficiency, we propose the next generation of talent search at LinkedIn, also referred to as Search By Ideal Candidates. In this system, a searcher provides one or several ideal candidates as the input to hire for a given position. The system then generates a query based on the ideal candidates and uses it to retrieve and rank results. Shifting from the traditional Query-By-Keyword to this new Query-By-Example system poses a number of challenges: How to generate a query that best describes the candidates? When moving to a completely different paradigm, how does one leverage previous product logs to learn ranking models and/or evaluate the new system with no existing usage logs? Finally, given the different nature between the two search paradigms, the ranking features typically used for Query-By-Keyword systems might not be optimal for Query-By-Example. This paper describes our approach to solving these challenges. We present experimental results confirming the effectiveness of the proposed solution, particularly on query building and search ranking tasks. As of writing this paper, the new system has been available to all LinkedIn members.	1,0,0,0,0,0
An efficient data structure for counting all linear extensions of a poset, calculating its jump number, and the likes	Achieving the goals in the title (and others) relies on a cardinality-wise scanning of the ideals of the poset. Specifically, the relevant numbers attached to the k+1 element ideals are inferred from the corresponding numbers of the k-element (order) ideals. Crucial in all of this is a compressed representation (using wildcards) of the ideal lattice. The whole scheme invites distributed computation.	1,0,0,0,0,0
EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial Action Unit Detection	In this paper, we propose a deep learning based approach for facial action unit detection by enhancing and cropping the regions of interest. The approach is implemented by adding two novel nets (layers): the enhancing layers and the cropping layers, to a pretrained CNN model. For the enhancing layers, we designed an attention map based on facial landmark features and applied it to a pretrained neural network to conduct enhanced learning (The E-Net). For the cropping layers, we crop facial regions around the detected landmarks and design convolutional layers to learn deeper features for each facial region (C-Net). We then fuse the E-Net and the C-Net to obtain our Enhancing and Cropping (EAC) Net, which can learn both feature enhancing and region cropping functions. Our approach shows significant improvement in performance compared to the state-of-the-art methods applied to BP4D and DISFA AU datasets.	1,0,0,0,0,0
Unsteady Propulsion by an Intermittent Swimming Gait	Inviscid computational results are presented on a self-propelled swimmer modeled as a virtual body combined with a two-dimensional hydrofoil pitching intermittently about its leading edge. Lighthill (1971) originally proposed that this burst-and-coast behavior can save fish energy during swimming by taking advantage of the viscous Bone-Lighthill boundary layer thinning mechanism. Here, an additional inviscid Garrick mechanism is discovered that allows swimmers to control the ratio of their added mass thrust-producing forces to their circulatory drag-inducing forces by decreasing their duty cycle, DC, of locomotion. This mechanism can save intermittent swimmers as much as 60% of the energy it takes to swim continuously at the same speed. The inviscid energy savings are shown to increase with increasing amplitude of motion, increase with decreasing Lighthill number, Li, and switch to an energetic cost above continuous swimming for sufficiently low DC. Intermittent swimmers are observed to shed four vortices per cycle that form into groups that are self-similar with the DC. In addition, previous thrust and power scaling laws of continuous self-propelled swimming are further generalized to include intermittent swimming. The key is that by averaging the thrust and power coefficients over only the bursting period then the intermittent problem can be transformed into a continuous one. Furthermore, the intermittent thrust and power scaling relations are extended to predict the mean speed and cost of transport of swimmers. By tuning a few coefficients with a handful of simulations these self-propelled relations can become predictive. In the current study, the mean speed and cost of transport are predicted to within 3% and 18% of their full-scale values by using these relations.	0,1,0,0,0,0
Optimal portfolio selection in an Itô-Markov additive market	We study a portfolio selection problem in a continuous-time Itô-Markov additive market with prices of financial assets described by Markov additive processes which combine Lévy processes and regime switching models. Thus the model takes into account two sources of risk: the jump diffusion risk and the regime switching risk. For this reason the market is incomplete. We complete the market by enlarging it with the use of a set of Markovian jump securities, Markovian power-jump securities and impulse regime switching securities. Moreover, we give conditions under which the market is asymptotic-arbitrage-free. We solve the portfolio selection problem in the Itô-Markov additive market for the power utility and the logarithmic utility.	0,0,0,0,0,1
Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras	We study the near-infrared properties of 690 Mira candidates in the central region of the Large Magellanic Cloud, based on time-series observations at JHKs. We use densely-sampled I-band observations from the OGLE project to generate template light curves in the near infrared and derive robust mean magnitudes at those wavelengths. We obtain near-infrared Period-Luminosity relations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We study the Period-Luminosity-Color relations and the color excesses of Carbon-rich Miras, which show evidence for a substantially different reddening law.	0,0,0,1,0,0
Topology Analysis of International Networks Based on Debates in the United Nations	In complex, high dimensional and unstructured data it is often difficult to extract meaningful patterns. This is especially the case when dealing with textual data. Recent studies in machine learning, information theory and network science have developed several novel instruments to extract the semantics of unstructured data, and harness it to build a network of relations. Such approaches serve as an efficient tool for dimensionality reduction and pattern detection. This paper applies semantic network science to extract ideological proximity in the international arena, by focusing on the data from General Debates in the UN General Assembly on the topics of high salience to international community. UN General Debate corpus (UNGDC) covers all high-level debates in the UN General Assembly from 1970 to 2014, covering all UN member states. The research proceeds in three main steps. First, Latent Dirichlet Allocation (LDA) is used to extract the topics of the UN speeches, and therefore semantic information. Each country is then assigned a vector specifying the exposure to each of the topics identified. This intermediate output is then used in to construct a network of countries based on information theoretical metrics where the links capture similar vectorial patterns in the topic distributions. Topology of the networks is then analyzed through network properties like density, path length and clustering. Finally, we identify specific topological features of our networks using the map equation framework to detect communities in our networks of countries.	1,0,1,1,0,0
A Game of Life on Penrose tilings	We define rules for cellular automata played on quasiperiodic tilings of the plane arising from the multigrid method in such a way that these cellular automata are isomorphic to Conway's Game of Life. Although these tilings are nonperiodic, determining the next state of each tile is a local computation, requiring only knowledge of the local structure of the tiling and the states of finitely many nearby tiles. As an example, we show a version of a "glider" moving through a region of a Penrose tiling. This constitutes a potential theoretical framework for a method of executing computations in non-periodically structured substrates such as quasicrystals.	0,1,1,0,0,0
On Quitting: Performance and Practice in Online Game Play	We study the relationship between performance and practice by analyzing the activity of many players of a casual online game. We find significant heterogeneity in the improvement of player performance, given by score, and address this by dividing players into similar skill levels and segmenting each player's activity into sessions, i.e., sequence of game rounds without an extended break. After disaggregating data, we find that performance improves with practice across all skill levels. More interestingly, players are more likely to end their session after an especially large improvement, leading to a peak score in their very last game of a session. In addition, success is strongly correlated with a lower quitting rate when the score drops, and only weakly correlated with skill, in line with psychological findings about the value of persistence and "grit": successful players are those who persist in their practice despite lower scores. Finally, we train an epsilon-machine, a type of hidden Markov model, and find a plausible mechanism of game play that can predict player performance and quitting the game. Our work raises the possibility of real-time assessment and behavior prediction that can be used to optimize human performance.	1,0,0,0,0,0
Deep Sets	We study the problem of designing models for machine learning tasks defined on \emph{sets}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \cite{poczos13aistats}, to anomaly detection in piezometer data of embankment dams \cite{Jung15Exploration}, to cosmology \cite{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.	1,0,0,1,0,0
Synergistic Team Composition	Effective teams are crucial for organisations, especially in environments that require teams to be constantly created and dismantled, such as software development, scientific experiments, crowd-sourcing, or the classroom. Key factors influencing team performance are competences and personality of team members. Hence, we present a computational model to compose proficient and congenial teams based on individuals' personalities and their competences to perform tasks of different nature. With this purpose, we extend Wilde's post-Jungian method for team composition, which solely employs individuals' personalities. The aim of this study is to create a model to partition agents into teams that are balanced in competences, personality and gender. Finally, we present some preliminary empirical results that we obtained when analysing student performance. Results show the benefits of a more informed team composition that exploits individuals' competences besides information about their personalities.	1,0,0,0,0,0
Hindsight policy gradients	A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. In sparse-reward environments, the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended appears crucial to enable sample efficient learning. However, reinforcement learning agents have only recently been endowed with such capacity for hindsight. In this paper, we demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms. Our experiments on a diverse selection of sparse-reward environments show that hindsight leads to a remarkable increase in sample efficiency.	1,0,0,0,0,0
Coaction functors, II	In further study of the application of crossed-product functors to the Baum-Connes Conjecture, Buss, Echterhoff, and Willett introduced various other properties that crossed-product functors may have. Here we introduce and study analogues of these properties for coaction functors, making sure that the properties are preserved when the coaction functors are composed with the full crossed product to make a crossed-product functor. The new properties for coaction functors studied here are functoriality for generalized homomorphisms and the correspondence property. We particularly study the connections with the ideal property. The study of functoriality for generalized homomorphisms requires a detailed development of the Fischer construction of maximalization of coactions with regard to possibly degenerate homomorphisms into multiplier algebras. We verify that all "KLQ" functors arising from large ideals of the Fourier-Stieltjes algebra $B(G)$ have all the properties we study, and at the opposite extreme we give an example of a coaction functor having none of the properties.	0,0,1,0,0,0
Cosmology from conservation of global energy	It is argued that many of the problems and ambiguities of standard cosmology derive from a single one: violation of conservation of energy in the standard paradigm. Standard cosmology satisfies conservation of local energy, however disregards the inherent global aspect of energy. We therefore explore conservation of the quasi-local Misner-Sharp energy within the causal horizon, which, as we argue, is necessarily an apparent horizon. Misner-Sharp energy assumes the presence of arbitrary mass-energy. Its conservation, however, yields "empty" de Sitter (open, flat, closed) as single cosmological solution, where Misner-Sharp total energy acts as cosmological constant and where the source of curvature energy is unidentified. It is argued that de Sitter is only apparently empty of matter. That is, total matter energy scales as curvature energy in open de Sitter, which causes evolution of the cosmic potential and induces gravitational time dilation. Curvature of time accounts completely for the extrinsic curvature, i.e., renders open de Sitter spatially flat. This explains the well known, surprising, spatial flatness of Misner-Sharp energy, even if extrinsic curvature is non-zero. The general relativistic derivation from Misner-Sharp energy is confirmed by a Machian equation of recessional and peculiar energy, which explicitly assumes the presence of matter. This relational model enhances interpretation. Time-dilated open de Sitter is spatially flat, dynamically close to $\Lambda$CDM, and is shown to be without the conceptual problems of concordance cosmology.	0,1,0,0,0,0
Variational Bayesian Complex Network Reconstruction	Complex network reconstruction is a hot topic in many fields. A popular data-driven reconstruction framework is based on lasso. However, it is found that, in the presence of noise, it may be inefficient for lasso to determine the network topology. This paper builds a new framework to cope with this problem. The key idea is to employ a series of linear regression problems to model the relationship between network nodes, and then to use an efficient variational Bayesian method to infer the unknown coefficients. Based on the obtained information, the network is finally reconstructed by determining whether two nodes connect with each other or not. The numerical experiments conducted with both synthetic and real data demonstrate that the new method outperforms lasso with regard to both reconstruction accuracy and running speed.	1,0,0,0,0,0
Scaling Law for Three-body Collisions in Identical Fermions with $p$-wave Interactions	We experimentally confirmed the threshold behavior and scattering length scaling law of the three-body loss coefficients in an ultracold spin-polarized gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the three-body loss coefficients as functions of temperature and scattering volume, and found that the threshold law and the scattering length scaling law hold in limited temperature and magnetic field regions. We also found that the breakdown of the scaling laws is due to the emergence of the effective-range term. This work is an important first step toward full understanding of the loss of identical fermions with $p$-wave interactions.	0,1,0,0,0,0
StackInsights: Cognitive Learning for Hybrid Cloud Readiness	Hybrid cloud is an integrated cloud computing environment utilizing a mix of public cloud, private cloud, and on-premise traditional IT infrastructures. Workload awareness, defined as a detailed full range understanding of each individual workload, is essential in implementing the hybrid cloud. While it is critical to perform an accurate analysis to determine which workloads are appropriate for on-premise deployment versus which workloads can be migrated to a cloud off-premise, the assessment is mainly performed by rule or policy based approaches. In this paper, we introduce StackInsights, a novel cognitive system to automatically analyze and predict the cloud readiness of workloads for an enterprise. Our system harnesses the critical metrics across the entire stack: 1) infrastructure metrics, 2) data relevance metrics, and 3) application taxonomy, to identify workloads that have characteristics of a) low sensitivity with respect to business security, criticality and compliance, and b) low response time requirements and access patterns. Since the capture of the data relevance metrics involves an intrusive and in-depth scanning of the content of storage objects, a machine learning model is applied to perform the business relevance classification by learning from the meta level metrics harnessed across stack. In contrast to traditional methods, StackInsights significantly reduces the total time for hybrid cloud readiness assessment by orders of magnitude.	1,0,0,0,0,0
An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog	We present a novel end-to-end trainable neural network model for task-oriented dialog systems. The model is able to track dialog state, issue API calls to knowledge base (KB), and incorporate structured KB query results into system responses to successfully complete task-oriented dialogs. The proposed model produces well-structured system responses by jointly learning belief tracking and KB result processing conditioning on the dialog history. We evaluate the model in a restaurant search domain using a dataset that is converted from the second Dialog State Tracking Challenge (DSTC2) corpus. Experiment results show that the proposed model can robustly track dialog state given the dialog history. Moreover, our model demonstrates promising results in producing appropriate system responses, outperforming prior end-to-end trainable neural network models using per-response accuracy evaluation metrics.	1,0,0,0,0,0
Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning	We found an easy and quick post-learning method named "Icing on the Cake" to enhance a classification performance in deep learning. The method is that we train only the final classifier again after an ordinary training is done.	0,0,0,1,0,0
Timelike surfaces in Minkowski space with a canonical null direction	Given a constant vector field $Z$ in Minkowski space, a timelike surface is said to have a canonical null direction with respect to $Z$ if the projection of $Z$ on the tangent space of the surface gives a lightlike vector field. In this paper we describe these surfaces in the ruled case. For example when the Minkowski space has three dimensions then a surface with a canonical null direction is minimal and flat. On the other hand, we describe several properties in the non ruled case and we partially describe these surfaces in four-dimensional Minkowski space. We give different ways for building these surfaces in four-dimensional Minkowski space and we finally use the Gauss map for describe another properties of these surfaces.	0,0,1,0,0,0
Alignment, Orientation, and Coulomb Explosion of Difluoroiodobenzene Studied with the Pixel Imaging Mass Spectrometry (PImMS) Camera	Laser-induced adiabatic alignment and mixed-field orientation of 2,6-difluoroiodobenzene (C6H3F2I) molecules are probed by Coulomb explosion imaging following either near-infrared strong-field ionization or extreme-ultraviolet multi-photon inner-shell ionization using free-electron laser pulses. The resulting photoelectrons and fragment ions are captured by a double-sided velocity map imaging spectrometer and projected onto two position-sensitive detectors. The ion side of the spectrometer is equipped with the Pixel Imaging Mass Spectrometry (PImMS) camera, a time-stamping pixelated detector that can record the hit positions and arrival times of up to four ions per pixel per acquisition cycle. Thus, the time-of-flight trace and ion momentum distributions for all fragments can be recorded simultaneously. We show that we can obtain a high degree of one- and three-dimensional alignment and mixed- field orientation, and compare the Coulomb explosion process induced at both wavelengths.	0,1,0,0,0,0
Derivation of the cutoff length from the quantum quadratic enhancement of a mass in vacuum energy constant Lambda	Ultraviolet self-interaction energies in field theory sometimes contain meaningful physical quantities. The self-energies in such as classical electrodynamics are usually subtracted from the rest mass. For the consistent treatment of energies as sources of curvature in the Einstein field equations, this study includes these subtracted self-energies into vacuum energy expressed by the constant Lambda (used in such as Lambda-CDM). In this study, the self-energies in electrodynamics and macroscopic classical Einstein field equations are examined, using the formalisms with the ultraviolet cutoff scheme. One of the cutoff formalisms is the field theory in terms of the step-function-type basis functions, developed by the present authors. The other is a continuum theory of a fundamental particle with the same cutoff length. Based on the effectiveness of the continuum theory with the cutoff length shown in the examination, the dominant self-energy is the quadratic term of the Higgs field at a quantum level (classical self-energies are reduced to logarithmic forms by quantum corrections). The cutoff length is then determined to reproduce today's tiny value of Lambda for vacuum energy. Additionally, a field with nonperiodic vanishing boundary conditions is treated, showing that the field has no zero-point energy.	0,1,0,0,0,0
Generating retinal flow maps from structural optical coherence tomography with artificial intelligence	Despite significant advances in artificial intelligence (AI) for computer vision, its application in medical imaging has been limited by the burden and limits of expert-generated labels. We used images from optical coherence tomography angiography (OCTA), a relatively new imaging modality that measures perfusion of the retinal vasculature, to train an AI algorithm to generate vasculature maps from standard structural optical coherence tomography (OCT) images of the same retinae, both exceeding the ability and bypassing the need for expert labeling. Deep learning was able to infer perfusion of microvasculature from structural OCT images with similar fidelity to OCTA and significantly better than expert clinicians (P < 0.00001). OCTA suffers from need of specialized hardware, laborious acquisition protocols, and motion artifacts; whereas our model works directly from standard OCT which are ubiquitous and quick to obtain, and allows unlocking of large volumes of previously collected standard OCT data both in existing clinical trials and clinical practice. This finding demonstrates a novel application of AI to medical imaging, whereby subtle regularities between different modalities are used to image the same body part and AI is used to generate detailed and accurate inferences of tissue function from structure imaging.	0,0,0,1,0,0
A Lichnerowicz estimate for the spectral gap of the sub-Laplacian	For a second order operator on a compact manifold satisfying the strong Hörmander condition, we give a bound for the spectral gap analogous to the Lichnerowicz estimate for the Laplacian of a Riemannian manifold. We consider a wide class of such operators which includes horizontal lifts of the Laplacian on Riemannian submersions with minimal leaves.	0,0,1,0,0,0
Models of fault-tolerant distributed computation via dynamic epistemic logic	The computability power of a distributed computing model is determined by the communication media available to the processes, the timing assumptions about processes and communication, and the nature of failures that processes can suffer. In a companion paper we showed how dynamic epistemic logic can be used to give a formal semantics to a given distributed computing model, to capture precisely the knowledge needed to solve a distributed task, such as consensus. Furthermore, by moving to a dual model of epistemic logic defined by simplicial complexes, topological invariants are exposed, which determine task solvability. In this paper we show how to extend the setting above to include in the knowledge of the processes, knowledge about the model of computation itself. The extension describes the knowledge processes gain about the current execution, in problems where processes have no input values at all.	1,0,1,0,0,0
Future Energy Consumption Prediction Based on Grey Forecast Model	We use grey forecast model to predict the future energy consumption of four states in the U.S, and make some improvments to the model.	0,0,0,1,0,0
Constructing confidence sets for the matrix completion problem	In the present note we consider the problem of constructing honest and adaptive confidence sets for the matrix completion problem. For the Bernoulli model with known variance of the noise we provide a realizable method for constructing confidence sets that adapt to the unknown rank of the true matrix.	0,0,1,1,0,0
Gender Disparities in Science? Dropout, Productivity, Collaborations and Success of Male and Female Computer Scientists	Scientific collaborations shape ideas as well as innovations and are both the substrate for, and the outcome of, academic careers. Recent studies show that gender inequality is still present in many scientific practices ranging from hiring to peer-review processes and grant applications. In this work, we investigate gender-specific differences in collaboration patterns of more than one million computer scientists over the course of 47 years. We explore how these patterns change over years and career ages and how they impact scientific success. Our results highlight that successful male and female scientists reveal the same collaboration patterns: compared to scientists in the same career age, they tend to collaborate with more colleagues than other scientists, seek innovations as brokers and establish longer-lasting and more repetitive collaborations. However, women are on average less likely to adapt the collaboration patterns that are related with success, more likely to embed into ego networks devoid of structural holes, and they exhibit stronger gender homophily as well as a consistently higher dropout rate than men in all career ages.	1,1,0,0,0,0
Nutrients and biomass dynamics in photo-sequencing batch reactors treating wastewater with high nutrients loadings	The present study investigates different strategies for the treatment of a mixture of digestate from an anaerobic digester diluted and secondary effluent from a high rate algal pond. To this aim, the performance of two photo-sequencing batch reactors (PSBRs) operated at high nutrients loading rates and different solids retention times (SRTs) were compared with a semi-continuous photobioreactor (SC). Performances were evaluated in terms of wastewater treatment, biomass composition and biopolymers accumulation during 30 days of operation. PSBRs were operated at a hydraulic retention time (HRT) of 2 days and SRTs of 10 and 5 days (PSBR2-10 and PSBR2-5, respectively), whereas the semi-continuous reactor was operated at a coupled HRT/SRT of 10 days (SC10-10). Results showed that PSBR2-5 achieved the highest removal rates in terms of TN (6.7 mg L-1 d-1), TP (0.31 mg L-1 d-1), TOC (29.32 mg L-1 d-1) and TIC (3.91 mg L-1 d-1). These results were in general 3-6 times higher than the removal rates obtained in the SC10-10 (TN 29.74 mg L-1 d-1, TP 0.96 mg L-1 d-1, TOC 29.32 mg L-1 d-1 and TIC 3.91 mg L-1 d-1). Furthermore, both PSBRs were able to produce biomass up to 0.09 g L-1 d-1, more than twofold the biomass produced by the semi-continuous reactor (0.04 g L-1 d-1), and achieved a biomass settleability of 86-92%. This study also demonstrated that the microbial composition could be controlled by the nutrients loads, since the three reactors were dominated by different species depending on the nutritional conditions. Concerning biopolymers accumulation, carbohydrates concentration achieved similar values in the three reactors (11%), whereas <0.5 % of polyhydrohybutyrates (PHB) was produced. These low values in biopolymers production could be related to the lack of microorganisms as cyanobacteria that are able to accumulate carbohydrates/PHB.	0,0,0,0,1,0
The closure of ideals of $\boldsymbol{\ell^1(Σ)}$ in its enveloping $\boldsymbol{\mathrm{C}^\ast}$-algebra	If $X$ is a compact Hausdorff space and $\sigma$ is a homeomorphism of $X$, then an involutive Banach algebra $\ell^1(\Sigma)$ of crossed product type is naturally associated with the topological dynamical system $\Sigma=(X,\sigma)$. We initiate the study of the relation between two-sided ideals of $\ell^1(\Sigma)$ and ${\mathrm C}^\ast(\Sigma)$, the enveloping $\mathrm{C}^\ast$-algebra ${\mathrm C}(X)\rtimes_\sigma \mathbb Z$ of $\ell^1(\Sigma)$. Among others, we prove that the closure of a proper two-sided ideal of $\ell^1(\Sigma)$ in ${\mathrm C}^\ast(\Sigma)$ is again a proper two-sided ideal of ${\mathrm C}^\ast(\Sigma)$.	0,0,1,0,0,0
Gaussian autoregressive process with dependent innovations. Some asymptotic results	In this paper we introduce a modified version of a gaussian standard first-order autoregressive process where we allow for a dependence structure between the state variable $Y_{t-1}$ and the next innovation $\xi_t$. We call this model dependent innovations gaussian AR(1) process (DIG-AR(1)). We analyze the moment and temporal dependence properties of the new model. After proving that the OLS estimator does not consistently estimate the autoregressive parameter, we introduce an infeasible estimator and we provide its $\sqrt{T}$-asymptotic normality.	0,0,1,1,0,0
Linear-time approximation schemes for planar minimum three-edge connected and three-vertex connected spanning subgraphs	We present the first polynomial-time approximation schemes, i.e., (1 + {\epsilon})-approximation algorithm for any constant {\epsilon} > 0, for the minimum three-edge connected spanning subgraph problem and the minimum three-vertex connected spanning subgraph problem in undirected planar graphs. Both the approximation schemes run in linear time.	1,0,0,0,0,0
ArchiveWeb: collaboratively extending and exploring web archive collections - How would you like to work with your collections?	Curated web archive collections contain focused digital content which is collected by archiving organizations, groups, and individuals to provide a representative sample covering specific topics and events to preserve them for future exploration and analysis. In this paper, we discuss how to best support collaborative construction and exploration of these collections through the ArchiveWeb system. ArchiveWeb has been developed using an iterative evaluation-driven design-based research approach, with considerable user feedback at all stages. The first part of this paper describes the important insights we gained from our initial requirements engineering phase during the first year of the project and the main functionalities of the current ArchiveWeb system for searching, constructing, exploring, and discussing web archive collections. The second part summarizes the feedback we received on this version from archiving organizations and libraries, as well as our corresponding plans for improving and extending the system for the next release.	1,0,0,0,0,0
Bendable Cuboid Robot Path Planning with Collision Avoidance using Generalized $L_p$ Norms	Optimal path planning problems for rigid and deformable (bendable) cuboid robots are considered by providing an analytic safety constraint using generalized $L_p$ norms. For regular cuboid robots, level sets of weighted $L_p$ norms generate implicit approximations of their surfaces. For bendable cuboid robots a weighted $L_p$ norm in polar coordinates implicitly approximates the surface boundary through a specified level set. Obstacle volumes, in the environment to navigate within, are presumed to be approximately described as sub-level sets of weighted $L_p$ norms. Using these approximate surface models, the optimal safe path planning problem is reformulated as a two stage optimization problem, where the safety constraint depends on a point on the robot which is closest to the obstacle in the obstacle's distance metric. A set of equality and inequality constraints are derived to replace the closest point problem, which is then defines additional analytic constraints on the original path planning problem. Combining all the analytic constraints with logical AND operations leads to a general optimal safe path planning problem. Numerically solving the problem involve conversion to a nonlinear programing problem. Simulations for rigid and bendable cuboid robot verify the proposed method.	1,0,0,0,0,0
The Formal Semantics of Rascal Light	Rascal is a high-level transformation language that aims to simplify software language engineering tasks like defining program syntax, analyzing and transforming programs, and performing code generation. The language provides several features including built-in collections (lists, sets, maps), algebraic data-types, powerful pattern matching operations with backtracking, and high-level traversals supporting multiple strategies. Interaction between different language features can be difficult to comprehend, since most features are semantically rich. The report provides a well-defined formal semantics for a large subset of Rascal, called Rascal Light, suitable for developing formal techniques, e.g., type systems and static analyses. Additionally, the report states and proofs a series of interesting properties of the semantics, including purity of backtracking, strong typing, partial progress and the existence of a terminating subset.	1,0,0,0,0,0
Tensor Completion Algorithms in Big Data Analytics	Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.	1,0,0,1,0,0
Two-photon imaging assisted by a dynamic random medium	Random scattering is usually viewed as a serious nuisance in optical imaging, and needs to be prevented in the conventional imaging scheme based on single-photon interference. Here we proposed a two-photon imaging scheme with the widely used lens replaced by a dynamic random medium. In contrast to destroying imaging process, the dynamic random medium in our scheme works as a crucial imaging element to bring constructive interference, and allows us to image an object from light field scattered by this dynamic random medium. On the one hand, our imaging scheme with incoherent two-photon illumination enables us to achieve super-resolution imaging with the resolution reaching Heisenberg limit. On the other hand, with coherent two-photon illumination, the image of a pure-phase object can be obtained in our imaging scheme. These results show new possibilities to overcome bottleneck of widely used single-photon imaging by developing imaging method based on multi-photon interference.	0,1,0,0,0,0
Theoretical aspects of microscale acoustofluidics	Henrik Bruus is professor of lab-chip systems and theoretical physics at the Technical University of Denmark. In this contribution, he summarizes some of the recent results within theory and simulation of microscale acoustofluidic systems that he has obtained in collaboration with his students and international colleagues. The main emphasis is on three dynamical effects induced by external ultrasound fields acting on aqueous solutions and particle suspensions: The acoustic radiation force acting on suspended micro- and nanoparticles, the acoustic streaming appearing in the fluid, and the newly discovered acoustic body force acting on inhomogeneous solutions.	0,0,0,0,1,0
Overfitting Mechanism and Avoidance in Deep Neural Networks	Assisted by the availability of data and high performance computing, deep learning techniques have achieved breakthroughs and surpassed human performance empirically in difficult tasks, including object recognition, speech recognition, and natural language processing. As they are being used in critical applications, understanding underlying mechanisms for their successes and limitations is imperative. In this paper, we show that overfitting, one of the fundamental issues in deep neural networks, is due to continuous gradient updating and scale sensitiveness of cross entropy loss. By separating samples into correctly and incorrectly classified ones, we show that they behave very differently, where the loss decreases in the correct ones and increases in the incorrect ones. Furthermore, by analyzing dynamics during training, we propose a consensus-based classification algorithm that enables us to avoid overfitting and significantly improve the classification accuracy especially when the number of training samples is limited. As each trained neural network depends on extrinsic factors such as initial values as well as training data, requiring consensus among multiple models reduces extrinsic factors substantially; for statistically independent models, the reduction is exponential. Compared to ensemble algorithms, the proposed algorithm avoids overgeneralization by not classifying ambiguous inputs. Systematic experimental results demonstrate the effectiveness of the proposed algorithm. For example, using only 1000 training samples from MNIST dataset, the proposed algorithm achieves 95% accuracy, significantly higher than any of the individual models, with 90% of the test samples classified.	1,0,0,1,0,0
Latent Mixture Modeling for Clustered Data	This article proposes a mixture modeling approach to estimating cluster-wise conditional distributions in clustered (grouped) data. We adapt the mixture-of-experts model to the latent distributions, and propose a model in which each cluster-wise density is represented as a mixture of latent experts with cluster-wise mixing proportions distributed as Dirichlet distribution. The model parameters are estimated by maximizing the marginal likelihood function using a newly developed Monte Carlo Expectation-Maximization algorithm. We also extend the model such that the distribution of cluster-wise mixing proportions depends on some cluster-level covariates. The finite sample performance of the proposed model is compared with some existing mixture modeling approaches as well as linear mixed model through the simulation studies. The proposed model is also illustrated with the posted land price data in Japan.	0,0,0,1,0,0
Hierarchy of exchange interactions in the triangular-lattice spin-liquid YbMgGaO$_{4}$	The spin-1/2 triangular lattice antiferromagnet YbMgGaO$_{4}$ has attracted recent attention as a quantum spin-liquid candidate with the possible presence of off-diagonal anisotropic exchange interactions induced by spin-orbit coupling. Whether a quantum spin-liquid is stabilized or not depends on the interplay of various exchange interactions with chemical disorder that is inherent to the layered structure of the compound. We combine time-domain terahertz spectroscopy and inelastic neutron scattering measurements in the field polarized state of YbMgGaO$_{4}$ to obtain better microscopic insights on its exchange interactions. Terahertz spectroscopy in this fashion functions as high-field electron spin resonance and probes the spin-wave excitations at the Brillouin zone center, ideally complementing neutron scattering. A global spin-wave fit to all our spectroscopic data at fields over 4T, informed by the analysis of the terahertz spectroscopy linewidths, yields stringent constraints on $g$-factors and exchange interactions. Our results paint YbMgGaO$_{4}$ as an easy-plane XXZ antiferromagnet with the combined and necessary presence of sub-leading next-nearest neighbor and weak anisotropic off-diagonal nearest-neighbor interactions. Moreover, the obtained $g$-factors are substantially different from previous reports. This works establishes the hierarchy of exchange interactions in YbMgGaO$_{4}$ from high-field data alone and thus strongly constrains possible mechanisms responsible for the observed spin-liquid phenomenology.	0,1,0,0,0,0
Vector bundles and modular forms for Fuchsian groups of genus zero	This article lays the foundations for the study of modular forms transforming with respect to representations of Fuchsian groups of genus zero. More precisely, we define geometrically weighted graded modules of such modular forms, where the graded structure comes from twisting with all isomorphism classes of line bundles on the corresponding compactified modular curve, and we study their structure by relating it to the structure of vector bundles over orbifold curves of genus zero. We prove that these modules are free whenever the Fuchsian group has at most two elliptic points. For three or more elliptic points, we give explicit constructions of indecomposable vector bundles of rank two over modular orbifold curves, which give rise to non-free modules of geometrically weighted modular forms.	0,0,1,0,0,0
Classical System of Martin-Lof's Inductive Definitions is not Equivalent to Cyclic Proofs	A cyclic proof system, called CLKID-omega, gives us another way of representing inductive definitions and effcient proof search. The 2011 paper by Brotherston and Simpson showed that the provability of CLKID-omega includes the provability of Martin-Lof's system of inductive definitions, called LKID, and conjectured the equivalence. Since then, the equivalence has been left an open question. This paper shows that CLKID-omega and LKID are indeed not equivalent. This paper considers a statement called 2-Hydra in these two systems with the first-order language formed by 0, the successor, the natural number predicate, and a binary predicate symbol used to express 2-Hydra. This paper shows that the 2-Hydra statement is provable in CLKID-omega, but the statement is not provable in LKID, by constructing some Henkin model where the statement is false.	1,0,0,0,0,0
On the normal centrosymmetric Nonnegative inverse eigenvalue problem	We give sufficient conditions of the nonnegative inverse eigenvalue problem (NIEP) for normal centrosymmetric matrices. These sufficient conditions are analogous to the sufficient conditions of the NIEP for normal matrices given by Xu [16] and Julio, Manzaneda and Soto [2].	0,0,1,0,0,0
Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition	This paper studies the emotion recognition from musical tracks in the 2-dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with the state-of-the-art method for the same task. We utilize one CNN layer followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the 'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset.	1,0,0,0,0,0
Core of communities in bipartite networks	We use the information present in a bipartite network to detect cores of communities of each set of the bipartite system. Cores of communities are found by investigating statistically validated projected networks obtained using information present in the bipartite network. Cores of communities are highly informative and robust with respect to the presence of errors or missing entries in the bipartite network. We assess the statistical robustness of cores by investigating an artificial benchmark network, the co-authorship network, and the actor-movie network. The accuracy and precision of the partition obtained with respect to the reference partition are measured in terms of the adjusted Rand index and of the adjusted Wallace index respectively. The detection of cores is highly precise although the accuracy of the methodology can be limited in some cases.	1,1,0,0,0,0
Learning K-way D-dimensional Discrete Code For Compact Embedding Representations	Embedding methods such as word embedding have become pillars for many applications containing discrete structures. Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying linear transformation based on "one-hot" encoding of the discrete symbols. Despite its simplicity, such approach yields number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the "one-hot" encoding. In "KD encoding", each symbol is represented by a $D$-dimensional code, and each of its dimension has a cardinality of $K$. The final symbol embedding vector can be generated by composing the code embedding vectors. To learn the semantically meaningful code, we derive a relaxed discrete optimization technique based on stochastic gradient descent. By adopting the new coding system, the efficiency of parameterization can be significantly improved (from linear to logarithmic), and this can also mitigate the over-fitting problem. In our experiments with language modeling, the number of embedding parameters can be reduced by 97\% while achieving similar or better performance.	1,0,0,1,0,0
Comparison of forcing functions in magnetohydrodynamic turbulence	Results are presented of direct numerical simulations of incompressible, homogeneous magnetohydrodynamic turbulence without a mean magnetic field, subject to different mechanical forcing functions commonly used in the literature. Specifically, the forces are negative damping (which uses the large-scale velocity field as a forcing function), a nonhelical random force, and a nonhelical static sinusoidal force (analogous to helical ABC forcing). The time evolution of the three ideal invariants (energy, magnetic helicity and cross helicity), the time-averaged energy spectra, the energy ratios and the dissipation ratios are examined. All three forcing functions produce qualitatively similar steady states with regards to the time evolution of the energy and magnetic helicity. However, differences in the cross helicity evolution are observed, particularly in the case of the static sinusoidal method of energy injection. Indeed, an ensemble of sinusoidally-forced simulations with identical parameters shows significant variations in the cross helicity over long time periods, casting some doubt on the validity of the principle of ergodicity in systems in which the injection of helicity cannot be controlled. Cross helicity can unexpectedly enter the system through the forcing function and must be carefully monitored.	0,1,0,0,0,0
A quantized physical framework for understanding the working mechanism of ion channels	A quantized physical framework, called the five-anchor model, is developed for a general understanding of the working mechanism of ion channels. According to the hypotheses of this model, the following two basic physical principles are assigned to each anchor: the polarity change induced by an electron transition and the mutual repulsion and attraction induced by an electrostatic force. Consequently, many unique phenomena, such as fast and slow inactivation, the stochastic gating pattern and constant conductance of a single ion channel, the difference between electrical and optical stimulation (optogenetics), nerve conduction block and the generation of an action potential, become intrinsic features of this physical model. Moreover, this model also provides a foundation for the probability equation used to calculate the results of electrical stimulation in our previous C-P theory.	0,0,0,0,1,0
First principles study of structural, magnetic and electronic properties of CrAs	We report ab initio density functional calculations of the structural and magnetic properties, and the electronic structure of CrAs. To simulate the observed pressure-driven experimental results, we perform our analysis for different volumes of the unit cell, showing that the structural, magnetic and electronic properties strongly depend on the size of the cell. We find that the calculated quantities are in good agreement with the experimental data, and we review our results in terms of the observed superconductivity.	0,1,0,0,0,0
Crime Prediction by Data-Driven Green's Function method	We develop an algorithm that forecasts cascading events, by employing a Green's function scheme on the basis of the self-exciting point process model. This method is applied to open data of 10 types of crimes happened in Chicago. It shows a good prediction accuracy superior to or comparable to the standard methods which are the expectation-maximization method and prospective hotspot maps method. We find a cascade influence of the crimes that has a long-time, logarithmic tail; this result is consistent with an earlier study on burglaries. This long-tail feature cannot be reproduced by the other standard methods. In addition, a merit of the Green's function method is the low computational cost in the case of high density of events and/or large amount of the training data.	1,1,0,1,0,0
Parametric Identification Using Weighted Null-Space Fitting	In identification of dynamical systems, the prediction error method using a quadratic cost function provides asymptotically efficient estimates under Gaussian noise and additional mild assumptions, but in general it requires solving a non-convex optimization problem. An alternative class of methods uses a non-parametric model as intermediate step to obtain the model of interest. Weighted null-space fitting (WNSF) belongs to this class. It is a weighted least-squares method consisting of three steps. In the first step, a high-order ARX model is estimated. In a second least-squares step, this high-order estimate is reduced to a parametric estimate. In the third step, weighted least squares is used to reduce the variance of the estimates. The method is flexible in parametrization and suitable for both open- and closed-loop data. In this paper, we show that WNSF provides estimates with the same asymptotic properties as PEM with a quadratic cost function when the model orders are chosen according to the true system. Also, simulation studies indicate that WNSF may be competitive with state-of-the-art methods.	1,0,0,0,0,0
Periodic solution for strongly nonlinear oscillators by He's new amplitude-frequency relationship	This paper applies He's new amplitude-frequency relationship recently established by Ji-Huan He (Int J Appl Comput Math 3 1557-1560, 2017) to study periodic solutions of strongly nonlinear systems with odd nonlinearities. Some examples are given to illustrate the effectiveness, ease and convenience of the method. In general, the results are valid for small as well as large oscillation amplitude. The method can be easily extended to other nonlinear systems with odd nonlinearities and can therefore be found widely applicable in engineering and other science. The method used in this paper can be applied directly to highly nonlinear problems without any discretization, linearization or additional requirements.	0,1,0,0,0,0
Polarization properties of turbulent synchrotron bubbles: an approach based on Chandrasekhar-Kendall functions	Synchrotron emitting bubbles arise when the outflow from a compact relativistic engine, either a Black Hole or a Neutron Star, impacts on the environment. The emission properties of synchrotron radiation are widely used to infer the dynamical properties of these bubbles, and from them the injection conditions of the engine. Radio polarization offers an important tool to investigate the level and spectrum of turbulence, the magnetic field configuration, and possibly the degree of mixing. Here we introduce a formalism based on Chandrasekhar-Kendall functions that allows us to properly take into account the geometry of the bubble, going beyond standard analysis based on periodic cartesian domains. We investigate how different turbulent spectra, magnetic helicity and particle distribution function, impact on global properties that are easily accessible to observations, even at low resolution, and we provide fitting formulae to relate observed quantities to the underlying magnetic field structure.	0,1,0,0,0,0
Inferring Generative Model Structure with Static Analysis	Obtaining enough labeled data to robustly train complex discriminative models is a major bottleneck in the machine learning pipeline. A popular solution is combining multiple sources of weak supervision using generative models. The structure of these models affects training label quality, but is difficult to learn without any ground truth labels. We instead rely on these weak supervision sources having some structure by virtue of being encoded programmatically. We present Coral, a paradigm that infers generative model structure by statically analyzing the code for these heuristics, thus reducing the data required to learn structure significantly. We prove that Coral's sample complexity scales quasilinearly with the number of heuristics and number of relations found, improving over the standard sample complexity, which is exponential in $n$ for identifying $n^{\textrm{th}}$ degree relations. Experimentally, Coral matches or outperforms traditional structure learning approaches by up to 3.81 F1 points. Using Coral to model dependencies instead of assuming independence results in better performance than a fully supervised model by 3.07 accuracy points when heuristics are used to label radiology data without ground truth labels.	1,0,0,1,0,0
ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity?	Generating molecules with desired chemical properties is important for drug discovery. The use of generative neural networks is promising for this task. However, from visual inspection, it often appears that generated samples lack diversity. In this paper, we quantify this internal chemical diversity, and we raise the following challenge: can a nontrivial AI model reproduce natural chemical diversity for desired molecules? To illustrate this question, we consider two generative models: a Reinforcement Learning model and the recently introduced ORGAN. Both fail at this challenge. We hope this challenge will stimulate research in this direction.	1,0,0,1,0,0
Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems over Large Graphs	A regularized optimization problem over a large unstructured graph is studied, where the regularization term is tied to the graph geometry. Typical regularization examples include the total variation and the Laplacian regularizations over the graph. When applying the proximal gradient algorithm to solve this problem, there exist quite affordable methods to implement the proximity operator (backward step) in the special case where the graph is a simple path without loops. In this paper, an algorithm, referred to as "Snake", is proposed to solve such regularized problems over general graphs, by taking benefit of these fast methods. The algorithm consists in properly selecting random simple paths in the graph and performing the proximal gradient algorithm over these simple paths. This algorithm is an instance of a new general stochastic proximal gradient algorithm, whose convergence is proven. Applications to trend filtering and graph inpainting are provided among others. Numerical experiments are conducted over large graphs.	1,0,0,1,0,0
The Chandra Deep Field South as a test case for Global Multi Conjugate Adaptive Optics	The era of the next generation of giant telescopes requires not only the advent of new technologies but also the development of novel methods, in order to exploit fully the extraordinary potential they are built for. Global Multi Conjugate Adaptive Optics (GMCAO) pursues this approach, with the goal of achieving good performance over a field of view of a few arcmin and an increase in sky coverage. In this article, we show the gain offered by this technique to an astrophysical application, such as the photometric survey strategy applied to the Chandra Deep Field South as a case study. We simulated a close-to-real observation of a 500 x 500 arcsec^2 extragalactic deep field with a 40-m class telescope that implements GMCAO. We analysed mock K-band images of 6000 high-redshift (up to z = 2.75) galaxies therein as if they were real to recover the initial input parameters. We attained 94.5 per cent completeness for source detection with SEXTRACTOR. We also measured the morphological parameters of all the sources with the two-dimensional fitting tools GALFIT. The agreement we found between recovered and intrinsic parameters demonstrates GMCAO as a reliable approach to assist extremely large telescope (ELT) observations of extragalactic interest.	0,1,0,0,0,0
Performance Scaling Law for Multi-Cell Multi-User Massive MIMO	This work provides a comprehensive scaling law based performance analysis for multi-cell multi-user massive multiple-input-multiple-output (MIMO) downlink systems. Imperfect channel state information (CSI), pilot contamination, and channel spatial correlation are all considered. First, a sum- rate lower bound is derived by exploiting the asymptotically deterministic property of the received signal power, while keeping the random nature of other components in the signal-to-interference-plus-noise-ratio (SINR) intact. Via a general scaling model on important network parameters, including the number of users, the channel training energy and the data transmission power, with respect to the number of base station antennas, the asymptotic scaling law of the effective SINR is obtained, which reveals quantitatively the tradeoff of the network parameters. More importantly, pilot contamination and pilot contamination elimination (PCE) are considered in the analytical framework. In addition, the applicability of the derived asymptotic scaling law in practical systems with large but finite antenna numbers are discussed. Finally, sufficient conditions on the parameter scalings for the SINR to be asymptotically deterministic in the sense of mean square convergence are provided, which covers existing results on such analysis as special cases and shows the effect of PCE explicitly.	1,0,0,0,0,0
Two-Player Games for Efficient Non-Convex Constrained Optimization	In recent years, constrained optimization has become increasingly relevant to the machine learning community, with applications including Neyman-Pearson classification, robust optimization, and fair machine learning. A natural approach to constrained optimization is to optimize the Lagrangian, but this is not guaranteed to work in the non-convex setting, and, if using a first-order method, cannot cope with non-differentiable constraints (e.g. constraints on rates or proportions). The Lagrangian can be interpreted as a two-player game played between a player who seeks to optimize over the model parameters, and a player who wishes to maximize over the Lagrange multipliers. We propose a non-zero-sum variant of the Lagrangian formulation that can cope with non-differentiable--even discontinuous--constraints, which we call the "proxy-Lagrangian". The first player minimizes external regret in terms of easy-to-optimize "proxy constraints", while the second player enforces the original constraints by minimizing swap regret. For this new formulation, as for the Lagrangian in the non-convex setting, the result is a stochastic classifier. For both the proxy-Lagrangian and Lagrangian formulations, however, we prove that this classifier, instead of having unbounded size, can be taken to be a distribution over no more than m+1 models (where m is the number of constraints). This is a significant improvement in practical terms.	0,0,0,1,0,0
A Stochastic Programming Approach for Electric Vehicle Charging Network Design	Advantages of electric vehicles (EV) include reduction of greenhouse gas and other emissions, energy security, and fuel economy. The societal benefits of large-scale adoption of EVs cannot be realized without adequate deployment of publicly accessible charging stations. We propose a two-stage stochastic programming model to determine the optimal network of charging stations for a community considering uncertainties in arrival and dwell time of vehicles, battery state of charge of arriving vehicles, walkable range and charging preferences of drivers, demand during weekdays and weekends, and rate of adoption of EVs within a community. We conducted studies using sample average approximation (SAA) method which asymptotically converges to an optimal solution for a two-stage stochastic problem, however it is computationally expensive for large-scale instances. Therefore, we developed a heuristic to produce near to optimal solutions quickly for our data instances. We conducted computational experiments using various publicly available data sources, and benefits of the solutions are evaluated both quantitatively and qualitatively for a given community.	1,0,1,0,0,0
Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories	Generative Adversarial Networks (GANs) represent a promising class of generative networks that combine neural networks with game theory. From generating realistic images and videos to assisting musical creation, GANs are transforming many fields of arts and sciences. However, their application to healthcare has not been fully realized, more specifically in generating electronic health records (EHR) data. In this paper, we propose a framework for exploring the value of GANs in the context of continuous laboratory time series data. We devise an unsupervised evaluation method that measures the predictive power of synthetic laboratory test time series. Further, we show that when it comes to predicting the impact of drug exposure on laboratory test data, incorporating representation learning of the training cohorts prior to training GAN models is beneficial.	1,0,0,1,0,0
Metastability versus collapse following a quench in attractive Bose-Einstein condensates	We consider a Bose-Einstein condensate (BEC) with attractive two-body interactions in a cigar-shaped trap, initially prepared in its ground state for a given negative scattering length, which is quenched to a larger absolute value of the scattering length. Using the mean-field approximation, we compute numerically, for an experimentally relevant range of aspect ratios and initial strengths of the coupling, two critical values of quench: one corresponds to the weakest attraction strength the quench to which causes the system to collapse before completing even a single return from the narrow configuration ("perihelion") in its breathing cycle. The other is a similar critical point for the occurrence of collapse before completing two returns. In the latter case, we also compute the limiting value, as we keep increasing the strength of the post-quench attraction towards its critical value, of the time interval between the first two perihelia. We also use a Gaussian variational model to estimate the critical quenched attraction strength below which the system is stable against the collapse for long times. These time intervals and critical attraction strengths---apart from being fundamental properties of nonlinear dynamics of self-attractive BECs---may provide clues to the design of upcoming experiments that are trying to create robust BEC breathers.	0,1,0,0,0,0
Deep Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching	We study the neural-linear bandit model for solving sequential decision-making problems with high dimensional side information. Neural-linear bandits leverage the representation power of deep neural networks and combine it with efficient exploration mechanisms, designed for linear contextual bandits, on top of the last hidden layer. Since the representation is being optimized during learning, information regarding exploration with "old" features is lost. Here, we propose the first limited memory neural-linear bandit that is resilient to this phenomenon, which we term catastrophic forgetting. We evaluate our method on a variety of real-world data sets, including regression, classification, and sentiment analysis, and observe that our algorithm is resilient to catastrophic forgetting and achieves superior performance.	1,0,0,1,0,0
Masses of Kepler-46b, c from Transit Timing Variations	We use 16 quarters of the \textit{Kepler} mission data to analyze the transit timing variations (TTVs) of the extrasolar planet Kepler-46b (KOI-872). Our dynamical fits confirm that the TTVs of this planet (period $P=33.648^{+0.004}_{-0.005}$ days) are produced by a non-transiting planet Kepler-46c ($P=57.325^{+0.116}_{-0.098}$ days). The Bayesian inference tool \texttt{MultiNest} is used to infer the dynamical parameters of Kepler-46b and Kepler-46c. We find that the two planets have nearly coplanar and circular orbits, with eccentricities $\simeq 0.03$ somewhat higher than previously estimated. The masses of the two planets are found to be $M_{b}=0.885^{+0.374}_{-0.343}$ and $M_{c}=0.362^{+0.016}_{-0.016}$ Jupiter masses, with $M_{b}$ being determined here from TTVs for the first time. Due to the precession of its orbital plane, Kepler-46c should start transiting its host star in a few decades from now.	0,1,0,0,0,0
Two-sided Facility Location	Recent years have witnessed the rise of many successful e-commerce marketplace platforms like the Amazon marketplace, AirBnB, Uber/Lyft, and Upwork, where a central platform mediates economic transactions between buyers and sellers. Motivated by these platforms, we formulate a set of facility location problems that we term Two-sided Facility location. In our model, agents arrive at nodes in an underlying metric space, where the metric distance between any buyer and seller captures the quality of the corresponding match. The platform posts prices and wages at the nodes, and opens a set of facilities to route the agents to. The agents at any facility are assumed to be matched. The platform ensures high match quality by imposing a distance constraint between a node and the facilities it is routed to. It ensures high service availability by ensuring flow to the facility is at least a pre-specified lower bound. Subject to these constraints, the goal of the platform is to maximize the social surplus (or gains from trade) subject to weak budget balance, i.e., profit being non-negative. We present an approximation algorithm for this problem that yields a $(1 + \epsilon)$ approximation to surplus for any constant $\epsilon > 0$, while relaxing the match quality (i.e., maximum distance of any match) by a constant factor. We use an LP rounding framework that easily extends to other objectives such as maximizing volume of trade or profit. We justify our models by considering a dynamic marketplace setting where agents arrive according to a stochastic process and have finite patience (or deadlines) for being matched. We perform queueing analysis to show that for policies that route agents to facilities and match them, ensuring a low abandonment probability of agents reduces to ensuring sufficient flow arrives at each facility.	1,0,0,0,0,0
Global optimization for low-dimensional switching linear regression and bounded-error estimation	The paper provides global optimization algorithms for two particularly difficult nonconvex problems raised by hybrid system identification: switching linear regression and bounded-error estimation. While most works focus on local optimization heuristics without global optimality guarantees or with guarantees valid only under restrictive conditions, the proposed approach always yields a solution with a certificate of global optimality. This approach relies on a branch-and-bound strategy for which we devise lower bounds that can be efficiently computed. In order to obtain scalable algorithms with respect to the number of data, we directly optimize the model parameters in a continuous optimization setting without involving integer variables. Numerical experiments show that the proposed algorithms offer a higher accuracy than convex relaxations with a reasonable computational burden for hybrid system identification. In addition, we discuss how bounded-error estimation is related to robust estimation in the presence of outliers and exact recovery under sparse noise, for which we also obtain promising numerical results.	1,0,0,1,0,0
Deep Recurrent Neural Networks for seizure detection and early seizure detection systems	Epilepsy is common neurological diseases, affecting about 0.6-0.8 % of world population. Epileptic patients suffer from chronic unprovoked seizures, which can result in broad spectrum of debilitating medical and social consequences. Since seizures, in general, occur infrequently and are unpredictable, automated seizure detection systems are recommended to screen for seizures during long-term electroencephalogram (EEG) recordings. In addition, systems for early seizure detection can lead to the development of new types of intervention systems that are designed to control or shorten the duration of seizure events. In this article, we investigate the utility of recurrent neural networks (RNNs) in designing seizure detection and early seizure detection systems. We propose a deep learning framework via the use of Gated Recurrent Unit (GRU) RNNs for seizure detection. We use publicly available data in order to evaluate our method and demonstrate very promising evaluation results with overall accuracy close to 100 %. We also systematically investigate the application of our method for early seizure warning systems. Our method can detect about 98% of seizure events within the first 5 seconds of the overall epileptic seizure duration.	1,0,0,0,0,0
Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models	Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex hand-crafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network (RNN) to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task.	1,0,0,0,0,0
Cross ratios on boundaries of symmetric spaces and Euclidean buildings	We generalize the natural cross ratio on the ideal boundary of a rank one symmetric spaces, or even $\mathrm{CAT}(-1)$ space, to higher rank symmetric spaces and (non-locally compact) Euclidean buildings - we obtain vector valued cross ratios defined on simplices of the building at infinity. We show several properties of those cross ratios; for example that (under some restrictions) periods of hyperbolic isometries give back the translation vector. In addition, we show that cross ratio preserving maps on the chamber set are induced by isometries and vice versa - motivating that the cross ratios bring the geometry of the symmetric space/Euclidean building to the boundary.	0,0,1,0,0,0
Prior-aware Dual Decomposition: Document-specific Topic Inference for Spectral Topic Models	Spectral topic modeling algorithms operate on matrices/tensors of word co-occurrence statistics to learn topic-specific word distributions. This approach removes the dependence on the original documents and produces substantial gains in efficiency and provable topic inference, but at a cost: the model can no longer provide information about the topic composition of individual documents. Recently Thresholded Linear Inverse (TLI) is proposed to map the observed words of each document back to its topic composition. However, its linear characteristics limit the inference quality without considering the important prior information over topics. In this paper, we evaluate Simple Probabilistic Inverse (SPI) method and novel Prior-aware Dual Decomposition (PADD) that is capable of learning document-specific topic compositions in parallel. Experiments show that PADD successfully leverages topic correlations as a prior, notably outperforming TLI and learning quality topic compositions comparable to Gibbs sampling on various data.	1,0,0,0,0,0
Improving the upper bound on the length of the shortest reset words	We improve the best known upper bound on the length of the shortest reset words of synchronizing automata. The new bound is slightly better than $114 n^3 / 685 + O(n^2)$. The Černý conjecture states that $(n-1)^2$ is an upper bound. So far, the best general upper bound was $(n^3-n)/6-1$ obtained by J.-E.~Pin and P.~Frankl in 1982. Despite a number of efforts, it remained unchanged for about 35 years. To obtain the new upper bound we utilize avoiding words. A word is avoiding for a state $q$ if after reading the word the automaton cannot be in $q$. We obtain upper bounds on the length of the shortest avoiding words, and using the approach of Trahtman from 2011 combined with the well known Frankl theorem from 1982, we improve the general upper bound on the length of the shortest reset words. For all the bounds, there exist polynomial algorithms finding a word of length not exceeding the bound.	1,0,0,0,0,0
Online Estimation and Adaptive Control for a Class of History Dependent Functional Differential Equations	This paper presents sufficient conditions for the convergence of online estimation methods and the stability of adaptive control strategies for a class of history dependent, functional differential equations. The study is motivated by the increasing interest in estimation and control techniques for robotic systems whose governing equations include history dependent nonlinearities. The functional differential equations in this paper are constructed using integral operators that depend on distributed parameters. As a consequence the resulting estimation and control equations are examples of distributed parameter systems whose states and distributed parameters evolve in finite and infinite dimensional spaces, respectively. suWell-posedness, existence, and uniqueness are discussed for the class of fully actuated robotic systems with history dependent forces in their governing equation of motion. By deriving rates of approximation for the class of history dependent operators in this paper, sufficient conditions are derived that guarantee that finite dimensional approximations of the online estimation equations converge to the solution of the infinite dimensional, distributed parameter system. The convergence and stability of a sliding mode adaptive control strategy for the history dependent, functional differential equations is established using Barbalat's lemma.	0,0,1,0,0,0
Adaptive Non-uniform Compressive Sampling for Time-varying Signals	In this paper, adaptive non-uniform compressive sampling (ANCS) of time-varying signals, which are sparse in a proper basis, is introduced. ANCS employs the measurements of previous time steps to distribute the sensing energy among coefficients more intelligently. To this aim, a Bayesian inference method is proposed that does not require any prior knowledge of importance levels of coefficients or sparsity of the signal. Our numerical simulations show that ANCS is able to achieve the desired non-uniform recovery of the signal. Moreover, if the signal is sparse in canonical basis, ANCS can reduce the number of required measurements significantly.	1,0,0,1,0,0
Reverse Quantum Annealing Approach to Portfolio Optimization Problems	We investigate a hybrid quantum-classical solution method to the mean-variance portfolio optimization problems. Starting from real financial data statistics and following the principles of the Modern Portfolio Theory, we generate parametrized samples of portfolio optimization problems that can be related to quadratic binary optimization forms programmable in the analog D-Wave Quantum Annealer 2000Q. The instances are also solvable by an industry-established Genetic Algorithm approach, which we use as a classical benchmark. We investigate several options to run the quantum computation optimally, ultimately discovering that the best results in terms of expected time-to-solution as a function of number of variables for the hardest instances set are obtained by seeding the quantum annealer with a solution candidate found by a greedy local search and then performing a reverse annealing protocol. The optimized reverse annealing protocol is found to be more than 100 times faster than the corresponding forward quantum annealing on average.	0,0,0,0,0,1
On absolutely normal and continued fraction normal numbers	We give a construction of a real number that is normal to all integer bases and continued fraction normal. The computation of the first n digits of its continued fraction expansion performs in the order of n^4 mathematical operations. The construction works by defining successive refinements of appropriate subintervals to achieve, in the limit, simple normality to all integer bases and continued fraction normality. The main diffculty is to control the length of these subintervals. To achieve this we adapt and combine known metric theorems on continued fractions and on expansions in integers bases.	0,0,1,0,0,0
Differentially Private Dropout	Large data collections required for the training of neural networks often contain sensitive information such as the medical histories of patients, and the privacy of the training data must be preserved. In this paper, we introduce a dropout technique that provides an elegant Bayesian interpretation to dropout, and show that the intrinsic noise added, with the primary goal of regularization, can be exploited to obtain a degree of differential privacy. The iterative nature of training neural networks presents a challenge for privacy-preserving estimation since multiple iterations increase the amount of noise added. We overcome this by using a relaxed notion of differential privacy, called concentrated differential privacy, which provides tighter estimates on the overall privacy loss. We demonstrate the accuracy of our privacy-preserving dropout algorithm on benchmark datasets.	1,0,0,1,0,0
Disorder Dependent Valley Properties in Monolayer WSe2	We investigate the effect on disorder potential on exciton valley polarization and valley coherence in monolayer WSe2. By analyzing polarization properties of photoluminescence, the valley coherence (VC) and valley polarization (VP) is quantified across the inhomogeneously broadened exciton resonance. We find that disorder plays a critical role in the exciton VC, while minimally affecting VP. For different monolayer samples with disorder characterized by their Stokes Shift (SS), VC decreases in samples with higher SS while VP again remains unchanged. These two methods consistently demonstrate that VC as defined by the degree of linearly polarized photoluminescence is more sensitive to disorder potential, motivating further theoretical studies.	0,1,0,0,0,0
Using solar and load predictions in battery scheduling at the residential level	Smart solar inverters can be used to store, monitor and manage a home's solar energy. We describe a smart solar inverter system with battery which can either operate in an automatic mode or receive commands over a network to charge and discharge at a given rate. In order to make battery storage financially viable and advantageous to the consumers, effective battery scheduling algorithms can be employed. Particularly, when time-of-use tariffs are in effect in the region of the inverter, it is possible in some cases to schedule the battery to save money for the individual customer, compared to the "automatic" mode. Hence, this paper presents and evaluates the performance of a novel battery scheduling algorithm for residential consumers of solar energy. The proposed battery scheduling algorithm optimizes the cost of electricity over next 24 hours for residential consumers. The cost minimization is realized by controlling the charging/discharging of battery storage system based on the predictions for load and solar power generation values. The scheduling problem is formulated as a linear programming problem. We performed computer simulations over 83 inverters using several months of hourly load and PV data. The simulation results indicate that key factors affecting the viability of optimization are the tariffs and the PV to Load ratio at each inverter. Depending on the tariff, savings of between 1% and 10% can be expected over the automatic approach. The prediction approach used in this paper is also shown to out-perform basic "persistence" forecasting approaches. We have also examined the approaches for improving the prediction accuracy and optimization effectiveness.	1,0,0,0,0,0
The Relation Between Fundamental Constants and Particle Physics Parameters	The observed constraints on the variability of the proton to electron mass ratio $\mu$ and the fine structure constant $\alpha$ are used to establish constraints on the variability of the Quantum Chromodynamic Scale and a combination of the Higgs Vacuum Expectation Value and the Yukawa couplings. Further model dependent assumptions provide constraints on the Higgs VEV and the Yukawa couplings separately. A primary conclusion is that limits on the variability of dimensionless fundamental constants such as $\mu$ and $\alpha$ provide important constraints on the parameter space of new physics and cosmologies.	0,1,0,0,0,0
Optimal Energy Distribution with Energy Packet Networks	We use Energy Packet Network paradigms to investigate energy distribution problems in a computer system with energy harvesting and storages units. Our goal is to minimize both the overall average response time of jobs at workstations and the total rate of energy lost in the network. Energy is lost when it arrives at idle workstations which are empty. Energy is also lost in storage leakages. We assume that the total rate of energy harvesting and the rate of jobs arriving at workstations are known. We also consider a special case in which the total rate of energy harvesting is sufficiently large so that workstations are less busy. In this case, energy is more likely to be sent to an idle workstation. Optimal solutions are obtained which minimize both the overall response time and energy loss under the constraint of a fixed energy harvesting rate.	1,0,0,0,0,0
Improving power of genetic association studies by extreme phenotype sampling: a review and some new results	Extreme phenotype sampling is a selective genotyping design for genetic association studies where only individuals with extreme values of a continuous trait are genotyped for a set of genetic variants. Under financial or other limitations, this design is assumed to improve the power to detect associations between genetic variants and the trait, compared to randomly selecting the same number of individuals for genotyping. Here we present extensions of likelihood models that can be used for inference when the data are sampled according to the extreme phenotype sampling design. Computational methods for parameter estimation and hypothesis testing are provided. We consider methods for common variant genetic effects and gene-environment interaction effects in linear regression models with a normally distributed trait. We use simulated and real data to show that extreme phenotype sampling can be powerful compared to random sampling, but that this does not hold for all extreme sampling methods and situations.	0,0,0,1,0,0
The First Planetary Microlensing Event with Two Microlensed Source Stars	We present the analysis of microlensing event MOA-2010-BLG-117, and show that the light curve can only be explained by the gravitational lensing of a binary source star system by a star with a Jupiter mass ratio planet. It was necessary to modify standard microlensing modeling methods to find the correct light curve solution for this binary-source, binary-lens event. We are able to measure a strong microlensing parallax signal, which yields the masses of the host star, $M_* = 0.58\pm 0.11 M_\odot$, and planet $m_p = 0.54\pm 0.10 M_{\rm Jup}$ at a projected star-planet separation of $a_\perp = 2.42\pm 0.26\,$AU, corresponding to a semi-major axis of $a = 2.9{+1.6\atop -0.6}\,$AU. Thus, the system resembles a half-scale model of the Sun-Jupiter system with a half-Jupiter mass planet orbiting a half-solar mass star at very roughly half of Jupiter's orbital distance from the Sun. The source stars are slightly evolved, and by requiring them to lie on the same isochrone, we can constrain the source to lie in the near side of the bulge at a distance of $D_S = 6.9 \pm 0.7\,$kpc, which implies a distance to the planetary lens system of $D_L = 3.5\pm 0.4\,$kpc. The ability to model unusual planetary microlensing events, like this one, will be necessary to extract precise statistical information from the planned large exoplanet microlensing surveys, such as the WFIRST microlensing survey.	0,1,0,0,0,0
Optimally Gathering Two Robots	We present an algorithm that ensures in finite time the gathering of two robots in the non-rigid ASYNC model. To circumvent established impossibility results, we assume robots are equipped with 2-colors lights and are able to measure distances between one another. Aside from its light, a robot has no memory of its past actions, and its protocol is deterministic. Since, in the same model, gathering is impossible when lights have a single color, our solution is optimal with respect to the number of used colors.	1,0,0,0,0,0
Analysis and X-ray tomography	These are lecture notes for the course "MATS4300 Analysis and X-ray tomography" given at the University of Jyväskylä in Fall 2017. The course is a broad overview of various tools in analysis that can be used to study X-ray tomography. The focus is on tools and ideas, not so much on technical details and minimal assumptions. Only very basic functional analysis is assumed as background. Exercise problems are included.	0,0,1,0,0,0
In situ Electric Field Skyrmion Creation in Magnetoelectric Cu$_2$OSeO$_3$	Magnetic skyrmions are localized nanometric spin textures with quantized winding numbers as the topological invariant. Rapidly increasing attention has been paid to the investigations of skyrmions since their experimental discovery in 2009, due both to the fundamental properties and the promising potential in spintronics based applications. However, controlled creation of skyrmions remains a pivotal challenge towards technological applications. Here, we report that skyrmions can be created locally by electric field in the magnetoelectric helimagnet Cu$\mathsf{_2}$OSeO$\mathsf{_3}$. Using Lorentz transmission electron microscopy, we successfully write skyrmions in situ from a helical spin background. Our discovery is highly coveted since it implies that skyrmionics can be integrated into contemporary field effect transistor based electronic technology, where very low energy dissipation can be achieved, and hence realizes a large step forward to its practical applications.	0,1,0,0,0,0
The quadratic M-convexity testing problem	M-convex functions, which are a generalization of valuated matroids, play a central role in discrete convex analysis. Quadratic M-convex functions constitute a basic and important subclass of M-convex functions, which has a close relationship with phylogenetics as well as valued constraint satisfaction problems. In this paper, we consider the quadratic M-convexity testing problem (QMCTP), which is the problem of deciding whether a given quadratic function on $\{0,1\}^n$ is M-convex. We show that QMCTP is co-NP-complete in general, but is polynomial-time solvable under a natural assumption. Furthermore, we propose an $O(n^2)$-time algorithm for solving QMCTP in the polynomial-time solvable case.	0,0,1,0,0,0
3D printable multimaterial cellular auxetics with tunable stiffness	Auxetic materials are a novel class of mechanical metamaterials which exhibit an interesting property of negative Poisson ratio by virtue of their architecture rather than composition. It has been well established that a wide range of negative Poisson ratio can be obtained by varying the geometry and architecture of the cellular materials. However, the limited range of stiffness values obtained from a given geometry restricts their applications. Research trials have revealed that multi-material cellular designs have the capability to generate range of stiffness values as per the requirement of application. With the advancements in 3D printing, multi-material cellular designs can be realized in practice. In this work, multi-material cellular designs are investigated using finite element method. It was observed that introduction of material gradient/distribution in the cell provides a means to tune cellular stiffness as per the specific requirement. These results will aid in the design of wearable auxetic impact protection devices which rely on stiffness gradients and variable auxeticity.	0,1,0,0,0,0
The population of SNe/SNRs in the starburst galaxy Arp 220. A self-consistent analysis of 20 years of VLBI monitoring	The nearby ultra-luminous infrared galaxy (ULIRG) Arp 220 is an excellent laboratory for studies of extreme astrophysical environments. For 20 years, Very Long Baseline Interferometry (VLBI) has been used to monitor a population of compact sources thought to be supernovae (SNe), supernova remnants (SNRs) and possibly active galactic nuclei (AGNs). Using new and archival VLBI data spanning 20 years, we obtain 23 high-resolution radio images of Arp 220 at wavelengths from 18 cm to 2 cm. From model-fitting to the images we obtain estimates of flux densities and sizes of all detected sources. We detect radio continuum emission from 97 compact sources and present flux densities and sizes for all analysed observation epochs. We find evidence for a LD-relation within Arp 220, with larger sources being less luminous. We find a compact source LF $n(L)\propto L^\beta$ with $\beta=-2.02\pm0.11$, similar to SNRs in normal galaxies. Based on simulations we argue that there are many relatively large and weak sources below our detection threshold. The rapidly declining object 0.2227+0.482 is proposed as a possible AGN candidate. The observations can be explained by a mixed population of SNe and SNRs, where the former expand in a dense circumstellar medium (CSM) and the latter interact with the surrounding interstellar medium (ISM). Several sources (9 of the 94 with fitted sizes) are likely luminous, type IIn SNe. This number of luminous SNe correspond to few percent of the total number of SNe in Arp 220 which is consistent with a total SN-rate of 4 yr$^{-1}$ as inferred from the total radio emission given a normal stellar initial mass function (IMF). Based on the fitted luminosity function, we argue that emission from all compact sources, also below our detection threshold, make up at most 20\% of the total radio emission at GHz frequencies.	0,1,0,0,0,0
Eco-evolutionary feedbacks - theoretical models and perspectives	1. Theoretical models pertaining to feedbacks between ecological and evolutionary processes are prevalent in multiple biological fields. An integrative overview is currently lacking, due to little crosstalk between the fields and the use of different methodological approaches. 2. Here we review a wide range of models of eco-evolutionary feedbacks and highlight their underlying assumptions. We discuss models where feedbacks occur both within and between hierarchical levels of ecosystems, including populations, communities, and abiotic environments, and consider feedbacks across spatial scales. 3. Identifying the commonalities among feedback models, and the underlying assumptions, helps us better understand the mechanistic basis of eco-evolutionary feedbacks. Eco-evolutionary feedbacks can be readily modelled by coupling demographic and evolutionary formalisms. We provide an overview of these approaches and suggest future integrative modelling avenues. 4. Our overview highlights that eco-evolutionary feedbacks have been incorporated in theoretical work for nearly a century. Yet, this work does not always include the notion of rapid evolution or concurrent ecological and evolutionary time scales. We discuss the importance of density- and frequency-dependent selection for feedbacks, as well as the importance of dispersal as a central linking trait between ecology and evolution in a spatial context.	0,0,0,0,1,0
The Memory Function Formalism: A Review	An introduction to the Zwanzig-Mori-Götze-Wölfle memory function formalism (or generalized Drude formalism) is presented. This formalism is used extensively in analyzing the experimentally obtained optical conductivity of strongly correlated systems like cuprates and Iron based superconductors etc. For a broader perspective both the generalised Langevin equation approach and the projection operator approach for the memory function formalism are given. The Götze-Wölfle perturbative expansion of memory function is presented and its application to the computation of the dynamical conductivity of metals is also reviewd. This review of the formalism contains all the mathematical details for pedagogical purposes.	0,1,0,0,0,0
Multi-rendezvous Spacecraft Trajectory Optimization with Beam P-ACO	The design of spacecraft trajectories for missions visiting multiple celestial bodies is here framed as a multi-objective bilevel optimization problem. A comparative study is performed to assess the performance of different Beam Search algorithms at tackling the combinatorial problem of finding the ideal sequence of bodies. Special focus is placed on the development of a new hybridization between Beam Search and the Population-based Ant Colony Optimization algorithm. An experimental evaluation shows all algorithms achieving exceptional performance on a hard benchmark problem. It is found that a properly tuned deterministic Beam Search always outperforms the remaining variants. Beam P-ACO, however, demonstrates lower parameter sensitivity, while offering superior worst-case performance. Being an anytime algorithm, it is then found to be the preferable choice for certain practical applications.	1,1,0,0,0,0
Learning Powers of Poisson Binomial Distributions	We introduce the problem of simultaneously learning all powers of a Poisson Binomial Distribution (PBD). A PBD of order $n$ is the distribution of a sum of $n$ mutually independent Bernoulli random variables $X_i$, where $\mathbb{E}[X_i] = p_i$. The $k$'th power of this distribution, for $k$ in a range $[m]$, is the distribution of $P_k = \sum_{i=1}^n X_i^{(k)}$, where each Bernoulli random variable $X_i^{(k)}$ has $\mathbb{E}[X_i^{(k)}] = (p_i)^k$. The learning algorithm can query any power $P_k$ several times and succeeds in learning all powers in the range, if with probability at least $1- \delta$: given any $k \in [m]$, it returns a probability distribution $Q_k$ with total variation distance from $P_k$ at most $\epsilon$. We provide almost matching lower and upper bounds on query complexity for this problem. We first show a lower bound on the query complexity on PBD powers instances with many distinct parameters $p_i$ which are separated, and we almost match this lower bound by examining the query complexity of simultaneously learning all the powers of a special class of PBD's resembling the PBD's of our lower bound. We study the fundamental setting of a Binomial distribution, and provide an optimal algorithm which uses $O(1/\epsilon^2)$ samples. Diakonikolas, Kane and Stewart [COLT'16] showed a lower bound of $\Omega(2^{1/\epsilon})$ samples to learn the $p_i$'s within error $\epsilon$. The question whether sampling from powers of PBDs can reduce this sampling complexity, has a negative answer since we show that the exponential number of samples is inevitable. Having sampling access to the powers of a PBD we then give a nearly optimal algorithm that learns its $p_i$'s. To prove our two last lower bounds we extend the classical minimax risk definition from statistics to estimating functions of sequences of distributions.	1,0,1,1,0,0
Global existence and convergence of $Q$-curvature flow on manifolds of even dimension	Using a negative gradient flow approach, we generalize and unify some existence theorems for the problem of prescribing $Q$-curvature first by Baird, Fardoun, and Regbaoui (Calc. Var. 27 75-104) for $4$-manifolds with a possible sign-changing curvature candidate then by Brendle (Ann. Math. 158 323-343) for $n$-manifolds with even $n$ with positive curvature candidate to the case of $n$-manifolds of all even dimension with sign-changing curvature candidates. Making use of the \L ojasiewicz--Simon inequality, we also address the rate of the convergence.	0,0,1,0,0,0
A Closer Look at the Alpha Persei Coronal Conundrum	A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest star, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness were similar to coronally active late-type dwarf members. Later, in 2010, a Hubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found far-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious offset of the ROSAT source, suggested that a late-type companion might be responsible for the X-rays. Recently, a multi-faceted program tested that premise. Groundbased optical coronography, and near-UV imaging with HST Wide Field Camera 3, searched for any close-in faint candidate coronal objects, but without success. Then, a Chandra pointing found the X-ray source single and coincident with the bright star. Significantly, the SiIV emissions of Alpha Persei, in a deeper FUV spectrum collected by HST COS as part of the joint program, aligned well with chromospheric atomic oxygen (which must be intrinsic to the luminous star), within the context of cooler late-F and early-G supergiants, including Cepheid variables. This pointed to the X-rays as the fundamental anomaly. The over-luminous X-rays still support the case for a hyperactive dwarf secondary, albeit now spatially unresolved. However, an alternative is that Alpha Persei represents a novel class of coronal source. Resolving the first possibility now has become more difficult, because the easy solution -- a well separated companion -- has been eliminated. Testing the other possibility will require a broader high-energy census of the early-F supergiants.	0,1,0,0,0,0
The Belgian repository of fundamental atomic data and stellar spectra (BRASS). I. Cross-matching atomic databases of astrophysical interest	Fundamental atomic parameters, such as oscillator strengths, play a key role in modelling and understanding the chemical composition of stars in the universe. Despite the significant work underway to produce these parameters for many astrophysically important ions, uncertainties in these parameters remain large and can propagate throughout the entire field of astronomy. The Belgian repository of fundamental atomic data and stellar spectra (BRASS) aims to provide the largest systematic and homogeneous quality assessment of atomic data to date in terms of wavelength, atomic and stellar parameter coverage. To prepare for it, we first compiled multiple literature occurrences of many individual atomic transitions, from several atomic databases of astrophysical interest, and assessed their agreement. Several atomic repositories were searched and their data retrieved and formatted in a consistent manner. Data entries from all repositories were cross-matched against our initial BRASS atomic line list to find multiple occurrences of the same transition. Where possible we used a non-parametric cross-match depending only on electronic configurations and total angular momentum values. We also checked for duplicate entries of the same physical transition, within each retrieved repository, using the non-parametric cross-match. We report the cross-matched transitions for each repository and compare their fundamental atomic parameters. We find differences in log(gf) values of up to 2 dex or more. We also find and report that ~2% of our line list and Vienna Atomic Line Database retrievals are composed of duplicate transitions. Finally we provide a number of examples of atomic spectral lines with different log(gf) values, and discuss the impact of these uncertain log(gf) values on quantitative spectroscopy. All cross-matched atomic data and duplicate transitions are available to download at brass.sdf.org.	0,1,0,0,0,0
Learning Neural Parsers with Deterministic Differentiable Imitation Learning	We explore the problem of learning to decompose spatial tasks into segments, as exemplified by the problem of a painting robot covering a large object. Inspired by the ability of classical decision tree algorithms to construct structured partitions of their input spaces, we formulate the problem of decomposing objects into segments as a parsing approach. We make the insight that the derivation of a parse-tree that decomposes the object into segments closely resembles a decision tree constructed by ID3, which can be done when the ground-truth available. We learn to imitate an expert parsing oracle, such that our neural parser can generalize to parse natural images without ground truth. We introduce a novel deterministic policy gradient update, DRAG (i.e., DeteRministically AGgrevate) in the form of a deterministic actor-critic variant of AggreVaTeD, to train our neural parser. From another perspective, our approach is a variant of the Deterministic Policy Gradient suitable for the imitation learning setting. The deterministic policy representation offered by training our neural parser with DRAG allows it to outperform state of the art imitation and reinforcement learning approaches.	1,0,0,1,0,0
Control of Asynchronous Imitation Dynamics on Networks	Imitation is widely observed in populations of decision-making agents. Using our recent convergence results for asynchronous imitation dynamics on networks, we consider how such networks can be efficiently driven to a desired equilibrium state by offering payoff incentives for using a certain strategy, either uniformly or targeted to individuals. In particular, if for each available strategy, agents playing that strategy receive maximum payoff when their neighbors play that same strategy, we show that providing incentives to agents in a network that is at equilibrium will result in convergence to a unique new equilibrium. For the case when a uniform incentive can be offered to all agents, this result allows the computation of the optimal incentive using a binary search algorithm. When incentives can be targeted to individual agents, we propose an algorithm to select which agents should be chosen based on iteratively maximizing a ratio of the number of agents who adopt the desired strategy to the payoff incentive required to get those agents to do so. Simulations demonstrate that the proposed algorithm computes near-optimal targeted payoff incentives for a range of networks and payoff distributions in coordination games.	1,1,0,0,0,0
SPIDER: CMB polarimetry from the edge of space	SPIDER is a balloon-borne instrument designed to map the polarization of the millimeter-wave sky at large angular scales. SPIDER targets the B-mode signature of primordial gravitational waves in the cosmic microwave background (CMB), with a focus on mapping a large sky area with high fidelity at multiple frequencies. SPIDER's first longduration balloon (LDB) flight in January 2015 deployed a total of 2400 antenna-coupled Transition Edge Sensors (TESs) at 90 GHz and 150 GHz. In this work we review the design and in-flight performance of the SPIDER instrument, with a particular focus on the measured performance of the detectors and instrument in a space-like loading and radiation environment. SPIDER's second flight in December 2018 will incorporate payload upgrades and new receivers to map the sky at 285 GHz, providing valuable information for cleaning polarized dust emission from CMB maps.	0,1,0,0,0,0
Data-Augmented Contact Model for Rigid Body Simulation	Accurately modeling contact behaviors for real-world, near-rigid materials remains a grand challenge for existing rigid-body physics simulators. This paper introduces a data-augmented contact model that incorporates analytical solutions with observed data to predict the 3D contact impulse which could result in rigid bodies bouncing, sliding or spinning in all directions. Our method enhances the expressiveness of the standard Coulomb contact model by learning the contact behaviors from the observed data, while preserving the fundamental contact constraints whenever possible. For example, a classifier is trained to approximate the transitions between static and dynamic frictions, while non-penetration constraint during collision is enforced analytically. Our method computes the aggregated effect of contact for the entire rigid body, instead of predicting the contact force for each contact point individually, removing the exponential decline in accuracy as the number of contact points increases.	1,0,0,0,0,0
A functional perspective on emergent supersymmetry	We investigate the emergence of ${\cal N}=1$ supersymmetry in the long-range behavior of three-dimensional parity-symmetric Yukawa systems. We discuss a renormalization approach that manifestly preserves supersymmetry whenever such symmetry is realized, and use it to prove that supersymmetry-breaking operators are irrelevant, thus proving that such operators are suppressed in the infrared. All our findings are illustrated with the aid of the $\epsilon$-expansion and a functional variant of perturbation theory, but we provide numerical estimates of critical exponents that are based on the non-perturbative functional renormalization group.	0,1,0,0,0,0
Learning convex bounds for linear quadratic control policy synthesis	Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a number of fields, from artificial intelligence and robotics, to medicine and finance. This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function. We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data. The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees. Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.	0,0,0,1,0,0
PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits	We consider the problem of identifying any $k$ out of the best $m$ arms in an $n$-armed stochastic multi-armed bandit. Framed in the PAC setting, this particular problem generalises both the problem of `best subset selection' and that of selecting `one out of the best m' arms [arcsk 2017]. In applications such as crowd-sourcing and drug-designing, identifying a single good solution is often not sufficient. Moreover, finding the best subset might be hard due to the presence of many indistinguishably close solutions. Our generalisation of identifying exactly $k$ arms out of the best $m$, where $1 \leq k \leq m$, serves as a more effective alternative. We present a lower bound on the worst-case sample complexity for general $k$, and a fully sequential PAC algorithm, \GLUCB, which is more sample-efficient on easy instances. Also, extending our analysis to infinite-armed bandits, we present a PAC algorithm that is independent of $n$, which identifies an arm from the best $\rho$ fraction of arms using at most an additive poly-log number of samples than compared to the lower bound, thereby improving over [arcsk 2017] and [Aziz+AKA:2018]. The problem of identifying $k > 1$ distinct arms from the best $\rho$ fraction is not always well-defined; for a special class of this problem, we present lower and upper bounds. Finally, through a reduction, we establish a relation between upper bounds for the `one out of the best $\rho$' problem for infinite instances and the `one out of the best $m$' problem for finite instances. We conjecture that it is more efficient to solve `small' finite instances using the latter formulation, rather than going through the former.	1,0,0,1,0,0
Optimal Scheduling of Multi-Energy Systems with Flexible Electrical and Thermal Loads	This paper proposes a detailed optimal scheduling model of an exemplar multi-energy system comprising combined cycle power plants (CCPPs), battery energy storage systems, renewable energy sources, boilers, thermal energy storage systems,electric loads and thermal loads. The proposed model considers the detailed start-up and shutdown power trajectories of the gas turbines, steam turbines and boilers. Furthermore, a practical,multi-energy load management scheme is proposed within the framework of the optimal scheduling problem. The proposed load management scheme utilizes the flexibility offered by system components such as flexible electrical pump loads, electrical interruptible loads and a flexible thermal load to reduce the overall energy cost of the system. The efficacy of the proposed model in reducing the energy cost of the system is demonstrated in the context of a day-ahead scheduling problem using four illustrative scenarios.	1,0,0,0,0,0
Scaling the Scattering Transform: Deep Hybrid Networks	We use the scattering network as a generic and fixed ini-tialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1 x 1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset.	1,0,0,0,0,0
Analytic heating rate of neutron star merger ejecta derived from Fermi's theory of beta decay	Macronovae (kilonovae) that arise in binary neutron star mergers are powered by radioactive beta decay of hundreds of $r$-process nuclides. We derive, using Fermi's theory of beta decay, an analytic estimate of the nuclear heating rate. We show that the heating rate evolves as a power law ranging between $t^{-6/5}$ to $t^{-4/3}$. The overall magnitude of the heating rate is determined by the mean values of nuclear quantities, e.g., the nuclear matrix elements of beta decay. These values are specified by using nuclear experimental data. We discuss the role of higher order beta transitions and the robustness of the power law. The robust and simple form of the heating rate suggests that observations of the late-time bolometric light curve $\propto t^{-\frac{4}{3}}$ would be a direct evidence of a $r$-process driven macronova. Such observations could also enable us to estimate the total amount of $r$-process nuclei produced in the merger.	0,1,0,0,0,0
Languages of Play: Towards semantic foundations for game interfaces	Formal models of games help us account for and predict behavior, leading to more robust and innovative designs. While the games research community has proposed many formalisms for both the "game half" (game models, game description languages) and the "human half" (player modeling) of a game experience, little attention has been paid to the interface between the two, particularly where it concerns the player expressing her intent toward the game. We describe an analytical and computational toolbox based on programming language theory to examine the phenomenon sitting between control schemes and game rules, which we identify as a distinct player intent language for each game.	1,0,0,0,0,0
Global Patterns of Synchronization in Human Communications	Social media are transforming global communication and coordination. The data derived from social media can reveal patterns of human behavior at all levels and scales of society. Using geolocated Twitter data, we have quantified collective behaviors across multiple scales, ranging from the commutes of individuals, to the daily pulse of 50 major urban areas and global patterns of human coordination. Human activity and mobility patterns manifest the synchrony required for contingency of actions between individuals. Urban areas show regular cycles of contraction and expansion that resembles heartbeats linked primarily to social rather than natural cycles. Business hours and circadian rhythms influence daily cycles of work, recreation, and sleep. Different urban areas have characteristic signatures of daily collective activities. The differences are consistent with a new emergent global synchrony that couples behavior in distant regions across the world. A globally synchronized peak that includes exchange of ideas and information across Europe, Africa, Asia and Australasia. We propose a dynamical model to explain the emergence of global synchrony in the context of increasing global communication and reproduce the observed behavior. The collective patterns we observe show how social interactions lead to interdependence of behavior manifest in the synchronization of communication. The creation and maintenance of temporally sensitive social relationships results in the emergence of complexity of the larger scale behavior of the social system.	1,1,0,0,0,0
Optimal stopping via reinforced regression	In this note we propose a new approach towards solving numerically optimal stopping problems via reinforced regression based Monte Carlo algorithms. The main idea of the method is to reinforce standard linear regression algorithms in each backward induction step by adding new basis functions based on previously estimated continuation values. The proposed methodology is illustrated by a numerical example from mathematical finance.	0,0,0,1,0,0
Input Perturbations for Adaptive Regulation and Learning	Design of adaptive algorithms for simultaneous regulation and estimation of MIMO linear dynamical systems is a canonical reinforcement learning problem. Efficient policies whose regret (i.e. increase in the cost due to uncertainty) scales at a square-root rate of time have been studied extensively in the recent literature. Nevertheless, existing strategies are computationally intractable and require a priori knowledge of key system parameters. The only exception is a randomized Greedy regulator, for which asymptotic regret bounds have been recently established. However, randomized Greedy leads to probable fluctuations in the trajectory of the system, which renders its finite time regret suboptimal. This work addresses the above issues by designing policies that utilize input signals perturbations. We show that perturbed Greedy guarantees non-asymptotic regret bounds of (nearly) square-root magnitude w.r.t. time. More generally, we establish high probability bounds on both the regret and the learning accuracy under arbitrary input perturbations. The settings where Greedy attains the information theoretic lower bound of logarithmic regret are also discussed. To obtain the results, state-of-the-art tools from martingale theory together with the recently introduced method of policy decomposition are leveraged. Beside adaptive regulators, analysis of input perturbations captures key applications including remote sensing and distributed control.	1,0,0,0,0,0
Realistic theory of electronic correlations in nanoscopic systems	Nanostructures with open shell transition metal or molecular constituents host often strong electronic correlations and are highly sensitive to atomistic material details. This tutorial review discusses method developments and applications of theoretical approaches for the realistic description of the electronic and magnetic properties of nanostructures with correlated electrons. First, the implementation of a flexible interface between density functional theory and a variant of dynamical mean field theory (DMFT) highly suitable for the simulation of complex correlated structures is explained and illustrated. On the DMFT side, this interface is largely based on recent developments of quantum Monte Carlo and exact diagonalization techniques allowing for efficient descriptions of general four fermion Coulomb interactions, reduced symmetries and spin-orbit coupling, which are explained here. With the examples of the Cr (001) surfaces, magnetic adatoms, and molecular systems it is shown how the interplay of Hubbard U and Hund's J determines charge and spin fluctuations and how these interactions drive different sorts of correlation effects in nanosystems. Non-local interactions and correlations present a particular challenge for the theory of low dimensional systems. We present our method developments addressing these two challenges, i.e., advancements of the dynamical vertex approximation and a combination of the constrained random phase approximation with continuum medium theories. We demonstrate how non-local interaction and correlation phenomena are controlled not only by dimensionality but also by coupling to the environment which is typically important for determining the physics of nanosystems.	0,1,0,0,0,0
Insense: Incoherent Sensor Selection for Sparse Signals	Sensor selection refers to the problem of intelligently selecting a small subset of a collection of available sensors to reduce the sensing cost while preserving signal acquisition performance. The majority of sensor selection algorithms find the subset of sensors that best recovers an arbitrary signal from a number of linear measurements that is larger than the dimension of the signal. In this paper, we develop a new sensor selection algorithm for sparse (or near sparse) signals that finds a subset of sensors that best recovers such signals from a number of measurements that is much smaller than the dimension of the signal. Existing sensor selection algorithms cannot be applied in such situations. Our proposed Incoherent Sensor Selection (Insense) algorithm minimizes a coherence-based cost function that is adapted from recent results in sparse recovery theory. Using six datasets, including two real-world datasets on microbial diagnostics and structural health monitoring, we demonstrate the superior performance of Insense for sparse-signal sensor selection.	1,0,0,0,0,0
Proof of Correspondence between Keys and Encoding Maps in an Authentication Code	In a former paper the authors introduced two new systematic authentication codes based on the Gray map over a Galois ring. In this paper, it is proved the one-to-one onto correspondence between keys and encoding maps for the second introduced authentication code.	1,0,1,0,0,0
Robust and Efficient Parametric Spectral Estimation in Atomic Force Microscopy	An atomic force microscope (AFM) is capable of producing ultra-high resolution measurements of nanoscopic objects and forces. It is an indispensable tool for various scientific disciplines such as molecular engineering, solid-state physics, and cell biology. Prior to a given experiment, the AFM must be calibrated by fitting a spectral density model to baseline recordings. However, since AFM experiments typically collect large amounts of data, parameter estimation by maximum likelihood can be prohibitively expensive. Thus, practitioners routinely employ a much faster least-squares estimation method, at the cost of substantially reduced statistical efficiency. Additionally, AFM data is often contaminated by periodic electronic noise, to which parameter estimates are highly sensitive. This article proposes a two-stage estimator to address these issues. Preliminary parameter estimates are first obtained by a variance-stabilizing procedure, by which the simplicity of least-squares combines with the efficiency of maximum likelihood. A test for spectral periodicities then eliminates high-impact outliers, considerably and robustly protecting the second-stage estimator from the effects of electronic noise. Simulation and experimental results indicate that a two- to ten-fold reduction in mean squared error can be expected by applying our methodology.	0,0,0,1,0,0
The Case for Learned Index Structures	Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.	1,0,0,0,0,0
Quantum Harmonic Analysis of the Density Matrix: Basics	In this Review we will study rigorously the notion of mixed states and their density matrices. We mostly give complete proofs. We will also discuss the quantum-mechanical consequences of possible variations of Planck's constant h. This Review has been written having in mind two readerships: mathematical physicists and quantum physicists. The mathematical rigor is maximal, but the language and notation we use throughout should be familiar to physicists.	0,0,1,0,0,0
Magnetic droplet nucleation with homochiral Neel domain wall	We investigate the effect of the Dzyaloshinskii Moriya interaction (DMI) on magnetic domain nucleation in a ferromagnetic thin film with perpendicular magnetic anisotropy. We propose an extended droplet model to determine the nucleation field as a function of the in-plane field. The model can explain the experimentally observed nucleation in a CoNi microstrip with the interfacial DMI. The results are also reproduced by micromagnetic simulation based on the string model. The electrical measurement method proposed in this study can be widely used to quantitatively determine the DMI energy density.	0,1,0,0,0,0
Securing Information-Centric Networking without negating Middleboxes	Information-Centric Networking is a promising networking paradigm that overcomes many of the limitations of current networking architectures. Various research efforts investigate solutions for securing ICN. Nevertheless, most of these solutions relax security requirements in favor of network performance. In particular, they weaken end-user privacy and the architecture's tolerance to security breaches in order to support middleboxes that offer services such as caching and content replication. In this paper, we adapt TLS, a widely used security standard, to an ICN context. We design solutions that allow session reuse and migration among multiple stakeholders and we propose an extension that allows authorized middleboxes to lawfully and transparently intercept secured communications.	1,0,0,0,0,0
How to Generate Pseudorandom Permutations Over Other Groups	Recent results by Alagic and Russell have given some evidence that the Even-Mansour cipher may be secure against quantum adversaries with quantum queries, if considered over other groups than $(\mathbb{Z}/2)^n$. This prompts the question as to whether or not other classical schemes may be generalized to arbitrary groups and whether classical results still apply to those generalized schemes. In this thesis, we generalize the Even-Mansour cipher and the Feistel cipher. We show that Even and Mansour's original notions of secrecy are obtained on a one-key, group variant of the Even-Mansour cipher. We generalize the result by Kilian and Rogaway, that the Even-Mansour cipher is pseudorandom, to super pseudorandomness, also in the one-key, group case. Using a Slide Attack we match the bound found above. After generalizing the Feistel cipher to arbitrary groups we resolve an open problem of Patel, Ramzan, and Sundaram by showing that the 3-round Feistel cipher over an arbitrary group is not super pseudorandom. We generalize a result by Gentry and Ramzan showing that the Even-Mansour cipher can be implemented using the Feistel cipher as the public permutation. In this result, we also consider the one-key case over a group and generalize their bound. Finally, we consider Zhandry's result on quantum pseudorandom permutations, showing that his result may be generalized to hold for arbitrary groups. In this regard, we consider whether certain card shuffles may be generalized as well.	1,0,1,0,0,0
Transition rates and radiative lifetimes of Ca I	We tabulate spontaneous emission rates for all possible 811 electric-dipole-allowed transitions between the 75 lowest-energy states of Ca I. These involve the $4sns$ ($n=4-8$), $4snp$ ($n=4-7$), $4snd$ ($n=3-6$), $4snf$ ($n=4-6$), $4p^2$, and $3d4p$ electronic configurations. We compile the transition rates by carrying out ab initio relativistic calculations using the combined method of configuration interaction and many-body perturbation theory. The results are compared to the available literature values. The tabulated rates can be useful in various applications, such as optimizing laser cooling in magneto-optical traps, estimating various systematic effects in optical clocks and evaluating static or dynamic polarizabilities and long-range atom-atom interaction coefficients and related atomic properties.	0,1,0,0,0,0
Asynchronous Decentralized Parallel Stochastic Gradient Descent	Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous algorithms using a parameter server suffer from 1) communication bottleneck at parameter servers when workers are many, and 2) significantly worse convergence when the traffic to parameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-possible convergence rate? In this paper, we propose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis shows AD-PSGD converges at the optimal $O(1/\sqrt{K})$ rate as SGD and has linear speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8X faster than its synchronous counterparts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.	1,0,0,1,0,0
General notions of regression depth function	As a measure for the centrality of a point in a set of multivariate data, statistical depth functions play important roles in multivariate analysis, because one may conveniently construct descriptive as well as inferential procedures relying on them. Many depth notions have been proposed in the literature to fit to different applications. However, most of them are mainly developed for the location setting. In this paper, we discuss the possibility of extending some of them into the regression setting. A general concept of regression depth function is also provided.	0,0,0,1,0,0
Unveiling Bias Compensation in Turbo-Based Algorithms for (Discrete) Compressed Sensing	In Compressed Sensing, a real-valued sparse vector has to be recovered from an underdetermined system of linear equations. In many applications, however, the elements of the sparse vector are drawn from a finite set. Adapted algorithms incorporating this additional knowledge are required for the discrete-valued setup. In this paper, turbo-based algorithms for both cases are elucidated and analyzed from a communications engineering perspective, leading to a deeper understanding of the algorithm. In particular, we gain the intriguing insight that the calculation of extrinsic values is equal to the unbiasing of a biased estimate and present an improved algorithm.	1,0,0,0,0,0
Weak Versus Strong Disorder Superfluid-Bose Glass Transition in One Dimension	Using large-scale simulations based on matrix product state and quantum Monte Carlo techniques, we study the superfluid to Bose glass-transition for one-dimensional attractive hard-core bosons at zero temperature, across the full regime from weak to strong disorder. As a function of interaction and disorder strength, we identify a Berezinskii-Kosterlitz-Thouless critical line with two different regimes. At small attraction where critical disorder is weak compared to the bandwidth, the critical Luttinger parameter $K_c$ takes its universal Giamarchi-Schulz value $K_{c}=3/2$. Conversely, a non-universal $K_c>3/2$ emerges for stronger attraction where weak-link physics is relevant. In this strong disorder regime, the transition is characterized by self-similar power-law distributed weak links with a continuously varying characteristic exponent $\alpha$.	0,1,0,0,0,0
Nevanlinna classes associated to a closed set on $\partial$D	We introduce Nevanlinna classes of holomorphic functions associated to a closed set on the boundary of the unit disc in the complex plane and we get Blaschke type theorems relative to these classes by use of several complex variables methods. This gives alternative proofs of some results of Favorov \& Golinskii, useful, in particular, for the study of eigenvalues of non self adjoint Schr{ö}dinger operators.	0,0,1,0,0,0
An Army of Me: Sockpuppets in Online Discussion Communities	In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as "I", and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets.	1,1,0,1,0,0
Fluid flows shaping organism morphology	A dynamic self-organized morphology is the hallmark of network-shaped organisms like slime moulds and fungi. Organisms continuously re-organize their flexible, undifferentiated body plans to forage for food. Among these organisms the slime mould Physarum polycephalum has emerged as a model to investigate how organism can self-organize their extensive networks and act as a coordinated whole. Cytoplasmic fluid flows flowing through the tubular networks have been identified as key driver of morphological dynamics. Inquiring how fluid flows can shape living matter from small to large scales opens up many new avenues for research.	0,0,0,0,1,0
Liu-Nagel phase diagrams in infinite dimension	We study Harmonic Soft Spheres as a model of thermal structural glasses in the limit of infinite dimensions. We show that cooling, compressing and shearing a glass lead to a Gardner transition and, hence, to a marginally stable amorphous solid as found for Hard Spheres systems. A general outcome of our results is that a reduced stability of the glass favors the appearance of the Gardner transition. Therefore using strong perturbations, e.g. shear and compression, on standard glasses or using weak perturbations on weakly stable glasses, e.g. the ones prepared close to the jamming point, are the generic ways to induce a Gardner transition. The formalism that we discuss allows to study general perturbations, including strain deformations that are important to study soft glassy rheology at the mean field level.	0,1,0,0,0,0
First non-icosahedral boron allotrope synthesized at high pressure and high temperature	Theoretical predictions of pressure-induced phase transformations often become long-standing enigmas because of limitations of contemporary available experimental possibilities. Hitherto the existence of a non-icosahedral boron allotrope has been one of them. Here we report on the first non-icosahedral boron allotrope, which we denoted as {\zeta}-B, with the orthorhombic {\alpha}-Ga-type structure (space group Cmce) synthesized in a diamond anvil cell at extreme high-pressure high-temperature conditions (115 GPa and 2100 K). The structure of {\zeta}-B was solved using single-crystal synchrotron X-ray diffraction and its compressional behavior was studied in the range of very high pressures (115 GPa to 135 GPa). Experimental validation of theoretical predictions reveals the degree of our up-to-date comprehension of condensed matter and promotes further development of the solid state physics and chemistry.	0,1,0,0,0,0
Variance-Reduced Stochastic Learning under Random Reshuffling	Several useful variance-reduced stochastic gradient algorithms, such as SVRG, SAGA, Finito, and SAG, have been proposed to minimize empirical risks with linear convergence properties to the exact minimizer. The existing convergence results assume uniform data sampling with replacement. However, it has been observed in related works that random reshuffling can deliver superior performance over uniform sampling and, yet, no formal proofs or guarantees of exact convergence exist for variance-reduced algorithms under random reshuffling. This paper makes two contributions. First, it resolves this open issue and provides the first theoretical guarantee of linear convergence under random reshuffling for SAGA; the argument is also adaptable to other variance-reduced algorithms. Second, under random reshuffling, the paper proposes a new amortized variance-reduced gradient (AVRG) algorithm with constant storage requirements compared to SAGA and with balanced gradient computations compared to SVRG. AVRG is also shown analytically to converge linearly.	1,0,0,1,0,0
Spontaneous currents in superconducting systems with strong spin-orbit coupling	We show that Rashba spin-orbit coupling at the interface between a superconductor and a ferromagnet should produce a spontaneous current in the atomic thickness region near the interface. This current is counter-balanced by the superconducting screening current flowing in the region of the width of the London penetration depth near the interface. Such current carrying state creates a magnetic field near the superconductor surface, generates a stray magnetic field outside the sample edges, changes the slope of the temperature dependence of the critical field $H_{c3}$ and may generate the spontaneous Abrikosov vortices near the interface.	0,1,0,0,0,0
Counterexample-Guided k-Induction Verification for Fast Bug Detection	Recently, the k-induction algorithm has proven to be a successful approach for both finding bugs and proving correctness. However, since the algorithm is an incremental approach, it might waste resources trying to prove incorrect programs. In this paper, we propose to extend the k-induction algorithm in order to shorten the number of steps required to find a property violation. We convert the algorithm into a meet-in-the-middle bidirectional search algorithm, using the counterexample produced from over-approximating the program. The preliminary results show that the number of steps required to find a property violation is reduced to $\lfloor\frac{k}{2} + 1\rfloor$ and the verification time for programs with large state space is reduced considerably.	1,0,0,0,0,0
Joint Beamforming and Antenna Selection for Sum Rate Maximization in Cognitive Radio Networks	This letter studies joint transmit beamforming and antenna selection at a secondary base station (BS) with multiple primary users (PUs) in an underlay cognitive radio multiple-input single-output broadcast channel. The objective is to maximize the sum rate subject to the secondary BS transmit power, minimum required rates for secondary users, and PUs' interference power constraints. The utility function of interest is nonconcave and the involved constraints are nonconvex, so this problem is hard to solve. Nevertheless, we propose a new iterative algorithm that finds local optima at the least. We use an inner approximation method to construct and solve a simple convex quadratic program of moderate dimension at each iteration of the proposed algorithm. Simulation results indicate that the proposed algorithm converges quickly and outperforms existing approaches.	1,0,0,0,0,0
Free LSD: Prior-Free Visual Landing Site Detection for Autonomous Planes	Full autonomy for fixed-wing unmanned aerial vehicles (UAVs) requires the capability to autonomously detect potential landing sites in unknown and unstructured terrain, allowing for self-governed mission completion or handling of emergency situations. In this work, we propose a perception system addressing this challenge by detecting landing sites based on their texture and geometric shape without using any prior knowledge about the environment. The proposed method considers hazards within the landing region such as terrain roughness and slope, surrounding obstacles that obscure the landing approach path, and the local wind field that is estimated by the on-board EKF. The latter enables applicability of the proposed method on small-scale autonomous planes without landing gear. A safe approach path is computed based on the UAV dynamics, expected state estimation and actuator uncertainty, and the on-board computed elevation map. The proposed framework has been successfully tested on photo-realistic synthetic datasets and in challenging real-world environments.	1,0,0,0,0,0
Degenerate cyclotomic Hecke algebras and higher level Heisenberg categorification	We associate a monoidal category $\mathcal{H}^\lambda$ to each dominant integral weight $\lambda$ of $\widehat{\mathfrak{sl}}_p$ or $\mathfrak{sl}_\infty$. These categories, defined in terms of planar diagrams, act naturally on categories of modules for the degenerate cyclotomic Hecke algebras associated to $\lambda$. We show that, in the $\mathfrak{sl}_\infty$ case, the level $d$ Heisenberg algebra embeds into the Grothendieck ring of $\mathcal{H}^\lambda$, where $d$ is the level of $\lambda$. The categories $\mathcal{H}^\lambda$ can be viewed as a graphical calculus describing induction and restriction functors between categories of modules for degenerate cyclotomic Hecke algebras, together with their natural transformations. As an application of this tool, we prove a new result concerning centralizers for degenerate cyclotomic Hecke algebras.	0,0,1,0,0,0
Second-order constrained variational problems on Lie algebroids: applications to optimal control	The aim of this work is to study, from an intrinsic and geometric point of view, second-order constrained variational problems on Lie algebroids, that is, optimization problems defined by a cost functional which depends on higher-order derivatives of admissible curves on a Lie algebroid. Extending the classical Skinner and Rusk formalism for the mechanics in the context of Lie algebroids, for second-order constrained mechanical systems, we derive the corresponding dynamical equations. We find a symplectic Lie subalgebroid where, under some mild regularity conditions, the second-order constrained variational problem, seen as a presymplectic Hamiltonian system, has a unique solution. We study the relationship of this formalism with the second-order constrained Euler-Poincaré and Lagrange-Poincaré equations, among others. Our study is applied to the optimal control of mechanical systems.	0,0,1,0,0,0
Brain Damage and Motor Cortex Impairment in Chronic Obstructive Pulmonary Disease: Implication of Nonrapid Eye Movement Sleep Desaturation	Nonrapid eye movement (NREM) sleep desaturation may cause neuronal damage due to the withdrawal of cerebrovascular reactivity. The current study (1) assessed the prevalence of NREM sleep desaturation in nonhypoxemic patients with chronic obstructive pulmonary disease (COPD) and (2) compared a biological marker of cerebral lesion and neuromuscular function in patients with and without NREM sleep desaturation.	0,1,0,0,0,0
Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge	We consider the use of Deep Learning methods for modeling complex phenomena like those occurring in natural physical processes. With the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics. However, despite considerable successes in a variety of application domains, the machine learning field is not yet ready to handle the level of complexity required by such problems. Using an example application, namely Sea Surface Temperature Prediction, we show how general background knowledge gained from physics could be used as a guideline for designing efficient Deep Learning models. In order to motivate the approach and to assess its generality we demonstrate a formal link between the solution of a class of differential equations underlying a large family of physical phenomena and the proposed model. Experiments and comparison with series of baselines including a state of the art numerical approach is then provided.	1,0,0,1,0,0
A new, large-scale map of interstellar reddening derived from HI emission	We present a new map of interstellar reddening, covering the 39\% of the sky with low {\rm HI} column densities ($N_{\rm HI} < 4\times10^{20}\,\rm cm^{-2}$ or $E(B-V)\approx 45\rm\, mmag$) at $16\overset{'}{.}1$ resolution, based on all-sky observations of Galactic HI emission by the HI4PI Survey. In this low column density regime, we derive a characteristic value of $N_{\rm HI}/E(B-V) = 8.8\times10^{21}\, \rm\, cm^{2}\, mag^{-1}$ for gas with $|v_{\rm LSR}| < 90\,\rm km\, s^{-1}$ and find no significant reddening associated with gas at higher velocities. We compare our HI-based reddening map with the Schlegel, Finkbeiner, and Davis (1998, SFD) reddening map and find them consistent to within a scatter of $\simeq 5\,\rm mmag$. Further, the differences between our map and the SFD map are in excellent agreement with the low resolution ($4\overset{\circ}{.}5$) corrections to the SFD map derived by Peek and Graves (2010) based on observed reddening toward passive galaxies. We therefore argue that our HI-based map provides the most accurate interstellar reddening estimates in the low column density regime to date. Our reddening map is made publicly available (this http URL).	0,1,0,0,0,0
Fast Monte-Carlo Localization on Aerial Vehicles using Approximate Continuous Belief Representations	Size, weight, and power constrained platforms impose constraints on computational resources that introduce unique challenges in implementing localization algorithms. We present a framework to perform fast localization on such platforms enabled by the compressive capabilities of Gaussian Mixture Model representations of point cloud data. Given raw structural data from a depth sensor and pitch and roll estimates from an on-board attitude reference system, a multi-hypothesis particle filter localizes the vehicle by exploiting the likelihood of the data originating from the mixture model. We demonstrate analysis of this likelihood in the vicinity of the ground truth pose and detail its utilization in a particle filter-based vehicle localization strategy, and later present results of real-time implementations on a desktop system and an off-the-shelf embedded platform that outperform localization results from running a state-of-the-art algorithm on the same environment.	1,0,0,0,0,0
Evolutionary Centrality and Maximal Cliques in Mobile Social Networks	This paper introduces an evolutionary approach to enhance the process of finding central nodes in mobile networks. This can provide essential information and important applications in mobile and social networks. This evolutionary approach considers the dynamics of the network and takes into consideration the central nodes from previous time slots. We also study the applicability of maximal cliques algorithms in mobile social networks and how it can be used to find the central nodes based on the discovered maximal cliques. The experimental results are promising and show a significant enhancement in finding the central nodes.	1,0,0,0,0,0
MIT SuperCloud Portal Workspace: Enabling HPC Web Application Deployment	The MIT SuperCloud Portal Workspace enables the secure exposure of web services running on high performance computing (HPC) systems. The portal allows users to run any web application as an HPC job and access it from their workstation while providing authentication, encryption, and access control at the system level to prevent unintended access. This capability permits users to seamlessly utilize existing and emerging tools that present their user interface as a website on an HPC system creating a portal workspace. Performance measurements indicate that the MIT SuperCloud Portal Workspace incurs marginal overhead when compared to a direct connection of the same service.	1,0,0,0,0,0
Hidden Community Detection in Social Networks	We introduce a new paradigm that is important for community detection in the realm of network analysis. Networks contain a set of strong, dominant communities, which interfere with the detection of weak, natural community structure. When most of the members of the weak communities also belong to stronger communities, they are extremely hard to be uncovered. We call the weak communities the hidden community structure. We present a novel approach called HICODE (HIdden COmmunity DEtection) that identifies the hidden community structure as well as the dominant community structure. By weakening the strength of the dominant structure, one can uncover the hidden structure beneath. Likewise, by reducing the strength of the hidden structure, one can more accurately identify the dominant structure. In this way, HICODE tackles both tasks simultaneously. Extensive experiments on real-world networks demonstrate that HICODE outperforms several state-of-the-art community detection methods in uncovering both the dominant and the hidden structure. In the Facebook university social networks, we find multiple non-redundant sets of communities that are strongly associated with residential hall, year of registration or career position of the faculties or students, while the state-of-the-art algorithms mainly locate the dominant ground truth category. In the Due to the difficulty of labeling all ground truth communities in real-world datasets, HICODE provides a promising approach to pinpoint the existing latent communities and uncover communities for which there is no ground truth. Finding this unknown structure is an extremely important community detection problem.	1,1,0,1,0,0
The $E$-cohomological Conley Index, Cup-Lengths and the Arnold Conjecture on $T^{2n}$	We give a new proof of the strong Arnold conjecture for $1$-periodic solutions of Hamiltonian systems on tori, that was first shown by C. Conley and E. Zehnder in 1983. Our proof uses other methods and is shorter than the previous one. We first show that the $E$-cohomological Conley index, that was introduced by the first author recently, has a natural module structure. This yields a new cup-length and a lower bound for the number of critical points of functionals. Then an existence result for the $E$-cohomological Conley index, which applies to the setting of the Arnold conjecture, paves the way to a new proof of it on tori.	0,0,1,0,0,0
Finding events in temporal networks: Segmentation meets densest-subgraph discovery	In this paper we study the problem of discovering a timeline of events in a temporal network. We model events as dense subgraphs that occur within intervals of network activity. We formulate the event-discovery task as an optimization problem, where we search for a partition of the network timeline into k non-overlapping intervals, such that the intervals span subgraphs with maximum total density. The output is a sequence of dense subgraphs along with corresponding time intervals, capturing the most interesting events during the network lifetime. A naive solution to our optimization problem has polynomial but prohibitively high running time complexity. We adapt existing recent work on dynamic densest-subgraph discovery and approximate dynamic programming to design a fast approximation algorithm. Next, to ensure richer structure, we adjust the problem formulation to encourage coverage of a larger set of nodes. This problem is NP-hard even for static graphs. However, on static graphs a simple greedy algorithm leads to approximate solution due to submodularity. We extended this greedy approach for the case of temporal networks. However, the approximation guarantee does not hold. Nevertheless, according to the experiments, the algorithm finds good quality solutions.	1,0,0,0,0,0
Safety-Aware Apprenticeship Learning	Apprenticeship learning (AL) is a kind of Learning from Demonstration techniques where the reward function of a Markov Decision Process (MDP) is unknown to the learning agent and the agent has to derive a good policy by observing an expert's demonstrations. In this paper, we study the problem of how to make AL algorithms inherently safe while still meeting its learning objective. We consider a setting where the unknown reward function is assumed to be a linear combination of a set of state features, and the safety property is specified in Probabilistic Computation Tree Logic (PCTL). By embedding probabilistic model checking inside AL, we propose a novel counterexample-guided approach that can ensure safety while retaining performance of the learnt policy. We demonstrate the effectiveness of our approach on several challenging AL scenarios where safety is essential.	1,0,0,0,0,0
Benchmarking gate-based quantum computers	With the advent of public access to small gate-based quantum processors, it becomes necessary to develop a benchmarking methodology such that independent researchers can validate the operation of these processors. We explore the usefulness of a number of simple quantum circuits as benchmarks for gate-based quantum computing devices and show that circuits performing identity operations are very simple, scalable and sensitive to gate errors and are therefore very well suited for this task. We illustrate the procedure by presenting benchmark results for the IBM Quantum Experience, a cloud-based platform for gate-based quantum computing.	0,1,0,0,0,0
Variational Dropout Sparsifies Deep Neural Networks	We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.	1,0,0,1,0,0
Calculation of thallium hyperfine anomaly	We suggest a method to calculate hyperfine anomaly for many-electron atoms and ions. At first, we tested this method by calculating hyperfine anomaly for hydrogen-like thallium ion and obtained fairly good agreement with analytical expressions. Then we did calculations for the neutral thallium and tested an assumption, that the the ratio between the anomalies for $s$ and $p_{1/2}$ states is the same for these two systems. Finally, we come up with recommendations about preferable atomic states for the precision measurements of the nuclear $g$ factors.	0,1,0,0,0,0
Gorenstein homological properties of tensor rings	Let $R$ be a two-sided noetherian ring and $M$ be a nilpotent $R$-bimodule, which is finitely generated on both sides. We study Gorenstein homological properties of the tensor ring $T_R(M)$. Under certain conditions, the ring $R$ is Gorenstein if and only if so is $T_R(M)$. We characterize Gorenstein projective $T_R(M)$-modules in terms of $R$-modules.	0,0,1,0,0,0
Riemannian stochastic quasi-Newton algorithm with variance reduction and its convergence analysis	Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large, but finite number of loss functions. The present paper proposes a Riemannian stochastic quasi-Newton algorithm with variance reduction (R-SQN-VR). The key challenges of averaging, adding, and subtracting multiple gradients are addressed with notions of retraction and vector transport. We present convergence analyses of R-SQN-VR on both non-convex and retraction-convex functions under retraction and vector transport operators. The proposed algorithm is evaluated on the Karcher mean computation on the symmetric positive-definite manifold and the low-rank matrix completion on the Grassmann manifold. In all cases, the proposed algorithm outperforms the state-of-the-art Riemannian batch and stochastic gradient algorithms.	1,0,1,1,0,0
Deep Text Classification Can be Fooled	In this paper, we present an effective method to craft text adversarial samples, revealing one important yet underestimated fact that DNN-based text classifiers are also prone to adversarial sample attack. Specifically, confronted with different adversarial scenarios, the text items that are important for classification are identified by computing the cost gradients of the input (white-box attack) or generating a series of occluded test samples (black-box attack). Based on these items, we design three perturbation strategies, namely insertion, modification, and removal, to generate adversarial samples. The experiment results show that the adversarial samples generated by our method can successfully fool both state-of-the-art character-level and word-level DNN-based text classifiers. The adversarial samples can be perturbed to any desirable classes without compromising their utilities. At the same time, the introduced perturbation is difficult to be perceived.	1,0,0,0,0,0
Historical Review of Recurrence Plots	In the last two decades recurrence plots (RPs) were introduced in many different scientific disciplines. It turned out how powerful this method is. After introducing approaches of quantification of RPs and by the study of relationships between RPs and fundamental properties of dynamical systems, this method attracted even more attention. After 20 years of RPs it is time to summarise this development in a historical context.	0,1,0,0,0,0
Transformation thermal convection: Cloaking, concentrating, and camouflage	Heat can generally transfer via thermal conduction, thermal radiation, and thermal convection. All the existing theories of transformation thermotics and optics can treat thermal conduction and thermal radiation, respectively. Unfortunately, thermal convection has never been touched in transformation theories due to the lack of a suitable theory, thus limiting applications associated with heat transfer through fluids (liquid or gas). Here, we develop, for the first time, a general theory of transformation thermal convection by considering the convection-diffusion equation, the Navier-Stokes equation, and the Darcy law. By introducing porous media, we get a set of coupled equations keeping their forms under coordinate transformation. As model applications, the theory helps to show the effects of cloaking, concentrating, and camouflage. Our finite element simulations confirm the theoretical findings. This work offers a general transformation theory for thermal convection, thus revealing some novel behaviors of thermal convection; it not only provides new hints on how to control heat transfer by combining thermal conduction, thermal radiation, and thermal convection, but also benefits the study of mass diffusion and other related fields that contain a set of equations and need to transform velocities at the same time.	0,1,0,0,0,0
Optimal Rates of Sketched-regularized Algorithms for Least-Squares Regression over Hilbert Spaces	We investigate regularized algorithms combining with projection for least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function. As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nyström regularized algorithms. Our results are the first ones with optimal, distribution-dependent rates that do not have any saturation effect for sketched/Nyström regularized algorithms, considering both the attainable and non-attainable cases.	0,0,0,1,0,0
Deep Room Recognition Using Inaudible Echos	Recent years have seen the increasing need of location awareness by mobile applications. This paper presents a room-level indoor localization approach based on the measured room's echos in response to a two-millisecond single-tone inaudible chirp emitted by a smartphone's loudspeaker. Different from other acoustics-based room recognition systems that record full-spectrum audio for up to ten seconds, our approach records audio in a narrow inaudible band for 0.1 seconds only to preserve the user's privacy. However, the short-time and narrowband audio signal carries limited information about the room's characteristics, presenting challenges to accurate room recognition. This paper applies deep learning to effectively capture the subtle fingerprints in the rooms' acoustic responses. Our extensive experiments show that a two-layer convolutional neural network fed with the spectrogram of the inaudible echos achieve the best performance, compared with alternative designs using other raw data formats and deep models. Based on this result, we design a RoomRecognize cloud service and its mobile client library that enable the mobile application developers to readily implement the room recognition functionality without resorting to any existing infrastructures and add-on hardware. Extensive evaluation shows that RoomRecognize achieves 99.7%, 97.7%, 99%, and 89% accuracy in differentiating 22 and 50 residential/office rooms, 19 spots in a quiet museum, and 15 spots in a crowded museum, respectively. Compared with the state-of-the-art approaches based on support vector machine, RoomRecognize significantly improves the Pareto frontier of recognition accuracy versus robustness against interfering sounds (e.g., ambient music).	1,0,0,0,0,0
On the Heat Kernel and Weyl Anomaly of Schrödinger invariant theory	We propose a method inspired from discrete light cone quantization (DLCQ) to determine the heat kernel for a Schrödinger field theory (Galilean boost invariant with $z=2$ anisotropic scaling symmetry) living in $d+1$ dimensions, coupled to a curved Newton-Cartan background starting from a heat kernel of a relativistic conformal field theory ($z=1$) living in $d+2$ dimensions. We use this method to show the Schrödinger field theory of a complex scalar field cannot have any Weyl anomalies. To be precise, we show that the Weyl anomaly $\mathcal{A}^{G}_{d+1}$ for Schrödinger theory is related to the Weyl anomaly of a free relativistic scalar CFT $\mathcal{A}^{R}_{d+2}$ via $\mathcal{A}^{G}_{d+1}= 2\pi \delta (m) \mathcal{A}^{R}_{d+2}$ where $m$ is the charge of the scalar field under particle number symmetry. We provide further evidence of vanishing anomaly by evaluating Feynman diagrams in all orders of perturbation theory. We present an explicit calculation of the anomaly using a regulated Schrödinger operator, without using the null cone reduction technique. We generalise our method to show that a similar result holds for one time derivative theories with even $z>2$.	0,1,0,0,0,0
Low-Latency Millimeter-Wave Communications: Traffic Dispersion or Network Densification?	This paper investigates two strategies to reduce the communication delay in future wireless networks: traffic dispersion and network densification. A hybrid scheme that combines these two strategies is also considered. The probabilistic delay and effective capacity are used to evaluate performance. For probabilistic delay, the violation probability of delay, i.e., the probability that the delay exceeds a given tolerance level, is characterized in terms of upper bounds, which are derived by applying stochastic network calculus theory. In addition, to characterize the maximum affordable arrival traffic for mmWave systems, the effective capacity, i.e., the service capability with a given quality-of-service (QoS) requirement, is studied. The derived bounds on the probabilistic delay and effective capacity are validated through simulations. These numerical results show that, for a given average system gain, traffic dispersion, network densification, and the hybrid scheme exhibit different potentials to reduce the end-to-end communication delay. For instance, traffic dispersion outperforms network densification, given high average system gain and arrival rate, while it could be the worst option, otherwise. Furthermore, it is revealed that, increasing the number of independent paths and/or relay density is always beneficial, while the performance gain is related to the arrival rate and average system gain, jointly. Therefore, a proper transmission scheme should be selected to optimize the delay performance, according to the given conditions on arrival traffic and system service capability.	1,0,0,0,0,0
Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field	Even- and odd-frequency superconductivity coexist due to broken time-reversal symmetry under magnetic field. In order to describe this mixing, we extend the linearized Eliashberg equation for the spin and charge fluctuation mechanism in strongly correlated electron systems. We apply this extended Eliashberg equation to the odd-frequency superconductivity on a quasi-one-dimensional isosceles triangular lattice under in-plane magnetic field and examine the effect of the even-frequency component.	0,1,0,0,0,0
Efficient exploration with Double Uncertain Value Networks	This paper studies directed exploration for reinforcement learning agents by tracking uncertainty about the value of each available action. We identify two sources of uncertainty that are relevant for exploration. The first originates from limited data (parametric uncertainty), while the second originates from the distribution of the returns (return uncertainty). We identify methods to learn these distributions with deep neural networks, where we estimate parametric uncertainty with Bayesian drop-out, while return uncertainty is propagated through the Bellman equation as a Gaussian distribution. Then, we identify that both can be jointly estimated in one network, which we call the Double Uncertain Value Network. The policy is directly derived from the learned distributions based on Thompson sampling. Experimental results show that both types of uncertainty may vastly improve learning in domains with a strong exploration challenge.	1,0,0,1,0,0
Exploring the Psychological Basis for Transitions in the Archaeological Record	In lieu of an abstract here is the first paragraph: No other species remotely approaches the human capacity for the cultural evolution of novelty that is accumulative, adaptive, and open-ended (i.e., with no a priori limit on the size or scope of possibilities). By culture we mean extrasomatic adaptations--including behavior and technology--that are socially rather than sexually transmitted. This chapter synthesizes research from anthropology, psychology, archaeology, and agent-based modeling into a speculative yet coherent account of two fundamental cognitive transitions underlying human cultural evolution that is consistent with contemporary psychology. While the chapter overlaps with a more technical paper on this topic (Gabora & Smith 2018), it incorporates new research and elaborates a genetic component to our overall argument. The ideas in this chapter grew out of a non-Darwinian framework for cultural evolution, referred to as the Self-other Reorganization (SOR) theory of cultural evolution (Gabora, 2013, in press; Smith, 2013), which was inspired by research on the origin and earliest stage in the evolution of life (Cornish-Bowden & Cárdenas 2017; Goldenfeld, Biancalani, & Jafarpour, 2017, Vetsigian, Woese, & Goldenfeld 2006; Woese, 2002). SOR bridges psychological research on fundamental aspects of our human nature such as creativity and our proclivity to reflect on ideas from different perspectives, with the literature on evolutionary approaches to cultural evolution that aspire to synthesize the behavioral sciences much as has been done for the biological scientists. The current chapter is complementary to this effort, but less abstract; it attempts to ground the theory of cultural evolution in terms of cognitive transitions as suggested by archaeological evidence.	0,0,0,0,1,0
Towards a New Interpretation of Separable Convolutions	In recent times, the use of separable convolutions in deep convolutional neural network architectures has been explored. Several researchers, most notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in their deep architectures and have demonstrated state of the art or close to state of the art performance. However, the underlying mechanism of action of separable convolutions are still not fully understood. Although their mathematical definition is well understood as a depthwise convolution followed by a pointwise convolution, deeper interpretations such as the extreme Inception hypothesis (Chollet, 2016) have failed to provide a thorough explanation of their efficacy. In this paper, we propose a hybrid interpretation that we believe is a better model for explaining the efficacy of separable convolutions.	1,0,0,1,0,0
On Sampling Strategies for Neural Network-based Collaborative Filtering	Recent advances in neural networks have inspired people to design hybrid recommendation algorithms that can incorporate both (1) user-item interaction information and (2) content information including image, audio, and text. Despite their promising results, neural network-based recommendation algorithms pose extensive computational costs, making it challenging to scale and improve upon. In this paper, we propose a general neural network-based recommendation framework, which subsumes several existing state-of-the-art recommendation algorithms, and address the efficiency issue by investigating sampling strategies in the stochastic gradient descent training for the framework. We tackle this issue by first establishing a connection between the loss functions and the user-item interaction bipartite graph, where the loss function terms are defined on links while major computation burdens are located at nodes. We call this type of loss functions "graph-based" loss functions, for which varied mini-batch sampling strategies can have different computational costs. Based on the insight, three novel sampling strategies are proposed, which can significantly improve the training efficiency of the proposed framework (up to $\times 30$ times speedup in our experiments), as well as improving the recommendation performance. Theoretical analysis is also provided for both the computational cost and the convergence. We believe the study of sampling strategies have further implications on general graph-based loss functions, and would also enable more research under the neural network-based recommendation framework.	1,0,0,1,0,0
Learning to Segment and Represent Motion Primitives from Driving Data for Motion Planning Applications	Developing an intelligent vehicle which can perform human-like actions requires the ability to learn basic driving skills from a large amount of naturalistic driving data. The algorithms will become efficient if we could decompose the complex driving tasks into motion primitives which represent the elementary compositions of driving skills. Therefore, the purpose of this paper is to segment unlabeled trajectory data into a library of motion primitives. By applying a probabilistic inference based on an iterative Expectation-Maximization algorithm, our method segments the collected trajectories while learning a set of motion primitives represented by the dynamic movement primitives. The proposed method utilizes the mutual dependencies between the segmentation and representation of motion primitives and the driving-specific based initial segmentation. By utilizing this mutual dependency and the initial condition, this paper presents how we can enhance the performance of both the segmentation and the motion primitive library establishment. We also evaluate the applicability of the primitive representation method to imitation learning and motion planning algorithms. The model is trained and validated by using the driving data collected from the Beijing Institute of Technology intelligent vehicle platform. The results show that the proposed approach can find the proper segmentation and establish the motion primitive library simultaneously.	1,0,0,0,0,0
An Upper Bound of the Minimal Dispersion via Delta Covers	For a point set of $n$ elements in the $d$-dimensional unit cube and a class of test sets we are interested in the largest volume of a test set which does not contain any point. For all natural numbers $n$, $d$ and under the assumption of a $delta$-cover with cardinality $\vert \Gamma_\delta \vert$ we prove that there is a point set, such that the largest volume of such a test set without any point is bounded by $\frac{\log \vert \Gamma_\delta \vert}{n} + \delta$. For axis-parallel boxes on the unit cube this leads to a volume of at most $\frac{4d}{n}\log(\frac{9n}{d})$ and on the torus to $\frac{4d}{n}\log (2n)$.	1,0,1,0,0,0
Braid group symmetries of Grassmannian cluster algebras	We define an action of the extended affine d-strand braid group on the open positroid stratum in the Grassmannian Gr(k,n), for d the greatest common divisor of k and n. The action is by quasi-automorphisms of the cluster structure on the Grassmannian, determining a homomorphism from the extended affine braid group to the cluster modular group. We also define a quasi-isomorphism between the Grassmannian Gr(k,rk) and the Fock-Goncharov configuration space of 2r-tuples of affine flags for SL(k). This identifies the cluster variables, clusters, and cluster modular groups, in these two cluster structures. Fomin and Pylyavskyy proposed a description of the cluster combinatorics for Gr(3,n) in terms of Kuperberg's basis of non-elliptic webs. As our main application, we prove many of their conjectures for Gr(3,9) and give a presentation for its cluster modular group. We establish similar results for Gr(4,8). These results rely on the fact that both of these Grassmannians have finite mutation type.	0,0,1,0,0,0
Geometric Rescaling Algorithms for Submodular Function Minimization	We present a new class of polynomial-time algorithms for submodular function minimization (SFM), as well as a unified framework to obtain strongly polynomial SFM algorithms. Our new algorithms are based on simple iterative methods for the minimum-norm problem, such as the conditional gradient and the Fujishige-Wolfe algorithms. We exhibit two techniques to turn simple iterative methods into polynomial-time algorithms. Firstly, we use the geometric rescaling technique, which has recently gained attention in linear programming. We adapt this technique to SFM and obtain a weakly polynomial bound $O((n^4\cdot EO + n^5)\log (n L))$. Secondly, we exhibit a general combinatorial black-box approach to turn any strongly polynomial $\varepsilon L$-approximate SFM oracle into a strongly polynomial exact SFM algorithm. This framework can be applied to a wide range of combinatorial and continuous algorithms, including pseudo-polynomial ones. In particular, we can obtain strongly polynomial algorithms by a repeated application of the conditional gradient or of the Fujishige-Wolfe algorithm. Combined with the geometric rescaling technique, the black-box approach provides a $O((n^5\cdot EO + n^6)\log^2 n)$ algorithm. Finally, we show that one of the techniques we develop in the paper can also be combined with the cutting-plane method of Lee, Sidford, and Wong, yielding a simplified variant of their $O(n^3 \log^2 n \cdot EO + n^4\log^{O(1)} n)$ algorithm.	1,0,1,0,0,0
Towards information optimal simulation of partial differential equations	Most simulation schemes for partial differential equations (PDEs) focus on minimizing a simple error norm of a discretized version of a field. This paper takes a fundamentally different approach; the discretized field is interpreted as data providing information about a real physical field that is unknown. This information is sought to be conserved by the scheme as the field evolves in time. Such an information theoretic approach to simulation was pursued before by information field dynamics (IFD). In this paper we work out the theory of IFD for nonlinear PDEs in a noiseless Gaussian approximation. The result is an action that can be minimized to obtain an informationally optimal simulation scheme. It can be brought into a closed form using field operators to calculate the appearing Gaussian integrals. The resulting simulation schemes are tested numerically in two instances for the Burgers equation. Their accuracy surpasses finite-difference schemes on the same resolution. The IFD scheme, however, has to be correctly informed on the subgrid correlation structure. In certain limiting cases we recover well-known simulation schemes like spectral Fourier Galerkin methods. We discuss implications of the approximations made.	0,1,0,1,0,0
Deformable Classifiers	Geometric variations of objects, which do not modify the object class, pose a major challenge for object recognition. These variations could be rigid as well as non-rigid transformations. In this paper, we design a framework for training deformable classifiers, where latent transformation variables are introduced, and a transformation of the object image to a reference instantiation is computed in terms of the classifier output, separately for each class. The classifier outputs for each class, after transformation, are compared to yield the final decision. As a by-product of the classification this yields a transformation of the input object to a reference pose, which can be used for downstream tasks such as the computation of object support. We apply a two-step training mechanism for our framework, which alternates between optimizing over the latent transformation variables and the classifier parameters to minimize the loss function. We show that multilayer perceptrons, also known as deep networks, are well suited for this approach and achieve state of the art results on the rotated MNIST and the Google Earth dataset, and produce competitive results on MNIST and CIFAR-10 when training on smaller subsets of training data.	0,0,0,1,0,0
Junctions of refined Wilson lines and one-parameter deformation of quantum groups	We study junctions of Wilson lines in refined SU(N) Chern-Simons theory and their local relations. We focus on junctions of Wilson lines in antisymmetric and symmetric powers of the fundamental representation and propose a set of local relations which realize one-parameter deformations of quantum groups $\dot{U}_{q}(\mathfrak{sl}_{m})$ and $\dot{U}_{q}(\mathfrak{sl}_{n|m})$.	0,0,1,0,0,0
The Helsinki Neural Machine Translation System	We introduce the Helsinki Neural Machine Translation system (HNMT) and how it is applied in the news translation task at WMT 2017, where it ranked first in both the human and automatic evaluations for English--Finnish. We discuss the success of English--Finnish translations and the overall advantage of NMT over a strong SMT baseline. We also discuss our submissions for English--Latvian, English--Chinese and Chinese--English.	1,0,0,0,0,0
Classification without labels: Learning from mixed samples in high energy physics	Modern machine learning techniques can be used to construct powerful models for difficult collider physics problems. In many applications, however, these models are trained on imperfect simulations due to a lack of truth-level information in the data, which risks the model learning artifacts of the simulation. In this paper, we introduce the paradigm of classification without labels (CWoLa) in which a classifier is trained to distinguish statistical mixtures of classes, which are common in collider physics. Crucially, neither individual labels nor class proportions are required, yet we prove that the optimal classifier in the CWoLa paradigm is also the optimal classifier in the traditional fully-supervised case where all label information is available. After demonstrating the power of this method in an analytical toy example, we consider a realistic benchmark for collider physics: distinguishing quark- versus gluon-initiated jets using mixed quark/gluon training samples. More generally, CWoLa can be applied to any classification problem where labels or class proportions are unknown or simulations are unreliable, but statistical mixtures of the classes are available.	0,0,0,1,0,0
Semiparametrical Gaussian Processes Learning of Forward Dynamical Models for Navigating in a Circular Maze	This paper presents a problem of model learning for the purpose of learning how to navigate a ball to a goal state in a circular maze environment with two degrees of freedom. The motion of the ball in the maze environment is influenced by several non-linear effects such as dry friction and contacts, which are difficult to model physically. We propose a semiparametric model to estimate the motion dynamics of the ball based on Gaussian Process Regression equipped with basis functions obtained from physics first principles. The accuracy of this semiparametric model is shown not only in estimation but also in prediction at n-steps ahead and its compared with standard algorithms for model learning. The learned model is then used in a trajectory optimization algorithm to compute ball trajectories. We propose the system presented in the paper as a benchmark problem for reinforcement and robot learning, for its interesting and challenging dynamics and its relative ease of reproducibility.	1,0,0,1,0,0
A new statistical method for characterizing the atmospheres of extrasolar planets	By detecting light from extrasolar planets,we can measure their compositions and bulk physical properties. The technologies used to make these measurements are still in their infancy, and a lack of self-consistency suggests that previous observations have underestimated their systemic errors.We demonstrate a statistical method, newly applied to exoplanet characterization, which uses a Bayesian formalism to account for underestimated errorbars. We use this method to compare photometry of a substellar companion, GJ 758b, with custom atmospheric models. Our method produces a probability distribution of atmospheric model parameters including temperature, gravity, cloud model (fsed), and chemical abundance for GJ 758b. This distribution is less sensitive to highly variant data, and appropriately reflects a greater uncertainty on parameter fits.	0,1,0,0,0,0
Inductive Freeness of Ziegler's Canonical Multiderivations for Reflection Arrangements	Let $A$ be a free hyperplane arrangement. In 1989, Ziegler showed that the restriction $A''$ of $A$ to any hyperplane endowed with the natural multiplicity is then a free multiarrangement. We initiate a study of the stronger freeness property of inductive freeness for these canonical free multiarrangements and investigate them for the underlying class of reflection arrangements. More precisely, let $A = A(W)$ be the reflection arrangement of a complex reflection group $W$. By work of Terao, each such reflection arrangement is free. Thus so is Ziegler's canonical multiplicity on the restriction $A''$ of $A$ to a hyperplane. We show that the latter is inductively free as a multiarrangement if and only if $A''$ itself is inductively free.	0,0,1,0,0,0
Cosmic quantum optical probing of quantum gravity through a gravitational lensLens	We consider the nonunitary quantum dynamics of neutral massless scalar particles used to model photons around a massive gravitational lens. The gravitational interaction between the lensing mass and asymptotically free particles is described by their second-quantized scattering wavefunctions. Remarkably, the zero-point spacetime fluctuations can induce significant decoherence of the scattered states with spontaneous emission of gravitons, thereby reducing the particles' coherence as well as energy. This new effect suggests that, when photon polarizations are negligible, such quantum gravity phenomena could lead to measurable anomalous redshift of recently studied astrophysical lasers through a gravitational lens in the range of black holes and galaxy clusters.	0,1,0,0,0,0
Shutting down or powering up a (U)LIRG? Merger components in distinctly different evolutionary states in IRAS 19115-2124 (The Bird)	We present new SINFONI near-infrared integral field unit (IFU) spectroscopy and SALT optical long-slit spectroscopy characterising the history of a nearby merging luminous infrared galaxy, dubbed the Bird (IRAS19115-2114). The NIR line-ratio maps of the IFU data-cubes and stellar population fitting of the SALT spectra now allow dating of the star formation (SF) over the triple system uncovered from our previous adaptive optics data. The distinct components separate very clearly in a line-ratio diagnostic diagram. An off-nuclear pure starburst dominates the current SF of the Bird with 60-70% of the total, with a 4-7 Myr age, and signs of a fairly constant long-term star formation of the underlying stellar population. The most massive nucleus, in contrast, is quenched with a starburst age of >40 Myr and shows hints of budding AGN activity. The secondary massive nucleus is at an intermediate stage. The two major components have a population of older stars, consistent with a starburst triggered 1 Gyr ago in a first encounter. The simplest explanation of the history is that of a triple merger, where the strongly star forming component has joined later. We detect multiple gas flows in different phases. The Bird offers an opportunity to witness multiple stages of galaxy evolution in the same system; triggering as well as quenching of SF, and the early appearance of AGN activity. It also serves as a cautionary note on interpretations of observations with lower spatial resolution and/or without infrared data. At high-redshift the system would look like a clumpy starburst with crucial pieces of its puzzle hidden, in danger of misinterpretations.	0,1,0,0,0,0
Limit multiplicities for ${\rm SL}_2(\mathcal{O}_F)$ in ${\rm SL}_2(\mathbb{R}^{r_1}\oplus\mathbb{C}^{r_2})$	We prove that the family of lattices ${\rm SL}_2(\mathcal{O}_F)$, $F$ running over number fields with fixed archimedean signature $(r_1, r_2)$, in ${\rm SL}_2(\mathbb{R}^{r_1}\oplus\mathbb{C}^{r_2})$ has the limit multiplicity property.	0,0,1,0,0,0
Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of learning relational order via reinforcement learning procedure?	In this article, we extend the conventional framework of convolutional-Restricted-Boltzmann-Machine to learn highly abstract features among abitrary number of time related input maps by constructing a layer of multiplicative units, which capture the relations among inputs. In many cases, more than two maps are strongly related, so it is wise to make multiplicative unit learn relations among more input maps, in other words, to find the optimal relational-order of each unit. In order to enable our machine to learn relational order, we developed a reinforcement-learning method whose optimality is proven to train the network.	1,0,0,1,0,0
Theory of Disorder-Induced Half-Integer Thermal Hall Conductance	Electrons that are confined to a single Landau level in a two dimensional electron gas realize the effects of strong electron-electron repulsion in its purest form. The kinetic energy of individual electrons is completely quenched and all physical properties are dictated solely by many-body effects. A remarkable consequence is the emergence of new quasiparticles with fractional charge and exotic quantum statistics of which the most exciting ones are non-Abelian quasiparticles. A non-integer quantized thermal Hall conductance $\kappa_{xy}$ (in units of temperature times the universal constant $\pi^2 k_B^2 /3 h$; $h$ is the Planck constant and $k_B$ the Boltzmann constant) necessitates the existence of such quasiparticles. It has been predicted, and verified numerically, that such states are realized in the clean half-filled first Landau level of electrons with Coulomb repulsion, with $\kappa_{xy}$ being either $3/2$ or $7/2$. Excitingly, a recent experiment has indeed observed a half-integer value, which was measured, however, to be $\kappa_{xy}=5/2$. We resolve this contradiction within a picture where smooth disorder results in the formation of mesoscopic puddles with locally $\kappa_{xy}=3/2$ or $7/2$. Interactions between these puddles generate a coherent macroscopic state, which is reflected in an extended plateau with quantized $\kappa_{xy}=5/2$. The topological properties of quasiparticles at large distances are determined by the macroscopic phase, and not by the microscopic puddle where they reside. In principle, the same mechanism might also allow non-Abelian quasiparticles to emerge from a system comprised of microscopic Abelian puddles.	0,1,0,0,0,0
Learning Credible Models	In many settings, it is important that a model be capable of providing reasons for its predictions (i.e., the model must be interpretable). However, the model's reasoning may not conform with well-established knowledge. In such cases, while interpretable, the model lacks \textit{credibility}. In this work, we formally define credibility in the linear setting and focus on techniques for learning models that are both accurate and credible. In particular, we propose a regularization penalty, expert yielded estimates (EYE), that incorporates expert knowledge about well-known relationships among covariates and the outcome of interest. We give both theoretical and empirical results comparing our proposed method to several other regularization techniques. Across a range of settings, experiments on both synthetic and real data show that models learned using the EYE penalty are significantly more credible than those learned using other penalties. Applied to a large-scale patient risk stratification task, our proposed technique results in a model whose top features overlap significantly with known clinical risk factors, while still achieving good predictive performance.	1,0,0,1,0,0
The distribution of symmetry of a naturally reductive nilpotent Lie group	We show that the distribution of symmetry of a naturally reductive nilpotent Lie group coincides with the invariant distribution induced by the set of fixed vectors of the isotropy. This extends a known result on compact naturally reductive spaces. We also address the study of the quotient by the foliation of symmetry.	0,0,1,0,0,0
NEON+: Accelerated Gradient Methods for Extracting Negative Curvature for Non-Convex Optimization	Accelerated gradient (AG) methods are breakthroughs in convex optimization, improving the convergence rate of the gradient descent method for optimization with smooth functions. However, the analysis of AG methods for non-convex optimization is still limited. It remains an open question whether AG methods from convex optimization can accelerate the convergence of the gradient descent method for finding local minimum of non-convex optimization problems. This paper provides an affirmative answer to this question. In particular, we analyze two renowned variants of AG methods (namely Polyak's Heavy Ball method and Nesterov's Accelerated Gradient method) for extracting the negative curvature from random noise, which is central to escaping from saddle points. By leveraging the proposed AG methods for extracting the negative curvature, we present a new AG algorithm with double loops for non-convex optimization~\footnote{this is in contrast to a single-loop AG algorithm proposed in a recent manuscript~\citep{AGNON}, which directly analyzed the Nesterov's AG method for non-convex optimization and appeared online on November 29, 2017. However, we emphasize that our work is an independent work, which is inspired by our earlier work~\citep{NEON17} and is based on a different novel analysis.}, which converges to second-order stationary point $\x$ such that $\|\nabla f(\x)\|\leq \epsilon$ and $\nabla^2 f(\x)\geq -\sqrt{\epsilon} I$ with $\widetilde O(1/\epsilon^{1.75})$ iteration complexity, improving that of gradient descent method by a factor of $\epsilon^{-0.25}$ and matching the best iteration complexity of second-order Hessian-free methods for non-convex optimization.	0,0,0,1,0,0
Superconductivity at 7.3 K in the 133-type Cr-based RbCr3As3 single crystals	Here we report the preparation and superconductivity of the 133-type Cr-based quasi-one-dimensional (Q1D) RbCr3As3 single crystals. The samples were prepared by the deintercalation of Rb+ ions from the 233-type Rb2Cr3As3 crystals which were grown from a high-temperature solution growth method. The RbCr3As3 compound crystallizes in a centrosymmetric structure with the space group of P63/m (No. 176) different with its non-centrosymmetric Rb2Cr3As3 superconducting precursor, and the refined lattice parameters are a = 9.373(3) {\AA} and c = 4.203(7) {\AA}. Electrical resistivity and magnetic susceptibility characterizations reveal the occurrence of superconductivity with an interestingly higher onset Tc of 7.3 K than other Cr-based superconductors, and a high upper critical field Hc2(0) near 70 T in this 133-type RbCr3As3 crystals.	0,1,0,0,0,0
Output Impedance Diffusion into Lossy Power Lines	Output impedances are inherent elements of power sources in the electrical grids. In this paper, we give an answer to the following question: What is the effect of output impedances on the inductivity of the power network? To address this question, we propose a measure to evaluate the inductivity of a power grid, and we compute this measure for various types of output impedances. Following this computation, it turns out that network inductivity highly depends on the algebraic connectivity of the network. By exploiting the derived expressions of the proposed measure, one can tune the output impedances in order to enforce a desired level of inductivity on the power system. Furthermore, the results show that the more "connected" the network is, the more the output impedances diffuse into the network. Finally, using Kron reduction, we provide examples that demonstrate the utility and validity of the method.	1,0,0,0,0,0
Neural Networks Compression for Language Modeling	In this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks (RNNs). It is known that conventional RNNs, e.g, LSTM-based networks in language modeling, are characterized with either high space complexity or substantial inference time. This problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. By using the Penn Treebank (PTB) dataset we compare pruning, quantization, low-rank factorization, tensor train decomposition for LSTM networks in terms of model size and suitability for fast inference.	1,0,0,1,0,0
A Bayesian Approach for Inferring Local Causal Structure in Gene Regulatory Networks	Gene regulatory networks play a crucial role in controlling an organism's biological processes, which is why there is significant interest in developing computational methods that are able to extract their structure from high-throughput genetic data. A typical approach consists of a series of conditional independence tests on the covariance structure meant to progressively reduce the space of possible causal models. We propose a novel efficient Bayesian method for discovering the local causal relationships among triplets of (normally distributed) variables. In our approach, we score the patterns in the covariance matrix in one go and we incorporate the available background knowledge in the form of priors over causal structures. Our method is flexible in the sense that it allows for different types of causal structures and assumptions. We apply the approach to the task of inferring gene regulatory networks by learning regulatory relationships between gene expression levels. We show that our algorithm produces stable and conservative posterior probability estimates over local causal structures that can be used to derive an honest ranking of the most meaningful regulatory relationships. We demonstrate the stability and efficacy of our method both on simulated data and on real-world data from an experiment on yeast.	0,0,0,1,1,0
Robust Implicit Backpropagation	Arguably the biggest challenge in applying neural networks is tuning the hyperparameters, in particular the learning rate. The sensitivity to the learning rate is due to the reliance on backpropagation to train the network. In this paper we present the first application of Implicit Stochastic Gradient Descent (ISGD) to train neural networks, a method known in convex optimization to be unconditionally stable and robust to the learning rate. Our key contribution is a novel layer-wise approximation of ISGD which makes its updates tractable for neural networks. Experiments show that our method is more robust to high learning rates and generally outperforms standard backpropagation on a variety of tasks.	0,0,0,1,0,0
The stability and energy exchange mechanism of divergent states with real energy	The eigenvalue of the hermitic Hamiltonian is real undoubtedly. Actually, The reality can also be guaranteed by the $PT$-symmetry. The hermiticity and the $PT$-symmetric quantum theory both have requirements regarding the boundary condition. There exists a reverse strategy to investigate the quantum problem. Namely, define the eigenvalue as real first, and, meanwhile, open the boundary condition. Then the behaviors of the wave function at the boundary become rich in meaning. This eigenfunction is generally divergent, and the extent and direction of divergence are closely linked to the energy. It was noted that these divergent behaviors can be well described by their energy-space uncertainty relation which is not trivial anymore. The divergent state is unstable and will certainly exchange energy with the outside. The mechanism of energy exchange is just in the energy-space uncertainty relation, which will benefit dynamic simulation, the many-body problem, and so on. There is no distinct dividing line between this kind of divergent unstable state and the convergent stable state. Their relationship is like that of the rational and irrational numbers. In practice, there are distinct advantages of speed and accuracy for the methods based on the laws of divergence.	0,1,0,0,0,0
Spatial heterogeneities shape collective behavior of signaling amoeboid cells	We present novel experimental results on pattern formation of signaling Dictyostelium discoideum amoeba in the presence of a periodic array of millimeter-sized pillars. We observe concentric cAMP waves that initiate almost synchronously at the pillars and propagate outwards. These waves have higher frequency than the other firing centers and dominate the system dynamics. The cells respond chemotactically to these circular waves and stream towards the pillars, forming periodic Voronoi domains that reflect the periodicity of the underlying lattice. We performed comprehensive numerical simulations of a reaction-diffusion model to study the characteristics of the boundary conditions given by the obstacles. Our simulations show that, the obstacles can act as the wave source depending on the imposed boundary condition. Interestingly, a critical minimum accumulation of cAMP around the obstacles is needed for the pillars to act as the wave source. This critical value is lower at smaller production rates of the intracellular cAMP which can be controlled in our experiments using caffeine. Experiments and simulations also show that in the presence of caffeine the number of firing centers is reduced which is crucial in our system for circular waves emitted from the pillars to successfully take over the dynamics. These results are crucial to understand the signaling mechanism of Dictyostelium cells that experience spatial heterogeneities in its natural habitat.	0,0,0,0,1,0
Faster Bounding Box Annotation for Object Detection in Indoor Scenes	This paper proposes an approach for rapid bounding box annotation for object detection datasets. The procedure consists of two stages: The first step is to annotate a part of the dataset manually, and the second step proposes annotations for the remaining samples using a model trained with the first stage annotations. We experimentally study which first/second stage split minimizes to total workload. In addition, we introduce a new fully labeled object detection dataset collected from indoor scenes. Compared to other indoor datasets, our collection has more class categories, different backgrounds, lighting conditions, occlusion and high intra-class differences. We train deep learning based object detectors with a number of state-of-the-art models and compare them in terms of speed and accuracy. The fully annotated dataset is released freely available for the research community.	0,0,0,1,0,0
On the Faithfulness of 1-dimensional Topological Quantum Field Theories	This paper explores 1-dimensional topological quantum field theories. We separately deal with strict and strong 1-dimensional topological quantum field theories. The strict one is regarded as a symmetric monoidal functor between the category of 1-cobordisms and the category of matrices, and the strong one is a symmetric monoidal functor between the category of 1-cobordisms and the category of finite dimensional vector spaces. It has been proved that both strict and strong 1-dimensional topological quantum field theories are faithful.	0,0,1,0,0,0
On the Parallel Parameterized Complexity of the Graph Isomorphism Problem	In this paper, we study the parallel and the space complexity of the graph isomorphism problem (\GI{}) for several parameterizations. Let $\mathcal{H}=\{H_1,H_2,\cdots,H_l\}$ be a finite set of graphs where $|V(H_i)|\leq d$ for all $i$ and for some constant $d$. Let $\mathcal{G}$ be an $\mathcal{H}$-free graph class i.e., none of the graphs $G\in \mathcal{G}$ contain any $H \in \mathcal{H}$ as an induced subgraph. We show that \GI{} parameterized by vertex deletion distance to $\mathcal{G}$ is in a parameterized version of $\AC^1$, denoted $\PL$-$\AC^1$, provided the colored graph isomorphism problem for graphs in $\mathcal{G}$ is in $\AC^1$. From this, we deduce that \GI{} parameterized by the vertex deletion distance to cographs is in $\PL$-$\AC^1$. The parallel parameterized complexity of \GI{} parameterized by the size of a feedback vertex set remains an open problem. Towards this direction we show that the graph isomorphism problem is in $\PL$-$\TC^0$ when parameterized by vertex cover or by twin-cover. Let $\mathcal{G}'$ be a graph class such that recognizing graphs from $\mathcal{G}'$ and the colored version of \GI{} for $\mathcal{G}'$ is in logspace ($\L$). We show that \GI{} for bounded vertex deletion distance to $\mathcal{G}'$ is in $\L$. From this, we obtain logspace algorithms for \GI{} for graphs with bounded vertex deletion distance to interval graphs and graphs with bounded vertex deletion distance to cographs.	1,0,0,0,0,0
Imaging the Schwarzschild-radius-scale Structure of M87 with the Event Horizon Telescope using Sparse Modeling	We propose a new imaging technique for radio and optical/infrared interferometry. The proposed technique reconstructs the image from the visibility amplitude and closure phase, which are standard data products of short-millimeter very long baseline interferometers such as the Event Horizon Telescope (EHT) and optical/infrared interferometers, by utilizing two regularization functions: the $\ell_1$-norm and total variation (TV) of the brightness distribution. In the proposed method, optimal regularization parameters, which represent the sparseness and effective spatial resolution of the image, are derived from data themselves using cross validation (CV). As an application of this technique, we present simulated observations of M87 with the EHT based on four physically motivated models. We confirm that $\ell_1$+TV regularization can achieve an optimal resolution of $\sim 20-30$% of the diffraction limit $\lambda/D_{\rm max}$, which is the nominal spatial resolution of a radio interferometer. With the proposed technique, the EHT can robustly and reasonably achieve super-resolution sufficient to clearly resolve the black hole shadow. These results make it promising for the EHT to provide an unprecedented view of the event-horizon-scale structure in the vicinity of the super-massive black hole in M87 and also the Galactic center Sgr A*.	0,1,0,0,0,0
Perception-based energy functions in seam-cutting	Image stitching is challenging in consumer-level photography, due to alignment difficulties in unconstrained shooting environment. Recent studies show that seam-cutting approaches can effectively relieve artifacts generated by local misalignment. Normally, seam-cutting is described in terms of energy minimization, however, few of existing methods consider human perception in their energy functions, which sometimes causes that a seam with minimum energy is not most invisible in the overlapping region. In this paper, we propose a novel perception-based energy function in the seam-cutting framework, which considers the nonlinearity and the nonuniformity of human perception in energy minimization. Our perception-based approach adopts a sigmoid metric to characterize the perception of color discrimination, and a saliency weight to simulate that human eyes incline to pay more attention to salient objects. In addition, our seam-cutting composition can be easily implemented into other stitching pipelines. Experiments show that our method outperforms the seam-cutting method of the normal energy function, and a user study demonstrates that our composed results are more consistent with human perception.	1,0,0,0,0,0
Finite-Time Stabilization of Longitudinal Control for Autonomous Vehicles via a Model-Free Approach	This communication presents a longitudinal model-free control approach for computing the wheel torque command to be applied on a vehicle. This setting enables us to overcome the problem of unknown vehicle parameters for generating a suitable control law. An important parameter in this control setting is made time-varying for ensuring finite-time stability. Several convincing computer simulations are displayed and discussed. Overshoots become therefore smaller. The driving comfort is increased and the robustness to time-delays is improved.	1,0,1,0,0,0
Protein Classification using Machine Learning and Statistical Techniques: A Comparative Analysis	In recent era prediction of enzyme class from an unknown protein is one of the challenging tasks in bioinformatics. Day to day the number of proteins is increases as result the prediction of enzyme class gives a new opportunity to bioinformatics scholars. The prime objective of this article is to implement the machine learning classification technique for feature selection and predictions also find out an appropriate classification technique for function prediction. In this article the seven different classification technique like CRT, QUEST, CHAID, C5.0, ANN (Artificial Neural Network), SVM and Bayesian has been implemented on 4368 protein data that has been extracted from UniprotKB databank and categories into six different class. The proteins data is high dimensional sequence data and contain a maximum of 48 features.To manipulate the high dimensional sequential protein data with different classification technique, the SPSS has been used as an experimental tool. Different classification techniques give different results for every model and shows that the data are imbalanced for class C4, C5 and C6. The imbalanced data affect the performance of model. In these three classes the precision and recall value is very less or negligible. The experimental results highlight that the C5.0 classification technique accuracy is more suited for protein feature classification and predictions. The C5.0 classification technique gives 95.56% accuracy and also gives high precision and recall value. Finally, we conclude that the features that is selected can be used for function prediction.	0,0,0,0,1,0
An improvement on LSB+ method	The Least Significant Bit (LSB) substitution is an old and simple data hiding method that could almost effortlessly be implemented in spatial or transform domain over any digital media. This method can be attacked by several steganalysis methods, because it detectably changes statistical and perceptual characteristics of the cover signal. A typical method for steganalysis of the LSB substitution is the histogram attack that attempts to diagnose anomalies in the cover image's histogram. A well-known method to stand the histogram attack is the LSB+ steganography that intentionally embeds some extra bits to make the histogram look natural. However, the LSB+ method still affects the perceptual and statistical characteristics of the cover signal. In this paper, we propose a new method for image steganography, called LSB++, which improves over the LSB+ image steganography by decreasing the amount of changes made to the perceptual and statistical attributes of the cover image. We identify some sensitive pixels affecting the signal characteristics, and then lock and keep them from the extra bit embedding process of the LSB+ method, by introducing a new embedding key. Evaluation results show that, without reducing the embedding capacity, our method can decrease potentially detectable changes caused by the embedding process.	1,0,0,0,0,0
Simultaneously Learning Neighborship and Projection Matrix for Supervised Dimensionality Reduction	Explicitly or implicitly, most of dimensionality reduction methods need to determine which samples are neighbors and the similarity between the neighbors in the original highdimensional space. The projection matrix is then learned on the assumption that the neighborhood information (e.g., the similarity) is known and fixed prior to learning. However, it is difficult to precisely measure the intrinsic similarity of samples in high-dimensional space because of the curse of dimensionality. Consequently, the neighbors selected according to such similarity might and the projection matrix obtained according to such similarity and neighbors are not optimal in the sense of classification and generalization. To overcome the drawbacks, in this paper we propose to let the similarity and neighbors be variables and model them in low-dimensional space. Both the optimal similarity and projection matrix are obtained by minimizing a unified objective function. Nonnegative and sum-to-one constraints on the similarity are adopted. Instead of empirically setting the regularization parameter, we treat it as a variable to be optimized. It is interesting that the optimal regularization parameter is adaptive to the neighbors in low-dimensional space and has intuitive meaning. Experimental results on the YALE B, COIL-100, and MNIST datasets demonstrate the effectiveness of the proposed method.	0,0,0,1,0,0
The strength of Ramsey's theorem for pairs and arbitrarily many colors	In this paper, we show that $\mathrm{RT}^{2}+\mathsf{WKL}_0$ is a $\Pi^{1}_{1}$-conservative extension of $\mathrm{B}\Sigma^0_3$.	0,0,1,0,0,0
A Broader View on Bias in Automated Decision-Making: Reflecting on Epistemology and Dynamics	Machine learning (ML) is increasingly deployed in real world contexts, supplying actionable insights and forming the basis of automated decision-making systems. While issues resulting from biases pre-existing in training data have been at the center of the fairness debate, these systems are also affected by technical and emergent biases, which often arise as context-specific artifacts of implementation. This position paper interprets technical bias as an epistemological problem and emergent bias as a dynamical feedback phenomenon. In order to stimulate debate on how to change machine learning practice to effectively address these issues, we explore this broader view on bias, stress the need to reflect on epistemology, and point to value-sensitive design methodologies to revisit the design and implementation process of automated decision-making systems.	1,0,0,1,0,0
Nanoscale superconducting memory based on the kinetic inductance of asymmetric nanowire loops	The demand for low-dissipation nanoscale memory devices is as strong as ever. As Moore's Law is staggering, and the demand for a low-power-consuming supercomputer is high, the goal of making information processing circuits out of superconductors is one of the central goals of modern technology and physics. So far, digital superconducting circuits could not demonstrate their immense potential. One important reason for this is that a dense superconducting memory technology is not yet available. Miniaturization of traditional superconducting quantum interference devices is difficult below a few micrometers because their operation relies on the geometric inductance of the superconducting loop. Magnetic memories do allow nanometer-scale miniaturization, but they are not purely superconducting (Baek et al 2014 Nat. Commun. 5 3888). Our approach is to make nanometer scale memory cells based on the kinetic inductance (and not geometric inductance) of superconducting nanowire loops, which have already shown many fascinating properties (Aprili 2006 Nat. Nanotechnol. 1 15; Hopkins et al 2005 Science 308 1762). This allows much smaller devices and naturally eliminates magnetic-field cross-talk. We demonstrate that the vorticity, i.e., the winding number of the order parameter, of a closed superconducting loop can be used for realizing a nanoscale nonvolatile memory device. We demonstrate how to alter the vorticity in a controlled fashion by applying calibrated current pulses. A reliable read-out of the memory is also demonstrated. We present arguments that such memory can be developed to operate without energy dissipation.	0,1,0,0,0,0
A finite field analogue for Appell series F_3	In this paper we introduce a finite field analogue for the Appell series F_3 and give some reduction formulae and certain generating functions for this function over finite fields.	0,0,1,0,0,0
The rigorous derivation of the linear Landau equation from a particle system in a weak-coupling limit	We consider a system of N particles interacting via a short-range smooth potential, in a intermediate regime between the weak-coupling and the low-density. We provide a rigorous derivation of the Linear Landau equation from this particle system. The strategy of the proof consists in showing the asymptotic equivalence between the one-particle marginal and the solution of the linear Boltzmann equation with vanishing mean free path.Then, following the ideas of Landau, we prove the asympotic equivalence between the solutions of the Boltzmann and Landau linear equation in the grazing collision limit.	0,0,1,0,0,0
Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey	Natural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks.	1,0,0,0,0,0
Split, Send, Reassemble: A Formal Specification of a CAN Bus Protocol Stack	We present a formal model for a fragmentation and a reassembly protocol running on top of the standardised CAN bus, which is widely used in automotive and aerospace applications. Although the CAN bus comes with an in-built mechanism for prioritisation, we argue that this is not sufficient and provide another protocol to overcome this shortcoming.	1,0,0,0,0,0
Local migration quantification method for scratch assays	Motivation: The scratch assay is a standard experimental protocol used to characterize cell migration. It can be used to identify genes that regulate migration and evaluate the efficacy of potential drugs that inhibit cancer invasion. In these experiments, a scratch is made on a cell monolayer and recolonisation of the scratched region is imaged to quantify cell migration rates. A drawback of this methodology is the lack of its reproducibility resulting in irregular cell-free areas with crooked leading edges. Existing quantification methods deal poorly with such resulting irregularities present in the data. Results: We introduce a new quantification method that can analyse low quality experimental data. By considering in-silico and in-vitro data, we show that the method provides a more accurate statistical classification of the migration rates than two established quantification methods. The application of this method will enable the quantification of migration rates of scratch assay data previously unsuitable for analysis. Availability and Implementation: The source code and the implementation of the algorithm as a GUI along with an example dataset and user instructions, are available in this https URL. The datasets are available in this https URL.	0,0,0,0,1,0
Closed-form approximations in derivatives pricing: The Kristensen-Mele approach	Kristensen and Mele (2011) developed a new approach to obtain closed-form approximations to continuous-time derivatives pricing models. The approach uses a power series expansion of the pricing bias between an intractable model and some known auxiliary model. Since the resulting approximation formula has closed-form it is straightforward to obtain approximations of greeks. In this thesis I will introduce Kristensen and Mele's methods and apply it to a variety of stochastic volatility models of European style options as well as a model for commodity futures. The focus of this thesis is the effect of different model choices and different model parameter values on the numerical stability of Kristensen and Mele's approximation.	0,0,0,0,0,1
Holographic Entanglement Entropy in Cyclic Cosmology	We discuss a cyclic cosmology in which the visible universe, or introverse, is all that is accessible to an observer while the extroverse represents the total spacetime originating from the time when the dark energy began to dominate. It is argued that entanglement entropy of the introverse is the more appropriate quantity to render infinitely cyclic, rather than the entropy of the total universe. Since vanishing entanglement entropy implies disconnected spacetimes, at the turnaround when the introverse entropy is zero the disconnected extroverse can be jettisoned with impunity.	0,1,0,0,0,0
A Characterization of Integral ISS for Switched and Time-varying Systems	Most of the existing characterizations of the integral input-to-state stability (iISS) property are not valid for time-varying or switched systems in cases where converse Lyapunov theorems for stability are not available. This note provides a characterization that is valid for switched and time-varying systems, and shows that natural extensions of some of the existing characterizations result in only sufficient but not necessary conditions. The results provided also pinpoint suitable iISS gains and relate these to supply functions and bounds on the function defining the system dynamics.	1,0,1,0,0,0
How Do Software Startups Pivot? Empirical Results from a Multiple Case Study	In order to handle intense time pressure and survive in dynamic market, software startups have to make crucial decisions constantly on whether to change directions or stay on chosen courses, or in the terms of Lean Startup, to pivot or to persevere. The existing research and knowledge on software startup pivots are very limited. In this study, we focused on understanding the pivoting processes of software startups, and identified the triggering factors and pivot types. To achieve this, we employed a multiple case study approach, and analyzed the data obtained from four software startups. The initial findings show that different software startups make different types of pivots related to business and technology during their product development life cycle. The pivots are triggered by various factors including negative customer feedback.	1,0,0,0,0,0
A geometric realization of the $m$-cluster categories of type $\tilde{D_n}$	We show that a subcategory of the $m$-cluster category of type $\tilde{D_n}$ is isomorphic to a category consisting of arcs in an $(n-2)m$-gon with two central $(m-1)$-gons inside of it. We show that the mutation of colored quivers and $m$-cluster-tilting objects is compatible with the flip of an $(m+2)$-angulation. In the final part of this paper, we detail an example of a quiver of type $\tilde{D_7}$.	0,0,1,0,0,0
Parsimonious Data: How a single Facebook like predicts voting behaviour in multiparty systems	Recently, two influential PNAS papers have shown how our preferences for 'Hello Kitty' and 'Harley Davidson', obtained through Facebook likes, can accurately predict details about our personality, religiosity, political attitude and sexual orientation (Konsinski et al. 2013; Youyou et al 2015). In this paper, we make the claim that though the wide variety of Facebook likes might predict such personal traits, even more accurate and generalizable results can be reached through applying a contexts-specific, parsimonious data strategy. We built this claim by predicting present day voter intention based solely on likes directed toward posts from political actors. Combining the online and offline, we join a subsample of surveyed respondents to their public Facebook activity and apply machine learning classifiers to explore the link between their political liking behaviour and actual voting intention. Through this work, we show how even a single well-chosen Facebook like, can reveal as much about our political voter intention as hundreds of random likes. Further, by including the entire political like history of the respondents, our model reaches prediction accuracies above previous multiparty studies (60-70%). We conclude the paper by discussing how a parsimonious data strategy applied, with some limitations, allow us to generalize our findings to the 1,4 million Danes with at least one political like and even to other political multiparty systems.	1,0,0,0,0,0
Bias correction in daily maximum and minimum temperature measurements through Gaussian process modeling	The Global Historical Climatology Network-Daily database contains, among other variables, daily maximum and minimum temperatures from weather stations around the globe. It is long known that climatological summary statistics based on daily temperature minima and maxima will not be accurate, if the bias due to the time at which the observations were collected is not accounted for. Despite some previous work, to our knowledge, there does not exist a satisfactory solution to this important problem. In this paper, we carefully detail the problem and develop a novel approach to address it. Our idea is to impute the hourly temperatures at the location of the measurements by borrowing information from the nearby stations that record hourly temperatures, which then can be used to create accurate summaries of temperature extremes. The key difficulty is that these imputations of the temperature curves must satisfy the constraint of falling between the observed daily minima and maxima, and attaining those values at least once in a twenty-four hour period. We develop a spatiotemporal Gaussian process model for imputing the hourly measurements from the nearby stations, and then develop a novel and easy to implement Markov Chain Monte Carlo technique to sample from the posterior distribution satisfying the above constraints. We validate our imputation model using hourly temperature data from four meteorological stations in Iowa, of which one is hidden and the data replaced with daily minima and maxima, and show that the imputed temperatures recover the hidden temperatures well. We also demonstrate that our model can exploit information contained in the data to infer the time of daily measurements.	0,0,0,1,0,0
Fidelity Lower Bounds for Stabilizer and CSS Quantum Codes	In this paper we estimate the fidelity of stabilizer and CSS codes. First, we derive a lower bound on the fidelity of a stabilizer code via its quantum enumerator. Next, we find the average quantum enumerators of the ensembles of finite length stabilizer and CSS codes. We use the average quantum enumerators for obtaining lower bounds on the average fidelity of these ensembles. We further improve the fidelity bounds by estimating the quantum enumerators of expurgated ensembles of stabilizer and CSS codes. Finally, we derive fidelity bounds in the asymptotic regime when the code length tends to infinity. These results tell us which code rate we can afford for achieving a target fidelity with codes of a given length. The results also show that in symmetric depolarizing channel a typical stabilizer code has better performance, in terms of fidelity and code rate, compared with a typical CSS codes, and that balanced CSS codes significantly outperform other CSS codes. Asymptotic results demonstrate that CSS codes have a fundamental performance loss compared to stabilizer codes.	1,0,0,0,0,0
On finite determinacy of complete intersection singularities	We give an elementary combinatorial proof of the following fact: Every real or complex analytic complete intersection germ X is equisingular -- in the sense of the Hilbert-Samuel function -- with a germ of an algebraic set defined by sufficiently long truncations of the defining equations of X.	0,0,1,0,0,0
Semi-Semantic Line-Cluster Assisted Monocular SLAM for Indoor Environments	This paper presents a novel method to reduce the scale drift for indoor monocular simultaneous localization and mapping (SLAM). We leverage the prior knowledge that in the indoor environment, the line segments form tight clusters, e.g. many door frames in a straight corridor are of the same shape, size and orientation, so the same edges of these door frames form a tight line segment cluster. We implement our method in the popular ORB-SLAM2, which also serves as our baseline. In the front end we detect the line segments in each frame and incrementally cluster them in the 3D space. In the back end, we optimize the map imposing the constraint that the line segments of the same cluster should be the same. Experimental results show that our proposed method successfully reduces the scale drift for indoor monocular SLAM.	1,0,0,0,0,0
Spin pumping into superconductors: A new probe of spin dynamics in a superconducting thin film	Spin pumping refers to the microwave-driven spin current injection from a ferromagnet into the adjacent target material. We theoretically investigate the spin pumping into superconductors by fully taking account of impurity spin-orbit scattering that is indispensable to describe diffusive spin transport with finite spin diffusion length. We calculate temperature dependence of the spin pumping signal and show that a pronounced coherence peak appears immediately below the superconducting transition temperature Tc, which survives even in the presence of the spin-orbit scattering. The phenomenon provides us with a new way of studying the dynamic spin susceptibility in a superconducting thin film. This is contrasted with the nuclear magnetic resonance technique used to study a bulk superconductor.	0,1,0,0,0,0
Model-free prediction of noisy chaotic time series by deep learning	We present a deep neural network for a model-free prediction of a chaotic dynamical system from noisy observations. The proposed deep learning model aims to predict the conditional probability distribution of a state variable. The Long Short-Term Memory network (LSTM) is employed to model the nonlinear dynamics and a softmax layer is used to approximate a probability distribution. The LSTM model is trained by minimizing a regularized cross-entropy function. The LSTM model is validated against delay-time chaotic dynamical systems, Mackey-Glass and Ikeda equations. It is shown that the present LSTM makes a good prediction of the nonlinear dynamics by effectively filtering out the noise. It is found that the prediction uncertainty of a multiple-step forecast of the LSTM model is not a monotonic function of time; the predicted standard deviation may increase or decrease dynamically in time.	1,1,0,0,0,0
Adjusting for bias introduced by instrumental variable estimation in the Cox Proportional Hazards Model	Instrumental variable (IV) methods are widely used for estimating average treatment effects in the presence of unmeasured confounders. However, the capability of existing IV procedures, and most notably the two-stage residual inclusion (2SRI) procedure recommended for use in nonlinear contexts, to account for unmeasured confounders in the Cox proportional hazard model is unclear. We show that instrumenting an endogenous treatment induces an unmeasured covariate, referred to as an individual frailty in survival analysis parlance, which if not accounted for leads to bias. We propose a new procedure that augments 2SRI with an individual frailty and prove that it is consistent under certain conditions. The finite sample-size behavior is studied across a broad set of conditions via Monte Carlo simulations. Finally, the proposed methodology is used to estimate the average effect of carotid endarterectomy versus carotid artery stenting on the mortality of patients suffering from carotid artery disease. Results suggest that the 2SRI-frailty estimator generally reduces the bias of both point and interval estimators compared to traditional 2SRI.	0,0,0,1,0,0
On the robustness of the H$β$ Lick index as a cosmic clock in passive early-type galaxies	We examine the H$\beta$ Lick index in a sample of $\sim 24000$ massive ($\rm log(M/M_{\odot})>10.75$) and passive early-type galaxies extracted from SDSS at z<0.3, in order to assess the reliability of this index to constrain the epoch of formation and age evolution of these systems. We further investigate the possibility of exploiting this index as "cosmic chronometer", i.e. to derive the Hubble parameter from its differential evolution with redshift, hence constraining cosmological models independently of other probes. We find that the H$\beta$ strength increases with redshift as expected in passive evolution models, and shows at each redshift weaker values in more massive galaxies. However, a detailed comparison of the observed index with the predictions of stellar population synthesis models highlights a significant tension, with the observed index being systematically lower than expected. By analyzing the stacked spectra, we find a weak [NII]$\lambda6584$ emission line (not detectable in the single spectra) which anti-correlates with the mass, that can be interpreted as a hint of the presence of ionized gas. We estimated the correction of the H$\beta$ index by the residual emission component exploiting different approaches, but find it very uncertain and model-dependent. We conclude that, while the qualitative trends of the observed H$\beta$-z relations are consistent with the expected passive and downsizing scenario, the possible presence of ionized gas even in the most massive and passive galaxies prevents to use this index for a quantitative estimate of the age evolution and for cosmological applications.	0,1,0,0,0,0
From optimal transport to generative modeling: the VEGAN cookbook	We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution $P_X$ and the latent variable model distribution $P_G$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from $P_X$ and $P_G$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.	0,0,0,1,0,0
Solving SDPs for synchronization and MaxCut problems via the Grothendieck inequality	A number of statistical estimation problems can be addressed by semidefinite programs (SDP). While SDPs are solvable in polynomial time using interior point methods, in practice generic SDP solvers do not scale well to high-dimensional problems. In order to cope with this problem, Burer and Monteiro proposed a non-convex rank-constrained formulation, which has good performance in practice but is still poorly understood theoretically. In this paper we study the rank-constrained version of SDPs arising in MaxCut and in synchronization problems. We establish a Grothendieck-type inequality that proves that all the local maxima and dangerous saddle points are within a small multiplicative gap from the global maximum. We use this structural information to prove that SDPs can be solved within a known accuracy, by applying the Riemannian trust-region method to this non-convex problem, while constraining the rank to be of order one. For the MaxCut problem, our inequality implies that any local maximizer of the rank-constrained SDP provides a $ (1 - 1/(k-1)) \times 0.878$ approximation of the MaxCut, when the rank is fixed to $k$. We then apply our results to data matrices generated according to the Gaussian ${\mathbb Z}_2$ synchronization problem, and the two-groups stochastic block model with large bounded degree. We prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information-theoretically optimal methods.	0,0,1,1,0,0
Fast Switching Dual Fabry-Perot-Cavity-based Optical Refractometry for Assessment of Gas Refractivity and Density - Estimates of Its Precision, Accuracy, and Temperature Dependence	Dual Fabry-Perot-Cavity-based Optical Refractometry (DFCB-OR) have been shown to have excellent potential for characterization of gases, in particular their refractivity and density. However, its performance has in practice been found to be limited by drifts. To remedy this, drift-free DFPC-OR (DF-DFCB-OR) has recently been proposed. Suggested methodologies for realization of a specific type of DF-DFCB-OR, termed Fast Switching DFCB-OR (FS-DFCB-OR), have been presented in an accompanying work. This paper scrutinizes the performance and the limitations of both DF- and FS-DFCB-OR for assessments of refractivity and gas density, in particular their precision, accuracy, and temperature dependence. It is shown that both refractivity and gas density can be assessed by FS-DFCB-OR with a precision in the 10$^{-9}$ range under STP conditions. It is demonstrated that the absolute accuracy is mainly limited by the accuracy by which the instantaneous deformation of the cavity or the higher order virial coefficients can be assessed. It is also shown that the internal accuracy, i.e. the accuracy by which the system can be characterized with respect to an internal standard, can be several orders of magnitude better than the absolute. It is concluded that the temperature dependence of FS-DFCB-OR is exceptionally small, typically in the 10$^{-8}$ to 10$^{-7}$/C range, and primarily caused by thermal expansion of the FPC-spacer material. Finally, this paper discusses means on how to design a FS-DFCB-or system for optimal performance and epitomizes the conclusions of this and our accompanying works regarding both DF- and FS-DFCB-OR in terms of performance and provides an outlook for both techniques. Our works can serve as a basis for future realizations of instrumentation for assessments of gas refractivity and density that can fully benefit from the extraordinary potential of FPC-OR.	0,1,0,0,0,0
When do we have the power to detect biological interactions in spatial point patterns?	Determining the relative importance of environmental factors, biotic interactions and stochasticity in assembling and maintaining species-rich communities remains a major challenge in ecology. In plant communities, interactions between individuals of different species are expected to leave a spatial signature in the form of positive or negative spatial correlations over distances relating to the spatial scale of interaction. Most studies using spatial point process tools have found relatively little evidence for interactions between pairs of species. More interactions tend to be detected in communities with fewer species. However, there is currently no understanding of how the power to detect spatial interactions may change with sample size, or the scale and intensity of interactions. We use a simple 2-species model where the scale and intensity of interactions are controlled to simulate point pattern data. In combination with an approximation to the variance of the spatial summary statistics that we sample, we investigate the power of current spatial point pattern methods to correctly reject the null model of bivariate species independence. We show that the power to detect interactions is positively related to the abundances of the species tested, and the intensity and scale of interactions. Increasing imbalance in abundances has a negative effect on the power to detect interactions. At population sizes typically found in currently available datasets for species-rich plant communities we find only a very low power to detect interactions. Differences in power may explain the increased frequency of interactions in communities with fewer species. Furthermore, the community-wide frequency of detected interactions is very sensitive to a minimum abundance criterion for including species in the analyses.	0,0,0,0,1,0
Phase diagrams of Bose-Hubbard model and antiferromagnetic spin-1/2 models on a honeycomb lattice	Motivated by the recent experimental realization of the Haldane model by ultracold fermions in an optical lattice, we investigate phase diagrams of the hard-core Bose-Hubbard model on a honeycomb lattice. This model is closely related with a spin-1/2 antiferromagnetic (AF) quantum spin model. Nearest-neighbor (NN) hopping amplitude is positive and it prefers an AF configurations of phases of Bose-Einstein condensates. On the other hand, an amplitude of the next-NN hopping depends on an angle variable as in the Haldane model. Phase diagrams are obtained by means of an extended path-integral Monte-Carlo simulations. Besides the AF state, a 120$^o$-order state, there appear other phases including a Bose metal in which no long-range orders exist.	0,1,0,0,0,0
Asymptotic properties of a componentwise ARH(1) plug-in predictor	This paper presents new results on prediction of linear processes in function spaces. The autoregressive Hilbertian process framework of order one (ARH(1) process framework) is adopted. A componentwise estimator of the autocorrelation operator is formulated, from the moment-based estimation of its diagonal coefficients, with respect to the orthogonal eigenvectors of the auto-covariance operator, which are assumed to be known. Mean-square convergence to the theoretical autocorrelation operator, in the space of Hilbert-Schmidt operators, is proved. Consistency then follows in that space. For the associated ARH(1) plug-in predictor, mean absolute convergence to the corresponding conditional expectation, in the considered Hilbert space, is obtained. Hence, consistency in that space also holds. A simulation study is undertaken to illustrate the finite-large sample behavior of the formulated componentwise estimator and predictor. The performance of the presented approach is compared with alternative approaches in the previous and current ARH(1) framework literature, including the case of unknown eigenvectors.	0,0,1,1,0,0
A Proof of the Herschel-Maxwell Theorem Using the Strong Law of Large Numbers	In this article, we use the strong law of large numbers to give a proof of the Herschel-Maxwell theorem, which characterizes the normal distribution as the distribution of the components of a spherically symmetric random vector, provided they are independent. We present shorter proofs under additional moment assumptions, and include a remark, which leads to another strikingly short proof of Maxwell's characterization using the central limit theorem.	0,0,1,0,0,0
MSO+nabla is undecidable	This paper is about an extension of monadic second-order logic over infinite trees, which adds a quantifier that says "the set of branches \pi which satisfy a formula \phi(\pi) has probability one". This logic was introduced by Michalewski and Mio; we call it MSO+nabla following Shelah and Lehmann. The logic MSO+nabla subsumes many qualitative probabilistic formalisms, including qualitative probabilistic CTL, probabilistic LTL, or parity tree automata with probabilistic acceptance conditions. We consider the decision problem: decide if a sentence of MSO+nabla is true in the infinite binary tree? For sentences from the weak variant of this logic (set quantifiers range only over finite sets) the problem was known to be decidable, but the question for the full logic remained open. In this paper we show that the problem for the full logic MSO+nabla is undecidable.	1,0,0,0,0,0
An Exploratory Study of Field Failures	Field failures, that is, failures caused by faults that escape the testing phase leading to failures in the field, are unavoidable. Improving verification and validation activities before deployment can identify and timely remove many but not all faults, and users may still experience a number of annoying problems while using their software systems. This paper investigates the nature of field failures, to understand to what extent further improving in-house verification and validation activities can reduce the number of failures in the field, and frames the need of new approaches that operate in the field. We report the results of the analysis of the bug reports of five applications belonging to three different ecosystems, propose a taxonomy of field failures, and discuss the reasons why failures belonging to the identified classes cannot be detected at design time but shall be addressed at runtime. We observe that many faults (70%) are intrinsically hard to detect at design-time.	1,0,0,0,0,0
Möbius topological superconductivity in UPt$_3$	Intensive studies for more than three decades have elucidated multiple superconducting phases and odd-parity Cooper pairs in a heavy fermion superconductor UPt$_3$. We identify a time-reversal invariant superconducting phase of UPt$_3$ as a recently proposed topological nonsymmorphic superconductivity. Combining the band structure of UPt$_3$, order parameter of $E_{\rm 2u}$ representation allowed by $P6_3/mmc$ space group symmetry, and topological classification by $K$-theory, we demonstrate the nontrivial $Z_2$-invariant of three-dimensional DIII class enriched by glide symmetry. Correspondingly, double Majorana cone surface states appear at the surface Brillouin zone boundary. Furthermore, we show a variety of surface states and clarify the topological protection by crystal symmetry. Majorana arcs corresponding to tunable Weyl points appear in the time-reversal symmetry broken B-phase. Majorana cone protected by mirror Chern number and Majorana flat band by glide-winding number are also revealed.	0,1,0,0,0,0
Trapping and displacement of liquid collars and plugs in rough-walled tubes	A liquid film wetting the interior of a long circular cylinder redistributes under the action of surface tension to form annular collars or occlusive plugs. These equilibrium structures are invariant under axial translation within a perfectly smooth uniform tube and therefore can be displaced axially by very weak external forcing. We consider how this degeneracy is disrupted when the tube wall is rough, and determine threshold conditions under which collars or plugs resist displacement under forcing. Wall roughness is modelled as a non-axisymmetric Gaussian random field of prescribed correlation length and small variance, mimicking some of the geometric irregularities inherent in applications such as lung airways. The thin film coating this surface is modelled using lubrication theory. When the roughness is weak, we show how the locations of equilibrium collars and plugs can be identified in terms of the azimuthally averaged tube radius; we derive conditions specifying equilibrium collar locations under an externally imposed shear flow, and plug locations under an imposed pressure gradient. We use these results to determine the probability of external forcing being sufficient to displace a collar or plug from a rough-walled tube, when the tube roughness is defined only in statistical terms.	0,1,0,0,0,0
Estimating a network from multiple noisy realizations	Complex interactions between entities are often represented as edges in a network. In practice, the network is often constructed from noisy measurements and inevitably contains some errors. In this paper we consider the problem of estimating a network from multiple noisy observations where edges of the original network are recorded with both false positives and false negatives. This problem is motivated by neuroimaging applications where brain networks of a group of patients with a particular brain condition could be viewed as noisy versions of an unobserved true network corresponding to the disease. The key to optimally leveraging these multiple observations is to take advantage of network structure, and here we focus on the case where the true network contains communities. Communities are common in real networks in general and in particular are believed to be presented in brain networks. Under a community structure assumption on the truth, we derive an efficient method to estimate the noise levels and the original network, with theoretical guarantees on the convergence of our estimates. We show on synthetic networks that the performance of our method is close to an oracle method using the true parameter values, and apply our method to fMRI brain data, demonstrating that it constructs stable and plausible estimates of the population network.	0,0,1,1,0,0
A Trio Neural Model for Dynamic Entity Relatedness Ranking	Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.	0,0,0,1,0,0
Improving text classification with vectors of reduced precision	This paper presents the analysis of the impact of a floating-point number precision reduction on the quality of text classification. The precision reduction of the vectors representing the data (e.g. TF-IDF representation in our case) allows for a decrease of computing time and memory footprint on dedicated hardware platforms. The impact of precision reduction on the classification quality was performed on 5 corpora, using 4 different classifiers. Also, dimensionality reduction was taken into account. Results indicate that the precision reduction improves classification accuracy for most cases (up to 25% of error reduction). In general, the reduction from 64 to 4 bits gives the best scores and ensures that the results will not be worse than with the full floating-point representation.	1,0,0,0,0,0
From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter	We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614 (2016)] where a functional for Fermi systems with anomalously large $s$-wave scattering length $a_s$ was proposed that has no free parameters. The functional is designed to correctly reproduce the unitary limit in Fermi gases together with the leading-order contributions in the s- and p-wave channels at low density. The functional is shown to be predictive up to densities $\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional retained in this work is further motivated. It is shown that the new functional corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One conclusion from the present work is that, except in the extremely low--density regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with respect to the unitary limit. Starting from the functional, we introduce density--dependent scales and show that scales associated to the bare interaction are strongly renormalized by medium effects. As a consequence, some of the scales at play around saturation are dominated by the unitary gas properties and not directly to low-energy constants. For instance, we show that the scale in the s-wave channel around saturation is proportional to the so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also point out that these scales are of the same order of magnitude than those empirically obtained in the Skyrme energy density functional. We finally propose a slight modification of the functional such that it becomes accurate up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$.	0,1,0,0,0,0
Improving pairwise comparison models using Empirical Bayes shrinkage	Comparison data arises in many important contexts, e.g. shopping, web clicks, or sports competitions. Typically we are given a dataset of comparisons and wish to train a model to make predictions about the outcome of unseen comparisons. In many cases available datasets have relatively few comparisons (e.g. there are only so many NFL games per year) or efficiency is important (e.g. we want to quickly estimate the relative appeal of a product). In such settings it is well known that shrinkage estimators outperform maximum likelihood estimators. A complicating matter is that standard comparison models such as the conditional multinomial logit model are only models of conditional outcomes (who wins) and not of comparisons themselves (who competes). As such, different models of the comparison process lead to different shrinkage estimators. In this work we derive a collection of methods for estimating the pairwise uncertainty of pairwise predictions based on different assumptions about the comparison process. These uncertainty estimates allow us both to examine model uncertainty as well as perform Empirical Bayes shrinkage estimation of the model parameters. We demonstrate that our shrunk estimators outperform standard maximum likelihood methods on real comparison data from online comparison surveys as well as from several sports contexts.	1,0,0,1,0,0
Automated Synthesis of Divide and Conquer Parallelism	This paper focuses on automated synthesis of divide-and-conquer parallelism, which is a common parallel programming skeleton supported by many cross-platform multithreaded libraries. The challenges of producing (manually or automatically) a correct divide-and-conquer parallel program from a given sequential code are two-fold: (1) assuming that individual worker threads execute a code identical to the sequential code, the programmer has to provide the extra code for dividing the tasks and combining the computation results, and (2) sometimes, the sequential code may not be usable as is, and may need to be modified by the programmer. We address both challenges in this paper. We present an automated synthesis technique for the case where no modifications to the sequential code are required, and we propose an algorithm for modifying the sequential code to make it suitable for parallelization when some modification is necessary. The paper presents theoretical results for when this {\em modification} is efficiently possible, and experimental evaluation of the technique and the quality of the produced parallel programs.	1,0,0,0,0,0
Biaxial magnetic field setup for angular magnetic measurements of thin films and spintronic nanodevices	The biaxial magnetic-field setup for angular magnetic measurements of thin film and spintronic devices is designed and presented. The setup allows for application of the in-plane magnetic field using a quadrupole electromagnet, controlled by power supply units and integrated with an electromagnet biaxial magnetic field sensor. In addition, the probe station is equipped with a microwave circuitry, which enables angle-resolved spin torque oscillation measurements. The angular dependencies of magnetoresistance and spin diode effect in a giant magnetoresistance strip are shown as an operational verification of the experimental setup. We adapted an analytical macrospin model to reproduce both the resistance and spin-diode angular dependency measurements.	0,1,0,0,0,0
Rate Optimal Binary Linear Locally Repairable Codes with Small Availability	A locally repairable code with availability has the property that every code symbol can be recovered from multiple, disjoint subsets of other symbols of small size. In particular, a code symbol is said to have $(r,t)$-availability if it can be recovered from $t$ disjoint subsets, each of size at most $r$. A code with availability is said to be 'rate-optimal', if its rate is maximum among the class of codes with given locality, availability, and alphabet size. This paper focuses on rate-optimal binary, linear codes with small availability, and makes four contributions. First, it establishes tight upper bounds on the rate of binary linear codes with $(r,2)$ and $(2,3)$ availability. Second, it establishes a uniqueness result for binary rate-optimal codes, showing that for certain classes of binary linear codes with $(r,2)$ and $(2,3)$-availability, any rate optimal code must be a direct sum of shorter rate optimal codes. Third, it presents novel upper bounds on the rates of binary linear codes with $(2,t)$ and $(r,3)$-availability. In particular, the main contribution here is a new method for bounding the number of cosets of the dual of a code with availability, using its covering properties. Finally, it presents a class of locally repairable linear codes associated with convex polyhedra, focusing on the codes associated with the Platonic solids. It demonstrates that these codes are locally repairable with $t = 2$, and that the codes associated with (geometric) dual polyhedra are (coding theoretic) duals of each other.	1,0,1,0,0,0
A New Backpressure Algorithm for Joint Rate Control and Routing with Vanishing Utility Optimality Gaps and Finite Queue Lengths	The backpressure algorithm has been widely used as a distributed solution to the problem of joint rate control and routing in multi-hop data networks. By controlling a parameter $V$ in the algorithm, the backpressure algorithm can achieve an arbitrarily small utility optimality gap. However, this in turn brings in a large queue length at each node and hence causes large network delay. This phenomenon is known as the fundamental utility-delay tradeoff. The best known utility-delay tradeoff for general networks is $[O(1/V), O(V)]$ and is attained by a backpressure algorithm based on a drift-plus-penalty technique. This may suggest that to achieve an arbitrarily small utility optimality gap, the existing backpressure algorithms necessarily yield an arbitrarily large queue length. However, this paper proposes a new backpressure algorithm that has a vanishing utility optimality gap, so utility converges to exact optimality as the algorithm keeps running, while queue lengths are bounded throughout by a finite constant. The technique uses backpressure and drift concepts with a new method for convex programming.	1,0,1,0,0,0
Two weight Commutators in the Dirichlet and Neumann Laplacian settings	In this paper we establish the characterization of the weighted BMO via two weight commutators in the settings of the Neumann Laplacian $\Delta_{N_+}$ on the upper half space $\mathbb{R}^n_+$ and the reflection Neumann Laplacian $\Delta_N$ on $\mathbb{R}^n$ with respect to the weights associated to $\Delta_{N_+}$ and $\Delta_{N}$ respectively. This in turn yields a weak factorization for the corresponding weighted Hardy spaces, where in particular, the weighted class associated to $\Delta_{N}$ is strictly larger than the Muckenhoupt weighted class and contains non-doubling weights. In our study, we also make contributions to the classical Muckenhoupt--Wheeden weighted Hardy space (BMO space respectively) by showing that it can be characterized via area function (Carleson measure respectively) involving the semigroup generated by the Laplacian on $\mathbb{R}^n$ and that the duality of these weighted Hardy and BMO spaces holds for Muckenhoupt $A^p$ weights with $p\in (1,2]$ while the previously known related results cover only $p\in (1,{n+1\over n}]$. We also point out that this two weight commutator theorem might not be true in the setting of general operators $L$, and in particular we show that it is not true when $L$ is the Dirichlet Laplacian $\Delta_{D_+}$ on $\mathbb{R}^n_+$.	0,0,1,0,0,0
Virtual refinements of the Vafa-Witten formula	We conjecture a formula for the generating function of virtual $\chi_y$-genera of moduli spaces of rank 2 sheaves on arbitrary surfaces with holomorphic 2-form. Specializing the conjecture to minimal surfaces of general type and to virtual Euler characteristics, we recover (part of) a formula of C. Vafa and E. Witten. These virtual $\chi_y$-genera can be written in terms of descendent Donaldson invariants. Using T. Mochizuki's formula, the latter can be expressed in terms of Seiberg-Witten invariants and certain explicit integrals over Hilbert schemes of points. These integrals are governed by seven universal functions, which are determined by their values on $\mathbb{P}^2$ and $\mathbb{P}^1 \times \mathbb{P}^1$. Using localization we calculate these functions up to some order, which allows us to check our conjecture in many cases. In an appendix by H. Nakajima and the first named author, the virtual Euler characteristic specialization of our conjecture is extended to include $\mu$-classes, thereby interpolating between Vafa-Witten's formula and Witten's conjecture for Donaldson invariants.	0,0,1,0,0,0
Scale out for large minibatch SGD: Residual network training on ImageNet-1K with improved accuracy and reduced time to train	For the past 5 years, the ILSVRC competition and the ImageNet dataset have attracted a lot of interest from the Computer Vision community, allowing for state-of-the-art accuracy to grow tremendously. This should be credited to the use of deep artificial neural network designs. As these became more complex, the storage, bandwidth, and compute requirements increased. This means that with a non-distributed approach, even when using the most high-density server available, the training process may take weeks, making it prohibitive. Furthermore, as datasets grow, the representation learning potential of deep networks grows as well by using more complex models. This synchronicity triggers a sharp increase in the computational requirements and motivates us to explore the scaling behaviour on petaflop scale supercomputers. In this paper we will describe the challenges and novel solutions needed in order to train ResNet-50 in this large scale environment. We demonstrate above 90\% scaling efficiency and a training time of 28 minutes using up to 104K x86 cores. This is supported by software tools from Intel's ecosystem. Moreover, we show that with regular 90 - 120 epoch train runs we can achieve a top-1 accuracy as high as 77\% for the unmodified ResNet-50 topology. We also introduce the novel Collapsed Ensemble (CE) technique that allows us to obtain a 77.5\% top-1 accuracy, similar to that of a ResNet-152, while training a unmodified ResNet-50 topology for the same fixed training budget. All ResNet-50 models as well as the scripts needed to replicate them will be posted shortly.	1,0,0,1,0,0
On the Limitation of Convolutional Neural Networks in Recognizing Negative Images	Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance on a variety of computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. In this paper, we examine whether CNNs are capable of learning the semantics of training data. To this end, we evaluate CNNs on negative images, since they share the same structure and semantics as regular images and humans can classify them correctly. Our experimental results indicate that when training on regular images and testing on negative images, the model accuracy is significantly lower than when it is tested on regular images. This leads us to the conjecture that current training methods do not effectively train models to generalize the concepts. We then introduce the notion of semantic adversarial examples - transformed inputs that semantically represent the same objects, but the model does not classify them correctly - and present negative images as one class of such inputs.	1,0,0,1,0,0
Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks	Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use "clean-labels"; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a $\textit{specific}$ test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot. We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a "watermarking" strategy that makes poisoning reliable using multiple ($\approx$50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.	0,0,0,1,0,0
A Composition Theorem for Randomized Query Complexity	Let the randomized query complexity of a relation for error probability $\epsilon$ be denoted by $R_\epsilon(\cdot)$. We prove that for any relation $f \subseteq \{0,1\}^n \times \mathcal{R}$ and Boolean function $g:\{0,1\}^m \rightarrow \{0,1\}$, $R_{1/3}(f\circ g^n) = \Omega(R_{4/9}(f)\cdot R_{1/2-1/n^4}(g))$, where $f \circ g^n$ is the relation obtained by composing $f$ and $g$. We also show that $R_{1/3}\left(f \circ \left(g^\oplus_{O(\log n)}\right)^n\right)=\Omega(\log n \cdot R_{4/9}(f) \cdot R_{1/3}(g))$, where $g^\oplus_{O(\log n)}$ is the function obtained by composing the xor function on $O(\log n)$ bits and $g^t$.	1,0,0,0,0,0
Using of heterogeneous corpora for training of an ASR system	The paper summarizes the development of the LVCSR system built as a part of the Pashto speech-translation system at the SCALE (Summer Camp for Applied Language Exploration) 2015 workshop on "Speech-to-text-translation for low-resource languages". The Pashto language was chosen as a good "proxy" low-resource language, exhibiting multiple phenomena which make the speech-recognition and and speech-to-text-translation systems development hard. Even when the amount of data is seemingly sufficient, given the fact that the data originates from multiple sources, the preliminary experiments reveal that there is little to no benefit in merging (concatenating) the corpora and more elaborate ways of making use of all of the data must be worked out. This paper concentrates only on the LVCSR part and presents a range of different techniques that were found to be useful in order to benefit from multiple different corpora	1,0,0,0,0,0
Bifurcation structure of cavity soliton dynamics in a VCSEL with saturable absorber and time-delayed feedback	We consider a wide-aperture surface-emitting laser with a saturable absorber section subjected to time-delayed feedback. We adopt the mean-field approach assuming a single longitudinal mode operation of the solitary VCSEL. We investigate cavity soliton dynamics under the effect of time- delayed feedback in a self-imaging configuration where diffraction in the external cavity is negligible. Using bifurcation analysis, direct numerical simulations and numerical path continuation methods, we identify the possible bifurcations and map them in a plane of feedback parameters. We show that for both the homogeneous and localized stationary lasing solutions in one spatial dimension the time-delayed feedback induces complex spatiotemporal dynamics, in particular a period doubling route to chaos, quasiperiodic oscillations and multistability of the stationary solutions.	0,1,0,0,0,0
Multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory	Learning and memory are intertwined in our brain and their relationship is at the core of several recent neural network models. In particular, the Attention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning network with an emphasis on biological plausibility of memory dynamics and learning. We find that the AuGMEnT network does not solve some hierarchical tasks, where higher-level stimuli have to be maintained over a long time, while lower-level stimuli need to be remembered and forgotten over a shorter timescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky or short-timescale and non-leaky or long-timescale units in memory, that allow to exchange lower-level information while maintaining higher-level one, thus solving both hierarchical and distractor tasks.	1,0,0,1,0,0
Contextually Customized Video Summaries via Natural Language	The best summary of a long video differs among different people due to its highly subjective nature. Even for the same person, the best summary may change with time or mood. In this paper, we introduce the task of generating customized video summaries through simple text. First, we train a deep architecture to effectively learn semantic embeddings of video frames by leveraging the abundance of image-caption data via a progressive and residual manner. Given a user-specific text description, our algorithm is able to select semantically relevant video segments and produce a temporally aligned video summary. In order to evaluate our textually customized video summaries, we conduct experimental comparison with baseline methods that utilize ground-truth information. Despite the challenging baselines, our method still manages to show comparable or even exceeding performance. We also show that our method is able to generate semantically diverse video summaries by only utilizing the learned visual embeddings.	1,0,0,0,0,0
Sine wave gating Silicon single-photon detectors for multiphoton entanglement experiments	Silicon single-photon detectors (SPDs) are the key devices for detecting single photons in the visible wavelength range. Here we present high detection efficiency silicon SPDs dedicated to the generation of multiphoton entanglement based on the technique of high-frequency sine wave gating. The silicon single-photon avalanche diodes (SPADs) components are acquired by disassembling 6 commercial single-photon counting modules (SPCMs). Using the new quenching electronics, the average detection efficiency of SPDs is increased from 68.6% to 73.1% at a wavelength of 785 nm. These sine wave gating SPDs are then applied in a four-photon entanglement experiment, and the four-fold coincidence count rate is increased by 30% without degrading its visibility compared with the original SPCMs.	0,1,0,0,0,0
Doubled Khovanov Homology	We define a homology theory of virtual links built out of the direct sum of the standard Khovanov complex with itself, motivating the name doubled Khovanov homology. We demonstrate that it can be used to show that some virtual links are non-classical, and that it yields a condition on a virtual knot being the connect sum of two unknots. Further, we show that doubled Khovanov homology possesses a perturbation analogous to that defined by Lee in the classical case and define a doubled Rasmussen invariant. This invariant is used to obtain various cobordism obstructions; in particular it is an obstruction to sliceness. Finally, we show that the doubled Rasmussen invariant contains the odd writhe of a virtual knot, and use this to show that knots with non-zero odd writhe are not slice.	0,0,1,0,0,0
Change of grading, injective dimension and dualizing complexes	Let $G,H$ be groups, $\phi: G \rightarrow H$ a group morphism, and $A$ a $G$-graded algebra. The morphism $\phi$ induces an $H$-grading on $A$, and on any $G$-graded $A$-module, which thus becomes an $H$-graded $A$-module. Given an injective $G$-graded $A$-module, we give bounds for its injective dimension when seen as $H$-graded $A$-module. Following ideas by Van den Bergh, we give an application of our results to the stability of dualizing complexes through change of grading.	0,0,1,0,0,0
Open data, open review and open dialogue in making social sciences plausible	Nowadays, protecting trust in social sciences also means engaging in open community dialogue, which helps to safeguard robustness and improve efficiency of research methods. The combination of open data, open review and open dialogue may sound simple but implementation in the real world will not be straightforward. However, in view of Begley and Ellis's (2012) statement that, "the scientific process demands the highest standards of quality, ethics and rigour," they are worth implementing. More importantly, they are feasible to work on and likely will help to restore plausibility to social sciences research. Therefore, I feel it likely that the triplet of open data, open review and open dialogue will gradually emerge to become policy requirements regardless of the research funding source.	0,0,0,1,0,0
Targeted Learning with Daily EHR Data	Electronic health records (EHR) data provide a cost and time-effective opportunity to conduct cohort studies of the effects of multiple time-point interventions in the diverse patient population found in real-world clinical settings. Because the computational cost of analyzing EHR data at daily (or more granular) scale can be quite high, a pragmatic approach has been to partition the follow-up into coarser intervals of pre-specified length. Current guidelines suggest employing a 'small' interval, but the feasibility and practical impact of this recommendation has not been evaluated and no formal methodology to inform this choice has been developed. We start filling these gaps by leveraging large-scale EHR data from a diabetes study to develop and illustrate a fast and scalable targeted learning approach that allows to follow the current recommendation and study its practical impact on inference. More specifically, we map daily EHR data into four analytic datasets using 90, 30, 15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation approach, the longitudinal TMLE, to estimate the causal effects of four dynamic treatment rules with each dataset, and compare the resulting inferences. To overcome the computational challenges presented by the size of these data, we propose a novel TMLE implementation, the 'long-format TMLE', and rely on the latest advances in scalable data-adaptive machine-learning software, xgboost and h2o, for estimation of the TMLE nuisance parameters.	0,0,0,1,0,0
Semantic Instance Segmentation with a Discriminative Loss Function	Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a representation of the image that can easily be clustered into instances with a simple post-processing step. The loss function encourages the network to map each pixel to a point in feature space so that pixels belonging to the same instance lie close together while different instances are separated by a wide margin. Our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and distinct from recent efforts in instance segmentation. In contrast to previous works, our method does not rely on object proposals or recurrent mechanisms. A key contribution of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on par with more complex methods. Moreover, we show that it does not suffer from some of the limitations of the popular detect-and-segment approaches. We achieve competitive performance on the Cityscapes and CVPPP leaf segmentation benchmarks.	1,0,0,0,0,0
Individualized Risk Prognosis for Critical Care Patients: A Multi-task Gaussian Process Model	We report the development and validation of a data-driven real-time risk score that provides timely assessments for the clinical acuity of ward patients based on their temporal lab tests and vital signs, which allows for timely intensive care unit (ICU) admissions. Unlike the existing risk scoring technologies, the proposed score is individualized; it uses the electronic health record (EHR) data to cluster the patients based on their static covariates into subcohorts of similar patients, and then learns a separate temporal, non-stationary multi-task Gaussian Process (GP) model that captures the physiology of every subcohort. Experiments conducted on data from a heterogeneous cohort of 6,094 patients admitted to the Ronald Reagan UCLA medical center show that our risk score significantly outperforms the state-of-the-art risk scoring technologies, such as the Rothman index and MEWS, in terms of timeliness, true positive rate (TPR), and positive predictive value (PPV). In particular, the proposed score increases the AUC with 20% and 38% as compared to Rothman index and MEWS respectively, and can predict ICU admissions 8 hours before clinicians at a PPV of 35% and a TPR of 50%. Moreover, we show that the proposed risk score allows for better decisions on when to discharge clinically stable patients from the ward, thereby improving the efficiency of hospital resource utilization.	1,0,0,0,0,0
Quantum chaos in an electron-phonon bad metal	We calculate the scrambling rate $\lambda_L$ and the butterfly velocity $v_B$ associated with the growth of quantum chaos for a solvable large-$N$ electron-phonon system. We study a temperature regime in which the electrical resistivity of this system exceeds the Mott-Ioffe-Regel limit and increases linearly with temperature - a sign that there are no long-lived charged quasiparticles - although the phonons remain well-defined quasiparticles. The long-lived phonons determine $\lambda_L$, rendering it parametrically smaller than the theoretical upper-bound $\lambda_L \ll \lambda_{max}=2\pi T/\hbar$. Significantly, the chaos properties seem to be intrinsic - $\lambda_L$ and $v_B$ are the same for electronic and phononic operators. We consider two models - one in which the phonons are dispersive, and one in which they are dispersionless. In either case, we find that $\lambda_L$ is proportional to the inverse phonon lifetime, and $v_B$ is proportional to the effective phonon velocity. The thermal and chaos diffusion constants, $D_E$ and $D_L\equiv v_B^2/\lambda_L$, are always comparable, $D_E \sim D_L$. In the dispersive phonon case, the charge diffusion constant $D_C$ satisfies $D_L\gg D_C$, while in the dispersionless case $D_L \ll D_C$.	0,1,0,0,0,0
Language Model Pre-training for Hierarchical Document Representations	Hierarchical neural architectures are often used to capture long-distance dependencies and have been applied to many document-level tasks such as summarization, document segmentation, and sentiment analysis. However, effective usage of such a large context can be difficult to learn, especially in the case where there is limited labeled data available. Building on the recent success of language model pretraining methods for learning flat representations of text, we propose algorithms for pre-training hierarchical document representations from unlabeled data. Unlike prior work, which has focused on pre-training contextual token representations or context-independent {sentence/paragraph} representations, our hierarchical document representations include fixed-length sentence/paragraph representations which integrate contextual information from the entire documents. Experiments on document segmentation, document-level question answering, and extractive document summarization demonstrate the effectiveness of the proposed pre-training algorithms.	1,0,0,0,0,0
One-shot and few-shot learning of word embeddings	Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.	1,0,0,1,0,0
A six-factor asset pricing model	The present study introduce the human capital component to the Fama and French five-factor model proposing an equilibrium six-factor asset pricing model. The study employs an aggregate of four sets of portfolios mimicking size and industry with varying dimensions. The first set consists of three set of six portfolios each sorted on size to B/M, size to investment, and size to momentum. The second set comprises of five index portfolios, third, a four-set of twenty-five portfolios each sorted on size to B/M, size to investment, size to profitability, and size to momentum, and the final set constitute thirty industry portfolios. To estimate the parameters of six-factor asset pricing model for the four sets of variant portfolios, we use OLS and Generalized method of moments based robust instrumental variables technique (IVGMM). The results obtained from the relevance, endogeneity, overidentifying restrictions, and the Hausman's specification, tests indicate that the parameter estimates of the six-factor model using IVGMM are robust and performs better than the OLS approach. The human capital component shares equally the predictive power alongside the factors in the framework in explaining the variations in return on portfolios. Furthermore, we assess the t-ratio of the human capital component of each IVGMM estimates of the six-factor asset pricing model for the four sets of variant portfolios. The t-ratio of the human capital of the eighty-three IVGMM estimates are more than 3.00 with reference to the standard proposed by Harvey et al. (2016). This indicates the empirical success of the six-factor asset-pricing model in explaining the variation in asset returns.	0,0,0,0,0,1
Epi-two-dimensional fluid flow: a new topological paradigm for dimensionality	While a variety of fundamental differences are known to separate two-dimensional (2D) and three-dimensional (3D) fluid flows, it is not well understood how they are related. Conventionally, dimensional reduction is justified by an \emph{a priori} geometrical framework; i.e., 2D flows occur under some geometrical constraint such as shallowness. However, deeper inquiry into 3D flow often finds the presence of local 2D-like structures without such a constraint, where 2D-like behavior may be identified by the integrability of vortex lines or vanishing local helicity. Here we propose a new paradigm of flow structure by introducing an intermediate class, termed epi-2-dimensional flow, and thereby build a topological bridge between 2D and 3D flows. The epi-2D property is local, and is preserved in fluid elements obeying ideal (inviscid and barotropic) mechanics; a local epi-2D flow may be regarded as a `particle' carrying a generalized enstrophy as its charge. A finite viscosity may cause `fusion' of two epi-2D particles, generating helicity from their charges giving rise to 3D flow.	0,1,0,0,0,0
Machine Translation in Indian Languages: Challenges and Resolution	English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using pre-ordering and suffix separation. The pre-ordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of pre-ordering and suffix separation helps in improving the quality of English to Indian Language machine translation.	1,0,0,0,0,0
Majorana Spin Liquids, Topology and Superconductivity in Ladders	We theoretically address spin chain analogs of the Kitaev quantum spin model on the honeycomb lattice. The emergent quantum spin liquid phases or Anderson resonating valence bond (RVB) states can be understood, as an effective model, in terms of p-wave superconductivity and Majorana fermions. We derive a generalized phase diagram for the two-leg ladder system with tunable interaction strengths between chains allowing us to vary the shape of the lattice (from square to honeycomb ribbon or brickwall ladder). We evaluate the winding number associated with possible emergent (topological) gapless modes at the edges. In the Az phase, as a result of the emergent Z2 gauge fields and pi-flux ground state, one may build spin-1/2 (loop) qubit operators by analogy to the toric code. In addition, we show how the intermediate gapless B phase evolves in the generalized ladder model. For the brickwall ladder, the $B$ phase is reduced to one line, which is analyzed through perturbation theory in a rung tensor product states representation and bosonization. Finally, we show that doping with a few holes can result in the formation of hole pairs and leads to a mapping with the Su-Schrieffer-Heeger model in polyacetylene; a superconducting-insulating quantum phase transition for these hole pairs is accessible, as well as related topological properties.	0,1,0,0,0,0
Conditional Accelerated Lazy Stochastic Gradient Descent	In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate $O\left(\frac{1}{\varepsilon^2}\right)$ improving over the projection-free, Online Frank-Wolfe based stochastic gradient descent of Hazan and Kale [2012] with convergence rate $O\left(\frac{1}{\varepsilon^4}\right)$.	1,0,0,1,0,0
Adversarial Training for Disease Prediction from Electronic Health Records with Missing Data	Electronic health records (EHRs) have contributed to the computerization of patient records and can thus be used not only for efficient and systematic medical services, but also for research on biomedical data science. However, there are many missing values in EHRs when provided in matrix form, which is an important issue in many biomedical EHR applications. In this paper, we propose a two-stage framework that includes missing data imputation and disease prediction to address the missing data problem in EHRs. We compared the disease prediction performance of generative adversarial networks (GANs) and conventional learning algorithms in combination with missing data prediction methods. As a result, we obtained a level of accuracy of 0.9777, sensitivity of 0.9521, specificity of 0.9925, area under the receiver operating characteristic curve (AUC-ROC) of 0.9889, and F-score of 0.9688 with a stacked autoencoder as the missing data prediction method and an auxiliary classifier GAN (AC-GAN) as the disease prediction method. The comparison results show that a combination of a stacked autoencoder and an AC-GAN significantly outperforms other existing approaches. Our results suggest that the proposed framework is more robust for disease prediction from EHRs with missing data.	1,0,0,1,0,0
Multi-Kernel LS-SVM Based Bio-Clinical Data Integration: Applications to Ovarian Cancer	The medical research facilitates to acquire a diverse type of data from the same individual for particular cancer. Recent studies show that utilizing such diverse data results in more accurate predictions. The major challenge faced is how to utilize such diverse data sets in an effective way. In this paper, we introduce a multiple kernel based pipeline for integrative analysis of high-throughput molecular data (somatic mutation, copy number alteration, DNA methylation and mRNA) and clinical data. We apply the pipeline on Ovarian cancer data from TCGA. After multiple kernels have been generated from the weighted sum of individual kernels, it is used to stratify patients and predict clinical outcomes. We examine the survival time, vital status, and neoplasm cancer status of each subtype to verify how well they cluster. We have also examined the power of molecular and clinical data in predicting dichotomized overall survival data and to classify the tumor grade for the cancer samples. It was observed that the integration of various data types yields higher log-rank statistics value. We were also able to predict clinical status with higher accuracy as compared to using individual data types.	0,0,0,1,0,0
Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation	Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to replicate some of these abilities with a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which the agent can move and interact with objects it sees, the agent learns a world model predicting the dynamic consequences of its actions. Simultaneously, the agent learns to take actions that adversarially challenge the developing world model, pushing the agent to explore novel and informative interactions with its environment. We demonstrate that this policy leads to the self-supervised emergence of a spectrum of complex behaviors, including ego motion prediction, object attention, and object gathering. Moreover, the world model that the agent learns supports improved performance on object dynamics prediction and localization tasks. Our results are a proof-of-principle that computational models of intrinsic motivation might account for key features of developmental visuomotor learning in infants.	0,0,0,1,0,0
Strong Consistency of Spectral Clustering for Stochastic Block Models	In this paper we prove the strong consistency of several methods based on the spectral clustering techniques that are widely used to study the community detection problem in stochastic block models (SBMs). We show that under some weak conditions on the minimal degree, the number of communities, and the eigenvalues of the probability block matrix, the K-means algorithm applied to the eigenvectors of the graph Laplacian associated with its first few largest eigenvalues can classify all individuals into the true community uniformly correctly almost surely. Extensions to both regularized spectral clustering and degree-corrected SBMs are also considered. We illustrate the performance of different methods on simulated networks.	0,0,0,1,0,0
Regularity of symbolic powers and Arboricity of matroids	Let $\Delta$ be a simplicial complex of a matroid $M$. In this paper, we explicitly compute the regularity of all the symbolic powers of a Stanley-Reisner ideal $I_\Delta$ in terms of combinatorial data of the matroid $M$. In order to do that, we provide a sharp bound between the arboricity of $M$ and the circumference of its dual $M^*$.	0,0,1,0,0,0
Notes on "Einstein metrics on compact simple Lie groups attached to standard triples"	In the paper "Einstein metrics on compact simple Lie groups attached to standard triples", the authors introduced the definition of standard triples and proved that every compact simple Lie group $G$ attached to a standard triple $(G,K,H)$ admits a left-invariant Einstein metric which is not naturally reductive except the standard triple $(\Sp(4),2\Sp(2),4\Sp(1))$. For the triple $(\Sp(4),2\Sp(2),4\Sp(1))$, we find there exists an involution pair of $\sp(4)$ such that $4\sp(1)$ is the fixed point of the pair, and then give the decomposition of $\sp(4)$ as a direct sum of irreducible $\ad(4\sp(1))$-modules. But $\Sp(4)/4\Sp(1)$ is not a generalized Wallach space. Furthermore we give left-invariant Einstein metrics on $\Sp(4)$ which are non-naturally reductive and $\Ad(4\Sp(1))$-invariant. For the general case $(\Sp(2n_1n_2),2\Sp(n_1n_2),2n_2\Sp(n_1))$, there exist $2n_2-1$ involutions of $\sp(2n_1n_2)$ such that $2n_2\sp(n_1))$ is the fixed point of these $2n_2-1$ involutions, and it follows the decomposition of $\sp(2n_1n_2)$ as a direct sum of irreducible $\ad(2n_2\sp(n_1))$-modules. In order to give new non-naturally reductive and $\Ad(2n_2\Sp(n_1)))$-invariant Einstein metrics on $\Sp(2n_1n_2)$, we prove a general result, i.e. $\Sp(2k+l)$ admits at least two non-naturally reductive Einstein metrics which are $\Ad(\Sp(k)\times\Sp(k)\times\Sp(l))$-invariant if $k<l$. It implies that every compact simple Lie group $\Sp(n)$ for $n\geq 4$ admits at least $2[\frac{n-1}{3}]$ non-naturally reductive left-invariant Einstein metrics.	0,0,1,0,0,0
Topological conjugacy of topological Markov shifts and Ruelle algebras	We will characterize topologically conjugate two-sided topological Markov shifts $(\bar{X}_A,\bar{\sigma}_A)$ in terms of the associated asymptotic Ruelle $C^*$-algebras ${\mathcal{R}}_A$ with its commutative $C^*$-subalgebras $C(\bar{X}_A)$ and the canonical circle actions. We will also show that extended Ruelle algebras ${\widetilde{\mathcal{R}}}_A$, which are purely infinite version of the asymptotic Ruelle algebras, with its commutative $C^*$-subalgebras $C(\bar{X}_A)$ and the canonical torus actions $\gamma^A$ are complete invariants for topological conjugacy of two-sided topological Markov shifts. We then have a computable topological conjugacy invariant, written in terms of the underlying matrix, of a two-sided topological Markov shift by using K-theory of the extended Ruelle algebra. The diagonal action of $\gamma^A$ has a unique KMS-state on ${\widetilde{\mathcal{R}}}_A$, which is an extension of the Parry measure on $\bar{X}_A$.	0,0,1,0,0,0
On Consistency of Graph-based Semi-supervised Learning	Graph-based semi-supervised learning is one of the most popular methods in machine learning. Some of its theoretical properties such as bounds for the generalization error and the convergence of the graph Laplacian regularizer have been studied in computer science and statistics literatures. However, a fundamental statistical property, the consistency of the estimator from this method has not been proved. In this article, we study the consistency problem under a non-parametric framework. We prove the consistency of graph-based learning in the case that the estimated scores are enforced to be equal to the observed responses for the labeled data. The sample sizes of both labeled and unlabeled data are allowed to grow in this result. When the estimated scores are not required to be equal to the observed responses, a tuning parameter is used to balance the loss function and the graph Laplacian regularizer. We give a counterexample demonstrating that the estimator for this case can be inconsistent. The theoretical findings are supported by numerical studies.	0,0,0,1,0,0
Laplace operators on holomorphic Lie algebroids	The paper introduces Laplace-type operators for functions defined on the tangent space of a Finsler Lie algebroid, using a volume form on the prolongation of the algebroid. It also presents the construction of a horizontal Laplace operator for forms defined on the prolongation of the algebroid. All of the Laplace operators considered in the paper are also locally expressed using the Chern-Finsler connection of the algebroid.	0,0,1,0,0,0
Emulating Simulations of Cosmic Dawn for 21cm Power Spectrum Constraints on Cosmology, Reionization, and X-ray Heating	Current and upcoming radio interferometric experiments are aiming to make a statistical characterization of the high-redshift 21cm fluctuation signal spanning the hydrogen reionization and X-ray heating epochs of the universe. However, connecting 21cm statistics to underlying physical parameters is complicated by the theoretical challenge of modeling the relevant physics at computational speeds quick enough to enable exploration of the high dimensional and weakly constrained parameter space. In this work, we use machine learning algorithms to build a fast emulator that mimics expensive simulations of the 21cm signal across a wide parameter space to high precision. We embed our emulator within a Markov-Chain Monte Carlo framework, enabling it to explore the posterior distribution over a large number of model parameters, including those that govern the Epoch of Reionization, the Epoch of X-ray Heating, and cosmology. As a worked example, we use our emulator to present an updated parameter constraint forecast for the Hydrogen Epoch of Reionization Array experiment, showing that its characterization of a fiducial 21cm power spectrum will considerably narrow the allowed parameter space of reionization and heating parameters, and could help strengthen Planck's constraints on $\sigma_8$. We provide both our generalized emulator code and its implementation specifically for 21cm parameter constraints as publicly available software.	0,1,0,0,0,0
Evidence for Two Hot Jupiter Formation Paths	Disk migration and high-eccentricity migration are two well-studied theories to explain the formation of hot Jupiters. The former predicts that these planets can migrate up until the planet-star Roche separation ($a_{Roche}$) and the latter predicts they will tidally circularize at a minimum distance of 2$a_{Roche}$. Considering long-running radial velocity and transit surveys have identified a couple hundred hot Jupiters to date, we can revisit the classic question of hot Jupiter formation in a data-driven manner. We approach this problem using data from several exoplanet surveys (radial velocity, Kepler, HAT, and WASP) allowing for either a single population or a mixture of populations associated with these formation channels, and applying a hierarchical Bayesian mixture model of truncated power laws of the form $x^{\gamma-1}$ to constrain the population-level parameters of interest (e.g., location of inner edges, $\gamma$, mixture fractions). Within the limitations of our chosen models, we find the current radial velocity and Kepler sample of hot Jupiters can be well explained with a single truncated power law distribution with a lower cutoff near 2$a_{Roche}$, a result that still holds after a decade, and $\gamma=-0.51\pm^{0.19}_{0.20}$. However, the HAT and WASP data show evidence for multiple populations (Bayes factor $\approx 10^{21}$). We find that $15\pm^{9}_{6}\%$ reside in a component consistent with disk migration ($\gamma=-0.04\pm^{0.53}_{1.27}$) and $85\pm^{6}_{9}\%$ in one consistent with high-eccentricity migration ($\gamma=-1.38\pm^{0.32}_{0.47}$). We find no immediately strong connections with some observed host star properties and speculate on how future exoplanet surveys could improve upon hot Jupiter population inference.	0,1,0,0,0,0
Ultracold heteronuclear three-body systems: How diabaticity limits the universality of recombination into shallow dimers	The mass-imbalanced three-body recombination process that forms a shallow dimer is shown to possess a rich Efimov-Stückelberg landscape, with corresponding spectra that differ fundamentally from the homonuclear case. A semi-analytical treatment of the three-body recombination predicts an unusual spectra with intertwined resonance peaks and minima, and yields in-depth insight into the behavior of the corresponding Efimov spectra. In particular, the patterns of the Efimov-Stückelberg landscape are shown to depend inherently on the degree of diabaticity of the three-body collisions, which strongly affects the universality of the heteronuclear Efimov states.	0,1,0,0,0,0
On some mellin transforms for the Riemann zeta function in the critical strip	We offer two new Mellin transform evaluations for the Riemann zeta function in the region $0<\Re(s)<1.$ Some discussion is offered in the way of evaluating some further Fourier integrals involving the Riemann xi function.	0,0,1,0,0,0
On the Difference between Physics and Biology: Logical Branching and Biomolecules	Physical emergence - crystals, rocks, sandpiles, turbulent eddies, planets, stars - is fundamentally different from biological emergence - amoeba, cells, mice, humans - even though the latter is based in the former. This paper points out that an essential difference is that as well as involving physical causation, causation in biological systems has a logical nature at each level of the hierarchy of emergence, from the biomolecular level up. The key link between physics and life enabling this to happen is provided by biomolecules, such as voltage gated ion channels, which enable branching logic to emerge from the underlying physics and hence enable logically based cell processes to take place in general, and in neurons in particular. These molecules can only have come into being via the contextually dependent processes of natural selection, which selects them for their biological function. A further major difference is between life in general and intelligent life. We characterise intelligent organisms as being engaged in deductive causation, which enables them to transcend the physical limitations of their bodies through the power of abstract thought, prediction, and planning. Ultimately this is enabled by the biomolecules that underlie the propagation of action potentials in neuronal axons in the brain.	0,1,0,0,0,0
If you are not paying for it, you are the product: How much do advertisers pay to reach you?	Online advertising is progressively moving towards a programmatic model in which ads are matched to actual interests of individuals collected as they browse the web. Letting the huge debate around privacy aside, a very important question in this area, for which little is known, is: How much do advertisers pay to reach an individual? In this study, we develop a first of its kind methodology for computing exactly that -- the price paid for a web user by the ad ecosystem -- and we do that in real time. Our approach is based on tapping on the Real Time Bidding (RTB) protocol to collect cleartext and encrypted prices for winning bids paid by advertisers in order to place targeted ads. Our main technical contribution is a method for tallying winning bids even when they are encrypted. We achieve this by training a model using as ground truth prices obtained by running our own "probe" ad-campaigns. We design our methodology through a browser extension and a back-end server that provides it with fresh models for encrypted bids. We validate our methodology using a one year long trace of 1600 mobile users and demonstrate that it can estimate a user's advertising worth with more than 82% accuracy.	1,0,0,0,0,0
Effective Description of Higher-Order Scalar-Tensor Theories	Most existing theories of dark energy and/or modified gravity, involving a scalar degree of freedom, can be conveniently described within the framework of the Effective Theory of Dark Energy, based on the unitary gauge where the scalar field is uniform. We extend this effective approach by allowing the Lagrangian in unitary gauge to depend on the time derivative of the lapse function. Although this dependence generically signals the presence of an extra scalar degree of freedom, theories that contain only one propagating scalar degree of freedom, in addition to the usual tensor modes, can be constructed by requiring the initial Lagrangian to be degenerate. Starting from a general quadratic action, we derive the dispersion relations for the linear perturbations around Minkowski and a cosmological background. Our analysis directly applies to the recently introduced Degenerate Higher-Order Scalar-Tensor (DHOST) theories. For these theories, we find that one cannot recover a Poisson-like equation in the static linear regime except for the subclass that includes the Horndeski and so-called "beyond Horndeski" theories. We also discuss Lorentz-breaking models inspired by Horava gravity.	0,1,0,0,0,0
Implementing GraphQL as a Query Language for Deductive Databases in SWI-Prolog Using DCGs, Quasi Quotations, and Dicts	The methods to access large relational databases in a distributed system are well established: the relational query language SQL often serves as a language for data access and manipulation, and in addition public interfaces are exposed using communication protocols like REST. Similarly to REST, GraphQL is the query protocol of an application layer developed by Facebook. It provides a unified interface between the client and the server for data fetching and manipulation. Using GraphQL's type system, it is possible to specify data handling of various sources and to combine, e.g., relational with NoSQL databases. In contrast to REST, GraphQL provides a single API endpoint and supports flexible queries over linked data. GraphQL can also be used as an interface for deductive databases. In this paper, we give an introduction of GraphQL and a comparison to REST. Using language features recently added to SWI-Prolog 7, we have developed the Prolog library GraphQL.pl, which implements the GraphQL type system and query syntax as a domain-specific language with the help of definite clause grammars (DCG), quasi quotations, and dicts. Using our library, the type system created for a deductive database can be validated, while the query system provides a unified interface for data access and introspection.	1,0,0,0,0,0
Power Plant Performance Modeling with Concept Drift	Power plant is a complex and nonstationary system for which the traditional machine learning modeling approaches fall short of expectations. The ensemble-based online learning methods provide an effective way to continuously learn from the dynamic environment and autonomously update models to respond to environmental changes. This paper proposes such an online ensemble regression approach to model power plant performance, which is critically important for operation optimization. The experimental results on both simulated and real data show that the proposed method can achieve performance with less than 1% mean average percentage error, which meets the general expectations in field operations.	1,0,0,1,0,0
Joint Scheduling and Transmission Power Control in Wireless Ad Hoc Networks	In this paper, we study how to determine concurrent transmissions and the transmission power level of each link to maximize spectrum efficiency and minimize energy consumption in a wireless ad hoc network. The optimal joint transmission packet scheduling and power control strategy are determined when the node density goes to infinity and the network area is unbounded. Based on the asymptotic analysis, we determine the fundamental capacity limits of a wireless network, subject to an energy consumption constraint. We propose a scheduling and transmission power control mechanism to approach the optimal solution to maximize spectrum and energy efficiencies in a practical network. The distributed implementation of the proposed scheduling and transmission power control scheme is presented based on our MAC framework proposed in [1]. Simulation results demonstrate that the proposed scheme achieves 40% higher throughput than existing schemes. Also, the energy consumption using the proposed scheme is about 20% of the energy consumed using existing power saving MAC protocols.	1,0,0,0,0,0
Entropic Causality and Greedy Minimum Entropy Coupling	We study the problem of identifying the causal relationship between two discrete random variables from observational data. We recently proposed a novel framework called entropic causality that works in a very general functional model but makes the assumption that the unobserved exogenous variable has small entropy in the true causal direction. This framework requires the solution of a minimum entropy coupling problem: Given marginal distributions of m discrete random variables, each on n states, find the joint distribution with minimum entropy, that respects the given marginals. This corresponds to minimizing a concave function of nm variables over a convex polytope defined by nm linear constraints, called a transportation polytope. Unfortunately, it was recently shown that this minimum entropy coupling problem is NP-hard, even for 2 variables with n states. Even representing points (joint distributions) over this space can require exponential complexity (in n, m) if done naively. In our recent work we introduced an efficient greedy algorithm to find an approximate solution for this problem. In this paper we analyze this algorithm and establish two results: that our algorithm always finds a local minimum and also is within an additive approximation error from the unknown global optimum.	1,0,0,1,0,0
Ride Sharing and Dynamic Networks Analysis	The potential of an efficient ride-sharing scheme to significantly reduce traffic congestion, lower emission level, as well as facilitating the introduction of smart cities has been widely demonstrated. This positive thrust however is faced with several delaying factors, one of which is the volatility and unpredictability of the potential benefit (or utilization) of ride-sharing at different times, and in different places. In this work the following research questions are posed: (a) Is ride-sharing utilization stable over time or does it undergo significant changes? (b) If ride-sharing utilization is dynamic, can it be correlated with some traceable features of the traffic? and (c) If ride-sharing utilization is dynamic, can it be predicted ahead of time? We analyze a dataset of over 14 Million taxi trips taken in New York City. We propose a dynamic travel network approach for modeling and forecasting the potential ride-sharing utilization over time, showing it to be highly volatile. In order to model the utilization's dynamics we propose a network-centric approach, projecting the aggregated traffic taken from continuous time periods into a feature space comprised of topological features of the network implied by this traffic. This feature space is then used to model the dynamics of ride-sharing utilization over time. The results of our analysis demonstrate the significant volatility of ride-sharing utilization over time, indicating that any policy, design or plan that would disregard this aspect and chose a static paradigm would undoubtably be either highly inefficient or provide insufficient resources. We show that using our suggested approach it is possible to model the potential utilization of ride sharing based on the topological properties of the rides network. We also show that using this method the potential utilization can be forecasting a few hours ahead of time.	1,1,0,0,0,0
Bianchi type-II universe with wet dark fluid in General Theory of Relativity	In this paper, dark energy models of the universe filled with wet dark fluid are constructed in the framework of LRS Bianchi type-II space-time in General Theory of Relativity. A new equation of state modeled on the equation of state $p$=$\gamma(\rho - \rho_*)$, which can describe a liquid including water, is used. The exact solutions of Einstein's field equations are obtained in quadrature form and the models corresponding to the cases $\gamma = 0$ and $\gamma = 1$ are discussed in detail.	0,1,0,0,0,0
Cycle Consistent Adversarial Denoising Network for Multiphase Coronary CT Angiography	In coronary CT angiography, a series of CT images are taken at different levels of radiation dose during the examination. Although this reduces the total radiation dose, the image quality during the low-dose phases is significantly degraded. To address this problem, here we propose a novel semi-supervised learning technique that can remove the noises of the CT images obtained in the low-dose phases by learning from the CT images in the routine dose phases. Although a supervised learning approach is not possible due to the differences in the underlying heart structure in two phases, the images in the two phases are closely related so that we propose a cycle-consistent adversarial denoising network to learn the non-degenerate mapping between the low and high dose cardiac phases. Experimental results showed that the proposed method effectively reduces the noise in the low-dose CT image while the preserving detailed texture and edge information. Moreover, thanks to the cyclic consistency and identity loss, the proposed network does not create any artificial features that are not present in the input images. Visual grading and quality evaluation also confirm that the proposed method provides significant improvement in diagnostic quality.	0,0,0,1,0,0
Response to "Counterexample to global convergence of DSOS and SDSOS hierarchies"	In a recent note [8], the author provides a counterexample to the global convergence of what his work refers to as "the DSOS and SDSOS hierarchies" for polynomial optimization problems (POPs) and purports that this refutes claims in our extended abstract [4] and slides in [3]. The goal of this paper is to clarify that neither [4], nor [3], and certainly not our full paper [5], ever defined DSOS or SDSOS hierarchies as it is done in [8]. It goes without saying that no claims about convergence properties of the hierarchies in [8] were ever made as a consequence. What was stated in [4,3] was completely different: we stated that there exist hierarchies based on DSOS and SDSOS optimization that converge. This is indeed true as we discuss in this response. We also emphasize that we were well aware that some (S)DSOS hierarchies do not converge even if their natural SOS counterparts do. This is readily implied by an example in our prior work [5], which makes the counterexample in [8] superfluous. Finally, we provide concrete counterarguments to claims made in [8] that aim to challenge the scalability improvements obtained by DSOS and SDSOS optimization as compared to sum of squares (SOS) optimization. [3] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS: More tractable alternatives to SOS. Slides at the meeting on Geometry and Algebra of Linear Matrix Inequalities, CIRM, Marseille, 2013. [4] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS optimization: LP and SOCP-based alternatives to sum of squares optimization. In proceedings of the 48th annual IEEE Conference on Information Sciences and Systems, 2014. [5] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS optimization: more tractable alternatives to sum of squares and semidefinite optimization. arXiv:1706.02586, 2017. [8] C. Josz. Counterexample to global convergence of DSOS and SDSOS hierarchies. arXiv:1707.02964, 2017.	1,0,0,1,0,0
Improving SIEM capabilities through an enhanced probe for encrypted Skype traffic detection	Nowadays, the Security Information and Event Management (SIEM) systems take on great relevance in handling security issues for critical infrastructures as Internet Service Providers. Basically, a SIEM has two main functions: i) the collection and the aggregation of log data and security information from disparate network devices (routers, firewalls, intrusion detection systems, ad hoc probes and others) and ii) the analysis of the gathered data by implementing a set of correlation rules aimed at detecting potential suspicious events as the presence of encrypted real-time traffic. In the present work, the authors propose an enhanced implementation of a SIEM where a particular focus is given to the detection of encrypted Skype traffic by using an ad-hoc developed enhanced probe (ESkyPRO) conveniently governed by the SIEM itself. Such enhanced probe, able to interact with an agent counterpart deployed into the SIEM platform, is designed by exploiting some machine learning concepts. The main purpose of the proposed ad-hoc SIEM is to correlate the information received by ESkyPRO and other types of data obtained by an Intrusion Detection System (IDS) probe in order to make the encrypted Skype traffic detection as accurate as possible.	1,0,0,0,0,0
Image-based immersed boundary model of the aortic root	Each year, approximately 300,000 heart valve repair or replacement procedures are performed worldwide, including approximately 70,000 aortic valve replacement surgeries in the United States alone. This paper describes progress in constructing anatomically and physiologically realistic immersed boundary (IB) models of the dynamics of the aortic root and ascending aorta. This work builds on earlier IB models of fluid-structure interaction (FSI) in the aortic root, which previously achieved realistic hemodynamics over multiple cardiac cycles, but which also were limited to simplified aortic geometries and idealized descriptions of the biomechanics of the aortic valve cusps. By contrast, the model described herein uses an anatomical geometry reconstructed from patient-specific computed tomography angiography (CTA) data, and employs a description of the elasticity of the aortic valve leaflets based on a fiber-reinforced constitutive model fit to experimental tensile test data. Numerical tests show that the model is able to resolve the leaflet biomechanics in diastole and early systole at practical grid spacings. The model is also used to examine differences in the mechanics and fluid dynamics yielded by fresh valve leaflets and glutaraldehyde-fixed leaflets similar to those used in bioprosthetic heart valves. Although there are large differences in the leaflet deformations during diastole, the differences in the open configurations of the valve models are relatively small, and nearly identical hemodynamics are obtained in all cases considered.	1,1,0,0,0,0
Topology determines force distributions in one-dimensional random spring networks	Networks of elastic fibers are ubiquitous in biological systems and often provide mechanical stability to cells and tissues. Fiber reinforced materials are also common in technology. An important characteristic of such materials is their resistance to failure under load. Rupture occurs when fibers break under excessive force and when that failure propagates. Therefore it is crucial to understand force distributions. Force distributions within such networks are typically highly inhomogeneous and are not well understood. Here we construct a simple one-dimensional model system with periodic boundary conditions by randomly placing linear springs on a circle. We consider ensembles of such networks that consist of $N$ nodes and have an average degree of connectivity $z$, but vary in topology. Using a graph-theoretical approach that accounts for the full topology of each network in the ensemble, we show that, surprisingly, the force distributions can be fully characterized in terms of the parameters $(N,z)$. Despite the universal properties of such $(N,z)$-ensembles, our analysis further reveals that a classical mean-field approach fails to capture force distributions correctly. We demonstrate that network topology is a crucial determinant of force distributions in elastic spring networks.	0,1,0,0,0,0
Direct and indirect seismic inversion: interpretation of certain mathematical theorems	Quantitative methods are more familiar to most geophysicists with direct inversion or indirect inversion. We will discuss seismic inversion in a high level sense without getting into the actual algorithms. We will stay with meta-equations and argue pros and cons based on certain mathematical theorems.	0,1,0,0,0,0
Ideal structure and pure infiniteness of ample groupoid $C^*$-algebras	In this paper, we study the ideal structure of reduced $C^*$-algebras $C^*_r(G)$ associated to étale groupoids $G$. In particular, we characterize when there is a one-to-one correspondence between the closed, two-sided ideals in $C_r^*(G)$ and the open invariant subsets of the unit space $G^{(0)}$ of $G$. As a consequence, we show that if $G$ is an inner exact, essentially principal, ample groupoid, then $C_r^*(G)$ is (strongly) purely infinite if and only if every non-zero projection in $C_0(G^{(0)})$ is properly infinite in $C_r^*(G)$. We also establish a sufficient condition on the ample groupoid $G$ that ensures pure infiniteness of $C_r^*(G)$ in terms of paradoxicality of compact open subsets of the unit space $G^{(0)}$. Finally, we introduce the type semigroup for ample groupoids and also obtain a dichotomy result: Let $G$ be an ample groupoid with compact unit space which is minimal and topologically principal. If the type semigroup is almost unperforated, then $C_r^*(G)$ is a simple $C^*$-algebra which is either stably finite or strongly purely infinite.	0,0,1,0,0,0
Analysis of error control in large scale two-stage multiple hypothesis testing	When dealing with the problem of simultaneously testing a large number of null hypotheses, a natural testing strategy is to first reduce the number of tested hypotheses by some selection (screening or filtering) process, and then to simultaneously test the selected hypotheses. The main advantage of this strategy is to greatly reduce the severe effect of high dimensions. However, the first screening or selection stage must be properly accounted for in order to maintain some type of error control. In this paper, we will introduce a selection rule based on a selection statistic that is independent of the test statistic when the tested hypothesis is true. Combining this selection rule and the conventional Bonferroni procedure, we can develop a powerful and valid two-stage procedure. The introduced procedure has several nice properties: (i) it completely removes the selection effect; (ii) it reduces the multiplicity effect; (iii) it does not "waste" data while carrying out both selection and testing. Asymptotic power analysis and simulation studies illustrate that this proposed method can provide higher power compared to usual multiple testing methods while controlling the Type 1 error rate. Optimal selection thresholds are also derived based on our asymptotic analysis.	0,0,1,1,0,0
The formation of the Milky Way halo and its dwarf satellites, a NLTE-1D abundance analysis. I. Homogeneous set of atmospheric parameters	We present a homogeneous set of accurate atmospheric parameters for a complete sample of very and extremely metal-poor stars in the dwarf spheroidal galaxies (dSphs) Sculptor, Ursa Minor, Sextans, Fornax, Boötes I, Ursa Major II, and Leo IV. We also deliver a Milky Way (MW) comparison sample of giant stars covering the -4 < [Fe/H] < -1.7 metallicity range. We show that, in the [Fe/H] > -3.5 regime, the non-local thermodynamic equilibrium (NLTE) calculations with non-spectroscopic effective temperature (Teff) and surface gravity (log~g) based on the photometric methods and known distance provide consistent abundances of the Fe I and Fe II lines. This justifies the Fe I/Fe II ionisation equilibrium method to determine log g for the MW halo giants with unknown distance. The atmospheric parameters of the dSphs and MW stars were checked with independent methods. In the [Fe/H] > -3.5 regime, the Ti I/Ti II ionisation equilibrium is fulfilled in the NLTE calculations. In the log~g - Teff plane, all the stars sit on the giant branch of the evolutionary tracks corresponding to [Fe/H] = -2 to -4, in line with their metallicities. For some of the most metal-poor stars of our sample, we hardly achieve consistent NLTE abundances from the two ionisation stages for both iron and titanium. We suggest that this is a consequence of the uncertainty in the Teff-colour relation at those metallicities. The results of these work provide the base for a detailed abundance analysis presented in a companion paper.	0,1,0,0,0,0
Efficient tracking of a growing number of experts	We consider a variation on the problem of prediction with expert advice, where new forecasters that were unknown until then may appear at each round. As often in prediction with expert advice, designing an algorithm that achieves near-optimal regret guarantees is straightforward, using aggregation of experts. However, when the comparison class is sufficiently rich, for instance when the best expert and the set of experts itself changes over time, such strategies naively require to maintain a prohibitive number of weights (typically exponential with the time horizon). By contrast, designing strategies that both achieve a near-optimal regret and maintain a reasonable number of weights is highly non-trivial. We consider three increasingly challenging objectives (simple regret, shifting regret and sparse shifting regret) that extend existing notions defined for a fixed expert ensemble; in each case, we design strategies that achieve tight regret bounds, adaptive to the parameters of the comparison class, while being computationally inexpensive. Moreover, our algorithms are anytime, agnostic to the number of incoming experts and completely parameter-free. Such remarkable results are made possible thanks to two simple but highly effective recipes: first the "abstention trick" that comes from the specialist framework and enables to handle the least challenging notions of regret, but is limited when addressing more sophisticated objectives. Second, the "muting trick" that we introduce to give more flexibility. We show how to combine these two tricks in order to handle the most challenging class of comparison strategies.	1,0,0,1,0,0
Andreev reflection in 2D relativistic materials with realistic tunneling transparency in normal-metal-superconductor junctions	The Andreev conductance across 2d normal metal (N)/superconductor (SC) junctions with relativistic Dirac spectrum is investigated theoretically in the Blonder-Tinkham-Klapwijk formalism. It is shown that for relativistic materials, due to the Klein tunneling instead of impurity potentials, the local strain in the junction is the key factor that determines the transparency of the junction. The local strain is shown to generate an effective Dirac $\delta$-gauge field. A remarkable suppression of the conductance are observed as the strength of the gauge field increases. The behaviors of the conductance are in well agreement with the results obtained in the case of 1d N/SC junction. We also study the Andreev reflection in a topological material near the chiral-to-helical phase transition in the presence of a local strain. The N side of the N/SC junction is modeled by the doped Kane-Mele (KM) model. The SC region is a doped correlated KM t-J (KMtJ) model, which has been shown to feature d+id'-wave spin-singlet pairing. With increasing intrinsic spin-orbit (SO) coupling, the doped KMtJ system undergoes a topological phase transition from the chiral d-wave superconductivity to the spin-Chern superconducting phase with helical Majorana fermions at edges. We explore the Andreev conductance at the two inequivalent Dirac points, respectively and predict the distinctive behaviors for the Andreev conductance across the topological phase transition. Relevance of our results for the adatom-doped graphene is discussed.	0,1,0,0,0,0
Veamy: an extensible object-oriented C++ library for the virtual element method	This paper summarizes the development of Veamy, an object-oriented C++ library for the virtual element method (VEM) on general polygonal meshes, whose modular design is focused on its extensibility. The linear elastostatic and Poisson problems in two dimensions have been chosen as the starting stage for the development of this library. The theory of the VEM, upon which Veamy is built, is presented using a notation and a terminology that resemble the language of the finite element method (FEM) in engineering analysis. Several examples are provided to demonstrate the usage of Veamy, and in particular, one of them features the interaction between Veamy and the polygonal mesh generator PolyMesher. A computational performance comparison between VEM and FEM is also conducted. Veamy is free and open source software.	1,0,0,0,0,0
Towards Plan Transformations for Real-World Pick and Place Tasks	In this paper, we investigate the possibility of applying plan transformations to general manipulation plans in order to specialize them to the specific situation at hand. We present a framework for optimizing execution and achieving higher performance by autonomously transforming robot's behavior at runtime. We show that plans employed by robotic agents in real-world environments can be transformed, despite their control structures being very complex due to the specifics of acting in the real world. The evaluation is carried out on a plan of a PR2 robot performing pick and place tasks, to which we apply three example transformations, as well as on a large amount of experiments in a fast plan projection environment.	1,0,0,0,0,0
Radiative nonrecoil nuclear finite size corrections of order $α(Z α)^5$ to the Lamb shift in light muonic atoms	On the basis of quasipotential method in quantum electrodynamics we calculate nuclear finite size radiative corrections of order $\alpha(Z \alpha)^5$ to the Lamb shift in muonic hydrogen and helium. To construct the interaction potential of particles, which gives the necessary contributions to the energy spectrum, we use the method of projection operators to states with a definite spin. Separate analytic expressions for the contributions of the muon self-energy, the muon vertex operator and the amplitude with spanning photon are obtained. We present also numerical results for these contributions using modern experimental data on the electromagnetic form factors of light nuclei.	0,1,0,0,0,0
A Proof of the Conjecture of Lehmer and of the Conjecture of Schinzel-Zassenhaus	The conjecture of Lehmer is proved to be true. The proof mainly relies upon: (i) the properties of the Parry Upper functions $f_{house(\alpha)}(z)$ associated with the dynamical zeta functions $\zeta_{house(\alpha)}(z)$ of the Rényi--Parry arithmetical dynamical systems, for $\alpha$ an algebraic integer $\alpha$ of house "$house(\alpha)$" greater than 1, (ii) the discovery of lenticuli of poles of $\zeta_{house(\alpha)}(z)$ which uniformly equidistribute at the limit on a limit "lenticular" arc of the unit circle, when $house(\alpha)$ tends to $1^+$, giving rise to a continuous lenticular minorant ${\rm M}_{r}(house(\alpha))$ of the Mahler measure ${\rm M}(\alpha)$, (iii) the Poincaré asymptotic expansions of these poles and of this minorant ${\rm M}_{r}(house(\alpha))$ as a function of the dynamical degree. With the same arguments the conjecture of Schinzel-Zassenhaus is proved to be true. An inequality improving those of Dobrowolski and Voutier ones is obtained. The set of Salem numbers is shown to be bounded from below by the Perron number $\theta_{31}^{-1} = 1.08545\ldots$, dominant root of the trinomial $-1 - z^{30} + z^{31}$. Whether Lehmer's number is the smallest Salem number remains open. A lower bound for the Weil height of nonzero totally real algebraic numbers, $\neq \pm 1$, is obtained (Bogomolov property). For sequences of algebraic integers of Mahler measure smaller than the smallest Pisot number, whose houses have a dynamical degree tending to infinity, the Galois orbit measures of conjugates are proved to converge towards the Haar measure on $|z|=1$ (limit equidistribution).	0,0,1,0,0,0
On constant multi-commodity flow-cut gaps for directed minor-free graphs	The multi-commodity flow-cut gap is a fundamental parameter that affects the performance of several divide \& conquer algorithms, and has been extensively studied for various classes of undirected graphs. It has been shown by Linial, London and Rabinovich \cite{linial1994geometry} and by Aumann and Rabani \cite{aumann1998log} that for general $n$-vertex graphs it is bounded by $O(\log n)$ and the Gupta-Newman-Rabinovich-Sinclair conjecture \cite{gupta2004cuts} asserts that it is $O(1)$ for any family of graphs that excludes some fixed minor. The flow-cut gap is poorly understood for the case of directed graphs. We show that for uniform demands it is $O(1)$ on directed series-parallel graphs, and on directed graphs of bounded pathwidth. These are the first constant upper bounds of this type for some non-trivial family of directed graphs. We also obtain $O(1)$ upper bounds for the general multi-commodity flow-cut gap on directed trees and cycles. These bounds are obtained via new embeddings and Lipschitz quasipartitions for quasimetric spaces, which generalize analogous results form the metric case, and could be of independent interest. Finally, we discuss limitations of methods that were developed for undirected graphs, such as random partitions, and random embeddings.	1,0,0,0,0,0
Sources of inter-model scatter in TRACMIP, the Tropical Rain belts with an Annual cycle and a Continent Model Intercomparison Project	We analyze the source of inter-model scatter in the surface temperature response to quadrupling CO2 in two sets of GCM simulations from the Tropical Rain Belts with an Annual cycle and a Continent Model Intercomparison Project (TRACMIP; Voigt et al, 2016). TRACMIP provides simulations of idealized climates that allow for studying the fundamental dynamics of tropical rainfall and its response to climate change. One configuration is an aquaplanet atmosphere (i.e., with zonally-symmetric boundary conditions) coupled to a slab ocean (AquaCTL and Aqua4x). The other includes an equatorial continent represented by a thin slab ocean with increased surface albedo and decreased evaporation (LandCTL and Land4x).	0,1,0,0,0,0
Revealing the cluster of slow transients behind a large slow slip event	Capable of reaching similar magnitudes to large megathrust earthquakes ($M_w>7$), slow slip events play a major role in accommodating tectonic motion on plate boundaries. These slip transients are the slow release of built-up tectonic stress that are geodetically imaged as a predominantly aseismic rupture, which is smooth in both time and space. We demonstrate here that large slow slip events are in fact a cluster of short-duration slow transients. Using a dense catalog of low-frequency earthquakes as a guide, we investigate the $M_w7.5$ slow slip event that occurred in 2006 along the subduction interface 40~km beneath Guerrero, Mexico. We show that while the long-period surface displacement as recorded by GPS suggests a six month duration, motion in the direction of tectonic release only sporadically occurs over 55 days and its surface signature is attenuated by rapid relocking of the plate interface.These results demonstrate that our current conceptual model of slow and continuous rupture is an artifact of low-resolution geodetic observations of a superposition of small, clustered slip events. Our proposed description of slow slip as a cluster of slow transients implies that we systematically overestimate the duration $T$ and underestimate the moment magnitude $M$ of large slow slip events.	0,1,0,0,0,0
Generalized Gray codes with prescribed ends of small dimensions	Given pairwise distinct vertices $\{\alpha_i , \beta_i\}^k_{i=1}$ of the $n$-dimensional hypercube $Q_n$ such that the distance of $\alpha_i$ and $\beta_i$ is odd, are there paths $P_i$ between $\alpha_i$ and $\beta_i$ such that $\{V (P_i)\}^k_{i=1}$ partitions $V(Q_n)$? A positive solution for every $n\ge1$ and $k=1$ is known as a Gray code of dimension $n$. In this paper we settle this problem for small values of $n$.	1,0,0,0,0,0
Reply to Hicks et al 2017, Reply to Morrison et al 2016 Refining the relevant population in forensic voice comparison, Reply to Hicks et al 2015 The importance of distinguishing info from evidence/observations when formulating propositions	The present letter to the editor is one in a series of publications discussing the formulation of hypotheses (propositions) for the evaluation of strength of forensic evidence. In particular, the discussion focusses on the issue of what information may be used to define the relevant population specified as part of the different-speaker hypothesis in forensic voice comparison. The previous publications in the series are: Hicks et al. 2015 <this http URL>; Morrison et al. (2016) <this http URL>; Hicks et al. (2017) <this http URL>. The latter letter to the editor mostly resolves the apparent disagreement between the two groups of authors. We briefly discuss one outstanding point of apparent disagreement, and attempt to correct a misinterpretation of our earlier remarks. We believe that at this point there is no actual disagreement, and that both groups of authors are calling for greater collaboration in order to reduce the likelihood of future misunderstandings.	0,0,0,1,0,0
Measuring the academic reputation through citation networks via PageRank	The objective assessment of the prestige of an academic institution is a difficult and hotly debated task. In the last few years, different types of University Rankings have been proposed to quantify the excellence of different research institutions in the world. Albeit met with criticism in some cases, the relevance of university rankings is being increasingly acknowledged: indeed, rankings are having a major impact on the design of research policies, both at the institutional and governmental level. Yet, the debate on what rankings are {\em exactly} measuring is enduring. Here, we address the issue by measuring a quantitive and reliable proxy of the academic reputation of a given institution and by evaluating its correlation with different university rankings. Specifically, we study citation patterns among universities in five different Web of Science Subject Categories and use the \pr~algorithm on the five resulting citation networks. The rationale behind our work is that scientific citations are driven by the reputation of the reference so that the PageRank algorithm is expected to yield a rank which reflects the reputation of an academic institution in a specific field. Our results allow to quantifying the prestige of a set of institutions in a certain research field based only on hard bibliometric data. Given the volume of the data analysed, our findings are statistically robust and less prone to bias, at odds with ad--hoc surveys often employed by ranking bodies in order to attain similar results. Because our findings are found to correlate extremely well with the ARWU Subject rankings, the approach we propose in our paper may open the door to new, Academic Ranking methodologies that go beyond current methods by reconciling the qualitative evaluation of Academic Prestige with its quantitative measurements via publication impact.	1,0,0,0,0,0
Evolution of Morphological and Physical Properties of Laboratory Interstellar Organic Residues with Ultraviolet Irradiation	Refractory organic compounds formed in molecular clouds are among the building blocks of the solar system objects and could be the precursors of organic matter found in primitive meteorites and cometary materials. However, little is known about the evolutionary pathways of molecular cloud organics from dense molecular clouds to planetary systems. In this study, we focus on the evolution of the morphological and viscoelastic properties of molecular cloud refractory organic matter. We found that the organic residue, experimentally synthesized at about 10 K from UV-irradiated H2O-CH3OH-NH3 ice, changed significantly in terms of its nanometer- to micrometer-scale morphology and viscoelastic properties after UV irradiation at room temperature. The dose of this irradiation was equivalent to that experienced after short residence in diffuse clouds (equal or less than 10,000 years) or irradiation in outer protoplanetary disks. The irradiated organic residues became highly porous and more rigid and formed amorphous nanospherules. These nanospherules are morphologically similar to organic nanoglobules observed in the least-altered chondrites, chondritic porous interplanetary dust particles, and cometary samples, suggesting that irradiation of refractory organics could be a possible formation pathway for such nanoglobules. The storage modulus (elasticity) of photo-irradiated organic residues is about 100 MPa irrespective of vibrational frequency, a value that is lower than the storage moduli of minerals and ice. Dust grains coated with such irradiated organics would therefore stick together efficiently, but growth to larger grains might be suppressed due to an increase in aggregate brittleness caused by the strong connections between grains.	0,1,0,0,0,0
The QKP limit of the quantum Euler-Poisson equation	In this paper, we consider the derivation of the Kadomtsev-Petviashvili (KP) equation for cold ion-acoustic wave in the long wavelength limit of the two-dimensional quantum Euler-Poisson system, under different scalings for varying directions in the Gardner-Morikawa transform. It is shown that the types of the KP equation depend on the scaled quantum parameter $H>0$. The QKP-I is derived for $H>2$, QKP-II for $0<H<2$ and the dispersive-less KP (dKP) equation for the critical case $H=2$. The rigorous proof for these limits is given in the well-prepared initial data case, and the norm that is chosen to close the proof is anisotropic in the two directions, in accordance with the anisotropic structure of the KP equation as well as the Gardner-Morikawa transform. The results can be generalized in several directions.	0,0,1,0,0,0
Outcrop fracture characterization on suppositional planes cutting through digital outcrop models (DOMs)	Conventional fracture data collection methods are usually implemented on planar surfaces or assuming they are planar; these methods may introduce sampling errors on uneven outcrop surfaces. Consequently, data collected on limited types of outcrop surfaces (mainly bedding surfaces) may not be a sufficient representation of fracture network characteristic in outcrops. Recent development of techniques that obtain DOMs from outcrops and extract the full extent of individual fractures offers the opportunity to address the problem of performing the conventional sampling methods on uneven outcrop surfaces. In this study, we propose a new method that performs outcrop fracture characterization on suppositional planes cutting through DOMs. The suppositional plane is the best fit plane of the outcrop surface, and the fracture trace map is extracted on the suppositional plane so that the fracture network can be further characterized. The amount of sampling errors introduced by the conventional methods and avoided by the new method on 16 uneven outcrop surfaces with different roughnesses are estimated. The results show that the conventional sampling methods don't apply to outcrops other than bedding surfaces or outcrops whose roughness > 0.04 m, and that the proposed method can greatly extend the types of outcrop surfaces for outcrop fracture characterization with the suppositional plane cutting through DOMs.	1,1,0,0,0,0
Computational Flows in Arithmetic	A computational flow is a pair consisting of a sequence of computational problems of a certain sort and a sequence of computational reductions among them. In this paper we will develop a theory for these computational flows and we will use it to make a sound and complete interpretation for bounded theories of arithmetic. This property helps us to decompose a first order arithmetical proof to a sequence of computational reductions by which we can extract the computational content of low complexity statements in some bounded theories of arithmetic such as $I\Delta_0$, $T^k_n$, $I\Delta_0+EXP$ and $PRA$. In the last section, by generalizing term-length flows to ordinal-length flows, we will extend our investigation from bounded theories to strong unbounded ones such as $I\Sigma_n$ and $PA+TI(\alpha)$ and we will capture their total $NP$ search problems as a consequence.	1,0,1,0,0,0
Knockoffs for the mass: new feature importance statistics with false discovery guarantees	An important problem in machine learning and statistics is to identify features that causally affect the outcome. This is often impossible to do from purely observational data, and a natural relaxation is to identify features that are correlated with the outcome even conditioned on all other observed features. For example, we want to identify that smoking really is correlated with cancer conditioned on demographics. The knockoff procedure is a recent breakthrough in statistics that, in theory, can identify truly correlated features while guaranteeing that the false discovery is limited. The idea is to create synthetic data -knockoffs- that captures correlations amongst the features. However there are substantial computational and practical challenges to generating and using knockoffs. This paper makes several key advances that enable knockoff application to be more efficient and powerful. We develop an efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we systematically evaluate knockoff test statistics and develop new statistics with improved power. The paper combines new mathematical guarantees with systematic experiments on real and synthetic data.	0,0,0,1,0,0
Group Field theory and Tensor Networks: towards a Ryu-Takayanagi formula in full quantum gravity	We establish a dictionary between group field theory (thus, spin networks and random tensors) states and generalized random tensor networks. Then, we use this dictionary to compute the Rényi entropy of such states and recover the Ryu-Takayanagi formula, in two different cases corresponding to two different truncations/approximations, suggested by the established correspondence.	0,1,0,0,0,0
Global stability of the Rate Control Protocol (RCP) and some implications for protocol design	The Rate Control Protocol (RCP) is a congestion control protocol that relies on explicit feedback from routers. RCP estimates the flow rate using two forms of feedback: rate mismatch and queue size. However, it remains an open design question whether queue size feedback in RCP is useful, given the presence of rate mismatch. The model we consider has RCP flows operating over a single bottleneck, with heterogeneous time delays. We first derive a sufficient condition for global stability, and then highlight how this condition favors the design choice of having only rate mismatch in the protocol definition.	1,0,0,0,0,0
Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking	This paper is concerned with the problem of top-$K$ ranking from pairwise comparisons. Given a collection of $n$ items and a few pairwise comparisons across them, one wishes to identify the set of $K$ items that receive the highest ranks. To tackle this problem, we adopt the logistic parametric model --- the Bradley-Terry-Luce model, where each item is assigned a latent preference score, and where the outcome of each pairwise comparison depends solely on the relative scores of the two items involved. Recent works have made significant progress towards characterizing the performance (e.g. the mean square error for estimating the scores) of several classical methods, including the spectral method and the maximum likelihood estimator (MLE). However, where they stand regarding top-$K$ ranking remains unsettled. We demonstrate that under a natural random sampling model, the spectral method alone, or the regularized MLE alone, is minimax optimal in terms of the sample complexity --- the number of paired comparisons needed to ensure exact top-$K$ identification, for the fixed dynamic range regime. This is accomplished via optimal control of the entrywise error of the score estimates. We complement our theoretical studies by numerical experiments, confirming that both methods yield low entrywise errors for estimating the underlying scores. Our theory is established via a novel leave-one-out trick, which proves effective for analyzing both iterative and non-iterative procedures. Along the way, we derive an elementary eigenvector perturbation bound for probability transition matrices, which parallels the Davis-Kahan $\sin\Theta$ theorem for symmetric matrices. This also allows us to close the gap between the $\ell_2$ error upper bound for the spectral method and the minimax lower limit.	1,0,1,1,0,0
Charge reconstruction study of the DAMPE Silicon-Tungsten Tracker with ion beams	The DArk Matter Particle Explorer (DAMPE) is one of the four satellites within Strategic Pioneer Research Program in Space Science of the Chinese Academy of Science (CAS). DAMPE can detect electrons, photons in a wide energy range (5 GeV to 10 TeV) and ions up to iron (100GeV to 100 TeV). Silicon-Tungsten Tracker (STK) is one of the four subdetectors in DAMPE, providing photon-electron conversion, track reconstruction and charge identification for ions. Ion beam test was carried out in CERN with 60GeV/u Lead primary beams. Charge reconstruction and charge resolution of STK detectors were investigated.	0,1,0,0,0,0
Mutually touching infinite cylinders in the 3D world of lines	Recently we gave arguments that only two unique topologically different configurations of 7 equal all mutually touching round cylinders (the configurations being mirror reflections of each other) are possible in 3D, although a whole world of configurations is possible already for round cylinders of arbitrary radii. It was found that as many as 9 round cylinders (all mutually touching) are possible in 3D while the upper bound for arbitrary cylinders was estimated to be not more than 14 under plausible arguments. Now by using the chirality and Ring matrices that we introduced earlier for the topological classification of line configurations, we have given arguments that the maximal number of mutually touching straight infinite cylinders of arbitrary cross-section (provided that its boundary is a smooth curve) in 3D cannot exceed 10. We generated numerically several configurations of 10 cylinders, restricting ourselves with elliptic cylinders. Configurations of 8 and 9 equal elliptic cylinders (all in mutually touching) are generated numerically as well. A possibility and restriction of continuous transformations from elliptic into round cylinder configurations are discussed. Some curious results concerning the properties of the chirality matrix (which coincides with Seidel's adjacency matrix important for the Graph theory) are presented.	0,0,1,0,0,0
Dynamics of cracks in disordered materials	Predicting when rupture occurs or cracks progress is a major challenge in numerous elds of industrial, societal and geophysical importance. It remains largely unsolved: Stress enhancement at cracks and defects, indeed, makes the macroscale dynamics extremely sensitive to the microscale material disorder. This results in giant statistical uctuations and non-trivial behaviors upon upscaling dicult to assess via the continuum approaches of engineering. These issues are examined here. We will see: How linear elastic fracture mechanics sidetracks the diculty by reducing the problem to that of the propagation of a single crack in an eective material free of defects, How slow cracks sometimes display jerky dynamics, with sudden violent events incompatible with the previous approach, and how some paradigms of statistical physics can explain it, How abnormally fast cracks sometimes emerge due to the formation of microcracks at very small scales.	0,1,0,0,0,0
Active Learning for Regression Using Greedy Sampling	Regression problems are pervasive in real-world applications. Generally a substantial amount of labeled samples are needed to build a regression model with good generalization ability. However, many times it is relatively easy to collect a large number of unlabeled samples, but time-consuming or expensive to label them. Active learning for regression (ALR) is a methodology to reduce the number of labeled samples, by selecting the most beneficial ones to label, instead of random selection. This paper proposes two new ALR approaches based on greedy sampling (GS). The first approach (GSy) selects new samples to increase the diversity in the output space, and the second (iGS) selects new samples to increase the diversity in both input and output spaces. Extensive experiments on 12 UCI and CMU StatLib datasets from various domains, and on 15 subjects on EEG-based driver drowsiness estimation, verified their effectiveness and robustness.	0,0,0,1,0,0
Experimental study of mini-magnetosphere	Magnetosphere at ion kinetic scales, or mini-magnetosphere, possesses unusual features as predicted by numerical simulations. However, there are practically no data on the subject from space observations and the data which are available are far too incomplete. In the present work we describe results of laboratory experiment on interaction of plasma flow with magnetic dipole with parameters such that ion inertia length is smaller than a size of observed magnetosphere. A detailed structure of non-coplanar or out-of-plane component of magnetic field has been obtained in meridian plane. Independence of this component on dipole moment reversal, as was reported in previous work, has been verified. In the tail distinct lobes and central current sheet have been observed. It was found that lobe regions adjacent to boundary layer are dominated by non-coplanar component of magnetic field. Tail-ward oriented electric current in plasma associated with that component appears to be equal to ion current in the frontal part of magnetosphere and in the tail current sheet implying that electrons are stationary in those regions while ions flow by. Obtained data strongly support the proposed model of mini-magnetosphere based on two-fluid effects as described by the Hall term.	0,1,0,0,0,0
On Study of the Reliable Fully Convolutional Networks with Tree Arranged Outputs (TAO-FCN) for Handwritten String Recognition	The handwritten string recognition is still a challengeable task, though the powerful deep learning tools were introduced. In this paper, based on TAO-FCN, we proposed an end-to-end system for handwritten string recognition. Compared with the conventional methods, there is no preprocess nor manually designed rules employed. With enough labelled data, it is easy to apply the proposed method to different applications. Although the performance of the proposed method may not be comparable with the state-of-the-art approaches, it's usability and robustness are more meaningful for practical applications.	1,0,0,0,0,0
Generalized End-to-End Loss for Speaker Verification	In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. "OK Google" and "Hey Google") as well as multiple dialects.	1,0,0,1,0,0
Stability Analysis of Piecewise Affine Systems with Multi-model Model Predictive Control	Constrained model predictive control (MPC) is a widely used control strategy, which employs moving horizon-based on-line optimisation to compute the optimum path of the manipulated variables. Nonlinear MPC can utilize detailed models but it is computationally expensive; on the other hand linear MPC may not be adequate. Piecewise affine (PWA) models can describe the underlying nonlinear dynamics more accurately, therefore they can provide a viable trade-off through their use in multi-model linear MPC configurations, which avoid integer programming. However, such schemes may introduce uncertainty affecting the closed loop stability. In this work, we propose an input to output stability analysis for closed loop systems, consisting of PWA models, where an observer and multi-model linear MPC are applied together, under unstructured uncertainty. Integral quadratic constraints (IQCs) are employed to assess the robustness of MPC under uncertainty. We create a model pool, by performing linearisation on selected transient points. All the possible uncertainties and nonlinearities (including the controller) can be introduced in the framework, assuming that they admit the appropriate IQCs, whilst the dissipation inequality can provide necessary conditions incorporating IQCs. We demonstrate the existence of static multipliers, which can reduce the conservatism of the stability analysis significantly. The proposed methodology is demonstrated through two engineering case studies.	1,0,0,0,0,0
Unstable normalized standing waves for the space periodic NLS	For the stationary nonlinear Schrödinger equation $-\Delta u+ V(x)u- f(u) = \lambda u$ with periodic potential $V$ we study the existence and stability properties of multibump solutions with prescribed $L^2$-norm. To this end we introduce a new nondegeneracy condition and develop new superposition techniques which allow to match the $L^2$-constraint. In this way we obtain the existence of infinitely many geometrically distinct solutions to the stationary problem. We then calculate the Morse index of these solutions with respect to the restriction of the underlying energy functional to the associated $L^2$-sphere, and we show their orbital instability with respect to the Schrödinger flow. Our results apply in both, the mass-subcritical and the mass-supercritical regime.	0,0,1,0,0,0
Batched High-dimensional Bayesian Optimization via Structural Kernel Learning	Optimization of high-dimensional black-box functions is an extremely challenging problem. While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings. In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method. Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes. Experimental validations on both synthetic and real-world functions demonstrate that the proposed method outperforms the existing state-of-the-art approaches.	1,0,1,1,0,0
QRT maps and related Laurent systems	In recent work it was shown how recursive factorisation of certain QRT maps leads to Somos-4 and Somos-5 recurrences with periodic coefficients, and to a fifth-order recurrence with the Laurent property. Here we recursively factorise the 12-parameter symmetric QRT map, given by a second-order recurrence, to obtain a system of three coupled recurrences which possesses the Laurent property. As degenerate special cases, we first derive systems of two coupled recurrences corresponding to the 5-parameter multiplicative and additive symmetric QRT maps. In all cases, the Laurent property is established using a generalisation of a result due to Hickerson, and exact formulae for degree growth are found from ultradiscrete (tropical) analogues of the recurrences. For the general 18-parameter QRT map it is shown that the components of the iterates can be written as a ratio of quantities that satisfy the same Somos-7 recurrence.	0,1,1,0,0,0
On the semisimplicity of the cyclotomic quiver Hecke algebra of type C	We provide criteria for the cyclotomic quiver Hecke algebras of type C to be semisimple. In the semisimple case, we construct the irreducible modules.	0,0,1,0,0,0
Piecewise linear generalized Alexander's theorem in dimension at most 5	We study piecewise linear co-dimension two embeddings of closed oriented manifolds in Euclidean space, and show that any such embedding can always be isotoped to be a closed braid as long as the ambient dimension is at most five, extending results of Alexander (in ambient dimension three), and Viro and independently Kamada (in ambient dimension four). We also show an analogous result for higher co-dimension embeddings.	0,0,1,0,0,0
Coresets for Dependency Networks	Many applications infer the structure of a probabilistic graphical model from data to elucidate the relationships between variables. But how can we train graphical models on a massive data set? In this paper, we show how to construct coresets -compressed data sets which can be used as proxy for the original data and have provably bounded worst case error- for Gaussian dependency networks (DNs), i.e., cyclic directed graphical models over Gaussians, where the parents of each variable are its Markov blanket. Specifically, we prove that Gaussian DNs admit coresets of size independent of the size of the data set. Unfortunately, this does not extend to DNs over members of the exponential family in general. As we will prove, Poisson DNs do not admit small coresets. Despite this worst-case result, we will provide an argument why our coreset construction for DNs can still work well in practice on count data. To corroborate our theoretical results, we empirically evaluated the resulting Core DNs on real data sets. The results	1,0,0,1,0,0
Faster Algorithms for Mean-Payoff Parity Games	Graph games provide the foundation for modeling and synthesis of reactive processes. Such games are played over graphs where the vertices are controlled by two adversarial players. We consider graph games where the objective of the first player is the conjunction of a qualitative objective (specified as a parity condition) and a quantitative objective (specified as a mean-payoff condition). There are two variants of the problem, namely, the threshold problem where the quantitative goal is to ensure that the mean-payoff value is above a threshold, and the value problem where the quantitative goal is to ensure the optimal mean-payoff value; in both cases ensuring the qualitative parity objective. The previous best-known algorithms for game graphs with $n$ vertices, $m$ edges, parity objectives with $d$ priorities, and maximal absolute reward value $W$ for mean-payoff objectives, are as follows: $O(n^{d+1} \cdot m \cdot W)$ for the threshold problem, and $O(n^{d+2} \cdot m \cdot W)$ for the value problem. Our main contributions are faster algorithms, and the running times of our algorithms are as follows: $O(n^{d-1} \cdot m \cdot W)$ for the threshold problem, and $O(n^{d} \cdot m \cdot W \cdot \log (n\cdot W))$ for the value problem. For mean-payoff parity objectives with two priorities, our algorithms match the best-known bounds of the algorithms for mean-payoff games (without conjunction with parity objectives). Our results are relevant in synthesis of reactive systems with both functional requirement (given as a qualitative objective) and performance requirement (given as a quantitative objective).	1,0,0,0,0,0
Ultra-wide-band slow light in photonic crystal coupled-cavity waveguides	Slow light propagation in structured materials is a highly promising approach for realizing on-chip integrated photonic devices based on enhanced optical nonlinearities. One of the most successful research avenues consists in engineering the band dispersion of light-guiding photonic crystal (PC) structures. The primary goal of such devices is to achieve slow-light operation over the largest possible bandwidth, with large group index, minimal index dispersion, and constant transmission spectrum. Here, we report on the experimental demonstration of to date record high GBP in silicon-based coupled-cavity waveguides (CCWs) operating at telecom wavelengths. Our results rely on novel CCW designs, optimized using a genetic algorithm, and refined nanofabrication processes.	0,1,0,0,0,0
Equilateral $p$-gons in $\mathbb R^d$ and deformed spheres and mod $p$ Fadell-Husseini index	We introduce the concept of $r$-equilateral $m$-gons. We prove the existence of $r$-equilateral $p$-gons in $\mathbb R^d$ if $r<d$ and the existence of equilateral $p$-gons in the image of continuous injective maps $f:S^d\to \mathbb R^{d+1}$. Our ideas are based mainly in the paper of Y. Soibelman \cite{soibelman}, in which the topological Borsuk number of $\mathbb{R}^2$ is calculated by means of topological methods and the paper of P. Blagojević and G. Ziegler \cite{blagojevictetrahedra} where Fadell-Husseini index is used for solving a problem related to the topological Borsuk problem for $\mathbb{R}^3$.	0,0,1,0,0,0
Simplicial Homotopy Theory, Link Homology and Khovanov Homology	The purpose of this note is to point out that simplicial methods and the well-known Dold-Kan construction in simplicial homotopy theory can be fruitfully applied to convert link homology theories into homotopy theories. Dold and Kan prove that there is a functor from the category of chain complexes over a commutative ring with unit to the category of simplicial objects over that ring such that chain homotopic maps go to homotopic maps in the simplicial category. Furthermore, this is an equivalence of categories. In this way, given a link homology theory, we construct a mapping taking link diagrams to a category of simplicial objects such that up to looping or delooping, link diagrams related by Reidemeister moves will give rise to homotopy equivalent simplicial objects, and the homotopy groups of these objects will be equal to the link homology groups of the original link homology theory. The construction is independent of the particular link homology theory. A simplifying point in producing a homotopy simplicial object in relation to a chain complex occurs when the chain complex is itself derived (via face maps) from a simplicial object that satisfies the Kan extension condition. Under these circumstances one can use that simplicial object rather than apply the Dold-Kan functor to the chain complex. We will give examples of this situation in regard to Khovanov homology. We will investigate detailed working out of this correspondence in separate papers. The purpose of this note is to announce the basic relationships for using simplicial methods in this domain. Thus we do more than just quote the Dold-Kan Theorem. We give a review of simplicial theory and we point to specific constructions, particularly in relation to Khovanov homology, that can be used to make simplicial homotopy types directly.	0,0,1,0,0,0
Linking Generative Adversarial Learning and Binary Classification	In this note, we point out a basic link between generative adversarial (GA) training and binary classification -- any powerful discriminator essentially computes an (f-)divergence between real and generated samples. The result, repeatedly re-derived in decision theory, has implications for GA Networks (GANs), providing an alternative perspective on training f-GANs by designing the discriminator loss function.	1,0,0,1,0,0
On Gallai's and Hajós' Conjectures for graphs with treewidth at most 3	A path (resp. cycle) decomposition of a graph $G$ is a set of edge-disjoint paths (resp. cycles) of $G$ that covers the edge set of $G$. Gallai (1966) conjectured that every graph on $n$ vertices admits a path decomposition of size at most $\lfloor (n+1)/2\rfloor$, and Hajós (1968) conjectured that every Eulerian graph on $n$ vertices admits a cycle decomposition of size at most $\lfloor (n-1)/2\rfloor$. Gallai's Conjecture was verified for many classes of graphs. In particular, Lovász (1968) verified this conjecture for graphs with at most one vertex of even degree, and Pyber (1996) verified it for graphs in which every cycle contains a vertex of odd degree. Hajós' Conjecture, on the other hand, was verified only for graphs with maximum degree $4$ and for planar graphs. In this paper, we verify Gallai's and Hajós' Conjectures for graphs with treewidth at most $3$. Moreover, we show that the only graphs with treewidth at most $3$ that do not admit a path decomposition of size at most $\lfloor n/2\rfloor$ are isomorphic to $K_3$ or $K_5-e$. Finally, we use the technique developed in this paper to present new proofs for Gallai's and Hajós' Conjectures for graphs with maximum degree at most $4$, and for planar graphs with girth at least $6$.	1,0,0,0,0,0
Cross-stream migration of a surfactant-laden deformable droplet in a Poiseuille flow	The motion of a viscous deformable droplet suspended in an unbounded Poiseuille flow in the presence of bulk-insoluble surfactants is studied analytically. Assuming the convective transport of fluid and heat to be negligible, we perform a small-deformation perturbation analysis to obtain the droplet migration velocity. The droplet dynamics strongly depends on the distribution of surfactants along the droplet interface, which is governed by the relative strength of convective transport of surfactants as compared with the diffusive transport of surfactants. The present study is focused on the following two limits: (i) when the surfactant transport is dominated by surface diffusion, and (ii) when the surfactant transport is dominated by surface convection. In the first limiting case, it is seen that the axial velocity of the droplet decreases with increase in the advection of the surfactants along the surface. The variation of cross-stream migration velocity, on the other hand, is analyzed over three different regimes based on the ratio of the viscosity of the droplet phase to that of the carrier phase. In the first regime the migration velocity decreases with increase in surface advection of the surfactants although there is no change in direction of droplet migration. For the second regime, the direction of the cross-stream migration of the droplet changes depending on different parameters. In the third regime, the migration velocity is merely affected by any change in the surfactant distribution. For the other limit of higher surface advection in comparison to surface diffusion of the surfactants, the axial velocity of the droplet is found to be independent of the surfactant distribution. However, the cross-stream velocity is found to decrease with increase in non-uniformity in surfactant distribution.	0,1,0,0,0,0
e-Fair: Aggregation in e-Commerce for Exploiting Economies of Scale	In recent years, many new and interesting models of successful online business have been developed, including competitive models such as auctions, where the product price tends to rise, and group-buying, where users cooperate obtaining a dynamic price that tends to go down. We propose the e-fair as a business model for social commerce, where both sellers and buyers are grouped to maximize benefits. e-Fairs extend the group-buying model aggregating demand and supply for price optimization as well as consolidating shipments and optimize withdrawals for guaranteeing additional savings. e-Fairs work upon multiple dimensions: time to aggregate buyers, their geographical distribution, price/quantity curves provided by sellers, and location of withdrawal points. We provide an analytical model for time and spatial optimization and simulate realistic scenarios using both real purchase data from an Italian marketplace and simulated ones. Experimental results demonstrate the potentials offered by e-fairs and show benefits for all the involved actors.	1,0,0,0,0,0
Espresso: Brewing Java For More Non-Volatility with Non-volatile Memory	Fast, byte-addressable non-volatile memory (NVM) embraces both near-DRAM latency and disk-like persistence, which has generated considerable interests to revolutionize system software stack and programming models. However, it is less understood how NVM can be combined with managed runtime like Java virtual machine (JVM) to ease persistence management. This paper proposes Espresso, a holistic extension to Java and its runtime, to enable Java programmers to exploit NVM for persistence management with high performance. Espresso first provides a general persistent heap design called Persistent Java Heap (PJH) to manage persistent data as normal Java objects. The heap is then strengthened with a recoverable mechanism to provide crash consistency for heap metadata. It then provides a new abstraction called Persistent Java Object (PJO) to provide an easy-to-use but safe persistent programming model for programmers to persist application data. The evaluation confirms that Espresso significantly outperforms state-of-art NVM support for Java (i.e., JPA and PCJ) while being compatible to existing data structures in Java programs.	1,0,0,0,0,0
Unstable Footwear as a Speed-Dependent Noise-Based Training Gear to Exercise Inverted Pendulum Motion During Walking	Previous research on unstable footwear has suggested that it may induce plantar mechanical noise during walking. The purpose of this study was to explore whether unstable footwear could be considered as a noise-based training gear to exercise body center of mass (CoM) motion during walking or not. Ground reaction forces were collected among 24 healthy young women walking at speeds between 3 and 6 km h-1 with control running shoes and unstable rocker-bottom shoes. The external mechanical work, the recovery of mechanical energy of the CoM during and within the step cycles, and the phase shift between potential and kinetic energy curves of the CoM were computed. Our findings support the idea that unstable rocker-bottom footwear could serve as a speed-dependent noise- based training gear to exercise CoM motion during walking. At slow speed, it acts as a stochastic resonance or facilitator, whereas at brisk speed it acts as a constraint.	0,1,0,0,0,0
A Classifying Variational Autoencoder with Application to Polyphonic Music Generation	The variational autoencoder (VAE) is a popular probabilistic generative model. However, one shortcoming of VAEs is that the latent variables cannot be discrete, which makes it difficult to generate data from different modes of a distribution. Here, we propose an extension of the VAE framework that incorporates a classifier to infer the discrete class of the modeled data. To model sequential data, we can combine our Classifying VAE with a recurrent neural network such as an LSTM. We apply this model to algorithmic music generation, where our model learns to generate musical sequences in different keys. Most previous work in this area avoids modeling key by transposing data into only one or two keys, as opposed to the 10+ different keys in the original music. We show that our Classifying VAE and Classifying VAE+LSTM models outperform the corresponding non-classifying models in generating musical samples that stay in key. This benefit is especially apparent when trained on untransposed music data in the original keys.	1,0,0,1,0,0
A note on self orbit equivalences of Anosov flows and bundles with fiberwise Anosov flows	We show that a self orbit equivalence of a transitive Anosov flow on a $3$-manifold which is homotopic to identity has to either preserve every orbit or the Anosov flow is $\mathbb{R}$-covered and the orbit equivalence has to be of a specific type. This result shows that one can remove a relatively unnatural assumption in a result of Farrell and Gogolev about the topological rigidity of bundles supporting a fiberwise Anosov flow when the fiber is $3$-dimensional.	0,0,1,0,0,0
A characterization of signed discrete infinitely divisible distributions	In this article, we give some reviews concerning negative probabilities model and quasi-infinitely divisible at the beginning. We next extend Feller's characterization of discrete infinitely divisible distributions to signed discrete infinitely divisible distributions, which are discrete pseudo compound Poisson (DPCP) distributions with connections to the Lévy-Wiener theorem. This is a special case of an open problem which is proposed by Sato(2014), Chaumont and Yor(2012). An analogous result involving characteristic functions is shown for signed integer-valued infinitely divisible distributions. We show that many distributions are DPCP by the non-zero p.g.f. property, such as the mixed Poisson distribution and fractional Poisson process. DPCP has some bizarre properties, and one is that the parameter $\lambda $ in the DPCP class cannot be arbitrarily small.	0,0,1,1,0,0
Phase Transitions in the Pooled Data Problem	In this paper, we study the pooled data problem of identifying the labels associated with a large collection of items, based on a sequence of pooled tests revealing the counts of each label within the pool. In the noiseless setting, we identify an exact asymptotic threshold on the required number of tests with optimal decoding, and prove a phase transition between complete success and complete failure. In addition, we present a novel noisy variation of the problem, and provide an information-theoretic framework for characterizing the required number of tests for general random noise models. Our results reveal that noise can make the problem considerably more difficult, with strict increases in the scaling laws even at low noise levels. Finally, we demonstrate similar behavior in an approximate recovery setting, where a given number of errors is allowed in the decoded labels.	1,0,0,1,0,0
Shape-dependence of the barrier for skyrmions on a two-lane racetrack	Single magnetic skyrmions are localized whirls in the magnetization with an integer winding number. They have been observed on nano-meter scales up to room temperature in multilayer structures. Due to their small size, topological winding number, and their ability to be manipulated by extremely tiny forces, they are often called interesting candidates for future memory devices. The two-lane racetrack has to exhibit two lanes that are separated by an energy barrier. The information is then encoded in the position of a skyrmion which is located in one of these close-by lanes. The artificial barrier between the lanes can be created by an additional nanostrip on top of the track. Here we study the dependence of the potential barrier on the shape of the additional nanostrip, calculating the potentials for a rectangular, triangular, and parabolic cross section, as well as interpolations between the first two. We find that a narrow barrier is always repulsive and that the height of the potential strongly depends on the shape of the nanostrip, whereas the shape of the potential is more universal. We finally show that the shape-dependence is redundant for possible applications.	0,1,0,0,0,0
Combinatorics of involutive divisions	The classical involutive division theory by Janet decomposes in the same way both the ideal and the escalier. The aim of this paper, following Janet's approach, is to discuss the combinatorial properties of involutive divisions, when defined on the set of all terms in a fixed degree D, postponing the discussion of ideal membership and related test. We adapt the theory by Gerdt and Blinkov, introducing relative involutive divisions and then, given a complete description of the combinatorial structure of a relative involutive division, we turn our attention to the problem of membership. In order to deal with this problem, we introduce two graphs as tools, one is strictly related to Seiler's L-graph, whereas the second generalizes it, to cover the case of "non-continuous" (in the sense of Gerdt-Blinkov) relative involutive divisions. Indeed, given an element in the ideal (resp. escalier), walking backwards (resp. forward) in the graph, we can identify all the other generators of the ideal (resp. elements of degree D in the escalier).	0,0,1,0,0,0
Statistical Latent Space Approach for Mixed Data Modelling and Applications	The analysis of mixed data has been raising challenges in statistics and machine learning. One of two most prominent challenges is to develop new statistical techniques and methodologies to effectively handle mixed data by making the data less heterogeneous with minimum loss of information. The other challenge is that such methods must be able to apply in large-scale tasks when dealing with huge amount of mixed data. To tackle these challenges, we introduce parameter sharing and balancing extensions to our recent model, the mixed-variate restricted Boltzmann machine (MV.RBM) which can transform heterogeneous data into homogeneous representation. We also integrate structured sparsity and distance metric learning into RBM-based models. Our proposed methods are applied in various applications including latent patient profile modelling in medical data analysis and representation learning for image retrieval. The experimental results demonstrate the models perform better than baseline methods in medical data and outperform state-of-the-art rivals in image dataset.	1,0,0,1,0,0
Clustering High Dimensional Dynamic Data Streams	We present data streaming algorithms for the $k$-median problem in high-dimensional dynamic geometric data streams, i.e. streams allowing both insertions and deletions of points from a discrete Euclidean space $\{1, 2, \ldots \Delta\}^d$. Our algorithms use $k \epsilon^{-2} poly(d \log \Delta)$ space/time and maintain with high probability a small weighted set of points (a coreset) such that for every set of $k$ centers the cost of the coreset $(1+\epsilon)$-approximates the cost of the streamed point set. We also provide algorithms that guarantee only positive weights in the coreset with additional logarithmic factors in the space and time complexities. We can use this positively-weighted coreset to compute a $(1+\epsilon)$-approximation for the $k$-median problem by any efficient offline $k$-median algorithm. All previous algorithms for computing a $(1+\epsilon)$-approximation for the $k$-median problem over dynamic data streams required space and time exponential in $d$. Our algorithms can be generalized to metric spaces of bounded doubling dimension.	1,0,0,0,0,0
Contemporary facets of business successes among leading companies, operating in Bulgaria	The current article unveils and analyzes some important factors, influencing diversity in strategic decision-making approaches in local companies. Researcher's attention is oriented to survey important characteristics of the strategic moves, undertaken by leading companies in Bulgaria.	0,0,0,0,0,1
Demo Abstract: CDMA-based IoT Services with Shared Band Operation of LTE in 5G	With the vision of deployment of massive Internet-of-Things (IoTs) in 5G network, existing 4G network and protocols are inefficient to handle sporadic IoT traffic with requirements of low-latency, low control overhead and low power. To suffice these requirements, we propose a design of a PHY/MAC layer using Software Defined Radios (SDRs) that is backward compatible with existing OFDM based LTE protocols and supports CDMA based transmissions for low power IoT devices as well. This demo shows our implemented system based on that design and the viability of the proposal under different network scenarios.	1,0,0,0,0,0
Adversarial examples for generative models	We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.	0,0,0,1,0,0
Robust Bayes-Like Estimation: Rho-Bayes estimation	We consider the problem of estimating the joint distribution $P$ of $n$ independent random variables within the Bayes paradigm from a non-asymptotic point of view. Assuming that $P$ admits some density $s$ with respect to a given reference measure, we consider a density model $\overline S$ for $s$ that we endow with a prior distribution $\pi$ (with support $\overline S$) and we build a robust alternative to the classical Bayes posterior distribution which possesses similar concentration properties around $s$ whenever it belongs to the model $\overline S$. Furthermore, in density estimation, the Hellinger distance between the classical and the robust posterior distributions tends to 0, as the number of observations tends to infinity, under suitable assumptions on the model and the prior, provided that the model $\overline S$ contains the true density $s$. However, unlike what happens with the classical Bayes posterior distribution, we show that the concentration properties of this new posterior distribution are still preserved in the case of a misspecification of the model, that is when $s$ does not belong to $\overline S$ but is close enough to it with respect to the Hellinger distance.	0,0,1,1,0,0
Portfolio Optimization in Fractional and Rough Heston Models	We consider a fractional version of the Heston volatility model which is inspired by [16]. Within this model we treat portfolio optimization problems for power utility functions. Using a suitable representation of the fractional part, followed by a reasonable approximation we show that it is possible to cast the problem into the classical stochastic control framework. This approach is generic for fractional processes. We derive explicit solutions and obtain as a by-product the Laplace transform of the integrated volatility. In order to get rid of some undesirable features we introduce a new model for the rough path scenario which is based on the Marchaud fractional derivative. We provide a numerical study to underline our results.	0,0,0,0,0,1
Accelerated Primal-Dual Proximal Block Coordinate Updating Methods for Constrained Convex Optimization	Block Coordinate Update (BCU) methods enjoy low per-update computational complexity because every time only one or a few block variables would need to be updated among possibly a large number of blocks. They are also easily parallelized and thus have been particularly popular for solving problems involving large-scale dataset and/or variables. In this paper, we propose a primal-dual BCU method for solving linearly constrained convex program in multi-block variables. The method is an accelerated version of a primal-dual algorithm proposed by the authors, which applies randomization in selecting block variables to update and establishes an $O(1/t)$ convergence rate under weak convexity assumption. We show that the rate can be accelerated to $O(1/t^2)$ if the objective is strongly convex. In addition, if one block variable is independent of the others in the objective, we then show that the algorithm can be modified to achieve a linear rate of convergence. The numerical experiments show that the accelerated method performs stably with a single set of parameters while the original method needs to tune the parameters for different datasets in order to achieve a comparable level of performance.	1,0,1,1,0,0
High Order Hierarchical Divergence-free Constrained Transport $H(div)$ Finite Element Method for Magnetic Induction Equation	In this paper, we will use the interior functions of an hierarchical basis for high order $BDM_p$ elements to enforce the divergence-free condition of a magnetic field $B$ approximated by the H(div) $BDM_p$ basis. The resulting constrained finite element method can be used to solve magnetic induction equation in MHD equations. The proposed procedure is based on the fact that the scalar $(p-1)$-th order polynomial space on each element can be decomposed as an orthogonal sum of the subspace defined by the divergence of the interior functions of the $p$-th order $BDM_p$ basis and the constant function. Therefore, the interior functions can be used to remove element-wise all higher order terms except the constant in the divergence error of the finite element solution of $B$-field. The constant terms from each element can be then easily corrected using a first order H(div) basis globally. Numerical results for a 3-D magnetic induction equation show the effectiveness of the proposed method in enforcing divergence-free condition of the magnetic field.	0,0,1,0,0,0
Heroes and Zeroes: Predicting the Impact of New Video Games on Twitch.tv	Video games and the playing thereof have been a fixture of American culture since their introduction in the arcades of the 1980s. However, it was not until the recent proliferation of broadband connections robust and fast enough to handle live video streaming that players of video games have transitioned from a content consumer role to a content producer role. Simultaneously, the rise of social media has revealed how interpersonal connections drive user engagement and interest. In this work, we discuss the recent proliferation of video game streaming, particularly on Twitch.tv, analyze trends and patterns in video game viewing, and develop predictive models for determining if a new game will have substantial impact on the streaming ecosystem.	1,0,0,0,0,0
Automatic Generation of Typographic Font from a Small Font Subset	This paper addresses the automatic generation of a typographic font from a subset of characters. Specifically, we use a subset of a typographic font to extrapolate additional characters. Consequently, we obtain a complete font containing a number of characters sufficient for daily use. The automated generation of Japanese fonts is in high demand because a Japanese font requires over 1,000 characters. Unfortunately, professional typographers create most fonts, resulting in significant financial and time investments for font generation. The proposed method can be a great aid for font creation because designers do not need to create the majority of the characters for a new font. The proposed method uses strokes from given samples for font generation. The strokes, from which we construct characters, are extracted by exploiting a character skeleton dataset. This study makes three main contributions: a novel method of extracting strokes from characters, which is applicable to both standard fonts and their variations; a fully automated approach for constructing characters; and a selection method for sample characters. We demonstrate our proposed method by generating 2,965 characters in 47 fonts. Objective and subjective evaluations verify that the generated characters are similar to handmade characters.	1,0,0,0,0,0
Characterizing the ionospheric current pattern response to southward and northward IMF turnings with dynamical SuperMAG correlation networks	We characterize the response of the quiet time (no substorms or storms) large-scale ionospheric transient equivalent currents to north-south and south-north IMF turnings by using a dynamical network of ground-based magnetometers. Canonical correlation between all pairs of SuperMAG magnetometer stations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\circ}$) is used to establish the extent of near-simultaneous magnetic response between regions of magnetic local time-MLAT. Parameters and maps that describe spatial-temporal correlation are used to characterize the system and its response to the turnings aggregated over several hundred events. We find that regions that experience large increases in correlation post turning coincide with typical locations of a two-cell convection system and are influenced by the interplanetary magnetic field $\mathit{B}_{y}$. The time between the turnings reaching the magnetopause and a network response is found to be $\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the nightside.	0,1,0,0,0,0
LQG Control and Sensing Co-design	Linear-Quadratic-Gaussian (LQG) control is concerned with the design of an optimal controller and estimator for linear Gaussian systems with imperfect state information. Standard LQG control assumes the set of sensor measurements to be fed to the estimator to be given. However, in many problems in networked systems and robotics, one is interested in designing a suitable set of sensors for LQG control. In this paper, we introduce the LQG control and sensing co-design problem, where one has to jointly design a suitable sensing, estimation, and control policy. We consider two dual instances of the co-design problem: the sensing-constrained LQG control problem, where the design maximizes the control performance subject to sensing constraints, and the minimum-sensing LQG control, where the design minimizes the amount of sensing subject to performance constraints. We focus on the case in which the sensing design has to be selected among a finite set of possible sensing modalities, where each modality is associated with a different cost. While we observe that the computation of the optimal sensing design is intractable in general, we present the first scalable LQG co-design algorithms to compute near-optimal policies with provable sub-optimality guarantees. To this end, (i) we show that a separation principle holds, which partially decouples the design of sensing, estimation, and control; (ii) we frame LQG co-design as the optimization of (approximately) supermodular set functions; (iii) we develop novel algorithms to solve the resulting optimization problems; (iv) we prove original results on the performance of these algorithms and establish connections between their sub-optimality gap and control-theoretic quantities. We conclude the paper by discussing two practical applications of the co-design problem, namely, sensing-constrained formation control and resource-constrained robot navigation.	1,0,0,0,0,0
A Dynamic Boosted Ensemble Learning Method Based on Random Forest	We propose a dynamic boosted ensemble learning method based on random forest (DBRF), a novel ensemble algorithm that incorporates the notion of hard example mining into Random Forest (RF) and thus combines the high accuracy of Boosting algorithm with the strong generalization of Bagging algorithm. Specifically, we propose to measure the quality of each leaf node of every decision tree in the random forest to determine hard examples. By iteratively training and then removing easy examples from training data, we evolve the random forest to focus on hard examples dynamically so as to learn decision boundaries better. Data can be cascaded through these random forests learned in each iteration in sequence to generate predictions, thus making RF deep. We also propose to use evolution mechanism and smart iteration mechanism to improve the performance of the model. DBRF outperforms RF on three UCI datasets and achieved state-of-the-art results compared to other deep models. Moreover, we show that DBRF is also a new way of sampling and can be very useful when learning from imbalanced data.	0,0,0,1,0,0
Exact results for directed random networks that grow by node duplication	We present exact analytical results for the degree distribution and for the distribution of shortest path lengths (DSPL) in a directed network model that grows by node duplication. Such models are useful in the study of the structure and growth dynamics of gene regulatory and scientific citation networks. Starting from an initial seed network, at each time step a random node, referred to as a mother node, is selected for duplication. Its daughter node is added to the network and duplicates, with probability p, each one of the outgoing links of the mother node. In addition, the daughter node forms a directed link to the mother node itself. Thus, the model is referred to as the corded directed-node-duplication (DND) model. We obtain analytical results for the in-degree distribution, $P_t(K_{in})$, and for the out-degree distribution, $P_t(K_{out})$, of the network at time t. It is found that the in-degrees follow a shifted power-law distribution, so the network is asymptotically scale free. In contrast, the out-degree distribution is a narrow distribution, that converges to a Poisson distribution in the sparse limit and to a Gaussian distribution in the dense limit. Using these distributions we calculate the mean degree, $\langle K_{in} \rangle_t = \langle K_{out} \rangle_t$. To calculate the DSPL we derive a master equation for the time evolution of the probability $P_t(L=\ell)$, $\ell=1,2,\dots$, that for two nodes, i and j, selected randomly at time t, the shortest path from i to j is of length $\ell$. Solving the master equation, we obtain a closed form expression for $P_t(L=\ell)$. It is found that the DSPL at time t consists of a convolution of the initial DSPL, $P_0(L=\ell)$, with a Poisson distribution and a sum of Poisson distributions. The mean distance, $<L>_t$, is found to depend logarithmically on the network size, $N_t$, namely the corded DND network is a small-world network.	1,0,0,0,0,0
Parity-Forbidden Transitions and Their Impacts on the Optical Absorption Properties of Lead-Free Metal Halide Perovskites and Double Perovskites	Using density-functional theory calculations, we analyze the optical absorption properties of lead (Pb)-free metal halide perovskites (AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or monovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent metal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is not Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions because of their indirect bandgap nature. Among the nine possible types of Pb-free metal halide double perovskites, six have direct bandgaps. Of these six types, four show inversion symmetry-induced parity-forbidden or weak transitions between band edges, making them not ideal for thin-film solar cell application. Only one type of Pb-free double perovskite shows optical absorption and electronic properties suitable for solar cell applications, namely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide important insights for designing new metal halide perovskites and double perovskites for optoelectronic applications.	0,1,0,0,0,0
Incidence systems on Cartesian powers of algebraic curves	We show that a reduct of the Zariski structure of an algebraic curve which is not locally modular interprets a field, answering a question of Zilber's.	0,0,1,0,0,0
Banach synaptic algebras	Using a representation theorem of Erik Alfsen, Frederic Schultz, and Erling Stormer for special JB-algebras, we prove that a synaptic algebra is norm complete (i.e., Banach) if and only if it is isomorphic to the self-adjoint part of a Rickart C*-algebra. Also, we give conditions on a Banach synaptic algebra that are equivalent to the condition that it is isomorphic to the self-adjoint part of an AW*-algebra. Moreover, we study some relationships between synaptic algebras and so-called generalized Hermitian algebras.	0,0,1,0,0,0
RVP-FLMS : A Robust Variable Power Fractional LMS Algorithm	In this paper, we propose an adaptive framework for the variable power of the fractional least mean square (FLMS) algorithm. The proposed algorithm named as robust variable power FLMS (RVP-FLMS) dynamically adapts the fractional power of the FLMS to achieve high convergence rate with low steady state error. For the evaluation purpose, the problems of system identification and channel equalization are considered. The experiments clearly show that the proposed approach achieves better convergence rate and lower steady-state error compared to the FLMS. The MATLAB code for the related simulation is available online at this https URL.	0,0,1,1,0,0
Representations of weakly multiplicative arithmetic matroids are unique	An arithmetic matroid is weakly multiplicative if the multiplicity of at least one of its bases is equal to the product of the multiplicities of its elements. We show that if such an arithmetic matroid can be represented by an integer matrix, then this matrix is uniquely determined. This implies that the integer cohomology ring of a centred toric arrangement whose arithmetic matroid is weakly multiplicative is determined by its poset of layers. This partially answers a question asked by Callegaro-Delucchi.	0,0,1,0,0,0
Uniqueness of stable capillary hypersurfaces in a ball	In this paper we prove that any immersed stable capillary hypersurfaces in a ball in space forms are totally umbilical. This solves completely a long-standing open problem. In the proof one of crucial ingredients is a new Minkowski type formula. We also prove a Heintze-Karcher-Ros type inequality for hypersurfaces in a ball, which, together with the new Minkowski formula, yields a new proof of Alexandrov's Theorem for embedded CMC hypersurfaces in a ball with free boundary.	0,0,1,0,0,0
Time-frequency analysis of ship wave patterns in shallow water: modelling and experiments	A spectrogram of a ship wake is a heat map that visualises the time-dependent frequency spectrum of surface height measurements taken at a single point as the ship travels by. Spectrograms are easy to compute and, if properly interpreted, have the potential to provide crucial information about various properties of the ship in question. Here we use geometrical arguments and analysis of an idealised mathematical model to identify features of spectrograms, concentrating on the effects of a finite-depth channel. Our results depend heavily on whether the flow regime is subcritical or supercritical. To support our theoretical predictions, we compare with data taken from experiments we conducted in a model test basin using a variety of realistic ship hulls. Finally, we note that vessels with a high aspect ratio appear to produce spectrogram data that contains periodic patterns. We can reproduce this behaviour in our mathematical model by using a so-called two-point wavemaker. These results highlight the role of wave interference effects in spectrograms of ship wakes.	0,1,0,0,0,0
Time-Optimal Path Tracking via Reachability Analysis	Given a geometric path, the Time-Optimal Path Tracking problem consists in finding the control strategy to traverse the path time-optimally while regulating tracking errors. A simple yet effective approach to this problem is to decompose the controller into two components: (i)~a path controller, which modulates the parameterization of the desired path in an online manner, yielding a reference trajectory; and (ii)~a tracking controller, which takes the reference trajectory and outputs joint torques for tracking. However, there is one major difficulty: the path controller might not find any feasible reference trajectory that can be tracked by the tracking controller because of torque bounds. In turn, this results in degraded tracking performances. Here, we propose a new path controller that is guaranteed to find feasible reference trajectories by accounting for possible future perturbations. The main technical tool underlying the proposed controller is Reachability Analysis, a new method for analyzing path parameterization problems. Simulations show that the proposed controller outperforms existing methods.	1,0,0,0,0,0
A weak law of large numbers for estimating the correlation in bivariate Brownian semistationary processes	This article presents various weak laws of large numbers for the so-called realised covariation of a bivariate stationary stochastic process which is not a semimartingale. More precisely, we consider two cases: Bivariate moving average processes with stochastic correlation and bivariate Brownian semistationary processes with stochastic correlation. In both cases, we can show that the (possibly scaled) realised covariation converges to the integrated (possibly volatility modulated) stochastic correlation process.	0,0,1,1,0,0
GCN-GAN: A Non-linear Temporal Link Prediction Model for Weighted Dynamic Networks	In this paper, we generally formulate the dynamics prediction problem of various network systems (e.g., the prediction of mobility, traffic and topology) as the temporal link prediction task. Different from conventional techniques of temporal link prediction that ignore the potential non-linear characteristics and the informative link weights in the dynamic network, we introduce a novel non-linear model GCN-GAN to tackle the challenging temporal link prediction task of weighted dynamic networks. The proposed model leverages the benefits of the graph convolutional network (GCN), long short-term memory (LSTM) as well as the generative adversarial network (GAN). Thus, the dynamics, topology structure and evolutionary patterns of weighted dynamic networks can be fully exploited to improve the temporal link prediction performance. Concretely, we first utilize GCN to explore the local topological characteristics of each single snapshot and then employ LSTM to characterize the evolving features of the dynamic networks. Moreover, GAN is used to enhance the ability of the model to generate the next weighted network snapshot, which can effectively tackle the sparsity and the wide-value-range problem of edge weights in real-life dynamic networks. To verify the model's effectiveness, we conduct extensive experiments on four datasets of different network systems and application scenarios. The experimental results demonstrate that our model achieves impressive results compared to the state-of-the-art competitors.	1,0,0,0,0,0
Learning Unsupervised Learning Rules	A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this goal is approached by minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise incidentally. In this work, we propose instead to directly target a later desired task by meta-learning an unsupervised learning rule, which leads to representations useful for that task. Here, our desired task (meta-objective) is the performance of the representation on semi-supervised classification, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations that perform well under this meta-objective. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to novel neural network architectures. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.	0,0,0,1,0,0
Mathematical model of immune response to hepatitis B	A new detailed mathematical model for dynamics of immune response to hepatitis B is proposed, which takes into account contributions from innate and adaptive immune responses, as well as cytokines. Stability analysis of different steady states is performed to identify parameter regions where the model exhibits clearance of infection, maintenance of a chronic infection, or periodic oscillations. Effects of nucleoside analogues and interferon treatments are analysed, and the critical drug efficiency is determined.	0,0,0,0,1,0
Wave and Dirac equations on manifolds	We review some recent results on geometric equations on Lorentzian manifolds such as the wave and Dirac equations. This includes well-posedness and stability for various initial value problems, as well as results on the structure of these equations on black-hole spacetimes (in particular, on the Kerr solution), the index theorem for hyperbolic Dirac operators and properties of the class of Green-hyperbolic operators.	0,0,1,0,0,0
Software Distribution Transparency and Auditability	A large user base relies on software updates provided through package managers. This provides a unique lever for improving the security of the software update process. We propose a transparency system for software updates and implement it for a widely deployed Linux package manager, namely APT. Our system is capable of detecting targeted backdoors without producing overhead for maintainers. In addition, in our system, the availability of source code is ensured, the binding between source and binary code is verified using reproducible builds, and the maintainer responsible for distributing a specific package can be identified. We describe a novel "hidden version" attack against current software transparency systems and propose as well as integrate a suitable defense. To address equivocation attacks by the transparency log server, we introduce tree root cross logging, where the log's Merkle tree root is submitted into a separately operated log server. This significantly relaxes the inter-operator cooperation requirements compared to other systems. Our implementation is evaluated by replaying over 3000 updates of the Debian operating system over the course of two years, demonstrating its viability and identifying numerous irregularities.	1,0,0,0,0,0
Converging Shock Flows for a Mie-Grüneisen Equation of State	Previous work has shown that the one-dimensional (1D) inviscid compressible flow (Euler) equations admit a wide variety of scale-invariant solutions (including the famous Noh, Sedov, and Guderley shock solutions) when the included equation of state (EOS) closure model assumes a certain scale-invariant form. However, this scale-invariant EOS class does not include even simple models used for shock compression of crystalline solids, including many broadly applicable representations of Mie-Grüneisen EOS. Intuitively, this incompatibility naturally arises from the presence of multiple dimensional scales in the Mie-Grüneisen EOS, which are otherwise absent from scale-invariant models that feature only dimensionless parameters (such as the adiabatic index in the ideal gas EOS). The current work extends previous efforts intended to rectify this inconsistency, by using a scale-invariant EOS model to approximate a Mie- Grüneisen EOS form. To this end, the adiabatic bulk modulus for the Mie-Grüneisen EOS is constructed, and its key features are used to motivate the selection of a scale-invariant approximation form. The remaining surrogate model parameters are selected through enforcement of the Rankine-Hugoniot jump conditions for an infinitely strong shock in a Mie-Grüneisen material. Finally, the approximate EOS is used in conjunction with the 1D inviscid Euler equations to calculate a semi-analytical, Guderley-like imploding shock solution in a metal sphere, and to determine if and when the solution may be valid for the underlying Mie-Grüneisen EOS.	0,1,0,0,0,0
A Theory of Solvability for Lossless Power Flow Equations -- Part I: Fixed-Point Power Flow	This two-part paper details a theory of solvability for the power flow equations in lossless power networks. In Part I, we derive a new formulation of the lossless power flow equations, which we term the fixed-point power flow. The model is stated for both meshed and radial networks, and is parameterized by several graph-theoretic matrices -- the power network stiffness matrices -- which quantify the internal coupling strength of the network. The model leads immediately to an explicit approximation of the high-voltage power flow solution. For standard test cases, we find that iterates of the fixed-point power flow converge rapidly to the high-voltage power flow solution, with the approximate solution yielding accurate predictions near base case loading. In Part II, we leverage the fixed-point power flow to study power flow solvability, and for radial networks we derive conditions guaranteeing the existence and uniqueness of a high-voltage power flow solution. These conditions (i) imply exponential convergence of the fixed-point power flow iteration, and (ii) properly generalize the textbook two-bus system results.	0,0,1,0,0,0
Simulating polaron biophysics with Rydberg atoms	Transport of excitations along proteins can be formulated in a quantum physics context, based on the periodicity and vibrational modes of the structures. Exact solutions are very challenging to obtain on classical computers, however, approximate solutions based on the Davydov ansatz have demonstrated the possibility of stabilized solitonic excitations along the protein. We propose an alternative study based on a chain of ultracold atoms. We investigate the experimental parameters to control such a quantum simulator based on dressed Rydberg atoms. We show that there is a feasible range of parameters where a quantum simulator can directly mimic the Davydov equations and their solutions. Such a quantum simulator opens up new directions for the study of transport phenomena in a biophysical context.	0,1,0,0,0,0
A GNS construction of three-dimensional abelian Dijkgraaf-Witten theories	We give a detailed account of the so-called "universal construction" that aims to extend invariants of closed manifolds, possibly with additional structure, to topological field theories and show that it amounts to a generalization of the GNS construction. We apply this construction to an invariant defined in terms of the groupoid cardinality of groupoids of bundles to recover Dijkgraaf-Witten theories, including the vector spaces obtained as a linearization of spaces of principal bundles.	0,0,1,0,0,0
Democratizing Design for Future Computing Platforms	Information and communications technology can continue to change our world. These advances will partially depend upon designs that synergistically combine software with specialized hardware. Today open-source software incubates rapid software-only innovation. The government can unleash software-hardware innovation with programs to develop open hardware components, tools, and design flows that simplify and reduce the cost of hardware design. Such programs will speed development for startup companies, established industry leaders, education, scientific research, and for government intelligence and defense platforms.	1,0,0,0,0,0
Minimax Estimation of the $L_1$ Distance	We consider the problem of estimating the $L_1$ distance between two discrete probability measures $P$ and $Q$ from empirical data in a nonasymptotic and large alphabet setting. When $Q$ is known and one obtains $n$ samples from $P$, we show that for every $Q$, the minimax rate-optimal estimator with $n$ samples achieves performance comparable to that of the maximum likelihood estimator (MLE) with $n\ln n$ samples. When both $P$ and $Q$ are unknown, we construct minimax rate-optimal estimators whose worst case performance is essentially that of the known $Q$ case with $Q$ being uniform, implying that $Q$ being uniform is essentially the most difficult case. The \emph{effective sample size enlargement} phenomenon, identified in Jiao \emph{et al.} (2015), holds both in the known $Q$ case for every $Q$ and the $Q$ unknown case. However, the construction of optimal estimators for $\|P-Q\|_1$ requires new techniques and insights beyond the approximation-based method of functional estimation in Jiao \emph{et al.} (2015).	0,0,1,1,0,0
On the Complexity of Approximating Wasserstein Barycenter	We study the complexity of approximating Wassertein barycenter of $m$ discrete measures, or histograms of size $n$ by contrasting two alternative approaches, both using entropic regularization. The first approach is based on the Iterative Bregman Projections (IBP) algorithm for which our novel analysis gives a complexity bound proportional to $\frac{mn^2}{\varepsilon^2}$ to approximate the original non-regularized barycenter. Using an alternative accelerated-gradient-descent-based approach, we obtain a complexity proportional to $\frac{mn^{2.5}}{\varepsilon} $. As a byproduct, we show that the regularization parameter in both approaches has to be proportional to $\varepsilon$, which causes instability of both algorithms when the desired accuracy is high. To overcome this issue, we propose a novel proximal-IBP algorithm, which can be seen as a proximal gradient method, which uses IBP on each iteration to make a proximal step. We also consider the question of scalability of these algorithms using approaches from distributed optimization and show that the first algorithm can be implemented in a centralized distributed setting (master/slave), while the second one is amenable to a more general decentralized distributed setting with an arbitrary network topology.	1,0,0,0,0,0
Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory	In meta-learning an agent extracts knowledge from observed tasks, aiming to facilitate learning of novel future tasks. Under the assumption that future tasks are 'related' to previous tasks, the accumulated knowledge should be learned in a way which captures the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of new tasks. We present a framework for meta-learning that is based on generalization error bounds, allowing us to extend various PAC-Bayes bounds to meta-learning. Learning takes place through the construction of a distribution over hypotheses based on the observed tasks, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting an experience-dependent prior for novel tasks. We develop a gradient-based algorithm which minimizes an objective function derived from the bounds and demonstrate its effectiveness numerically with deep neural networks. In addition to establishing the improved performance available through meta-learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.	1,0,0,1,0,0
Honey from the Hives: A Theoretical and Computational Exploration of Combinatorial Hives	In the first half of this manuscript, we begin with a brief review of combinatorial hives as introduced by Knutson and Tao, and focus on a conjecture by Danilov and Koshevoy for generating such a hive from Hermitian matrix pairs through an optimization scheme. We examine a proposal by Appleby and Whitehead in the spirit of this conjecture and analytically elucidate an obstruction in their construction for guaranteeing hive generation, while detailing stronger conditions under which we can produce hives with almost certain probability. We provide the first mapping of this prescription onto a practical algorithmic space that enables us to produce affirming computational results and open a new area of research into the analysis of the random geometries and curvatures of hive surfaces from select matrix ensembles. The second part of this manuscript concerns Littlewood-Richardson coefficients and methods of estimating them from the hive construction. We illustrate experimental confirmation of two numerical algorithms that we provide as tools for the community: one as a rounded estimator on the continuous hive polytope volume following a proposal by Narayanan, and the other as a novel construction using a coordinate hit-and-run on the hive lattice itself. We compare the advantages of each, and include numerical results on their accuracies for some tested cases.	0,0,0,1,0,0
PatternListener: Cracking Android Pattern Lock Using Acoustic Signals	Pattern lock has been widely used for authentication to protect user privacy on mobile devices (e.g., smartphones and tablets). Given its pervasive usage, the compromise of pattern lock could lead to serious consequences. Several attacks have been constructed to crack the lock. However, these approaches require the attackers to either be physically close to the target device or be able to manipulate the network facilities (e.g., WiFi hotspots) used by the victims. Therefore, the effectiveness of the attacks is significantly impacted by the environment of mobile devices. Also, these attacks are not scalable since they cannot easily infer unlock patterns of a large number of devices. Motivated by an observation that fingertip motions on the screen of a mobile device can be captured by analyzing surrounding acoustic signals on it, we propose PatternListener, a novel acoustic attack that cracks pattern lock by analyzing imperceptible acoustic signals reflected by the fingertip. It leverages speakers and microphones of the victim's device to play imperceptible audio and record the acoustic signals reflected by the fingertip. In particular, it infers each unlock pattern by analyzing individual lines that compose the pattern and are the trajectories of the fingertip. We propose several algorithms to construct signal segments according to the captured signals for each line and infer possible candidates of each individual line according to the signal segments. Finally, we map all line candidates into grid patterns and thereby obtain the candidates of the entire unlock pattern. We implement a PatternListener prototype by using off-the-shelf smartphones and thoroughly evaluate it using 130 unique patterns. The real experimental results demonstrate that PatternListener can successfully exploit over 90% patterns within five attempts.	1,0,0,0,0,0
Second-order and local characteristics of network intensity functions	The last decade has witnessed an increase of interest in the spatial analysis of structured point patterns over networks whose analysis is challenging because of geometrical complexities and unique methodological problems. In this context, it is essential to incorporate the network specificity into the analysis as the locations of events are restricted to areas covered by line segments. Relying on concepts originating from graph theory, we extend the notions of first-order network intensity functions to second-order and local network intensity functions. We consider two types of local indicators of network association functions which can be understood as adaptations of the primary ideas of local analysis on the plane. We develop the node-wise and cross-hierarchical type of local functions. A real dataset on urban disturbances is also presented.	0,0,0,1,0,0
Unifying the micro and macro properties of AGN feeding and feedback	We unify the feeding and feedback of supermassive black holes with the global properties of galaxies, groups, and clusters, by linking for the first time the physical mechanical efficiency at the horizon and Mpc scale. The macro hot halo is tightly constrained by the absence of overheating and overcooling as probed by X-ray data and hydrodynamic simulations ($\varepsilon_{\rm BH} \simeq$ 10$^{-3}\, T_{\rm x,7.4}$). The micro flow is shaped by general relativistic effects tracked by state-of-the-art GR-RMHD simulations ($\varepsilon_\bullet \simeq$ 0.03). The SMBH properties are tied to the X-ray halo temperature $T_{\rm x}$, or related cosmic scaling relation (as $L_{\rm x}$). The model is minimally based on first principles, as conservation of energy and mass recycling. The inflow occurs via chaotic cold accretion (CCA), the rain of cold clouds condensing out of the quenched cooling flow and recurrently funneled via inelastic collisions. Within 100s gravitational radii, the accretion energy is transformed into ultrafast 10$^4$ km s$^{-1}$ outflows (UFOs) ejecting most of the inflowing mass. At larger radii the energy-driven outflow entrains progressively more mass: at roughly kpc scale, the velocities of the hot/warm/cold outflows are a few 10$^3$, 1000, 500 km s$^{-1}$, with median mass rates ~10, 100, several 100 M$_\odot$ yr$^{-1}$, respectively. The unified CCA model is consistent with the observations of nuclear UFOs, and ionized, neutral, and molecular macro outflows. We provide step-by-step implementation for subgrid simulations, (semi)analytic works, or observational interpretations which require self-regulated AGN feedback at coarse scales, avoiding the a-posteriori fine-tuning of efficiencies.	0,1,0,0,0,0
Spatial point processes intensity estimation with a diverging number of covariates	Feature selection procedures for spatial point processes parametric intensity estimation have been recently developed since more and more applications involve a large number of covariates. In this paper, we investigate the setting where the number of covariates diverges as the domain of observation increases. In particular, we consider estimating equations based on Campbell theorems derived from Poisson and logistic regression likelihoods regularized by a general penalty function. We prove that, under some conditions, the consistency, the sparsity, and the asymptotic normality are valid for such a setting. We support the theoretical results by numerical ones obtained from simulation experiments and an application to forestry datasets.	0,0,1,1,0,0
Embedding for bulk systems using localized atomic orbitals	We present an embedding approach for semiconductors and insulators based on or- bital rotations in the space of occupied Kohn-Sham orbitals. We have implemented our approach in the popular VASP software package. We demonstrate its power for defect structures in silicon and polaron formation in titania, two challenging cases for conventional Kohn-Sham density functional theory.	0,1,0,0,0,0
Sorting Phenomena in a Mathematical Model For Two Mutually Attracting/Repelling Species	Macroscopic models for systems involving diffusion, short-range repulsion, and long-range attraction have been studied extensively in the last decades. In this paper we extend the analysis to a system for two species interacting with each other according to different inner- and intra-species attractions. Under suitable conditions on this self- and crosswise attraction an interesting effect can be observed, namely phase separation into neighbouring regions, each of which contains only one of the species. We prove that the intersection of the support of the stationary solutions of the continuum model for the two species has zero Lebesgue measure, while the support of the sum of the two densities is simply connected. Preliminary results indicate the existence of phase separation, i.e. spatial sorting of the different species. A detailed analysis in one spatial dimension follows. The existence and shape of segregated stationary solutions is shown via the Krein-Rutman theorem. Moreover, for small repulsion/nonlinear diffusion, also uniqueness of these stationary states is proved.	0,0,1,0,0,0
Variants of RMSProp and Adagrad with Logarithmic Regret Bounds	Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.	1,0,0,1,0,0
The Dependence of the Mass-Metallicity Relation on Large Scale Environment	We examine the relation between gas-phase oxygen abundance and stellar mass---the MZ relation---as a function of the large scale galaxy environment parameterized by the local density. The dependence of the MZ relation on the environment is small. The metallicity where the MZ relation saturates and the slope of the MZ relation are both independent of the local density. The impact of the large scale environment is completely parameterized by the anti-correlation between local density and the turnover stellar mass where the MZ relation begins to saturate. Analytical modeling suggests that the anti-correlation between the local density and turnover stellar mass is a consequence of a variation in the gas content of star-forming galaxies. Across $\sim1$ order of magnitude in local density, the gas content at a fixed stellar mass varies by $\sim5\%$. Variation of the specific star formation rate with environment is consistent with this interpretation. At a fixed stellar mass, galaxies in low density environments have lower metallicities because they are slightly more gas-rich than galaxies in high density environments. Modeling the shape of the mass-metallicity relation thus provides an indirect means to probe subtle variations in the gas content of star-forming galaxies.	0,1,0,0,0,0
List Decoding of Insertions and Deletions	List decoding of insertions and deletions in the Levenshtein metric is considered. The Levenshtein distance between two sequences is the minimum number of insertions and deletions needed to turn one of the sequences into the other. In this paper, a Johnson-like upper bound on the maximum list size when list decoding in the Levenshtein metric is derived. This bound depends only on the length and minimum Levenshtein distance of the code, the length of the received word, and the alphabet size. It shows that polynomial-time list decoding beyond half the Levenshtein distance is possible for many parameters. Further, we also prove a lower bound on list decoding of deletions with with the well-known binary Varshamov-Tenengolts (VT) codes which shows that the maximum list size grows exponentially with the number of deletions. Finally, an efficient list decoding algorithm for two insertions/deletions with VT codes is given. This decoder can be modified to a polynomial-time list decoder of any constant number of insertions/deletions.	1,0,1,0,0,0
Gravitational Wave signatures of inflationary models from Primordial Black Hole Dark Matter	Primordial Black Holes (PBH) could be the cold dark matter of the universe. They could have arisen from large (order one) curvature fluctuations produced during inflation that reentered the horizon in the radiation era. At reentry, these fluctuations source gravitational waves (GW) via second order anisotropic stresses. These GW, together with those (possibly) sourced during inflation by the same mechanism responsible for the large curvature fluctuations, constitute a primordial stochastic GW background (SGWB) that unavoidably accompanies the PBH formation. We study how the amplitude and the range of frequencies of this signal depend on the statistics (Gaussian versus $\chi^2$) of the primordial curvature fluctuations, and on the evolution of the PBH mass function due to accretion and merging. We then compare this signal with the sensitivity of present and future detectors, at PTA and LISA scales. We find that this SGWB will help to probe, or strongly constrain, the early universe mechanism of PBH production. The comparison between the peak mass of the PBH distribution and the peak frequency of this SGWB will provide important information on the merging and accretion evolution of the PBH mass distribution from their formation to the present era. Different assumptions on the statistics and on the PBH evolution also result in different amounts of CMB $\mu$-distortions. Therefore the above results can be complemented by the detection (or the absence) of $\mu$-distortions with an experiment such as PIXIE.	0,1,0,0,0,0
Introduction to a Temporal Graph Benchmark	A temporal graph is a data structure, consisting of nodes and edges in which the edges are associated with time labels. To analyze the temporal graph, the first step is to find a proper graph dataset/benchmark. While many temporal graph datasets exist online, none could be found that used the interval labels in which each edge is associated with a starting and ending time. Therefore we create a temporal graph data based on Wikipedia reference graph for temporal analysis. This report aims to provide more details of this graph benchmark to those who are interested in using it.	1,1,0,0,0,0
Affine Metrics and Associated Algebroid Structures: Application to General Relativity	In this paper, algebroid bundle associated to affine metrics provide an structure for unification of gravity and electromagnetism and, geometrization of matter.	0,0,1,0,0,0
Sequence-to-Sequence Models Can Directly Translate Foreign Speech	We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points.	1,0,0,1,0,0
Computationally Inferred Genealogical Networks Uncover Long-Term Trends in Assortative Mating	Genealogical networks, also known as family trees or population pedigrees, are commonly studied by genealogists wanting to know about their ancestry, but they also provide a valuable resource for disciplines such as digital demography, genetics, and computational social science. These networks are typically constructed by hand through a very time-consuming process, which requires comparing large numbers of historical records manually. We develop computational methods for automatically inferring large-scale genealogical networks. A comparison with human-constructed networks attests to the accuracy of the proposed methods. To demonstrate the applicability of the inferred large-scale genealogical networks, we present a longitudinal analysis on the mating patterns observed in a network. This analysis shows a consistent tendency of people choosing a spouse with a similar socioeconomic status, a phenomenon known as assortative mating. Interestingly, we do not observe this tendency to consistently decrease (nor increase) over our study period of 150 years.	1,0,0,0,1,0
Robust Bayesian Optimization with Student-t Likelihood	Bayesian optimization has recently attracted the attention of the automatic machine learning community for its excellent results in hyperparameter tuning. BO is characterized by the sample efficiency with which it can optimize expensive black-box functions. The efficiency is achieved in a similar fashion to the learning to learn methods: surrogate models (typically in the form of Gaussian processes) learn the target function and perform intelligent sampling. This surrogate model can be applied even in the presence of noise; however, as with most regression methods, it is very sensitive to outlier data. This can result in erroneous predictions and, in the case of BO, biased and inefficient exploration. In this work, we present a GP model that is robust to outliers which uses a Student-t likelihood to segregate outliers and robustly conduct Bayesian optimization. We present numerical results evaluating the proposed method in both artificial functions and real problems.	1,0,0,1,0,0
New zirconium hydrides predicted by structure search method based on first principles calculations	The formation of precipitated zirconium (Zr) hydrides is closely related to the hydrogen embrittlement problem for the cladding materials of pressured water reactors (PWR). In this work, we systematically investigated the crystal structures of zirconium hydride (ZrHx) with different hydrogen concentrations (x = 0~2, atomic ratio) by combining the basin hopping algorithm with first principles calculations. We conclude that the P3m1 {\zeta}-ZrH0.5 is dynamically unstable, while a novel dynamically stable P3m1 ZrH0.5 structure was discovered in the structure search. The stability of bistable P42/nnm ZrH1.5 structures and I4/mmm ZrH2 structures are also revisited. We find that the P42/nnm (c/a > 1) ZrH1.5 is dynamically unstable, while the I4/mmm (c/a = 1.57) ZrH2 is dynamically stable.The P42/nnm (c/a < 1) ZrH1.5 might be a key intermediate phase for the transition of {\gamma}->{\delta}->{\epsilon} phases. Additionally, by using the thermal dynamic simulations, we find that {\delta}-ZrH1.5 is the most stable structure at high temperature while ZrH2 is the most stable hydride at low temperature. Slow cooling process will promote the formation of {\delta}-ZrH1.5, and fast cooling process will promote the formation of {\gamma}-ZrH. These results may help to understand the phase transitions of zirconium hydrides.	0,1,0,0,0,0
A Polynomial Time Match Test for Large Classes of Extended Regular Expressions	In the present paper, we study the match test for extended regular expressions. We approach this NP-complete problem by introducing a novel variant of two-way multihead automata, which reveals that the complexity of the match test is determined by a hidden combinatorial property of extended regular expressions, and it shows that a restriction of the corresponding parameter leads to rich classes with a polynomial time match test. For presentational reasons, we use the concept of pattern languages in order to specify extended regular expressions. While this decision, formally, slightly narrows the scope of our results, an extension of our concepts and results to more general notions of extended regular expressions is straightforward.	1,0,0,0,0,0
Shape and fission instabilities of ferrofluids in non-uniform magnetic fields	We study static distributions of ferrofluid submitted to non-uniform magnetic fields. We show how the normal-field instability is modified in the presence of a weak magnetic field gradient. Then we consider a ferrofluid droplet and show how the gradient affects its shape. A rich phase transitions phenomenology is found. We also investigate the creation of droplets by successive splits when a magnet is vertically approached from below and derive theoretical expressions which are solved numerically to obtain the number of droplets and their aspect ratio as function of the field configuration. A quantitative comparison is performed with previous experimental results, as well as with our own experiments, and yields good agreement with the theoretical modeling.	0,1,0,0,0,0
Gridbot: An autonomous robot controlled by a Spiking Neural Network mimicking the brain's navigational system	It is true that the "best" neural network is not necessarily the one with the most "brain-like" behavior. Understanding biological intelligence, however, is a fundamental goal for several distinct disciplines. Translating our understanding of intelligence to machines is a fundamental problem in robotics. Propelled by new advancements in Neuroscience, we developed a spiking neural network (SNN) that draws from mounting experimental evidence that a number of individual neurons is associated with spatial navigation. By following the brain's structure, our model assumes no initial all-to-all connectivity, which could inhibit its translation to a neuromorphic hardware, and learns an uncharted territory by mapping its identified components into a limited number of neural representations, through spike-timing dependent plasticity (STDP). In our ongoing effort to employ a bioinspired SNN-controlled robot to real-world spatial mapping applications, we demonstrate here how an SNN may robustly control an autonomous robot in mapping and exploring an unknown environment, while compensating for its own intrinsic hardware imperfections, such as partial or total loss of visual input.	0,0,0,0,1,0
Twin Primes In Quadratic Arithmetic Progressions	A recent heuristic argument based on basic concepts in spectral analysis showed that the twin prime conjecture and a few other related primes counting problems are valid. A rigorous version of the spectral method, and a proof for the existence of infinitely many quadratic twin primes $n^{2}+1$ and $n^{2}+3$, $n \geq 1$, are proposed in this note.	0,0,1,0,0,0
A response to: "NIST experts urge caution in use of courtroom evidence presentation method"	A press release from the National Institute of Standards and Technology (NIST)could potentially impede progress toward improving the analysis of forensic evidence and the presentation of forensic analysis results in courts in the United States and around the world. "NIST experts urge caution in use of courtroom evidence presentation method" was released on October 12, 2017, and was picked up by the phys.org news service. It argues that, except in exceptional cases, the results of forensic analyses should not be reported as "likelihood ratios". The press release, and the journal article by NIST researchers Steven P. Lund & Harri Iyer on which it is based, identifies some legitimate points of concern, but makes a strawman argument and reaches an unjustified conclusion that throws the baby out with the bathwater.	0,0,0,1,0,0
Image Registration and Predictive Modeling: Learning the Metric on the Space of Diffeomorphisms	We present a method for metric optimization in the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, by treating the induced Riemannian metric on the space of diffeomorphisms as a kernel in a machine learning context. For simplicity, we choose the kernel Fischer Linear Discriminant Analysis (KLDA) as the framework. Optimizing the kernel parameters in an Expectation-Maximization framework, we define model fidelity via the hinge loss of the decision function. The resulting algorithm optimizes the parameters of the LDDMM norm-inducing differential operator as a solution to a group-wise registration and classification problem. In practice, this may lead to a biology-aware registration, focusing its attention on the predictive task at hand such as identifying the effects of disease. We first tested our algorithm on a synthetic dataset, showing that our parameter selection improves registration quality and classification accuracy. We then tested the algorithm on 3D subcortical shapes from the Schizophrenia cohort Schizconnect. Our Schizpohrenia-Control predictive model showed significant improvement in ROC AUC compared to baseline parameters.	0,0,0,1,0,0
Baselines and a datasheet for the Cerema AWP dataset	This paper presents the recently published Cerema AWP (Adverse Weather Pedestrian) dataset for various machine learning tasks and its exports in machine learning friendly format. We explain why this dataset can be interesting (mainly because it is a greatly controlled and fully annotated image dataset) and present baseline results for various tasks. Moreover, we decided to follow the very recent suggestions of datasheets for dataset, trying to standardize all the available information of the dataset, with a transparency objective.	0,0,0,1,0,0
The Complexity of Abstract Machines	The lambda-calculus is a peculiar computational model whose definition does not come with a notion of machine. Unsurprisingly, implementations of the lambda-calculus have been studied for decades. Abstract machines are implementations schema for fixed evaluation strategies that are a compromise between theory and practice: they are concrete enough to provide a notion of machine and abstract enough to avoid the many intricacies of actual implementations. There is an extensive literature about abstract machines for the lambda-calculus, and yet-quite mysteriously-the efficiency of these machines with respect to the strategy that they implement has almost never been studied. This paper provides an unusual introduction to abstract machines, based on the complexity of their overhead with respect to the length of the implemented strategies. It is conceived to be a tutorial, focusing on the case study of implementing the weak head (call-by-name) strategy, and yet it is an original re-elaboration of known results. Moreover, some of the observation contained here never appeared in print before.	1,0,0,0,0,0
Advancements in Continuum Approximation Models for Logistics and Transportation Systems: 1996 - 2016	Continuum Approximation (CA) is an efficient and parsimonious technique for modeling complex logistics problems. In this paper,we review recent studies that develop CA models for transportation, distribution and logistics problems with the aim of synthesizing recent advancements and identifying current research gaps. This survey focuses on important principles and key results from CA models. In particular, we consider how these studies fill the gaps identified by the most recent literature reviews in this field. We observe that CA models are used in a wider range of applications, especially in the areas of facility location and integrated supply chain management. Most studies use CA as an alternative to exact solution approaches; however, CA can also be used in combination with exact approaches. We also conclude with promising areas of future work.	0,0,1,0,0,0
On The Asymptotic Efficiency of Selection Procedures for Independent Gaussian Populations	The field of discrete event simulation and optimization techniques motivates researchers to adjust classic ranking and selection (R&S) procedures to the settings where the number of populations is large. We use insights from extreme value theory in order to reveal the asymptotic properties of R&S procedures. Namely, we generalize the asymptotic result of Robbins and Siegmund regarding selection from independent Gaussian populations with known constant variance by their means to the case of selecting a subset of varying size out of a given set of populations. In addition, we revisit the problem of selecting the population with the highest mean among independent Gaussian populations with unknown and possibly different variances. Particularly, we derive the relative asymptotic efficiency of Dudewicz and Dalal's and Rinott's procedures, showing that the former can be asymptotically superior by a multiplicative factor which is larger than one, but this factor may be reduced by proper choice of parameters. We also use our asymptotic results to suggest that the sample size in the first stage of the two procedures should be logarithmic in the number of populations.	0,0,1,1,0,0
The Young L Dwarf 2MASS J11193254-1137466 is a Planetary-Mass Binary	We have discovered that the extremely red, low-gravity L7 dwarf 2MASS J11193254-1137466 is a 0.14" (3.6 AU) binary using Keck laser guide star adaptive optics imaging. 2MASS J11193254-1137466 has previously been identified as a likely member of the TW Hydrae Association (TWA). Using our updated photometric distance and proper motion, a kinematic analysis based on the BANYAN II model gives an 82% probability of TWA membership. At TWA's 10$\pm$3 Myr age and using hot-start evolutionary models, 2MASS J11193254-1137466AB is a pair of $3.7^{+1.2}_{-0.9}$ $M_{\rm Jup}$ brown dwarfs, making it the lowest-mass binary discovered to date. We estimate an orbital period of $90^{+80}_{-50}$ years. One component is marginally brighter in $K$ band but fainter in $J$ band, making this a probable flux-reversal binary, the first discovered with such a young age. We also imaged the spectrally similar TWA L7 dwarf WISEA J114724.10-204021.3 with Keck and found no sign of binarity. Our evolutionary model-derived $T_{\rm eff}$ estimate for WISEA J114724.10-204021.3 is $\approx$230 K higher than for 2MASS J11193254-1137466AB, at odds with their spectral similarity. This discrepancy suggests that WISEA J114724.10-204021.3 may actually be a tight binary with masses and temperatures very similar to 2MASS J11193254-1137466AB, or further supporting the idea that near-infrared spectra of young ultracool dwarfs are shaped by factors other than temperature and gravity. 2MASS J11193254-1137466AB will be an essential benchmark for testing evolutionary and atmospheric models in the young planetary-mass regime.	0,1,0,0,0,0
Conformal blocks attached to twisted groups	The aim of this paper is to generalize the notion of conformal blocks to the situation in which the Lie algebra they are attached to is not defined over a field, but depends on covering data of curves. The result will be a sheaf of conformal blocks on the Hurwitz stack parametrizing Galois coverings of curves. Many features of the classical sheaves of conformal blocks are proved to hold in this more general setting, in particular the fusion rules, the propagation of vacua and the WZW connection.	0,0,1,0,0,0
Mean field repulsive Kuramoto models: Phase locking and spatial signs	The phenomenon of self-synchronization in populations of oscillatory units appears naturally in neurosciences. However, in some situations, the formation of a coherent state is damaging. In this article we study a repulsive mean-field Kuramoto model that describes the time evolution of n points on the unit circle, which are transformed into incoherent phase-locked states. It has been recently shown that such systems can be reduced to a three-dimensional system of ordinary differential equations, whose mathematical structure is strongly related to hyperbolic geometry. The orbits of the Kuramoto dynamical system are then described by a ow of Möbius transformations. We show this underlying dynamic performs statistical inference by computing dynamically M-estimates of scatter matrices. We also describe the limiting phase-locked states for random initial conditions using Tyler's transformation matrix. Moreover, we show the repulsive Kuramoto model performs dynamically not only robust covariance matrix estimation, but also data processing: the initial configuration of the n points is transformed by the dynamic into a limiting phase-locked state that surprisingly equals the spatial signs from nonparametric statistics. That makes the sign empirical covariance matrix to equal 1 2 id2, the variance-covariance matrix of a random vector that is uniformly distributed on the unit circle.	0,0,0,0,1,0
Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters	As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such "in-the-tail" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To allow for large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right "priors" or parameters for synthesis: we would like realistic data with poses and object configurations that mimic true Precarious Pedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem the Adversarial Imposters. We demonstrate that this simple pipeline allows one to synthesize realistic training data by making use of rendering/animation engines within a GAN framework. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Adversarial Imposters can also be used for "in-the-tail" validation at test-time, a notoriously difficult challenge for real-world deployment.	1,0,0,0,0,0
Defect-induced large spin-orbit splitting in the monolayer of PtSe$_2$	The effect of spin-orbit coupling (SOC) on the electronic properties of monolayer (ML) PtSe$_2$ is dictated by the presence of the crystal inversion symmetry to exhibit spin polarized band without characteristic of spin splitting. Through fully-relativistic density-functional theory calculations, we show that large spin-orbit splitting can be induced by introducing point defects. We calculate stability of native point defects such as a Se vacancy (V$_{\texttt{Se}}$), a Se interstitial (Se$_{i}$), a Pt vacancy (V$_{\texttt{Pt}}$), and a Pt interstitial (Pt$_{i}$), and find that both the V$_{\texttt{Se}}$ and Se$_{i}$ have the lowest formation energy. We also find that in contrast to the Se$_{i}$ case exhibiting spin degeneracy in the defect states, the large spin-orbit splitting up to 152 meV is observed in the defect states of the V$_{\texttt{Se}}$. Our analyses of orbital contributions to the defect states show that the large spin splitting is originated from the strong hybridization between Pt-$d_{x{^2}+y{^2}}+d_{xy}$ and Se-$p_{x}+p_{y}$ orbitals. Our study clarifies that the defects play an important role in the spin splitting properties of the PtSe$_2$ ML, which is important for designing future spintronic devices.	0,1,0,0,0,0
Discreteness of silting objects and t-structures in triangulated categories	We introduce the notion of ST-pairs of triangulated subcategories, a prototypical example of which is the pair of the bound homotopy category and the bound derived category of a finite-dimensional algebra. For an ST-pair $(\C,\D)$, we construct an order-preserving map from silting objects in $\C$ to bounded $t$-structures on $\D$ and show that the map is bijective if and only if $\C$ is silting-discrete if and only if $\D$ is $t$-discrete. Based on a work of Qiu and Woolf, the above result is applied to show that if $\C$ is silting-discrete then the stability space of $\D$ is contractible. This is used to obtain the contractibility of the stability spaces of some Calabi--Yau triangulated categories associated to Dynkin quivers.	0,0,1,0,0,0
Soft Pneumatic Gelatin Actuator for Edible Robotics	We present a fully edible pneumatic actuator based on gelatin-glycerol composite. The actuator is monolithic, fabricated via a molding process, and measures 90 mm in length, 20 mm in width, and 17 mm in thickness. Thanks to the composite mechanical characteristics similar to those of silicone elastomers, the actuator exhibits a bending angle of 170.3 ° and a blocked force of 0.34 N at the applied pressure of 25 kPa. These values are comparable to elastomer based pneumatic actuators. As a validation example, two actuators are integrated to form a gripper capable of handling various objects, highlighting the high performance and applicability of the edible actuator. These edible actuators, combined with other recent edible materials and electronics, could lay the foundation for a new type of edible robots.	1,0,0,0,0,0
Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations	Non-negative matrix factorization is a basic tool for decomposing data into the feature and weight matrices under non-negativity constraints, and in practice is often solved in the alternating minimization framework. However, it is unclear whether such algorithms can recover the ground-truth feature matrix when the weights for different features are highly correlated, which is common in applications. This paper proposes a simple and natural alternating gradient descent based algorithm, and shows that with a mild initialization it provably recovers the ground-truth in the presence of strong correlations. In most interesting cases, the correlation can be in the same order as the highest possible. Our analysis also reveals its several favorable features including robustness to noise. We complement our theoretical results with empirical studies on semi-synthetic datasets, demonstrating its advantage over several popular methods in recovering the ground-truth.	1,0,0,1,0,0
Quantum communication by means of collapse of the wave function	We show that quantum communication by means of collapse of the wave function is possible. In this study, quantum communication does not mean quantum teleportation or quantum cryptography, but transmission of information itself. Because of consistency with special relativity, the possibility of the quantum communication leads to another conclusion that the collapse of the wave function must propagate at the speed of light or slower. We show this requirement is consistent with nonlocality in quantum mechanics. We also demonstrate that the Einstein-Podolsky-Rosen experiment does not disprove our conclusion.	0,1,0,0,0,0
Generalized Lambert series and arithmetic nature of odd zeta values	It is pointed out that the generalized Lambert series $\displaystyle\sum_{n=1}^{\infty}\frac{n^{N-2h}}{e^{n^{N}x}-1}$ studied by Kanemitsu, Tanigawa and Yoshimoto can be found on page $332$ of Ramanujan's Lost Notebook in a slightly more general form. We extend an important transformation of this series obtained by Kanemitsu, Tanigawa and Yoshimoto by removing restrictions on the parameters $N$ and $h$ that they impose. From our extension we deduce a beautiful new generalization of Ramanujan's famous formula for odd zeta values which, for $N$ odd and $m>0$, gives a relation between $\zeta(2m+1)$ and $\zeta(2Nm+1)$. A result complementary to the aforementioned generalization is obtained for any even $N$ and $m\in\mathbb{Z}$. It generalizes a transformation of Wigert and can be regarded as a formula for $\zeta\left(2m+1-\frac{1}{N}\right)$. Applications of these transformations include a generalization of the transformation for the logarithm of Dedekind eta-function $\eta(z)$, Zudilin- and Rivoal-type results on transcendence of certain values, and a transcendence criterion for Euler's constant $\gamma$.	0,0,1,0,0,0
Modeling Interference Via Symmetric Treatment Decomposition	Classical causal inference assumes a treatment meant for a given unit does not have an effect on other units. When this "no interference" assumption is violated, new types of spillover causal effects arise, and causal inference becomes much more difficult. In addition, interference introduces a unique complication where outcomes may transmit treatment influences to each other, which is a relationship that has some features of a causal one, but is symmetric. In settings where detailed temporal information on outcomes is not available, addressing this complication using statistical inference methods based on Directed Acyclic Graphs (DAGs) (Ogburn & VanderWeele, 2014) leads to conceptual difficulties. In this paper, we develop a new approach to decomposing the spillover effect into direct (also known as the contagion effect) and indirect (also known as the infectiousness effect) components that extends the DAG based treatment decomposition approach to mediation found in (Robins & Richardson, 2010) to causal chain graph models (Lauritzen & Richardson, 2002). We show that when these components of the spillover effect are identified in these models, they have an identifying functional, which we call the symmetric mediation formula, that generalizes the mediation formula in DAGs (Pearl, 2011). We further show that, unlike assumptions in classical mediation analysis, an assumption permitting identification in our setting leads to restrictions on the observed data law, making the assumption empirically falsifiable. Finally, we discuss statistical inference for the components of the spillover effect in the special case of two interacting outcomes, and discuss a maximum likelihood estimator, and a doubly robust estimator.	0,0,0,1,0,0
Mobile phone identification through the built-in magnetometers	Mobile phones identification through their built in components has been demonstrated in literature for various types of sensors including the camera, microphones and accelerometers. The identification is performed by the exploitation of the small but significant differences in the electronic circuits generated during the production process. Thus, these differences become an intrinsic property of the electronic components, which can be detected and become an unique fingerprint of the component and of the mobile phone. In this paper, we investigate the identification of mobile phones through their builtin magnetometers, which has not been reported in literature yet. Magnetometers are stimulated with different waveforms using a solenoid connected to a computer s audio board. The identification is performed analyzing the digital output of the magnetometer through the use of statistical features and the Support Vector Machine (SVM) machine learning algorithm. We prove that this technique can distinguish different models and brands with very high accuracy but it can only distinguish phones of the same model with limited accuracy.	1,0,0,0,0,0
Betweenness and Diversity in Journal Citation Networks as Measures of Interdisciplinarity -- A Tribute to Eugene Garfield --	Journals were central to Eugene Garfield's research interests. Among other things, journals are considered as units of analysis for bibliographic databases such as the Web of Science (WoS) and Scopus. In addition to disciplinary classifications of journals, journal citation patterns span networks across boundaries to variable extents. Using betweenness centrality (BC) and diversity, we elaborate on the question of how to distinguish and rank journals in terms of interdisciplinarity. Interdisciplinarity, however, is difficult to operationalize in the absence of an operational definition of disciplines, the diversity of a unit of analysis is sample-dependent. BC can be considered as a measure of multi-disciplinarity. Diversity of co-citation in a citing document has been considered as an indicator of knowledge integration, but an author can also generate trans-disciplinary--that is, non-disciplined--variation by citing sources from other disciplines. Diversity in the bibliographic coupling among citing documents can analogously be considered as diffusion of knowledge across disciplines. Because the citation networks in the cited direction reflect both structure and variation, diversity in this direction is perhaps the best available measure of interdisciplinarity at the journal level. Furthermore, diversity is based on a summation and can therefore be decomposed, differences among (sub)sets can be tested for statistical significance. In an appendix, a general-purpose routine for measuring diversity in networks is provided.	1,0,0,0,0,0
Extracting Epistatic Interactions in Type 2 Diabetes Genome-Wide Data Using Stacked Autoencoder	2 Diabetes is a leading worldwide public health concern, and its increasing prevalence has significant health and economic importance in all nations. The condition is a multifactorial disorder with a complex aetiology. The genetic determinants remain largely elusive, with only a handful of identified candidate genes. Genome wide association studies (GWAS) promised to significantly enhance our understanding of genetic based determinants of common complex diseases. To date, 83 single nucleotide polymorphisms (SNPs) for type 2 diabetes have been identified using GWAS. Standard statistical tests for single and multi-locus analysis such as logistic regression, have demonstrated little effect in understanding the genetic architecture of complex human diseases. Logistic regression is modelled to capture linear interactions but neglects the non-linear epistatic interactions present within genetic data. There is an urgent need to detect epistatic interactions in complex diseases as this may explain the remaining missing heritability in such diseases. In this paper, we present a novel framework based on deep learning algorithms that deal with non-linear epistatic interactions that exist in genome wide association data. Logistic association analysis under an additive genetic model, adjusted for genomic control inflation factor, is conducted to remove statistically improbable SNPs to minimize computational overheads.	0,0,0,1,0,0
Early Routability Assessment in VLSI Floorplans: A Generalized Routing Model	Multiple design iterations are inevitable in nanometer Integrated Circuit (IC) design flow until desired printability and performance metrics are achieved. This starts with placement optimization aimed at improving routability, wirelength, congestion and timing in the design. Contrarily, no such practice exists on a floorplanned layout, during the early stage of the design flow. Recently, STAIRoute \cite{karb2} aimed to address that by identifying the shortest routing path of a net through a set of routing regions in the floorplan in multiple metal layers. Since the blocks in hierarchical ASIC/SoC designs do not use all the permissible routing layers for the internal routing corresponding to standard cell connectivity, the proposed STAIRoute framework is not an effective for early global routability assessment. This leads to improper utilization of routing area, specifically in higher routing layers with fewer routing blockages, as the lack of placement of standard cells does not facilitates any routing of their interconnections. This paper presents a generalized model for early global routability assessment, HGR, by utilizing the free regions over the blocks beyond certain metal layers. The proposed (hybrid) routing model comprises of (a) the junction graph model in STAIRoute routing through the block boundary regions in lower routing layers, and (ii) the grid graph model for routing in higher layers over the free regions of the blocks. Experiment with the latest floorplanning benchmarks exhibit an average reduction of $4\%$, $54\%$ and $70\%$ in netlength, via count, and congestion respectively when HGR is used over STAIRoute. Further, we conducted another experiment on an industrial design flow targeted for $45nm$ process, and the results are encouraging with $~3$X runtime boost when early global routing is used in conjunction with the existing physical design flow.	1,0,0,0,0,0
Characterization theorems for $Q$-independent random variables with values in a locally compact Abelian group	Let $X$ be a locally compact Abelian group, $Y$ be its character group. Following A. Kagan and G. Székely we introduce a notion of $Q$-independence for random variables with values in $X$. We prove group analogues of the Cramér, Kac-Bernstein, Skitovich-Darmois and Heyde theorems for $Q$-independent random variables with values in $X$. The proofs of these theorems are reduced to solving some functional equations on the group $Y$.	0,0,1,0,0,0
Stellar streams as gravitational experiments I. The case of Sagittarius	Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy offer a unique way to constrain the shape of galactic gravitational potentials. Such streams can be used as leaning tower gravitational experiments on galactic scales. The most well motivated modification of gravity proposed as an alternative to dark matter on galactic scales is Milgromian dynamics (MOND), and we present here the first ever N-body simulations of the dynamical evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a realistic baryonic mass model for the Milky Way, we attempt to reproduce the present-day spatial and kinematic structure of the Sagittarius dwarf and its immense tidal stream that wraps around the Milky Way. With very little freedom on the original structure of the progenitor, constrained by the total luminosity of the Sagittarius structure and by the observed stellar mass-size relation for isolated dwarf galaxies, we find reasonable agreement between our simulations and observations of this system. The observed stellar velocities in the leading arm can be reproduced if we include a massive hot gas corona around the Milky Way that is flattened in the direction of the principal plane of its satellites. This is the first time that tidal dissolution in MOND has been tested rigorously at these mass and acceleration scales.	0,1,0,0,0,0
Tunnelling Spectroscopy of Andreev States in Graphene	A normal conductor placed in good contact with a superconductor can inherit its remarkable electronic properties. This proximity effect microscopically originates from the formation in the conductor of entangled electron-hole states, called Andreev states. Spectroscopic studies of Andreev states have been performed in just a handful of systems. The unique geometry, electronic structure and high mobility of graphene make it a novel platform for studying Andreev physics in two dimensions. Here we use a full van der Waals heterostructure to perform tunnelling spectroscopy measurements of the proximity effect in superconductor-graphene-superconductor junctions. The measured energy spectra, which depend on the phase difference between the superconductors, reveal the presence of a continuum of Andreev bound states. Moreover, our device heterostructure geometry and materials enable us to measure the Andreev spectrum as a function of the graphene Fermi energy, showing a transition between different mesoscopic regimes. Furthermore, by experimentally introducing a novel concept, the supercurrent spectral density, we determine the supercurrent-phase relation in a tunnelling experiment, thus establishing the connection between Andreev physics at finite energy and the Josephson effect. This work opens up new avenues for probing exotic topological phases of matter in hybrid superconducting Dirac materials.	0,1,0,0,0,0
GANs for Biological Image Synthesis	In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multi-channel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images.	1,0,0,1,0,0
Fast Linear Transformations in Python	This paper introduces a new free library for the Python programming language, which provides a collection of structured linear transforms, that are not represented as explicit two dimensional arrays but in a more efficient way by exploiting the structural knowledge. This allows fast and memory savy forward and backward transformations while also provding a clean but still flexible interface to these effcient algorithms, thus making code more readable, scable and adaptable. We first outline the goals of this library, then how they were achieved and lastly we demonstrate the performance compared to current state of the art packages available for Python. This library is released and distributed under a free license.	1,0,0,0,0,0
Efficient Estimation for Dimension Reduction with Censored Data	We propose a general index model for survival data, which generalizes many commonly used semiparametric survival models and belongs to the framework of dimension reduction. Using a combination of geometric approach in semiparametrics and martingale treatment in survival data analysis, we devise estimation procedures that are feasible and do not require covariate-independent censoring as assumed in many dimension reduction methods for censored survival data. We establish the root-$n$ consistency and asymptotic normality of the proposed estimators and derive the most efficient estimator in this class for the general index model. Numerical experiments are carried out to demonstrate the empirical performance of the proposed estimators and an application to an AIDS data further illustrates the usefulness of the work.	0,0,1,1,0,0
Human perception in computer vision	Computer vision has made remarkable progress in recent years. Deep neural network (DNN) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and, remarkably, some generalization ability: new visual problems can now be solved more easily based on previous learning. Biological vision (learned in life and through evolution) is also accurate and general-purpose. Is it possible that these different learning regimes converge to similar problem-dependent optimal computations? We therefore asked whether the human system-level computation of visual perception has DNN correlates and considered several anecdotal test cases. We found that perceptual sensitivity to image changes has DNN mid-computation correlates, while sensitivity to segmentation, crowding and shape has DNN end-computation correlates. Our results quantify the applicability of using DNN computation to estimate perceptual loss, and are consistent with the fascinating theoretical view that properties of human perception are a consequence of architecture-independent visual learning.	1,0,0,0,0,0
Lattice implementation of Abelian gauge theories with Chern-Simons number and an axion field	Real time evolution of classical gauge fields is relevant for a number of applications in particle physics and cosmology, ranging from the early Universe to dynamics of quark-gluon plasma. We present a lattice formulation of the interaction between a $shift$-symmetric field and some $U(1)$ gauge sector, $a(x)\tilde{F}_{\mu\nu}F^{\mu\nu}$, reproducing the continuum limit to order $\mathcal{O}(dx_\mu^2)$ and obeying the following properties: (i) the system is gauge invariant and (ii) shift symmetry is exact on the lattice. For this end we construct a definition of the {\it topological number density} $Q = \tilde{F}_{\mu\nu}F^{\mu\nu}$ that admits a lattice total derivative representation $Q = \Delta_\mu^+ K^\mu$, reproducing to order $\mathcal{O}(dx_\mu^2)$ the continuum expression $Q = \partial_\mu K^\mu \propto \vec E \cdot \vec B$. If we consider a homogeneous field $a(x) = a(t)$, the system can be mapped into an Abelian gauge theory with Hamiltonian containing a Chern-Simons term for the gauge fields. This allow us to study in an accompanying paper the real time dynamics of fermion number non-conservation (or chirality breaking) in Abelian gauge theories at finite temperature. When $a(x) = a(\vec x,t)$ is inhomogeneous, the set of lattice equations of motion do not admit however a simple explicit local solution (while preserving an $\mathcal{O}(dx_\mu^2)$ accuracy). We discuss an iterative scheme allowing to overcome this difficulty.	0,1,0,0,0,0
Second Order Statistics Analysis and Comparison between Arithmetic and Geometric Average Fusion	Two fundamental approaches to information averaging are based on linear and logarithmic combination, yielding the arithmetic average (AA) and geometric average (GA) of the fusing initials, respectively. In the context of target tracking, the two most common formats of data to be fused are random variables and probability density functions, namely $v$-fusion and $f$-fusion, respectively. In this work, we analyze and compare the second order statistics (including variance and mean square error) of AA and GA in terms of both $v$-fusion and $f$-fusion. The case of weighted Gaussian mixtures representing multitarget densities in the presence of false alarms and misdetection (whose weight sums are not necessarily unit) is also considered, the result of which appears significantly different from that for a single target. In addition to exact derivation, exemplifying analysis and illustrations are provided.	1,0,1,1,0,0
A Generalization of Convolutional Neural Networks to Graph-Structured Data	This paper introduces a generalization of Convolutional Neural Networks (CNNs) from low-dimensional grid data, such as images, to graph-structured data. We propose a novel spatial convolution utilizing a random walk to uncover the relations within the input, analogous to the way the standard convolution uses the spatial neighborhood of a pixel on the grid. The convolution has an intuitive interpretation, is efficient and scalable and can also be used on data with varying graph structure. Furthermore, this generalization can be applied to many standard regression or classification problems, by learning the the underlying graph. We empirically demonstrate the performance of the proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular activity data set.	1,0,0,1,0,0
Study of secondary neutron interactions with $^{232}$Th, $^{129}$I, and $^{127}$I nuclei with the uranium assembly "QUINTA" at 2, 4, and 8 GeV deuteron beams of the JINR Nuclotron accelerator	The natural uranium assembly, "QUINTA", was irradiated with 2, 4, and 8 GeV deuterons. The $^{232}$Th, $^{127}$I, and $^{129}$I samples have been exposed to secondary neutrons produced in the assembly at a 20-cm radial distance from the deuteron beam axis. The spectra of gamma rays emitted by the activated $^{232}$Th, $^{127}$I, and $^{129}$I samples have been analyzed and several tens of product nuclei have been identified. For each of those products, neutron-induced reaction rates have been determined. The transmutation power for the $^{129}$I samples is estimated. Experimental results were compared to those calculated with well-known stochastic and deterministic codes.	0,1,0,0,0,0
Vector bundles over classifying spaces of p-local finite groups and Benson-Carlson duality	In this paper we obtain a description of the Grothendieck group of complex vector bundles over the classifying space of a p-local finite group in terms of representation rings of subgroups of its Sylow. We also prove a stable elements formula for generalized cohomological invariants of p-local finite groups, which is used to show the existence of unitary embeddings of p-local finite groups. Finally, we show that the augmentation map for the cochains of the classifying space of a p-local finite group is Gorenstein in the sense of Dwyer-Greenlees-Iyengar and obtain some consequences about the cohomology ring of these classifying spaces.	0,0,1,0,0,0
Henri Bénard: Thermal convection and vortex shedding	We present in this article the work of Henri Bénard (1874-1939), French physicist who began the systematic experimental study of two hydrodynamic systems: the thermal convection of fluids heated from below (the Rayleigh-Bénard convection and the Bénard-Marangoni convection) and the periodical vortex shedding behind a bluff body in a flow (the Bénard-Kármán vortex street). Across his scientific biography, we review the interplay between experiments and theory in these two major subjects of fluid mechanics.	0,1,0,0,0,0
Wadge Degrees of $ω$-Languages of Petri Nets	We prove that $\omega$-languages of (non-deterministic) Petri nets and $\omega$-languages of (non-deterministic) Turing machines have the same topological complexity: the Borel and Wadge hierarchies of the class of $\omega$-languages of (non-deterministic) Petri nets are equal to the Borel and Wadge hierarchies of the class of $\omega$-languages of (non-deterministic) Turing machines which also form the class of effective analytic sets. In particular, for each non-null recursive ordinal $\alpha < \omega\_1^{\rm CK} $ there exist some ${\bf \Sigma}^0\_\alpha$-complete and some ${\bf \Pi}^0\_\alpha$-complete $\omega$-languages of Petri nets, and the supremum of the set of Borel ranks of $\omega$-languages of Petri nets is the ordinal $\gamma\_2^1$, which is strictly greater than the first non-recursive ordinal $\omega\_1^{\rm CK}$. We also prove that there are some ${\bf \Sigma}\_1^1$-complete, hence non-Borel, $\omega$-languages of Petri nets, and that it is consistent with ZFC that there exist some $\omega$-languages of Petri nets which are neither Borel nor ${\bf \Sigma}\_1^1$-complete. This answers the question of the topological complexity of $\omega$-languages of (non-deterministic) Petri nets which was left open in [DFR14,FS14].	1,0,1,0,0,0
Fully Bayesian Estimation Under Informative Sampling	Bayesian estimation is increasingly popular for performing model based inference to support policymaking. These data are often collected from surveys under informative sampling designs where subject inclusion probabilities are designed to be correlated with the response variable of interest. Sampling weights constructed from marginal inclusion probabilities are typically used to form an exponentiated pseudo likelihood that adjusts the population likelihood for estimation on the sample due to ease-of-estimation. We propose an alternative adjustment based on a Bayes rule construction that simultaneously performs weight smoothing and estimates the population model parameters in a fully Bayesian construction. We formulate conditions on known marginal and pairwise inclusion probabilities that define a class of sampling designs where $L_{1}$ consistency of the joint posterior is guaranteed. We compare performances between the two approaches on synthetic data, which reveals that our fully Bayesian approach better estimates posterior uncertainty without a requirement to calibrate the normalization of the sampling weights. We demonstrate our method on an application concerning the National Health and Nutrition Examination Survey exploring the relationship between caffeine consumption and systolic blood pressure.	0,0,1,1,0,0
$μ$-constant monodromy groups and Torelli results for the quadrangle singularities and the bimodal series	This paper is a sequel to [He11] and [GH17]. In [He11] a notion of marking of isolated hypersurface singularities was defined, and a moduli space $M_\mu^{mar}$ for marked singularities in one $\mu$-homotopy class of isolated hypersurface singularities was established. It is an analogue of a Teichmüller space. It comes together with a $\mu$-constant monodromy group $G^{mar}\subset G_{\mathbb{Z}}$. Here $G_{\mathbb{Z}}$ is the group of automorphisms of a Milnor lattice which respect the Seifert form. It was conjectured that $M_\mu^{mar}$ is connected. This is equivalent to $G^{mar}= G_{\mathbb{Z}}$. Also Torelli type conjectures were formulated. In [He11] and [GH17] $M_\mu^{mar}, G_{\mathbb{Z}}$ and $G^{mar}$ were determined and all conjectures were proved for the simple, the unimodal and the exceptional bimodal singularities. In this paper the quadrangle singularities and the bimodal series are treated. The Torelli type conjectures are true. But the conjecture $G^{mar}= G_{\mathbb{Z}}$ and $M_\mu^{mar}$ connected does not hold for certain subseries of the bimodal series.	0,0,1,0,0,0
JADE - A Platform for Research on Cooperation of Physical and Virtual Agents	In the ICS, WUT a platform for simulation of cooperation of physical and virtual mobile agents is under development. The paper describes the motivation of the research, an organization of the platform, a model of agent, and the principles of design of the platform. Several experimental simulations are briefly described.	1,0,0,0,0,0
Modeling of hysteresis loop and its applications in ferroelectric materials	In order to understand the physical hysteresis loops clearly, we constructed a novel model, which is combined with the electric field, the temperature, and the stress as one synthetically parameter. This model revealed the shape of hysteresis loop was determined by few variables in ferroelectric materials: the saturation of polarization, the coercive field, the electric susceptibility and the equivalent field. Comparison with experimental results revealed the model can retrace polarization versus electric field and temperature. As a applications of this model, the calculate formula of energy storage efficiency, the electrocaloric effect, and the P(E,T) function have also been included in this article.	0,1,0,0,0,0
Asymptotic behaviour methods for the Heat Equation. Convergence to the Gaussian	In this expository work we discuss the asymptotic behaviour of the solutions of the classical heat equation posed in the whole Euclidean space. After an introductory review of the main facts on the existence and properties of solutions, we proceed with the proofs of convergence to the Gaussian fundamental solution, a result that holds for all integrable solutions, and represents in the PDE setting the Central Limit Theorem of probability. We present several methods of proof: first, the scaling method. Then several versions of the representation method. This is followed by the functional analysis approach that leads to the famous related equations, Fokker-Planck and Ornstein-Uhlenbeck. The analysis of this connection is also given in rather complete form here. Finally, we present the Boltzmann entropy method, coming from kinetic equations. The different methods are interesting because of the possible extension to prove the asymptotic behaviour or stabilization analysis for more general equations, linear or nonlinear. It all depends a lot on the particular features, and only one or some of the methods work in each case.Other settings of the Heat Equation are briefly discussed in Section 9 and a longer mention of results for different equations is done in Section 10.	0,0,1,0,0,0
Deep Neural Network for Analysis of DNA Methylation Data	Many researches demonstrated that the DNA methylation, which occurs in the context of a CpG, has strong correlation with diseases, including cancer. There is a strong interest in analyzing the DNA methylation data to find how to distinguish different subtypes of the tumor. However, the conventional statistical methods are not suitable for analyzing the highly dimensional DNA methylation data with bounded support. In order to explicitly capture the properties of the data, we design a deep neural network, which composes of several stacked binary restricted Boltzmann machines, to learn the low dimensional deep features of the DNA methylation data. Experiments show these features perform best in breast cancer DNA methylation data cluster analysis, comparing with some state-of-the-art methods.	0,0,0,1,1,0
Dynamic Uplink/Downlink Resource Management in Flexible Duplex-Enabled Wireless Networks	Flexible duplex is proposed to adapt to the channel and traffic asymmetry for future wireless networks. In this paper, we propose two novel algorithms within the flexible duplex framework for joint uplink and downlink resource allocation in multi-cell scenario, named SAFP and RMDI, based on the awareness of interference coupling among wireless links. Numerical results show significant performance gain over the baseline system with fixed uplink/downlink resource configuration, and over the dynamic TDD scheme that independently adapts the configuration to time-varying traffic volume in each cell. The proposed algorithms achieve two-fold increase when compared with the baseline scheme, measured by the worst-case quality of service satisfaction level, under a low level of traffic asymmetry. The gain is more significant when the traffic is highly asymmetric, as it achieves three-fold increase.	1,0,0,0,0,0
On Detecting Adversarial Perturbations	Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small "detector" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.	1,0,0,1,0,0
Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro Gesture	In the research of the impact of gestures using by a lecturer, one challenging task is to infer the attention of a group of audiences. Two important measurements that can help infer the level of attention are eye movement data and Electroencephalography (EEG) data. Under the fundamental assumption that a group of people would look at the same place if they all pay attention at the same time, we apply a method, "Time Warp Edit Distance", to calculate the similarity of their eye movement trajectories. Moreover, we also cluster eye movement pattern of audiences based on these pair-wised similarity metrics. Besides, since we don't have a direct metric for the "attention" ground truth, a visual assessment would be beneficial to evaluate the gesture-attention relationship. Thus we also implement a visualization tool.	1,0,0,0,0,0
Can the Journal Impact Factor Be Used as a Criterion for the Selection of Junior Researchers? A Large-Scale Empirical Study Based on ResearcherID Data	Early in researchers' careers, it is difficult to assess how good their work is or how important or influential the scholars will eventually be. Hence, funding agencies, academic departments, and others often use the Journal Impact Factor (JIF) of where the authors have published to assess their work and provide resources and rewards for future work. The use of JIFs in this way has been heavily criticized, however. Using a large data set with many thousands of publication profiles of individual researchers, this study tests the ability of the JIF (in its normalized variant) to identify, at the beginning of their careers, those candidates who will be successful in the long run. Instead of bare JIFs and citation counts, the metrics used here are standardized according to Web of Science subject categories and publication years. The results of the study indicate that the JIF (in its normalized variant) is able to discriminate between researchers who published papers later on with a citation impact above or below average in a field and publication year - not only in the short term, but also in the long term. However, the low to medium effect sizes of the results also indicate that the JIF (in its normalized variant) should not be used as the sole criterion for identifying later success: other criteria, such as the novelty and significance of the specific research, academic distinctions, and the reputation of previous institutions, should also be considered.	1,1,0,0,0,0
On architectural choices in deep learning: From network structure to gradient convergence and parameter estimation	We study mechanisms to characterize how the asymptotic convergence of backpropagation in deep architectures, in general, is related to the network structure, and how it may be influenced by other design choices including activation type, denoising and dropout rate. We seek to analyze whether network architecture and input data statistics may guide the choices of learning parameters and vice versa. Given the broad applicability of deep architectures, this issue is interesting both from theoretical and a practical standpoint. Using properties of general nonconvex objectives (with first-order information), we first build the association between structural, distributional and learnability aspects of the network vis-à-vis their interaction with parameter convergence rates. We identify a nice relationship between feature denoising and dropout, and construct families of networks that achieve the same level of convergence. We then derive a workflow that provides systematic guidance regarding the choice of network sizes and learning parameters often mediated4 by input statistics. Our technical results are corroborated by an extensive set of evaluations, presented in this paper as well as independent empirical observations reported by other groups. We also perform experiments showing the practical implications of our framework for choosing the best fully-connected design for a given problem.	1,0,1,1,0,0
Bottom-up Object Detection by Grouping Extreme and Center Points	With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6% Mask AP.	1,0,0,0,0,0
Boundedness in a fully parabolic chemotaxis system with nonlinear diffusion and sensitivity, and logistic source	In this paper we study the zero-flux chemotaxis-system \begin{equation*} \begin{cases} u_{ t}=\nabla \cdot ((u+1)^{m-1} \nabla u-(u+1)^\alpha \chi(v)\nabla v) + ku-\mu u^2 & x\in \Omega, t>0, \\ v_{t} = \Delta v-vu & x\in \Omega, t>0,\\ \end{cases} \end{equation*} $\Omega$ being a bounded and smooth domain of $\mathbb{R}^n$, $n\geq 1$, and where $m,k \in \mathbb{R}$, $\mu>0$ and $\alpha < \frac{m+1}{2}$. For any $v\geq 0$ the chemotactic sensitivity function is assumed to behave as the prototype $\chi(v) = \frac{\chi_0}{(1+av)^2}$, with $a\geq 0$ and $\chi_0>0$. We prove that for nonnegative and sufficiently regular initial data $u(x,0)$ and $v(x,0),$ the corresponding initial-boundary value problem admits a global bounded classical solution provided $\mu$ is large enough.	0,0,1,0,0,0
Resource Allocation for Wireless Networks: A Distributed Optimization Approach	We consider the multi-cell joint power control and scheduling problem in cellular wireless networks as a weighted sum-rate maximization problem. This formulation is very general and applies to a wide range of applications and QoS requirements. The problem is inherently hard due to objective's non-convexity and the knapsack-like constraints. Moreover, practical system requires a distributed operation. We applied an existing algorithm proposed by Scutari et al. in distributed optimization literature to our problem. The algorithm performs local optimization followed by consensus update repeatedly. However, it is not fully applicable to our problem, as it requires all decision variables to be maintained at every base station (BS), which is impractical for large-scale networks; also, it relies on the Lipschitz continuity of the objective function's gradient, which does not hold here. We exploited the nature of our objective function, and proposed a localized version of the algorithm. Furthermore, we relaxed the requirements of Lipschitz continuity with the proximal approximation. Convergence to local optimal solutions was proved under some conditions. Future work includes proving the above results from a stochastic approximation perspective, and investigating non-linear consensus schemes to speed up the convergence.	0,0,1,0,0,0
Fuel-Efficient En Route Formation of Truck Platoons	The problem of how to coordinate a large fleet of trucks with given itinerary to enable fuel-efficient platooning is considered. Platooning is a promising technology that enables trucks to save significant amounts of fuel by driving close together and thus reducing air drag. A setting is considered in which each truck in a fleet is provided with a start location, a destination, a departure time, and an arrival deadline from a higher planning level. Fuel-efficient plans should be computed. The plans consist of routes and speed profiles that allow trucks to arrive by their arrival deadlines. Hereby, trucks can meet on common parts of their routes and form platoons, resulting in decreased fuel consumption. We formulate a combinatorial optimization problem that combines plans involving only two vehicles. We show that this problem is hard to solve for large problem instances. Hence a heuristic algorithm is proposed. The resulting plans are further optimized using convex optimization techniques. The method is evaluated with Monte Carlo simulations in a realistic setting. We demonstrate that the proposed algorithm can compute plans for thousands of trucks and that significant fuel savings can be achieved.	1,0,0,0,0,0
Binary Tomography Reconstructions With Few Projections	We approach the tomographic problem in terms of linear system of equations $A\mathbf{x}=\mathbf{p}$ in an $(M\times N)$-sized lattice grid $\mathcal{A}$. Using a finite number of directions always yields the presence of ghosts, so preventing uniqueness. Ghosts can be managed by increasing the number of directions, which implies that also the number of collected projections (also called bins) increases. Therefore, for a best performing outcome, a kind of compromise should be sought among the number of employed directions, the number of collected projections, and the percentage of exactly reconstructed image. In this paper we wish to investigate such a problem in the case of binary images. We move from a theoretical result that allow uniqueness in $\mathcal{A}$ with just four suitably selected X-ray directions. This is exploited in studying the structure of the allowed ghosts in the given lattice grid. The knowledge of the ghost sizes, combined with geometrical information concerning the real valued solution of $A\mathbf{x}=\mathbf{p}$ having minimal Euclidean norm, leads to an explicit implementation of the previously obtained uniqueness theorem. This provides an easy binary algorithm (BRA) that, in the grid model, quickly returns perfect noise-free tomographic reconstructions. Then we focus on the tomography-side relevant problem of reducing the number of collected projections and, in the meantime, preserving a good quality of reconstruction. It turns out that, using sets of just four suitable directions, a high percentage of reconstructed pixel is preserved, even when the size of the projection vector $\mathbf{p}$ is considerably smaller than the size of the image to be reconstructed. Results are commented and discussed, also showing applications of BRA on phantoms with different features.	1,0,0,0,0,0
Courcelle's Theorem Made Dynamic	Dynamic complexity is concerned with updating the output of a problem when the input is slightly changed. We study the dynamic complexity of model checking a fixed monadic second-order formula over evolving subgraphs of a fixed maximal graph having bounded tree-width; here the subgraph evolves by losing or gaining edges (from the maximal graph). We show that this problem is in DynFO (with LOGSPACE precomputation), via a reduction to a Dyck reachability problem on an acyclic automaton.	1,0,0,0,0,0
Theory of mechano-chemical patterning in biphasic biological tissues	The formation of self-organized patterns is key to the morphogenesis of multicellular organisms, although a comprehensive theory of biological pattern formation is still lacking. Here, we propose a minimal model combining tissue mechanics to morphogen turnover and transport in order to explore new routes to patterning. Our active description couples morphogen reaction-diffusion, which impact on cell differentiation and tissue mechanics, to a two-phase poroelastic rheology, where one tissue phase consists of a poroelastic cell network and the other of a permeating extracellular fluid, which provides a feedback by actively transporting morphogens. While this model encompasses previous theories approximating tissues to inert monophasic media, such as Turing's reaction-diffusion model, it overcomes some of their key limitations permitting pattern formation via any two-species biochemical kinetics thanks to mechanically induced cross-diffusion flows. Moreover, we describe a qualitatively different advection-driven Keller-Segel instability which allows for the formation of patterns with a single morphogen, and whose fundamental mode pattern robustly scales with tissue size. We discuss the potential relevance of these findings for tissue morphogenesis.	0,0,0,0,1,0
Expected Time to Extinction of SIS Epidemic Model Using Quasy Stationary Distribution	We study that the breakdown of epidemic depends on some parameters, that is expressed in epidemic reproduction ratio number. It is noted that when $R_0 $ exceeds 1, the stochastic model have two different results. But, eventually the extinction will be reached even though the major epidemic occurs. The question is how long this process will reach extinction. In this paper, we will focus on the Markovian process of SIS model when major epidemic occurs. Using the approximation of quasi--stationary distribution, the expected mean time of extinction only occurs when the process is one step away from being extinct. Combining the theorm from Ethier and Kurtz, we use CLT to find the approximation of this quasi distribution and successfully determine the asymptotic mean time to extinction of SIS model without demography.	0,0,0,0,1,0
Distribution uniformity of laser-accelerated proton beams	Compared with conventional accelerators, laser plasma accelerators can generate high energy ions at a greatly reduced scale, due to their TV/m acceleration gradient. A compact laser plasma accelerator (CLAPA) has been built at the Institute of Heavy Ion Physics at Peking University. It will be used for applied research like biological irradiation, astrophysics simulations, etc. A beamline system with multiple quadrupoles and an analyzing magnet for laser-accelerated ions is proposed here. Since laser-accelerated ion beams have broad energy spectra and large angular divergence, the parameters (beam waist position in the Y direction, beam line layout, drift distance, magnet angles etc.) of the beamline system are carefully designed and optimised to obtain a radially symmetric proton distribution at the irradiation platform. Requirements of energy selection and differences in focusing or defocusing in application systems greatly influence the evolution of proton distributions. With optimal parameters, radially symmetric proton distributions can be achieved and protons with different energy spread within 5% have similar transverse areas at the experiment target.	0,1,0,0,0,0
Using Inertial Sensors for Position and Orientation Estimation	In recent years, MEMS inertial sensors (3D accelerometers and 3D gyroscopes) have become widely available due to their small size and low cost. Inertial sensor measurements are obtained at high sampling rates and can be integrated to obtain position and orientation information. These estimates are accurate on a short time scale, but suffer from integration drift over longer time scales. To overcome this issue, inertial sensors are typically combined with additional sensors and models. In this tutorial we focus on the signal processing aspects of position and orientation estimation using inertial sensors. We discuss different modeling choices and a selected number of important algorithms. The algorithms include optimization-based smoothing and filtering as well as computationally cheaper extended Kalman filter and complementary filter implementations. The quality of their estimates is illustrated using both experimental and simulated data.	1,0,0,0,0,0
Operationalizing Conflict and Cooperation between Automated Software Agents in Wikipedia: A Replication and Expansion of 'Even Good Bots Fight'	This paper replicates, extends, and refutes conclusions made in a study published in PLoS ONE ("Even Good Bots Fight"), which claimed to identify substantial levels of conflict between automated software agents (or bots) in Wikipedia using purely quantitative methods. By applying an integrative mixed-methods approach drawing on trace ethnography, we place these alleged cases of bot-bot conflict into context and arrive at a better understanding of these interactions. We found that overwhelmingly, the interactions previously characterized as problematic instances of conflict are typically better characterized as routine, productive, even collaborative work. These results challenge past work and show the importance of qualitative/quantitative collaboration. In our paper, we present quantitative metrics and qualitative heuristics for operationalizing bot-bot conflict. We give thick descriptions of kinds of events that present as bot-bot reverts, helping distinguish conflict from non-conflict. We computationally classify these kinds of events through patterns in edit summaries. By interpreting found/trace data in the socio-technical contexts in which people give that data meaning, we gain more from quantitative measurements, drawing deeper understandings about the governance of algorithmic systems in Wikipedia. We have also released our data collection, processing, and analysis pipeline, to facilitate computational reproducibility of our findings and to help other researchers interested in conducting similar mixed-method scholarship in other platforms and contexts.	1,0,0,0,0,0
Conditionally conjugate mean-field variational Bayes for logistic models	Variational Bayes (VB) is a common strategy for approximate Bayesian inference, but simple methods are only available for specific classes of models including, in particular, representations having conditionally conjugate constructions within an exponential family. Models with logit components are an apparently notable exception to this class, due to the absence of conjugacy between the logistic likelihood and the Gaussian priors for the coefficients in the linear predictor. To facilitate approximate inference within this widely used class of models, Jaakkola and Jordan (2000) proposed a simple variational approach which relies on a family of tangent quadratic lower bounds of logistic log-likelihoods, thus restoring conjugacy between these approximate bounds and the Gaussian priors. This strategy is still implemented successfully, but less attempts have been made to formally understand the reasons underlying its excellent performance. To cover this key gap, we provide a formal connection between the above bound and a recent Pólya-gamma data augmentation for logistic regression. Such a result places the computational methods associated with the aforementioned bounds within the framework of variational inference for conditionally conjugate exponential family models, thereby allowing recent advances for this class to be inherited also by the methods relying on Jaakkola and Jordan (2000).	0,0,1,1,0,0
Note on the geodesic Monte Carlo	Geodesic Monte Carlo (gMC) is a powerful algorithm for Bayesian inference on non-Euclidean manifolds. The original gMC algorithm was cleverly derived in terms of its progenitor, the Riemannian manifold Hamiltonian Monte Carlo (RMHMC). Here, it is shown that alternative and theoretically simpler derivations are available in which the original algorithm is a special case of two general classes of algorithms characterized by non-trivial mass matrices. The proposed derivations work entirely in embedding coordinates and thus clarify the original algorithm as applied to manifolds embedded in Euclidean space.	0,0,0,1,0,0
Exponential Ergodicity of the Bouncy Particle Sampler	Non-reversible Markov chain Monte Carlo schemes based on piecewise deterministic Markov processes have been recently introduced in applied probability, automatic control, physics and statistics. Although these algorithms demonstrate experimentally good performance and are accordingly increasingly used in a wide range of applications, geometric ergodicity results for such schemes have only been established so far under very restrictive assumptions. We give here verifiable conditions on the target distribution under which the Bouncy Particle Sampler algorithm introduced in \cite{P_dW_12} is geometrically ergodic. This holds whenever the target satisfies a curvature condition and has tails decaying at least as fast as an exponential and at most as fast as a Gaussian distribution. This allows us to provide a central limit theorem for the associated ergodic averages. When the target has tails thinner than a Gaussian distribution, we propose an original modification of this scheme that is geometrically ergodic. For thick-tailed target distributions, such as $t$-distributions, we extend the idea pioneered in \cite{J_G_12} in a random walk Metropolis context. We apply a change of variable to obtain a transformed target satisfying the tail conditions for geometric ergodicity. By sampling the transformed target using the Bouncy Particle Sampler and mapping back the Markov process to the original parameterization, we obtain a geometrically ergodic algorithm.	0,0,0,1,0,0
On the validity of the formal Edgeworth expansion for posterior densities	We consider a fundamental open problem in parametric Bayesian theory, namely the validity of the formal Edgeworth expansion of the posterior density. While the study of valid asymptotic expansions for posterior distributions constitutes a rich literature, the validity of the formal Edgeworth expansion has not been rigorously established. Several authors have claimed connections of various posterior expansions with the classical Edgeworth expansion, or have simply assumed its validity. Our main result settles this open problem. We also prove a lemma concerning the order of posterior cumulants which is of independent interest in Bayesian parametric theory. The most relevant literature is synthesized and compared to the newly-derived Edgeworth expansions. Numerical investigations illustrate that our expansion has the behavior expected of an Edgeworth expansion, and that it has better performance than the other existing expansion which was previously claimed to be of Edgeworth-type.	0,0,1,1,0,0
On the $E$-polynomial of parabolic $\mathrm{Sp}_{2n}$-character varieties	We find the $E$-polynomials of a family of parabolic $\mathrm{Sp}_{2n}$-character varieties $\mathcal{M}^{\xi}_{n}$ of Riemann surfaces by constructing a stratification, proving that each stratum has polynomial count, applying a result of Katz regarding the counting functions, and finally adding up the resulting $E$-polynomials of the strata. To count the number of $\mathbb{F}_{q}$-points of the strata, we invoke a formula due to Frobenius. Our calculation make use of a formula for the evaluation of characters on semisimple elements coming from Deligne-Lusztig theory, applied to the character theory of $\mathrm{Sp}{\left(2n,\mathbb{F}_{q}\right)}$, and Möbius inversion on the poset of set-partitions. We compute the Euler characteristic of the $\mathcal{M}^{\xi}_{n}$ with these polynomials, and show they are connected.	0,0,1,0,0,0
Optic Disc and Cup Segmentation Methods for Glaucoma Detection with Modification of U-Net Convolutional Neural Network	Glaucoma is the second leading cause of blindness all over the world, with approximately 60 million cases reported worldwide in 2010. If undiagnosed in time, glaucoma causes irreversible damage to the optic nerve leading to blindness. The optic nerve head examination, which involves measurement of cup-to-disc ratio, is considered one of the most valuable methods of structural diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation of optic disc and optic cup on eye fundus images and can be performed by modern computer vision algorithms. This work presents universal approach for automatic optic disc and cup segmentation, which is based on deep learning, namely, modification of U-Net convolutional neural network. Our experiments include comparison with the best known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation, our method achieves quality comparable to current state-of-the-art methods, outperforming them in terms of the prediction time.	1,0,0,1,0,0
Decentralized Optimal Control for Connected Automated Vehicles at Intersections Including Left and Right Turns	In prior work, we addressed the problem of optimally controlling on line connected and automated vehicles crossing two adjacent intersections in an urban area to minimize fuel consumption while achieving maximal throughput without any explicit traffic signaling and without considering left and right turns. In this paper, we extend the solution of this problem to account for left and right turns under hard safety constraints. Furthermore, we formulate and solve another optimization problem to minimize a measure of passenger discomfort while the vehicle turns at the intersection and we investigate the associated tradeoff between minimizing fuel consumption and passenger discomfort.	0,0,1,0,0,0
Spectral analysis of stationary random bivariate signals	A novel approach towards the spectral analysis of stationary random bivariate signals is proposed. Using the Quaternion Fourier Transform, we introduce a quaternion-valued spectral representation of random bivariate signals seen as complex-valued sequences. This makes possible the definition of a scalar quaternion-valued spectral density for bivariate signals. This spectral density can be meaningfully interpreted in terms of frequency-dependent polarization attributes. A natural decomposition of any random bivariate signal in terms of unpolarized and polarized components is introduced. Nonparametric spectral density estimation is investigated, and we introduce the polarization periodogram of a random bivariate signal. Numerical experiments support our theoretical analysis, illustrating the relevance of the approach on synthetic data.	0,0,0,1,0,0
Condition number and matrices	It is well known the concept of the condition number $\kappa(A) = \|A\|\|A^{-1}\|$, where $A$ is a $n \times n$ real or complex matrix and the norm used is the spectral norm. Although it is very common to think in $\kappa(A)$ as "the" condition number of $A$, the truth is that condition numbers are associated to problems, not just instance of problems. Our goal is to clarify this difference. We will introduce the general concept of condition number and apply it to the particular case of real or complex matrices. After this, we will introduce the classic condition number $\kappa(A)$ of a matrix and show some known results.	0,0,1,0,0,0
Control Problems with Vanishing Lie Bracket Arising from Complete Odd Circulant Evolutionary Games	We study an optimal control problem arising from a generalization of rock-paper-scissors in which the number of strategies may be selected from any positive odd number greater than 1 and in which the payoff to the winner is controlled by a control variable $\gamma$. Using the replicator dynamics as the equations of motion, we show that a quasi-linearization of the problem admits a special optimal control form in which explicit dynamics for the controller can be identified. We show that all optimal controls must satisfy a specific second order differential equation parameterized by the number of strategies in the game. We show that as the number of strategies increases, a limiting case admits a closed form for the open-loop optimal control. In performing our analysis we show necessary conditions on an optimal control problem that allow this analytic approach to function.	1,0,0,0,0,0
Ensuring patients privacy in a cryptographic-based-electronic health records using bio-cryptography	Several recent works have proposed and implemented cryptography as a means to preserve privacy and security of patients health data. Nevertheless, the weakest point of electronic health record (EHR) systems that relied on these cryptographic schemes is key management. Thus, this paper presents the development of privacy and security system for cryptography-based-EHR by taking advantage of the uniqueness of fingerprint and iris characteristic features to secure cryptographic keys in a bio-cryptography framework. The results of the system evaluation showed significant improvements in terms of time efficiency of this approach to cryptographic-based-EHR. Both the fuzzy vault and fuzzy commitment demonstrated false acceptance rate (FAR) of 0%, which reduces the likelihood of imposters gaining successful access to the keys protecting patients protected health information. This result also justifies the feasibility of implementing fuzzy key binding scheme in real applications, especially fuzzy vault which demonstrated a better performance during key reconstruction.	1,0,0,0,0,0
On the convergence of a fully discrete scheme of LES type to physically relevant solutions of the incompressible Navier-Stokes	Obtaining reliable numerical simulations of turbulent fluids is a challenging problem in computational fluid mechanics. The Large Eddy Simulations (LES) models are efficient tools to approximate turbulent fluids and an important step in the validation of these models is the ability to reproduce relevant properties of the flow. In this paper we consider a fully discrete approximation of the Navier-Stokes-Voigt model by an implicit Euler algorithm (with respect to the time variable) and a Fourier-Galerkin method (in the space variables). We prove the convergence to weak solutions of the incompressible Navier-Stokes equations satisfying the natural local entropy condition, hence selecting the so-called physically relevant solutions	0,0,1,0,0,0
Bounded solutions for a class of Hamiltonian systems	We obtain bounded for all $t$ solutions of ordinary differential equations as limits of the solutions of the corresponding Dirichlet problems on $(-L,L)$, with $L \rightarrow \infty$. We derive a priori estimates for the Dirichlet problems, allowing passage to the limit, via a diagonal sequence. This approach carries over to the PDE case.	0,0,1,0,0,0
A Heat Equation on some Adic Completions of Q and Ultrametric Analysis	This article deals with a Markov process related to the fundamental solution of a heat equation on the direct product ring Q_S, where Q_S is a finite direct product of p-adic fields. The techniques developed here are different from the well known ones: they are geometrical and very simple. As a result, the techniques developed here provides a general framework of these problems on other related ultrametric groups.	0,0,1,0,0,0
Rigorous Analysis for Efficient Statistically Accurate Algorithms for Solving Fokker-Planck Equations in Large Dimensions	This article presents a rigorous analysis for efficient statistically accurate algorithms for solving the Fokker-Planck equations associated with high-dimensional nonlinear turbulent dynamical systems with conditional Gaussian structures. Despite the conditional Gaussianity, these nonlinear systems contain many strong non-Gaussian features such as intermittency and fat-tailed probability density functions (PDFs). The algorithms involve a hybrid strategy that requires only a small number of samples $L$ to capture both the transient and the equilibrium non-Gaussian PDFs with high accuracy. Here, a conditional Gaussian mixture in a high-dimensional subspace via an extremely efficient parametric method is combined with a judicious Gaussian kernel density estimation in the remaining low-dimensional subspace. Rigorous analysis shows that the mean integrated squared error in the recovered PDFs in the high-dimensional subspace is bounded by the inverse square root of the determinant of the conditional covariance, where the conditional covariance is completely determined by the underlying dynamics and is independent of $L$. This is fundamentally different from a direct application of kernel methods to solve the full PDF, where $L$ needs to increase exponentially with the dimension of the system and the bandwidth shrinks. A detailed comparison between different methods justifies that the efficient statistically accurate algorithms are able to overcome the curse of dimensionality. It is also shown with mathematical rigour that these algorithms are robust in long time provided that the system is controllable and stochastically stable. Particularly, dynamical systems with energy-conserving quadratic nonlinearity as in many geophysical and engineering turbulence are proved to have these properties.	0,0,1,1,0,0
From sudden quench to adiabatic dynamics in the attractive Hubbard model	We study the crossover between the sudden quench limit and the adiabatic dynamics of superconducting states in the attractive Hubbard model. We focus on the dynamics induced by the change of the attractive interaction during a finite ramp time which is varied in order to track the evolution of the dynamical phase diagram from the sudden quench to the equilibrium limit. Two different dynamical regimes are realized for quenches towards weak and strong coupling interactions. At weak coupling the dynamics depends only on the energy injected into the system, whereas a dynamics retaining memory of the initial state takes place at strong coupling. We show that this is related to a sharp transition between a weak and a strong coupling quench dynamical regime, which defines the boundaries beyond which a dynamics independent from the initial state is recovered. Comparing the dynamics in the superconducting and non-superconducting phases we argue that this is due to the lack of an adiabatic connection to the equilibrium ground state for non-equilibrium superconducting states in the strong coupling quench regime.	0,1,0,0,0,0
The Elasticity of Puiseux Monoids	Let $M$ be an atomic monoid and let $x$ be a non-unit element of $M$. The elasticity of $x$, denoted by $\rho(x)$, is the ratio of its largest factorization length to its shortest factorization length, and it measures how far is $x$ from having a unique factorization. The elasticity $\rho(M)$ of $M$ is the supremum of the elasticities of all non-unit elements of $M$. The monoid $M$ has accepted elasticity if $\rho(M) = \rho(m)$ for some $m \in M$. In this paper, we study the elasticity of Puiseux monoids (i.e., additive submonoids of $\mathbb{Q}_{\ge 0}$). First, we characterize the Puiseux monoids $M$ having finite elasticity and find a formula for $\rho(M)$. Then we classify the Puiseux monoids having accepted elasticity in terms of their sets of atoms. When $M$ is a primary Puiseux monoid, we describe the topology of the set of elasticities of $M$, including a characterization of when $M$ is a bounded factorization monoid. Lastly, we give an example of a Puiseux monoid that is bifurcus (that is, every nonzero element has a factorization of length at most $2$).	0,0,1,0,0,0
A Non-Gaussian, Nonparametric Structure for Gene-Gene and Gene-Environment Interactions in Case-Control Studies Based on Hierarchies of Dirichlet Processes	It is becoming increasingly clear that complex interactions among genes and environmental factors play crucial roles in triggering complex diseases. Thus, understanding such interactions is vital, which is possible only through statistical models that adequately account for such intricate, albeit unknown, dependence structures. Bhattacharya & Bhattacharya (2016b) attempt such modeling, relating finite mixtures composed of Dirichlet processes that represent unknown number of genetic sub-populations through a hierarchical matrix-normal structure that incorporates gene-gene interactions, and possible mutations, induced by environmental variables. However, the product dependence structure implied by their matrix-normal model seems to be too simple to be appropriate for general complex, realistic situations. In this article, we propose and develop a novel nonparametric Bayesian model for case-control genotype data using hierarchies of Dirichlet processes that offers a more realistic and nonparametric dependence structure between the genes, induced by the environmental variables. In this regard, we propose a novel and highly parallelisable MCMC algorithm that is rendered quite efficient by the combination of modern parallel computing technology, effective Gibbs sampling steps, retrospective sampling and Transformation based Markov Chain Monte Carlo (TMCMC). We use appropriate Bayesian hypothesis testing procedures to detect the roles of genes and environment in case-control studies. We apply our ideas to 5 biologically realistic case-control genotype datasets simulated under distinct set-ups, and obtain encouraging results in each case. We finally apply our ideas to a real, myocardial infarction dataset, and obtain interesting results on gene-gene and gene-environment interaction, while broadly agreeing with the results reported in the literature.	0,0,0,1,0,0
Correlation decay in fermionic lattice systems with power-law interactions at non-zero temperature	We study correlations in fermionic lattice systems with long-range interactions in thermal equilibrium. We prove a bound on the correlation decay between anti-commuting operators and generalize a long-range Lieb-Robinson type bound. Our results show that in these systems of spatial dimension $D$ with, not necessarily translation invariant, two-site interactions decaying algebraically with the distance with an exponent $\alpha \geq 2\,D$, correlations between such operators decay at least algebraically with an exponent arbitrarily close to $\alpha$ at any non-zero temperature. Our bound is asymptotically tight, which we demonstrate by a high temperature expansion and by numerically analyzing density-density correlations in the 1D quadratic (free, exactly solvable) Kitaev chain with long-range pairing.	0,1,0,0,0,0
Maximum Margin Principal Components	Principal Component Analysis (PCA) is a very successful dimensionality reduction technique, widely used in predictive modeling. A key factor in its widespread use in this domain is the fact that the projection of a dataset onto its first $K$ principal components minimizes the sum of squared errors between the original data and the projected data over all possible rank $K$ projections. Thus, PCA provides optimal low-rank representations of data for least-squares linear regression under standard modeling assumptions. On the other hand, when the loss function for a prediction problem is not the least-squares error, PCA is typically a heuristic choice of dimensionality reduction -- in particular for classification problems under the zero-one loss. In this paper we target classification problems by proposing a straightforward alternative to PCA that aims to minimize the difference in margin distribution between the original and the projected data. Extensive experiments show that our simple approach typically outperforms PCA on any particular dataset, in terms of classification error, though this difference is not always statistically significant, and despite being a filter method is frequently competitive with Partial Least Squares (PLS) and Lasso on a wide range of datasets.	1,0,0,1,0,0
Deep Graph Infomax	We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.	1,0,0,1,0,0
Noise-synchronizability of opinion dynamics	With the analysis of noise-induced synchronization of opinion dynamics with bounded confidence (BC), a natural and fundamental question is what opinion structures can be synchronized by noise. In the traditional Hegselmann-Krause (HK) model, each agent examines the opinion values of all the other ones and then choose neighbors to update its own opinion according to the BC scheme. In reality, people are more likely to interchange opinions with only some individuals, resulting in a predetermined local discourse relationship as introduced by the DeGroot model. In this paper, we consider an opinion dynamics that combines the schemes of BC and local discourse topology and investigate its synchronization induced by noise. The new model endows the heterogeneous HK model with a time-varying discourse topology. With the proposed definition of noise-synchronizability, it is shown that the compound noisy model is almost surely noise-synchronizable if and only if the time-varying discourse graph is uniformly jointly connected, taking the noise-induced synchronization of the classical heterogeneous HK model as a special case. As a natural implication, the result for the first time builds the equivalence between the connectivity of discourse graph and the beneficial effect of noise for opinion consensus.	1,0,0,0,0,0
Motion Planning in Irreducible Path Spaces	The motion of a mechanical system can be defined as a path through its configuration space. Computing such a path has a computational complexity scaling exponentially with the dimensionality of the configuration space. We propose to reduce the dimensionality of the configuration space by introducing the irreducible path --- a path having a minimal swept volume. The paper consists of three parts: In part I, we define the space of all irreducible paths and show that planning a path in the irreducible path space preserves completeness of any motion planning algorithm. In part II, we construct an approximation to the irreducible path space of a serial kinematic chain under certain assumptions. In part III, we conduct motion planning using the irreducible path space for a mechanical snake in a turbine environment, for a mechanical octopus with eight arms in a pipe system and for the sideways motion of a humanoid robot moving through a room with doors and through a hole in a wall. We demonstrate that the concept of an irreducible path can be applied to any motion planning algorithm taking curvature constraints into account.	1,0,0,0,0,0
Parallel G-duplex and C-duplex DNA with Uninterrupted Spines of AgI-Mediated Base Pairs	Hydrogen bonding between nucleobases produces diverse DNA structural motifs, including canonical duplexes, guanine (G) quadruplexes and cytosine (C) i-motifs. Incorporating metal-mediated base pairs into nucleic acid structures can introduce new functionalities and enhanced stabilities. Here we demonstrate, using mass spectrometry (MS), ion mobility spectrometry (IMS) and fluorescence resonance energy transfer (FRET), that parallel-stranded structures consisting of up to 20 G-Ag(I)-G contiguous base pairs are formed when natural DNA sequences are mixed with silver cations in aqueous solution. FRET indicates that duplexes formed by poly(cytosine) strands with 20 contiguous C-Ag(I)-C base pairs are also parallel. Silver-mediated G-duplexes form preferentially over G-quadruplexes, and the ability of Ag+ to convert G-quadruplexes into silver-paired duplexes may provide a new route to manipulating these biologically relevant structures. IMS indicates that G-duplexes are linear and more rigid than B-DNA. DFT calculations were used to propose structures compatible with the IMS experiments. Such inexpensive, defect-free and soluble DNA-based nanowires open new directions in the design of novel metal-mediated DNA nanotechnology.	0,0,0,0,1,0
Frames of exponentials and sub-multitiles in LCA groups	In this note we investigate the existence of frames of exponentials for $L^2(\Omega)$ in the setting of LCA groups. Our main result shows that sub-multitiling properties of $\Omega \subset \widehat{G}$ with respect to a uniform lattice $\Gamma$ of $\widehat{G}$ guarantee the existence of a frame of exponentials with frequencies in a finite number of translates of the annihilator of $\Gamma$. We also prove the converse of this result and provide conditions for the existence of these frames. These conditions extend recent results on Riesz bases of exponentials and multitilings to frames.	0,0,1,0,0,0
$\mathcal{G}$-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space	It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant. Conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which is, however, not positively scale-invariant. This mismatch may lead to problems during the optimization process. Then, a natural question is: \emph{can we construct a new vector space that is positively scale-invariant and sufficient to represent ReLU neural networks so as to better facilitate the optimization process }? In this paper, we provide our positive answer to this question. First, we conduct a formal study on the positive scaling operators which forms a transformation group, denoted as $\mathcal{G}$. We show that the value of a path (i.e. the product of the weights along the path) in the neural network is invariant to positive scaling and prove that the value vector of all the paths is sufficient to represent the neural networks under mild conditions. Second, we show that one can identify some basis paths out of all the paths and prove that the linear span of their value vectors (denoted as $\mathcal{G}$-space) is an invariant space with lower dimension under the positive scaling group. Finally, we design stochastic gradient descent algorithm in $\mathcal{G}$-space (abbreviated as $\mathcal{G}$-SGD) to optimize the value vector of the basis paths of neural networks with little extra cost by leveraging back-propagation. Our experiments show that $\mathcal{G}$-SGD significantly outperforms the conventional SGD algorithm in optimizing ReLU networks on benchmark datasets.	0,0,0,1,0,0
Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples	We present a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN. We do this using Angluin's L* algorithm as a learner and the trained RNN as an oracle. Our technique efficiently extracts accurate automata from trained RNNs, even when the state vectors are large and require fine differentiation.	1,0,0,0,0,0
Synthesizing Neural Network Controllers with Probabilistic Model based Reinforcement Learning	We present an algorithm for rapidly learning controllers for robotics systems. The algorithm follows the model-based reinforcement learning paradigm, and improves upon existing algorithms; namely Probabilistic learning in Control (PILCO) and a sample-based version of PILCO with neural network dynamics (Deep-PILCO). We propose training a neural network dynamics model using variational dropout with truncated Log-Normal noise. This allows us to obtain a dynamics model with calibrated uncertainty, which can be used to simulate controller executions via rollouts. We also describe set of techniques, inspired by viewing PILCO as a recurrent neural network model, that are crucial to improve the convergence of the method. We test our method on a variety of benchmark tasks, demonstrating data-efficiency that is competitive with PILCO, while being able to optimize complex neural network controllers. Finally, we assess the performance of the algorithm for learning motor controllers for a six legged autonomous underwater vehicle. This demonstrates the potential of the algorithm for scaling up the dimensionality and dataset sizes, in more complex control tasks.	1,0,0,0,0,0
Statistical Analysis of Precipitation Events	In the present paper we demonstrate the results of a statistical analysis of some characteristics of precipitation events and propose a kind of a theoretical explanation of the proposed models in terms of mixed Poisson and mixed exponential distributions based on the information-theoretical entropy reasoning. The proposed models can be also treated as the result of following the popular Bayesian approach.	0,0,0,1,0,0
The temporalized Massey's method	We propose and throughly investigate a temporalized version of the popular Massey's technique for rating actors in sport competitions. The method can be described as a dynamic temporal process in which team ratings are updated at every match according to their performance during the match and the strength of the opponent team. Using the Italian soccer dataset, we empirically show that the method has a good foresight prediction accuracy.	1,1,0,0,0,0
A Review on Deep Learning Techniques Applied to Semantic Segmentation	Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.	1,0,0,0,0,0
Entropy? Honest!	Here we deconstruct, and then in a reasoned way reconstruct, the concept of "entropy of a system," paying particular attention to where the randomness may be coming from. We start with the core concept of entropy as a COUNT associated with a DESCRIPTION; this count (traditionally expressed in logarithmic form for a number of good reasons) is in essence the number of possibilities---specific instances or "scenarios," that MATCH that description. Very natural (and virtually inescapable) generalizations of the idea of description are the probability distribution and of its quantum mechanical counterpart, the density operator. We track the process of dynamically updating entropy as a system evolves. Three factors may cause entropy to change: (1) the system's INTERNAL DYNAMICS; (2) unsolicited EXTERNAL INFLUENCES on it; and (3) the approximations one has to make when one tries to predict the system's future state. The latter task is usually hampered by hard-to-quantify aspects of the original description, limited data storage and processing resource, and possibly algorithmic inadequacy. Factors 2 and 3 introduce randomness into one's predictions and accordingly degrade them. When forecasting, as long as the entropy bookkeping is conducted in an HONEST fashion, this degradation will ALWAYS lead to an entropy increase. To clarify the above point we introduce the notion of HONEST ENTROPY, which coalesces much of what is of course already done, often tacitly, in responsible entropy-bookkeping practice. This notion, we believe, will help to fill an expressivity gap in scientific discourse. With its help we shall prove that ANY dynamical system---not just our physical universe---strictly obeys Clausius's original formulation of the second law of thermodynamics IF AND ONLY IF it is invertible. Thus this law is a TAUTOLOGICAL PROPERTY of invertible systems!	0,1,0,0,0,0
Statistical inference using SGD	We present a novel method for frequentist statistical inference in $M$-estimation problems, based on stochastic gradient descent (SGD) with a fixed step size: we demonstrate that the average of such SGD sequences can be used for statistical inference, after proper scaling. An intuitive analysis using the Ornstein-Uhlenbeck process suggests that such averages are asymptotically normal. From a practical perspective, our SGD-based inference procedure is a first order method, and is well-suited for large scale problems. To show its merits, we apply it to both synthetic and real datasets, and demonstrate that its accuracy is comparable to classical statistical methods, while requiring potentially far less computation.	1,0,1,1,0,0
A unimodular Liouville hyperbolic souvlaki --- an appendix to [arXiv:1603.06712]	Carmesin, Federici, and Georgakopoulos [arXiv:1603.06712] constructed a transient hyperbolic graph that has no transient subtrees and that has the Liouville property for harmonic functions. We modify their construction to get a unimodular random graph with the same properties.	0,0,1,0,0,0
A comment on `An improved macroscale model for gas slip flow in porous media'	In a recent paper by Lasseux, Valdés-Parada and Porter (J.~Fluid~Mech. \textbf{805} (2016) 118-146), it is found that the apparent gas permeability of the porous media is a nonlinear function of the Knudsen number. However, this result is highly questionable, because the adopted Navier-Stokes equations and the first-order velocity-slip boundary condition are first-order (in terms of the Knudsen number) approximations of the Boltzmann equation and the kinetic boundary condition for rarefied gas flows. Our numerical simulations based on the Bhatnagar-Gross-Krook kinetic equation and regularized 20-moment equations prove that the Navier-Stokes equations with the first-order velocity-slip boundary condition are only accurate at a very small Knudsen number limit, where the apparent gas permeability is a linear function of the Knudsen number.	0,1,0,0,0,0
Common change point estimation in panel data from the least squares and maximum likelihood viewpoints	We establish the convergence rates and asymptotic distributions of the common break change-point estimators, obtained by least squares and maximum likelihood in panel data models and compare their asymptotic variances. Our model assumptions accommodate a variety of commonly encountered probability distributions and, in particular, models of particular interest in econometrics beyond the commonly analyzed Gaussian model, including the zero-inflated Poisson model for count data, and the probit and tobit models. We also provide novel results for time dependent data in the signal-plus-noise model, with emphasis on a wide array of noise processes, including Gaussian process, MA$(\infty)$ and $m$-dependent processes. The obtained results show that maximum likelihood estimation requires a stronger signal-to-noise model identifiability condition compared to its least squares counterpart. Finally, since there are three different asymptotic regimes that depend on the behavior of the norm difference of the model parameters before and after the change point, which cannot be realistically assumed to be known, we develop a novel data driven adaptive procedure that provides valid confidence intervals for the common break, without requiring a priori knowledge of the asymptotic regime the problem falls in.	0,0,1,1,0,0
A Note on Bayesian Model Selection for Discrete Data Using Proper Scoring Rules	We consider the problem of choosing between parametric models for a discrete observable, taking a Bayesian approach in which the within-model prior distributions are allowed to be improper. In order to avoid the ambiguity in the marginal likelihood function in such a case, we apply a homogeneous scoring rule. For the particular case of distinguishing between Poisson and Negative Binomial models, we conduct simulations that indicate that, applied prequentially, the method will consistently select the true model.	0,0,1,1,0,0
Deep Generative Models with Learnable Knowledge Constraints	The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.	0,0,0,1,0,0
Heterogeneous inputs to central pattern generators can shape insect gaits	In our previous work, we studied an interconnected bursting neuron model for insect locomotion, and its corresponding phase oscillator model, which at high speed can generate stable tripod gaits with three legs off the ground simultaneously in swing, and at low speed can generate stable tetrapod gaits with two legs off the ground simultaneously in swing. However, at low speed several other stable locomotion patterns, that are not typically observed as insect gaits, may coexist. In the present paper, by adding heterogeneous external input to each oscillator, we modify the bursting neuron model so that its corresponding phase oscillator model produces only one stable gait at each speed, specifically: a unique stable tetrapod gait at low speed, a unique stable tripod gait at high speed, and a unique branch of stable transition gaits connecting them. This suggests that control signals originating in the brain and central nervous system can modify gait patterns.	0,0,0,0,1,0
Constraining the contribution of active galactic nuclei to reionisation	Recent results have suggested that active galactic nuclei (AGN) could provide enough photons to reionise the Universe. We assess the viability of this scenario using a semi-numerical framework for modeling reionisation, to which we add a quasar contribution by constructing a Quasar Halo Occupation Distribution (QHOD) based on Giallongo et al. observations. Assuming a constant QHOD, we find that an AGN-only model cannot simultaneously match observations of the optical depth $\tau_e$, neutral fraction, and ionising emissivity. Such a model predicts $\tau_e$ too low by $\sim 2\sigma$ relative to Planck constraints, and reionises the Universe at $z\lesssim 5$. Arbitrarily increasing the AGN emissivity to match these results yields a strong mismatch with the observed ionising emissivity at $z\sim 5$. If we instead assume a redshift-independent AGN luminosity function yielding an emissivity evolution like that assumed in Madau & Haardt model, then we can match $\tau_e$ albeit with late reionisation, however such evolution is inconsistent with observations at $z\sim 4-6$ and poorly motivated physically. These results arise because AGN are more biased towards massive halos than typical reionising galaxies, resulting in stronger clustering and later formation times. AGN-dominated models produce larger ionising bubbles that are reflected in $\sim\times 2$ more 21cm power on all scales. A model with equal parts galaxies and AGN contribution is still (barely) consistent with observations, but could be distinguished using next-generation 21cm experiments HERA and SKA-low. We conclude that, even with recent claims of more faint AGN than previously thought, AGN are highly unlikely to dominate the ionising photon budget for reionisation.	0,1,0,0,0,0
Pattern Search Multidimensional Scaling	We present a novel view of nonlinear manifold learning using derivative-free optimization techniques. Specifically, we propose an extension of the classical multi-dimensional scaling (MDS) method, where instead of performing gradient descent, we sample and evaluate possible "moves" in a sphere of fixed radius for each point in the embedded space. A fixed-point convergence guarantee can be shown by formulating the proposed algorithm as an instance of General Pattern Search (GPS) framework. Evaluation on both clean and noisy synthetic datasets shows that pattern search MDS can accurately infer the intrinsic geometry of manifolds embedded in high-dimensional spaces. Additionally, experiments on real data, even under noisy conditions, demonstrate that the proposed pattern search MDS yields state-of-the-art results.	0,0,0,1,0,0
A Survey on Hypergraph Products (Erratum)	A surprising diversity of different products of hypergraphs have been discussed in the literature. Most of the hypergraph products can be viewed as generalizations of one of the four standard graph products. The most widely studied variant, the so-called square product, does not have this property, however. Here we survey the literature on hypergraph products with an emphasis on comparing the alternative generalizations of graph products and the relationships among them. In this context the so-called 2-sections and L2-sections are considered. These constructions are closely linked to related colored graph structures that seem to be a useful tool for the prime factor decompositions w.r.t.\ specific hypergraph products. We summarize the current knowledge on the propagation of hypergraph invariants under the different hypergraph multiplications. While the overwhelming majority of the material concerns finite (undirected) hypergraphs, the survey also covers a summary of the few results on products of infinite and directed hypergraphs.	1,0,0,0,0,0
Theoretical derivation of laser-dressed atomic states by using a fractal space	The derivation of approximate wave functions for an electron submitted to both a coulomb and a time-dependent laser electric fields, the so-called Coulomb-Volkov (CV) state, is addressed. Despite its derivation for continuum states does not exhibit any particular problem within the framework of the standard theory of quantum mechanics (QM), difficulties arise when considering an initially bound atomic state. Indeed the natural way of translating the unperturbed momentum by the laser vector potential is no longer possible since a bound state does not exhibit a plane wave form including explicitely a momentum. The use of a fractal space permits to naturally define a momentum for a bound wave function. Within this framework, it is shown how the derivation of laser-dressed bound states can be performed. Based on a generalized eikonal approach, a new expression for the laser-dressed states is also derived, fully symmetric relative to the continuum or bound nature of the initial unperturbed wave function. It includes an additional crossed term in the Volkov phase which was not obtained within the standard theory of quantum mechanics. The derivations within this fractal framework have highlighted other possible ways to derive approximate laser-dressed states in QM. After comparing the various obtained wave functions, an application to the prediction of the ionization probability of hydrogen targets by attosecond XUV pulses within the sudden approximation is provided. This approach allows to make predictions in various regimes depending on the laser intensity, going from the non-resonant multiphoton absorption to tunneling and barrier-suppression ionization.	0,1,0,0,0,0
Decidability problems in automaton semigroups	We consider decidability problems in self-similar semigroups, and in particular in semigroups of automatic transformations of $X^*$. We describe algorithms answering the word problem, and bound its complexity under some additional assumptions. We give a partial algorithm that decides in a group generated by an automaton, given $x,y$, whether an Engel identity ($[\cdots[[x,y],y],\dots,y]=1$ for a long enough commutator sequence) is satisfied. This algorithm succeeds, importantly, in proving that Grigorchuk's $2$-group is not Engel. We consider next the problem of recognizing Engel elements, namely elements $y$ such that the map $x\mapsto[x,y]$ attracts to $\{1\}$. Although this problem seems intractable in general, we prove that it is decidable for Grigorchuk's group: Engel elements are precisely those of order at most $2$. We include, in the text, a large number of open problems. Our computations were implemented using the package "Fr" within the computer algebra system "Gap".	1,0,1,0,0,0
STFT spectral loss for training a neural speech waveform model	This paper proposes a new loss using short-time Fourier transform (STFT) spectra for the aim of training a high-performance neural speech waveform model that predicts raw continuous speech waveform samples directly. Not only amplitude spectra but also phase spectra obtained from generated speech waveforms are used to calculate the proposed loss. We also mathematically show that training of the waveform model on the basis of the proposed loss can be interpreted as maximum likelihood training that assumes the amplitude and phase spectra of generated speech waveforms following Gaussian and von Mises distributions, respectively. Furthermore, this paper presents a simple network architecture as the speech waveform model, which is composed of uni-directional long short-term memories (LSTMs) and an auto-regressive structure. Experimental results showed that the proposed neural model synthesized high-quality speech waveforms.	1,0,0,0,0,0
Generic Cospark of a Matrix Can Be Computed in Polynomial Time	The cospark of a matrix is the cardinality of the sparsest vector in the column space of the matrix. Computing the cospark of a matrix is well known to be an NP hard problem. Given the sparsity pattern (i.e., the locations of the non-zero entries) of a matrix, if the non-zero entries are drawn from independently distributed continuous probability distributions, we prove that the cospark of the matrix equals, with probability one, to a particular number termed the generic cospark of the matrix. The generic cospark also equals to the maximum cospark of matrices consistent with the given sparsity pattern. We prove that the generic cospark of a matrix can be computed in polynomial time, and offer an algorithm that achieves this.	1,0,0,0,0,0
Collective irregular dynamics in balanced networks of leaky integrate-and-fire neurons	We extensively explore networks of weakly unbalanced, leaky integrate-and-fire (LIF) neurons for different coupling strength, connectivity, and by varying the degree of refractoriness, as well as the delay in the spike transmission. We find that the neural network does not only exhibit a microscopic (single-neuron) stochastic-like evolution, but also a collective irregular dynamics (CID). Our analysis is based on the computation of a suitable order parameter, typically used to characterize synchronization phenomena and on a detailed scaling analysis (i.e. simulations of different network sizes). As a result, we can conclude that CID is a true thermodynamic phase, intrinsically different from the standard asynchronous regime.	0,0,0,0,1,0
Is Smaller Better: A Proposal To Consider Bacteria For Biologically Inspired Modeling	Bacteria are easily characterizable model organisms with an impressively complicated set of capabilities. Among their capabilities is quorum sensing, a detailed cell-cell signaling system that may have a common origin with eukaryotic cell-cell signaling. Not only are the two phenomena similar, but quorum sensing, as is the case with any bacterial phenomenon when compared to eukaryotes, is also easier to study in depth than eukaryotic cell-cell signaling. This ease of study is a contrast to the only partially understood cellular dynamics of neurons. Here we review the literature on the strikingly neuron-like qualities of bacterial colonies and biofilms, including ion-based and hormonal signaling, and action potential-like behavior. This allows them to feasibly act as an analog for neurons that could produce more detailed and more accurate biologically-based computational models. Using bacteria as the basis for biologically feasible computational models may allow models to better harness the tremendous ability of biological organisms to make decisions and process information. Additionally, principles gleaned from bacterial function have the potential to influence computational efforts divorced from biology, just as neuronal function has in the abstract influenced countless machine learning efforts.	1,0,0,0,0,0
EasyInterface: A toolkit for rapid development of GUIs for research prototype tools	In this paper we describe EasyInterface, an open-source toolkit for rapid development of web-based graphical user interfaces (GUIs). This toolkit addresses the need of researchers to make their research prototype tools available to the community, and integrating them in a common environment, rapidly and without being familiar with web programming or GUI libraries in general. If a tool can be executed from a command-line and its output goes to the standard output, then in few minutes one can make it accessible via a web-interface or within Eclipse. Moreover, the toolkit defines a text-based language that can be used to get more sophisticated GUIs, e.g., syntax highlighting, dialog boxes, user interactions, etc. EasyInterface was originally developed for building a common frontend for tools developed in the Envisage project.	1,0,0,0,0,0
PD-ML-Lite: Private Distributed Machine Learning from Lighweight Cryptography	Privacy is a major issue in learning from distributed data. Recently the cryptographic literature has provided several tools for this task. However, these tools either reduce the quality/accuracy of the learning algorithm---e.g., by adding noise---or they incur a high performance penalty and/or involve trusting external authorities. We propose a methodology for {\sl private distributed machine learning from light-weight cryptography} (in short, PD-ML-Lite). We apply our methodology to two major ML algorithms, namely non-negative matrix factorization (NMF) and singular value decomposition (SVD). Our resulting protocols are communication optimal, achieve the same accuracy as their non-private counterparts, and satisfy a notion of privacy---which we define---that is both intuitive and measurable. Our approach is to use lightweight cryptographic protocols (secure sum and normalized secure sum) to build learning algorithms rather than wrap complex learning algorithms in a heavy-cost MPC framework. We showcase our algorithms' utility and privacy on several applications: for NMF we consider topic modeling and recommender systems, and for SVD, principal component regression, and low rank approximation.	1,0,0,1,0,0
Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs	The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.	0,0,0,1,0,0
Dispersive Regimes of the Dicke Model	We study two dispersive regimes in the dynamics of $N$ two-level atoms interacting with a bosonic mode for long interaction times. Firstly, we analyze the dispersive multiqubit quantum Rabi model for the regime in which the qubit frequencies are equal and smaller than the mode frequency, and for values of the coupling strength similar or larger than the mode frequency, namely, the deep strong coupling regime. Secondly, we address an interaction that is dependent on the photon number, where the coupling strength is comparable to the geometric mean of the qubit and mode frequencies. We show that the associated dynamics is analytically tractable and provide useful frameworks with which to analyze the system behavior. In the deep strong coupling regime, we unveil the structure of unexpected resonances for specific values of the coupling, present for $N\ge2$, and in the photon-number-dependent regime we demonstrate that all the nontrivial dynamical behavior occurs in the atomic degrees of freedom for a given Fock state. We verify these assertions with numerical simulations of the qubit population and photon-statistic dynamics.	0,1,0,0,0,0
Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks	The paper evaluates three variants of the Gated Recurrent Unit (GRU) in recurrent neural networks (RNN) by reducing parameters in the update and reset gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and show that these GRU-RNN variant models perform as well as the original GRU RNN model while reducing the computational expense.	1,0,0,1,0,0
Singular perturbation for abstract elliptic equations and application	Boundary value problem for complete second order elliptic equation is considered in Banach space. The equation and boundary conditions involve a small and spectral parameter. The uniform L_{p}-regularity properties with respect to space variable and parameters are established. Here, the explicit formula for the solution is given and behavior of solution is derived when the small parameter approaches zero. It used to obtain singular perturbation result for abstract elliptic equation	0,0,1,0,0,0
Supervised Learning of Labeled Pointcloud Differences via Cover-Tree Entropy Reduction	We introduce a new algorithm, called CDER, for supervised machine learning that merges the multi-scale geometric properties of Cover Trees with the information-theoretic properties of entropy. CDER applies to a training set of labeled pointclouds embedded in a common Euclidean space. If typical pointclouds corresponding to distinct labels tend to differ at any scale in any sub-region, CDER can identify these differences in (typically) linear time, creating a set of distributional coordinates which act as a feature extraction mechanism for supervised learning. We describe theoretical properties and implementation details of CDER, and illustrate its benefits on several synthetic examples.	1,0,0,1,0,0
On certain type of difference polynomials of meromorphic functions	In this paper, we investigate zeros of difference polynomials of the form $f(z)^nH(z, f)-s(z)$, where $f(z)$ is a meromorphic function, $H(z, f)$ is a difference polynomial of $f(z)$ and $s(z)$ is a small function. We first obtain some inequalities for the relationship of the zero counting function of $f(z)^nH(z, f)-s(z)$ and the characteristic function and pole counting function of $f(z)$. Based on these inequalities, we establish some difference analogues of a classical result of Hayman for meromorphic functions. Some special cases are also investigated. These results improve previous findings.	0,0,1,0,0,0
Audio Super Resolution using Neural Networks	We introduce a new audio processing technique that increases the sampling rate of signals such as speech or music using deep convolutional neural networks. Our model is trained on pairs of low and high-quality audio examples; at test-time, it predicts missing samples within a low-resolution signal in an interpolation process similar to image super-resolution. Our method is simple and does not involve specialized audio processing techniques; in our experiments, it outperforms baselines on standard speech and music benchmarks at upscaling ratios of 2x, 4x, and 6x. The method has practical applications in telephony, compression, and text-to-speech generation; it demonstrates the effectiveness of feed-forward convolutional architectures on an audio generation task.	1,0,0,0,0,0
Decentralized Task Allocation in Multi-Robot Systems via Bipartite Graph Matching Augmented with Fuzzy Clustering	Robotic systems, working together as a team, are becoming valuable players in different real-world applications, from disaster response to warehouse fulfillment services. Centralized solutions for coordinating multi-robot teams often suffer from poor scalability and vulnerability to communication disruptions. This paper develops a decentralized multi-agent task allocation (Dec-MATA) algorithm for multi-robot applications. The task planning problem is posed as a maximum-weighted matching of a bipartite graph, the solution of which using the blossom algorithm allows each robot to autonomously identify the optimal sequence of tasks it should undertake. The graph weights are determined based on a soft clustering process, which also plays a problem decomposition role seeking to reduce the complexity of the individual-agents' task assignment problems. To evaluate the new Dec-MATA algorithm, a series of case studies (of varying complexity) are performed, with tasks being distributed randomly over an observable 2D environment. A centralized approach, based on a state-of-the-art MILP formulation of the multi-Traveling Salesman problem is used for comparative analysis. While getting within 7-28% of the optimal cost obtained by the centralized algorithm, the Dec-MATA algorithm is found to be 1-3 orders of magnitude faster and minimally sensitive to task-to-robot ratios, unlike the centralized algorithm.	1,0,0,0,0,0
Generating Sentence Planning Variations for Story Telling	There has been a recent explosion in applications for dialogue interaction ranging from direction-giving and tourist information to interactive story systems. Yet the natural language generation (NLG) component for many of these systems remains largely handcrafted. This limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content. We propose that a solution to this problem lies in new methods for developing language generation resources. We describe the ES-Translator, a computational language generator that has previously been applied only to fables, and quantitatively evaluate the domain independence of the EST by applying it to personal narratives from weblogs. We then take advantage of recent work on language generation to create a parameterized sentence planner for story generation that provides aggregation operations, variations in discourse and in point of view. Finally, we present a user evaluation of different personal narrative retellings.	1,0,0,0,0,0
Skin Lesion Classification Using Hybrid Deep Neural Networks	Skin cancer is one of the major types of cancers and its incidence has been increasing over the past decades. Skin lesions can arise from various dermatologic disorders and can be classified to various types according to their texture, structure, color and other morphological features. The accuracy of diagnosis of skin lesions, specifically the discrimination of benign and malignant lesions, is paramount to ensure appropriate patient treatment. Machine learning-based classification approaches are among popular automatic methods for skin lesion classification. While there are many existing methods, convolutional neural networks (CNN) have shown to be superior over other classical machine learning methods for object detection and classification tasks. In this work, a fully automatic computerized method is proposed, which employs well established pre-trained convolutional neural networks and ensembles learning to classify skin lesions. We trained the networks using 2000 skin lesion images available from the ISIC 2017 challenge, which has three main categories and includes 374 melanoma, 254 seborrheic keratosis and 1372 benign nevi images. The trained classifier was then tested on 150 unlabeled images. The results, evaluated by the challenge organizer and based on the area under the receiver operating characteristic curve (AUC), were 84.8% and 93.6% for Melanoma and seborrheic keratosis binary classification problem, respectively. The proposed method achieved competitive results to experienced dermatologist. Further improvement and optimization of the proposed method with a larger training dataset could lead to a more precise, reliable and robust method for skin lesion classification.	1,0,0,0,0,0
Domination between different products and finiteness of associated semi-norms	In this note we determine all possible dominations between different products of manifolds, when none of the factors of the codomain is dominated by products. As a consequence, we determine the finiteness of every product-associated functorial semi-norm on the fundamental classes of the aforementioned products. These results give partial answers to questions of M. Gromov.	0,0,1,0,0,0
Estimating Large Precision Matrices via Modified Cholesky Decomposition	We introduce the $k$-banded Cholesky prior for estimating a high-dimensional bandable precision matrix via the modified Cholesky decomposition. The bandable assumption is imposed on the Cholesky factor of the decomposition. We obtained the P-loss convergence rate under the spectral norm and the matrix $\ell_{\infty}$ norm and the minimax lower bounds. Since the P-loss convergence rate (Lee and Lee (2017)) is stronger than the posterior convergence rate, the rates obtained are also posterior convergence rates. Furthermore, when the true precision matrix is a $k_0$-banded matrix with some finite $k_0$, the obtained P-loss convergence rates coincide with the minimax rates. The established convergence rates are slightly slower than the minimax lower bounds, but these are the fastest rates for bandable precision matrices among the existing Bayesian approaches. A simulation study is conducted to compare the performance to the other competitive estimators in various scenarios.	0,0,1,1,0,0
An Estimate of the First Eigenvalue of a Schrödinger Operator on Closed Surfaces	Based on the work of Schoen-Yau, we derive an estimate of the first eigenvalue of a Schrödinger Operator (the Jaocbi operator of minimal surfaces in flat 3-spaces) on surfaces.	0,0,1,0,0,0
A Calculus of Truly Concurrent Mobile Processes	We make a mixture of Milner's $\pi$-calculus and our previous work on truly concurrent process algebra, which is called $\pi_{tc}$. We introduce syntax and semantics of $\pi_{tc}$, its properties based on strongly truly concurrent bisimilarities. Also, we include an axiomatization of $\pi_{tc}$. $\pi_{tc}$ can be used as a formal tool in verifying mobile systems in a truly concurrent flavor.	1,0,0,0,0,0
Learning Role-based Graph Embeddings	Random walks are at the heart of many existing network embedding methods. However, such algorithms have many limitations that arise from the use of random walks, e.g., the features resulting from these methods are unable to transfer to new nodes and graphs as they are tied to vertex identity. In this work, we introduce the Role2Vec framework which uses the flexible notion of attributed random walks, and serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many others that leverage random walks. Our proposed framework enables these methods to be more widely applicable for both transductive and inductive learning as well as for use on graphs with attributes (if available). This is achieved by learning functions that generalize to new nodes and graphs. We show that our proposed framework is effective with an average AUC improvement of 16.55% while requiring on average 853x less space than existing methods on a variety of graphs.	1,0,0,1,0,0
Active matter invasion of a viscous fluid: unstable sheets and a no-flow theorem	We investigate the dynamics of a dilute suspension of hydrodynamically interacting motile or immotile stress-generating swimmers or particles as they invade a surrounding viscous fluid. Colonies of aligned pusher particles are shown to elongate in the direction of particle orientation and undergo a cascade of transverse concentration instabilities, governed at small times by an equation which also describes the Saffman-Taylor instability in a Hele-Shaw cell, or Rayleigh-Taylor instability in two-dimensional flow through a porous medium. Thin sheets of aligned pusher particles are always unstable, while sheets of aligned puller particles can either be stable (immotile particles), or unstable (motile particles) with a growth rate which is non-monotonic in the force dipole strength. We also prove a surprising "no-flow theorem": a distribution initially isotropic in orientation loses isotropy immediately but in such a way that results in no fluid flow everywhere and for all time.	0,0,0,0,1,0
Unbiased Shrinkage Estimation	Shrinkage estimation usually reduces variance at the cost of bias. But when we care only about some parameters of a model, I show that we can reduce variance without incurring bias if we have additional information about the distribution of covariates. In a linear regression model with homoscedastic Normal noise, I consider shrinkage estimation of the nuisance parameters associated with control variables. For at least three control variables and exogenous treatment, I establish that the standard least-squares estimator is dominated with respect to squared-error loss in the treatment effect even among unbiased estimators and even when the target parameter is low-dimensional. I construct the dominating estimator by a variant of James-Stein shrinkage in a high-dimensional Normal-means problem. It can be interpreted as an invariant generalized Bayes estimator with an uninformative (improper) Jeffreys prior in the target parameter.	0,0,1,1,0,0
Mechanisms of Lagrangian analyticity in fluids	Certain systems of inviscid fluid dynamics have the property that for solutions that are only slightly better than differentiable in Eulerian variables, the corresponding Lagrangian trajectories are analytic in time. We elucidate the mechanisms in fluid dynamics systems that give rise to this automatic Lagrangian analyticity, as well as mechanisms in some particular fluids systems which prevent it from occurring. We give a conceptual argument for a general fluids model which shows that the fulfillment of a basic set of criteria results in the analyticity of the trajectory maps in time. We then apply this to the incompressible Euler equations to prove analyticity of trajectories for vortex patch solutions. We also use the method to prove the Lagrangian trajectories are analytic for solutions to the pressureless Euler-Poisson equations, for initial data with moderate regularity. We then examine the compressible Euler equations, and find that the finite speed of propagation in the system is incompatible with the Lagrangian analyticity property. By taking advantage of this finite speed we are able to construct smooth initial data with the property that some corresponding Lagrangian trajectory is not analytic in time. We also study the Vlasov-Poisson system, uncovering another mechanism that deters the analyticity of trajectories. In this instance, we find that a key nonlocal operator does not preserve analytic dependence in time. For this system we can also construct smooth initial data for which the corresponding solution has some non-analytic Lagrangian trajectory. This provides a counterexample to Lagrangian analyticity for a system in which there is an infinite speed of propagation, unlike the compressible Euler equations.	0,0,1,0,0,0
Couple microscale periodic patches to simulate macroscale emergent dynamics	This article proposes a new way to construct computationally efficient `wrappers' around fine scale, microscopic, detailed descriptions of dynamical systems, such as molecular dynamics, to make predictions at the macroscale `continuum' level. It is often significantly easier to code a microscale simulator with periodicity: so the challenge addressed here is to develop a scheme that uses only a given periodic microscale simulator; specifically, one for atomistic dynamics. Numerical simulations show that applying a suitable proportional controller within `action regions' of a patch of atomistic simulation effectively predicts the macroscale transport of heat. Theoretical analysis establishes that such an approach will generally be effective and efficient, and also determines good values for the strength of the proportional controller. This work has the potential to empower systematic analysis and understanding at a macroscopic system level when only a given microscale simulator is available.	0,0,1,0,0,0
Hessian corrections to Hybrid Monte Carlo	A method for the introduction of second-order derivatives of the log likelihood into HMC algorithms is introduced, which does not require the Hessian to be evaluated at each leapfrog step but only at the start and end of trajectories.	0,0,0,1,0,0
Probing the gravitational redshift with an Earth-orbiting satellite	We present an approach to testing the gravitational redshift effect using the RadioAstron satellite. The experiment is based on a modification of the Gravity Probe A scheme of nonrelativistic Doppler compensation and benefits from the highly eccentric orbit and ultra-stable atomic hydrogen maser frequency standard of the RadioAstron satellite. Using the presented techniques we expect to reach an accuracy of the gravitational redshift test of order $10^{-5}$, a magnitude better than that of Gravity Probe A. Data processing is ongoing, our preliminary results agree with the validity of the Einstein Equivalence Principle.	0,1,0,0,0,0
On Geometry of Manifolds with Some Tensor Structures and Metrics of Norden Type	The object of study in the present dissertation are some topics in differential geometry of smooth manifolds with additional tensor structures and metrics of Norden type. There are considered four cases depending on the dimension of the manifold: 2n, 2n + 1, 4n and 4n + 3. The studied tensor structures, which are counterparts in the different related dimensions, are the almost complex/contact/hypercomplex structure and the almost contact 3-structure. The considered metric on the 2n-dimensional case is the Norden metric, and the metrics in the other three cases are generated by it. The purpose of the dissertation is to carry out the following: 1. Further investigations of almost complex manifolds with Norden metric including studying of natural connections with conditions for their torsion and invariant tensors under the twin interchange of Norden metrics. 2. Further investigations of almost contact manifolds with B-metric including studying of natural connections with conditions for their torsion and associated Schouten-van Kampen connections as well as a classification of affine connections. 3. Introducing and studying of Sasaki-like almost contact complex Riemannian manifolds. 4. Further investigations of almost hypercomplex manifolds with Hermitian-Norden metrics including studying of integrable structures of the considered type on 4-dimensional Lie algebra and tangent bundles with the complete lift of the base metric; introducing of associated Nijenhuis tensors in relation with natural connections having totally skew-symmetric torsion as well as quaternionic Kähler manifolds with Hermitian-Norden metrics. 5. Introducing and studying of manifolds with almost contact 3-structures and metrics of Hermitian-Norden type and, in particular, associated Nijenhuis tensors and their relationship with natural connections having totally skew-symmetric torsion.	0,0,1,0,0,0
The finiteness dimension of modules and relative Cohen-Macaulayness	Let $R$ be a commutative Noetherian ring, $\mathfrak a$ and $\mathfrak b$ ideals of $R$. In this paper, we study the finiteness dimension $f_{\mathfrak a}(M)$ of $M$ relative to $\mathfrak a$ and the $\mathfrak b$-minimum $\mathfrak a$-adjusted depth $\lambda_{\mathfrak a}^{\mathfrak b}(M)$ of $M$, where the underlying module $M$ is relative Cohen-Macaulay w.r.t $\mathfrak a$. Some applications of such modules are given.	0,0,1,0,0,0
Stall force of a cargo driven by N interacting motor proteins	We study a generic one-dimensional model for an intracellular cargo driven by N motor proteins against an external applied force. The model includes motor-cargo and motor-motor interactions. The cargo motion is described by an over-damped Langevin equation, while motor dynamics is specified by hopping rates which follow a local detailed balance condition with respect to change in energy per hopping event. Based on this model, we show that the stall force, the mean external force corresponding to zero mean cargo velocity, is completely independent of the details of the interactions and is, therefore, always equal to the sum of the stall forces of the individual motors. This exact result is arrived on the basis of a simple assumption: the (macroscopic) state of stall of the cargo is analogous to a state of thermodynamic equilibrium, and is characterized by vanishing net probability current between any two microstates, with the latter specified by motor positions relative to the cargo. The corresponding probability distribution of the microstates under stall is also determined. These predictions are in complete agreement with numerical simulations, carried out using specific forms of interaction potentials.	0,1,0,0,0,0
Decay Estimates for 1-D Parabolic PDEs with Boundary Disturbances	In this work decay estimates are derived for the solutions of 1-D linear parabolic PDEs with disturbances at both boundaries and distributed disturbances. The decay estimates are given in the L2 and H1 norms of the solution and discontinuous disturbances are allowed. Although an eigenfunction expansion for the solution is exploited for the proof of the decay estimates, the estimates do not require knowledge of the eigenvalues and the eigenfunctions of the corresponding Sturm-Liouville operator. Examples show that the obtained results can be applied for the stability analysis of parabolic PDEs with nonlocal terms.	1,0,1,0,0,0
A short note on Godbersen's Conjecture	In this short note we improve the best to date bound in Godbersen's conjecture, and show some implications for unbalanced difference bodies.	0,0,1,0,0,0
Size, Shape, and Phase Control in Ultrathin CdSe Nanosheets	Ultrathin two-dimensional nanosheets raise a rapidly increasing interest due to their unique dimensionality-dependent properties. Most of the two-dimensional materials are obtained by exfoliation of layered bulk materials or are grown on substrates by vapor deposition methods. To produce free-standing nanosheets, solution-based colloidal methods are emerging as promising routes. In this work, we demonstrate ultrathin CdSe nanosheets with controllable size, shape and phase. The key of our approach is the use of halogenated alkanes as additives in a hot-injection synthesis. Increasing concentrations of bromoalkanes can tune the shape from sexangular to quadrangular to triangular and the phase from zinc blende to wurtzite. Geometry and crystal structure evolution of the nanosheets take place in the presence of halide ions, acting as cadmium complexing agents and as surface X-type ligands, according to mass spectrometry and X-ray photoelectron spectroscopies. Our experimental findings show that the degree of these changes depends on the molecular structure of the halogen alkanes and the type of halogen atom.	0,1,0,0,0,0
CoT: Cooperative Training for Generative Modeling of Discrete Data	We propose Cooperative Training (CoT) for training generative models that measure a tractable density for discrete data. CoT coordinately trains a generator $G$ and an auxiliary predictive mediator $M$. The training target of $M$ is to estimate a mixture density of the learned distribution $G$ and the target distribution $P$, and that of $G$ is to minimize the Jensen-Shannon divergence estimated through $M$. CoT achieves independent success without the necessity of pre-training via Maximum Likelihood Estimation or involving high-variance algorithms like REINFORCE. This low-variance algorithm is theoretically proved to be unbiased for both generative and predictive tasks. We also theoretically and empirically show the superiority of CoT over most previous algorithms in terms of generative quality and diversity, predictive generalization ability and computational cost.	0,0,0,1,0,0
The structure of a minimal $n$-chart with two crossings II: Neighbourhoods of $Γ_1\cupΓ_{n-1}$	Given a 2-crossing minimal chart $\Gamma$, a minimal chart with two crossings, set $\alpha=\min\{~i~|~$there exists an edge of label $i$ containing a white vertex$\}$, and $\beta=\max\{~i~|~$there exists an edge of label $i$ containing a white vertex$\}$. In this paper we study the structure of a neighbourhood of $\Gamma_\alpha\cup\Gamma_\beta$, and propose a normal form for 2-crossing minimal $n$-charts, here $\Gamma_\alpha$ and $\Gamma_\beta$ mean the union of all the edges of label $\alpha$ and $\beta$ respectively.	0,0,1,0,0,0
Aggressive Economic Incentives and Physical Activity: The Role of Choice and Technology Decision Aids	Aggressive incentive schemes that allow individuals to impose economic punishment on themselves if they fail to meet health goals present a promising approach for encouraging healthier behavior. However, the element of choice inherent in these schemes introduces concerns that only non-representative sectors of the population will select aggressive incentives, leaving value on the table for those who don't opt in. In a field experiment conducted over a 29 week period on individuals wearing Fitbit activity trackers, we find modest and short lived increases in physical activity for those provided the choice of aggressive incentives. In contrast, we find significant and persistent increases for those assigned (oftentimes against their stated preference) to the same aggressive incentives. The modest benefits for those provided a choice seems to emerge because those who benefited most from the aggressive incentives were the least likely to choose them, and it was those who did not need them who opted in. These results are confirmed in a follow up lab experiment. We also find that benefits to individuals assigned to aggressive incentives were pronounced if they also updated their step target in the Fitbit mobile application to match the new activity goal we provided them. Our findings have important implications for incentive based interventions to improve health behavior. For firms and policy makers, our results suggest that one effective strategy for encouraging sustained healthy behavior combines exposure to aggressive incentive schemes to jolt individuals out of their comfort zones with technology decision aids that help individuals sustain this behavior after incentives end.	0,0,0,0,0,1
On separated solutions of logistic population equation with harvesting	We provide a surprising answer to a question raised in S. Ahmad and A.C. Lazer [2], and extend the results of that paper.	0,0,1,0,0,0
A Maximum Matching Algorithm for Basis Selection in Spectral Learning	We present a solution to scale spectral algorithms for learning sequence functions. We are interested in the case where these functions are sparse (that is, for most sequences they return 0). Spectral algorithms reduce the learning problem to the task of computing an SVD decomposition over a special type of matrix called the Hankel matrix. This matrix is designed to capture the relevant statistics of the training sequences. What is crucial is that to capture long range dependencies we must consider very large Hankel matrices. Thus the computation of the SVD becomes a critical bottleneck. Our solution finds a subset of rows and columns of the Hankel that realizes a compact and informative Hankel submatrix. The novelty lies in the way that this subset is selected: we exploit a maximal bipartite matching combinatorial algorithm to look for a sub-block with full structural rank, and show how computation of this sub-block can be further improved by exploiting the specific structure of Hankel matrices.	1,0,0,1,0,0
Probing Primordial-Black-Hole Dark Matter with Gravitational Waves	Primordial black holes (PBHs) have long been suggested as a candidate for making up some or all of the dark matter in the Universe. Most of the theoretically possible mass range for PBH dark matter has been ruled out with various null observations of expected signatures of their interaction with standard astrophysical objects. However, current constraints are significantly less robust in the 20 M_sun < M_PBH < 100 M_sun mass window, which has received much attention recently, following the detection of merging black holes with estimated masses of ~30 M_sun by LIGO and the suggestion that these could be black holes formed in the early Universe. We consider the potential of advanced LIGO (aLIGO) operating at design sensitivity to probe this mass range by looking for peaks in the mass spectrum of detected events. To quantify the background, which is due to black holes that are formed from dying stars, we model the shape of the stellar-black-hole mass function and calibrate its amplitude to match the O1 results. Adopting very conservative assumptions about the PBH and stellar-black-hole merger rates, we show that ~5 years of aLIGO data can be used to detect a contribution of >20 M_sun PBHs to dark matter down to f_PBH<0.5 at >99.9% confidence level. Combined with other probes that already suggest tension with f_PBH=1, the obtainable independent limits from aLIGO will thus enable a firm test of the scenario that PBHs make up all of dark matter.	0,1,0,0,0,0
Cobwebs from the Past and Present: Extracting Large Social Networks using Internet Archive Data	Social graph construction from various sources has been of interest to researchers due to its application potential and the broad range of technical challenges involved. The World Wide Web provides a huge amount of continuously updated data and information on a wide range of topics created by a variety of content providers, and makes the study of extracted people networks and their temporal evolution valuable for social as well as computer scientists. In this paper we present SocGraph - an extraction and exploration system for social relations from the content of around 2 billion web pages collected by the Internet Archive over the 17 years time period between 1996 and 2013. We describe methods for constructing large social graphs from extracted relations and introduce an interface to study their temporal evolution.	1,1,0,0,0,0
Effect of the non-thermal Sunyaev-Zel'dovich Effect on the temperature determination of galaxy clusters	A recent stacking analysis of Planck HFI data of galaxy clusters (Hurier 2016) allowed to derive the cluster temperatures by using the relativistic corrections to the Sunyaev-Zel'dovich effect (SZE). However, the temperatures of high-temperature clusters, as derived from this analysis, resulted to be basically higher than the temperatures derived from X-ray measurements, at a moderate statistical significance of $1.5\sigma$. This discrepancy has been attributed by Hurier (2016) to calibration issues. In this paper we discuss an alternative explanation for this discrepancy in terms of a non-thermal SZE astrophysical component. We find that this explanation can work if non-thermal electrons in galaxy clusters have a low value of their minimum momentum ($p_1\sim0.5-1$), and if their pressure is of the order of $20-30\%$ of the thermal gas pressure. Both these conditions are hard to obtain if the non-thermal electrons are mixed with the hot gas in the intra cluster medium, but can be possibly obtained if the non-thermal electrons are mainly confined in bubbles with high content of non-thermal plasma and low content of thermal plasma, or in giant radio lobes/relics located in the outskirts of clusters. In order to derive more precise results on the properties of non-thermal electrons in clusters, and in view of more solid detections of a discrepancy between X-rays and SZE derived clusters temperatures that cannot be explained in other ways, it would be necessary to reproduce the full analysis done by Hurier (2016) by adding systematically the non-thermal component of the SZE.	0,1,0,0,0,0
Sensor Selection and Random Field Reconstruction for Robust and Cost-effective Heterogeneous Weather Sensor Networks for the Developing World	We address the two fundamental problems of spatial field reconstruction and sensor selection in heterogeneous sensor networks: (i) how to efficiently perform spatial field reconstruction based on measurements obtained simultaneously from networks with both high and low quality sensors; and (ii) how to perform query based sensor set selection with predictive MSE performance guarantee. For the first problem, we developed a low complexity algorithm based on the spatial best linear unbiased estimator (S-BLUE). Next, building on the S-BLUE, we address the second problem, and develop an efficient algorithm for query based sensor set selection with performance guarantee. Our algorithm is based on the Cross Entropy method which solves the combinatorial optimization problem in an efficient manner.	0,0,0,1,0,0
Robust Unsupervised Domain Adaptation for Neural Networks via Moment Alignment	A novel approach for unsupervised domain adaptation for neural networks is proposed. It relies on metric-based regularization of the learning process. The metric-based regularization aims at domain-invariant latent feature representations by means of maximizing the similarity between domain-specific activation distributions. The proposed metric results from modifying an integral probability metric such that it becomes less translation-sensitive on a polynomial function space. The metric has an intuitive interpretation in the dual space as the sum of differences of higher order central moments of the corresponding activation distributions. Under appropriate assumptions on the input distributions, error minimization is proven for the continuous case. As demonstrated by an analysis of standard benchmark experiments for sentiment analysis, object recognition and digit recognition, the outlined approach is robust regarding parameter changes and achieves higher classification accuracies than comparable approaches. The source code is available at this https URL.	1,0,0,1,0,0
On Optimal Weighted-Delay Scheduling in Input-Queued Switches	Motivated by relatively few delay-optimal scheduling results, in comparison to results on throughput optimality, we investigate an input-queued switch scheduling problem in which the objective is to minimize a linear function of the queue-length vector. Theoretical properties of variants of the well-known MaxWeight scheduling algorithm are established within this context, which includes showing that these algorithms exhibit optimal heavy-traffic queue-length scaling. For the case of $2 \times 2$ input-queued switches, we derive an optimal scheduling policy and establish its theoretical properties, demonstrating fundamental differences with the variants of MaxWeight scheduling. Our theoretical results are expected to be of interest more broadly than input-queued switches. Computational experiments demonstrate and quantify the benefits of our optimal scheduling policy.	0,0,1,0,0,0
Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP	A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. \cite{jin2018q} proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \emph{without} accessing a generative model. We show that the \textit{sample complexity of exploration} of our algorithm is bounded by $\tilde{O}({\frac{SA}{\epsilon^2(1-\gamma)^7}})$. This improves the previously best known result of $\tilde{O}({\frac{SA}{\epsilon^4(1-\gamma)^8}})$ in this setting achieved by delayed Q-learning \cite{strehl2006pac}, and matches the lower bound in terms of $\epsilon$ as well as $S$ and $A$ except for logarithmic factors.	1,0,0,1,0,0
Markov State Models from short non-Equilibrium Simulations - Analysis and Correction of Estimation Bias	Many state of the art methods for the thermodynamic and kinetic characterization of large and complex biomolecular systems by simulation rely on ensemble approaches, where data from large numbers of relatively short trajectories are integrated. In this context, Markov state models (MSMs) are extremely popular because they can be used to compute stationary quantities and long-time kinetics from ensembles of short simulations, provided that these short simulations are in "local equilibrium" within the MSM states. However, in the last over 15 years since the inception of MSMs, it has been controversially discussed and not yet been answered how deviations from local equilibrium can be detected, whether these deviations induce a practical bias in MSM estimation, and how to correct for them. In this paper, we address these issues: We systematically analyze the estimation of Markov state models (MSMs) from short non-equilibrium simulations, and we provide an expression for the error between unbiased transition probabilities and the expected estimate from many short simulations. We show that the unbiased MSM estimate can be obtained even from relatively short non-equilibrium simulations in the limit of long lag times and good discretization. Further, we exploit observable operator model (OOM) theory to derive an unbiased estimator for the MSM transition matrix that corrects for the effect of starting out of equilibrium, even when short lag times are used. Finally, we show how the OOM framework can be used to estimate the exact eigenvalues or relaxation timescales of the system without estimating an MSM transition matrix, which allows us to practically assess the discretization quality of the MSM. Applications to model systems and molecular dynamics simulation data of alanine dipeptide are included for illustration. The improved MSM estimator is implemented in PyEMMA as of version 2.3.	0,1,1,0,0,0
Optimal control of a Vlasov-Poisson plasma by an external magnetic field - Analysis of a tracking type optimal control problem	In the paper "Optimal control of a Vlasov-Poisson plasma by an external magnetic field - The basics for variational calculus" [arXiv:1708.02464] we have already introduced a set of admissible magnetic fields and we have proved that each of those fields induces a unique strong solution of the Vlasov-Poisson system. We have also established that the field-state operator that maps any admissible field onto its corresponding solution is continuous and weakly compact. In this paper we will show that this operator is also Fréchet differentiable and we will continue to analyze the optimal control problem that was introduced in [arXiv:1708.02464]. More precisely, we will establish necessary and sufficient conditions for local optimality and we will show that an optimal solution is unique under certain conditions.	0,0,1,0,0,0
Cosmological Evolution and Exact Solutions in a Fourth-order Theory of Gravity	A fourth-order theory of gravity is considered which in terms of dynamics has the same degrees of freedom and number of constraints as those of scalar-tensor theories. In addition it admits a canonical point-like Lagrangian description. We study the critical points of the theory and we show that it can describe the matter epoch of the universe and that two accelerated phases can be recovered one of which describes a de Sitter universe. Finally for some models exact solutions are presented.	0,1,1,0,0,0
Systematic Quantum Mechanical Region Determination in QM/MM Simulation	Hybrid quantum mechanical-molecular mechanical (QM/MM) simulations are widely used in enzyme simulation. Over ten convergence studies of QM/MM methods have revealed over the past several years that key energetic and structural properties approach asymptotic limits with only very large (ca. 500-1000 atom) QM regions. This slow convergence has been observed to be due in part to significant charge transfer between the core active site and surrounding protein environment, which cannot be addressed by improvement of MM force fields or the embedding method employed within QM/MM. Given this slow convergence, it becomes essential to identify strategies for the most atom-economical determination of optimal QM regions and to gain insight into the crucial interactions captured only in large QM regions. Here, we extend and develop two methods for quantitative determination of QM regions. First, in the charge shift analysis (CSA) method, we probe the reorganization of electron density when core active site residues are removed completely, as determined by large-QM region QM/MM calculations. Second, we introduce the highly-parallelizable Fukui shift analysis (FSA), which identifies how core/substrate frontier states are altered by the presence of an additional QM residue on smaller initial QM regions. We demonstrate that the FSA and CSA approaches are complementary and consistent on three test case enzymes: catechol O-methyltransferase, cytochrome P450cam, and hen eggwhite lysozyme. We also introduce validation strategies and test sensitivities of the two methods to geometric structure, basis set size, and electronic structure methodology. Both methods represent promising approaches for the systematic, unbiased determination of quantum mechanical effects in enzymes and large systems that necessitate multi-scale modeling.	0,1,0,0,0,0
A second main theorem for holomorphic curve intersecting hypersurfaces	In this paper, we establish a second main theorem for holomorphic curve intersecting hypersurfaces in general position in projective space with level of truncation. As an application, we reduce the number hypersurfaces in uniqueness problem for holomorphic curve of authors before.	0,0,1,0,0,0
Damping of gravitational waves by matter	We develop a unified description, via the Boltzmann equation, of damping of gravitational waves by matter, incorporating collisions. We identify two physically distinct damping mechanisms -- collisional and Landau damping. We first consider damping in flat spacetime, and then generalize the results to allow for cosmological expansion. In the first regime, maximal collisional damping of a gravitational wave, independent of the details of the collisions in the matter is, as we show, significant only when its wavelength is comparable to the size of the horizon. Thus damping by intergalactic or interstellar matter for all but primordial gravitational radiation can be neglected. Although collisions in matter lead to a shear viscosity, they also act to erase anisotropic stresses, thus suppressing the damping of gravitational waves. Damping of primordial gravitational waves remains possible. We generalize Weinberg's calculation of gravitational wave damping, now including collisions and particles of finite mass, and interpret the collisionless limit in terms of Landau damping. While Landau damping of gravitational waves cannot occur in flat spacetime, the expansion of the universe allows such damping by spreading the frequency of a gravitational wave of given wavevector.	0,1,0,0,0,0
Aggregating multiple types of complex data in stock market prediction: A model-independent framework	The increasing richness in volume, and especially types of data in the financial domain provides unprecedented opportunities to understand the stock market more comprehensively and makes the price prediction more accurate than before. However, they also bring challenges to classic statistic approaches since those models might be constrained to a certain type of data. Aiming at aggregating differently sourced information and offering type-free capability to existing models, a framework for predicting stock market of scenarios with mixed data, including scalar data, compositional data (pie-like) and functional data (curve-like), is established. The presented framework is model-independent, as it serves like an interface to multiple types of data and can be combined with various prediction models. And it is proved to be effective through numerical simulations. Regarding to price prediction, we incorporate the trading volume (scalar data), intraday return series (functional data), and investors' emotions from social media (compositional data) through the framework to competently forecast whether the market goes up or down at opening in the next day. The strong explanatory power of the framework is further demonstrated. Specifically, it is found that the intraday returns impact the following opening prices differently between bearish market and bullish market. And it is not at the beginning of the bearish market but the subsequent period in which the investors' "fear" comes to be indicative. The framework would help extend existing prediction models easily to scenarios with multiple types of data and shed light on a more systemic understanding of the stock market.	0,0,0,1,0,1
Steklov problem on differential forms	In this paper we study spectral properties of Dirichlet-to-Neumann map on differential forms obtained by a slight modification of the definition due to Belishev and Sharafutdinov. The resulting operator $\Lambda$ is shown to be self-adjoint on the subspace of coclosed forms and to have purely discrete spectrum there.We investigate properies of eigenvalues of $\Lambda$ and prove a Hersch-Payne-Schiffer type inequality relating products of those eigenvalues to eigenvalues of Hodge Laplacian on the boundary. Moreover, non-trivial eigenvalues of $\Lambda$ are always at least as large as eigenvalues of Dirichlet-to-Neumann map defined by Raulot and Savo. Finally, we remark that a particular case of $p$-forms on the boundary of $2p+2$-dimensional manifold shares a lot of important properties with the classical Steklov eigenvalue problem on surfaces.	0,0,1,0,0,0
The content correlation of multiple streaming edges	We study how to detect clusters in a graph defined by a stream of edges, without storing the entire graph. We extend the approach to dynamic graphs defined by the most recent edges of the stream and to several streams. The {\em content correlation }of two streams $\rho(t)$ is the Jaccard similarity of their clusters in the windows before time $t$. We propose a simple and efficient method to approximate this correlation online and show that for dynamic random graphs which follow a power law degree distribution, we can guarantee a good approximation. As an application, we follow Twitter streams and compute their content correlations online. We then propose a {\em search by correlation} where answers to sets of keywords are entirely based on the small correlations of the streams. Answers are ordered by the correlations, and explanations can be traced with the stored clusters.	1,0,0,0,0,0
Greed Works - Online Algorithms For Unrelated Machine Stochastic Scheduling	This paper establishes the first performance guarantees for a combinatorial online algorithm that schedules stochastic, nonpreemptive jobs on unrelated machines to minimize the expected total weighted completion time. Prior work on unrelated machine scheduling with stochastic jobs was restricted to the offline case, and required sophisticated linear or convex programming relaxations for the assignment of jobs to machines. The algorithm introduced in this paper is based on a purely combinatorial assignment of jobs to machines, hence it also works online. The performance bounds are of the same order of magnitude as those of earlier work, and depend linearly on an upper bound $\Delta$ on the squared coefficient of variation of the jobs' processing times. They are $4+2\Delta$ when there are no release dates, and $12+6\Delta$ when jobs are released over time. For the special case of deterministic processing times, without and with release times, this paper shows that the same combinatorial greedy algorithm has a competitive ratio of 4 and 6, respectively. As to the technical contribution, the paper shows for the first time how dual fitting techniques can be used for stochastic and nonpreemptive scheduling problems.	1,0,0,0,0,0
Searching for chemical signatures of brown dwarf formation	Recent studies have shown that close-in brown dwarfs in the mass range 35-55 M$_{\rm Jup}$ are almost depleted as companions to stars, suggesting that objects with masses above and below this gap might have different formation mechanisms. We determine the fundamental stellar parameters, as well as individual abundances for a large sample of stars known to have a substellar companion in the brown dwarf regime. The sample is divided into stars hosting "massive" and "low-mass" brown dwarfs. Following previous works a threshold of 42.5 M$_{\rm Jup}$ was considered. Our results confirm that stars with brown dwarf companions do not follow the well-established gas-giant planet metallicity correlation seen in main-sequence planet hosts. Stars harbouring "massive" brown dwarfs show similar metallicity and abundance distribution as stars without known planets or with low-mass planets. We find a tendency of stars harbouring "less-massive" brown dwarfs of having slightly larger metallicity, [X$_{\rm Fe}$/Fe] values, and abundances of Sc II, Mn I, and Ni I in comparison with the stars having the massive brown dwarfs. The data suggest, as previously reported, that massive and low-mass brown dwarfs might present differences in period and eccentricity. We find evidence of a non-metallicity dependent mechanism for the formation of massive brown dwarfs. Our results agree with a scenario in which massive brown dwarfs are formed as stars. At high-metallicities, the core-accretion mechanism might become efficient in the formation of low-mass brown dwarfs while at lower metallicities low-mass brown dwarfs could form by gravitational instability in turbulent protostellar discs.	0,1,0,0,0,0
Private Data System Enabling Self-Sovereign Storage Managed by Executable Choreographies	With the increased use of Internet, governments and large companies store and share massive amounts of personal data in such a way that leaves no space for transparency. When a user needs to achieve a simple task like applying for college or a driving license, he needs to visit a lot of institutions and organizations, thus leaving a lot of private data in many places. The same happens when using the Internet. These privacy issues raised by the centralized architectures along with the recent developments in the area of serverless applications demand a decentralized private data layer under user control. We introduce the Private Data System (PDS), a distributed approach which enables self-sovereign storage and sharing of private data. The system is composed of nodes spread across the entire Internet managing local key-value databases. The communication between nodes is achieved through executable choreographies, which are capable of preventing information leakage when executing across different organizations with different regulations in place. The user has full control over his private data and is able to share and revoke access to organizations at any time. Even more, the updates are propagated instantly to all the parties which have access to the data thanks to the system design. Specifically, the processing organizations may retrieve and process the shared information, but are not allowed under any circumstances to store it on long term. PDS offers an alternative to systems that aim to ensure self-sovereignty of specific types of data through blockchain inspired techniques but face various problems, such as low performance. Both approaches propose a distributed database, but with different characteristics. While the blockchain-based systems are built to solve consensus problems, PDS's purpose is to solve the self-sovereignty aspects raised by the privacy laws, rules and principles.	1,0,0,0,0,0
Contemporary machine learning: a guide for practitioners in the physical sciences	Machine learning is finding increasingly broad application in the physical sciences. This most often involves building a model relationship between a dependent, measurable output and an associated set of controllable, but complicated, independent inputs. We present a tutorial on current techniques in machine learning -- a jumping-off point for interested researchers to advance their work. We focus on deep neural networks with an emphasis on demystifying deep learning. We begin with background ideas in machine learning and some example applications from current research in plasma physics. We discuss supervised learning techniques for modeling complicated functions, beginning with familiar regression schemes, then advancing to more sophisticated deep learning methods. We also address unsupervised learning and techniques for reducing the dimensionality of input spaces. Along the way, we describe methods for practitioners to help ensure that their models generalize from their training data to as-yet-unseen test data. We describe classes of tasks -- predicting scalars, handling images, fitting time-series -- and prepare the reader to choose an appropriate technique. We finally point out some limitations to modern machine learning and speculate on some ways that practitioners from the physical sciences may be particularly suited to help.	1,1,0,0,0,0
Geometric Ergodicity of the MUCOGARCH(1,1) process	For the multivariate COGARCH(1,1) volatility process we show sufficient conditions for the existence of a unique stationary distribution, for the geometric ergodicity and for the finiteness of moments of the stationary distribution. One of the conditions demands a sufficiently fast exponential decay of the MUCOGARCH(1,1) volatility process. Furthermore, we show easily applicable sufficient conditions for the needed irreducibility of the volatility process living in the cone of positive semidefinite matrices, if the driving Lévy process is a compound Poisson process.	0,0,1,0,0,0
Probabilistic Surfel Fusion for Dense LiDAR Mapping	With the recent development of high-end LiDARs, more and more systems are able to continuously map the environment while moving and producing spatially redundant information. However, none of the previous approaches were able to effectively exploit this redundancy in a dense LiDAR mapping problem. In this paper, we present a new approach for dense LiDAR mapping using probabilistic surfel fusion. The proposed system is capable of reconstructing a high-quality dense surface element (surfel) map from spatially redundant multiple views. This is achieved by a proposed probabilistic surfel fusion along with a geometry considered data association. The proposed surfel data association method considers surface resolution as well as high measurement uncertainty along its beam direction which enables the mapping system to be able to control surface resolution without introducing spatial digitization. The proposed fusion method successfully suppresses the map noise level by considering measurement noise caused by laser beam incident angle and depth distance in a Bayesian filtering framework. Experimental results with simulated and real data for the dense surfel mapping prove the ability of the proposed method to accurately find the canonical form of the environment without further post-processing.	1,0,0,0,0,0
Higher-genus quasimap wall-crossing via localization	We give a new proof of Ciocan-Fontanine and Kim's wall-crossing formula relating the virtual classes of the moduli spaces of $\epsilon$-stable quasimaps for different $\epsilon$ in any genus, whenever the target is a complete intersection in projective space and there is at least one marked point. Our techniques involve a twisted graph space, which we expect to generalize to yield wall-crossing formulas for general gauged linear sigma models.	0,0,1,0,0,0
SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities	The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by many vulnerabilities reported on a daily basis. This calls for machine learning methods to automate vulnerability detection. Deep learning is attractive for this purpose because it does not require human experts to manually define features. Despite the tremendous success of deep learning in other domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations (SySeVR), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been "silently" patched by the vendors when releasing newer versions of the products.	0,0,0,1,0,0
Transductive Zero-Shot Learning with Adaptive Structural Embedding	Zero-shot learning (ZSL) endows the computer vision system with the inferential capability to recognize instances of a new category that has never seen before. Two fundamental challenges in it are visual-semantic embedding and domain adaptation in cross-modality learning and unseen class prediction steps, respectively. To address both challenges, this paper presents two corresponding methods named Adaptive STructural Embedding (ASTE) and Self-PAsed Selective Strategy (SPASS), respectively. Specifically, ASTE formulates the visualsemantic interactions in a latent structural SVM framework to adaptively adjust the slack variables to embody the different reliableness among training instances. In this way, the reliable instances are imposed with small punishments, wheras the less reliable instances are imposed with more severe punishments. Thus, it ensures a more discriminative embedding. On the other hand, SPASS offers a framework to alleviate the domain shift problem in ZSL, which exploits the unseen data in an easy to hard fashion. Particularly, SPASS borrows the idea from selfpaced learning by iteratively selecting the unseen instances from reliable to less reliable to gradually adapt the knowledge from the seen domain to the unseen domain. Subsequently, by combining SPASS and ASTE, we present a self-paced Transductive ASTE (TASTE) method to progressively reinforce the classification capacity. Extensive experiments on three benchmark datasets (i.e., AwA, CUB, and aPY) demonstrate the superiorities of ASTE and TASTE. Furthermore, we also propose a fast training (FT) strategy to improve the efficiency of most of existing ZSL methods. The FT strategy is surprisingly simple and general enough, which can speed up the training time of most existing methods by 4~300 times while holding the previous performance.	1,0,0,0,0,0
Tensor ring decomposition	Tensor decompositions such as the canonical format and the tensor train format have been widely utilized to reduce storage costs and operational complexities for high-dimensional data, achieving linear scaling with the input dimension instead of exponential scaling. In this paper, we investigate even lower storage-cost representations in the tensor ring format, which is an extension of the tensor train format with variable end-ranks. Firstly, we introduce two algorithms for converting a tensor in full format to tensor ring format with low storage cost. Secondly, we detail a rounding operation for tensor rings and show how this requires new definitions of common linear algebra operations in the format to obtain storage-cost savings. Lastly, we introduce algorithms for transforming the graph structure of graph-based tensor formats, with orders of magnitude lower complexity than existing literature. The efficiency of all algorithms is demonstrated on a number of numerical examples, and we achieve up to more than an order of magnitude higher compression ratios than previous approaches to using the tensor ring format.	1,0,0,0,0,0
Mining Density Contrast Subgraphs	Dense subgraph discovery is a key primitive in many graph mining applications, such as detecting communities in social networks and mining gene correlation from biological data. Most studies on dense subgraph mining only deal with one graph. However, in many applications, we have more than one graph describing relations among a same group of entities. In this paper, given two graphs sharing the same set of vertices, we investigate the problem of detecting subgraphs that contrast the most with respect to density. We call such subgraphs Density Contrast Subgraphs, or DCS in short. Two widely used graph density measures, average degree and graph affinity, are considered. For both density measures, mining DCS is equivalent to mining the densest subgraph from a "difference" graph, which may have both positive and negative edge weights. Due to the existence of negative edge weights, existing dense subgraph detection algorithms cannot identify the subgraph we need. We prove the computational hardness of mining DCS under the two graph density measures and develop efficient algorithms to find DCS. We also conduct extensive experiments on several real-world datasets to evaluate our algorithms. The experimental results show that our algorithms are both effective and efficient.	1,0,0,0,0,0
Stein's Method for Stationary Distributions of Markov Chains and Application to Ising Models	We develop a new technique, based on Stein's method, for comparing two stationary distributions of irreducible Markov Chains whose update rules are `close enough'. We apply this technique to compare Ising models on $d$-regular expander graphs to the Curie-Weiss model (complete graph) in terms of pairwise correlations and more generally $k$th order moments. Concretely, we show that $d$-regular Ramanujan graphs approximate the $k$th order moments of the Curie-Weiss model to within average error $k/\sqrt{d}$ (averaged over the size $k$ subsets). The result applies even in the low-temperature regime; we also derive some simpler approximation results for functionals of Ising models that hold only at high enough temperatures.	0,0,1,0,0,0
Fair Kernel Learning	New social and economic activities massively exploit big data and machine learning algorithms to do inference on people's lives. Applications include automatic curricula evaluation, wage determination, and risk assessment for credits and loans. Recently, many governments and institutions have raised concerns about the lack of fairness, equity and ethics in machine learning to treat these problems. It has been shown that not including sensitive features that bias fairness, such as gender or race, is not enough to mitigate the discrimination when other related features are included. Instead, including fairness in the objective function has been shown to be more efficient. We present novel fair regression and dimensionality reduction methods built on a previously proposed fair classification framework. Both methods rely on using the Hilbert Schmidt independence criterion as the fairness term. Unlike previous approaches, this allows us to simplify the problem and to use multiple sensitive variables simultaneously. Replacing the linear formulation by kernel functions allows the methods to deal with nonlinear problems. For both linear and nonlinear formulations the solution reduces to solving simple matrix inversions or generalized eigenvalue problems. This simplifies the evaluation of the solutions for different trade-off values between the predictive error and fairness terms. We illustrate the usefulness of the proposed methods in toy examples, and evaluate their performance on real world datasets to predict income using gender and/or race discrimination as sensitive variables, and contraceptive method prediction under demographic and socio-economic sensitive descriptors.	0,0,0,1,0,0
Fast sampling of parameterised Gaussian random fields	Gaussian random fields are popular models for spatially varying uncertainties, arising for instance in geotechnical engineering, hydrology or image processing. A Gaussian random field is fully characterised by its mean function and covariance operator. In more complex models these can also be partially unknown. In this case we need to handle a family of Gaussian random fields indexed with hyperparameters. Sampling for a fixed configuration of hyperparameters is already very expensive due to the nonlocal nature of many classical covariance operators. Sampling from multiple configurations increases the total computational cost severely. In this report we employ parameterised Karhunen-Loève expansions for sampling. To reduce the cost we construct a reduced basis surrogate built from snapshots of Karhunen-Loève eigenvectors. In particular, we consider Matérn-type covariance operators with unknown correlation length and standard deviation. We suggest a linearisation of the covariance function and describe the associated online-offline decomposition. In numerical experiments we investigate the approximation error of the reduced eigenpairs. As an application we consider forward uncertainty propagation and Bayesian inversion with an elliptic partial differential equation where the logarithm of the diffusion coefficient is a parameterised Gaussian random field. In the Bayesian inverse problem we employ Markov chain Monte Carlo on the reduced space to generate samples from the posterior measure. All numerical experiments are conducted in 2D physical space, with non-separable covariance operators, and finite element grids with $\sim 10^4$ degrees of freedom.	1,0,0,1,0,0
Generalized singular value thresholding operator to affine matrix rank minimization problem	It is well known that the affine matrix rank minimization problem is NP-hard and all known algorithms for exactly solving it are doubly exponential in theory and in practice due to the combinational nature of the rank function. In this paper, a generalized singular value thresholding operator is generated to solve the affine matrix rank minimization problem. Numerical experiments show that our algorithm performs effectively in finding a low-rank matrix compared with some state-of-art methods.	0,0,1,0,0,0
Variable Selection Methods for Model-based Clustering	Model-based clustering is a popular approach for clustering multivariate data which has seen applications in numerous fields. Nowadays, high-dimensional data are more and more common and the model-based clustering approach has adapted to deal with the increasing dimensionality. In particular, the development of variable selection techniques has received a lot of attention and research effort in recent years. Even for small size problems, variable selection has been advocated to facilitate the interpretation of the clustering results. This review provides a summary of the methods developed for variable selection in model-based clustering. Existing R packages implementing the different methods are indicated and illustrated in application to two data analysis examples.	0,0,0,1,0,0
Nonlinear Flexoelectricity in Non-centrosymmetric Crystals	We analytically derive the elastic, dielectric, piezoelectric, and the flexoelectric phenomenological coefficients as functions of microscopic model parameters such as ionic positions and spring constants in the two-dimensional square-lattice model with rock-salt-type ionic arrangement. Monte-Carlo simulation reveals that a difference in the given elastic constants of the diagonal springs, each of which connects the same cations or anions, is responsible for the linear flexoelectric effect in the model. We show the quadratic flexoelectric effect is present only in non-centrosymmetric systems and it can overwhelm the linear effect in feasibly large strain gradients.	0,1,0,0,0,0
Optimizing the Wisdom of the Crowd: Inference, Learning, and Teaching	The unprecedented demand for large amount of data has catalyzed the trend of combining human insights with machine learning techniques, which facilitate the use of crowdsourcing to enlist label information both effectively and efficiently. The classic work on crowdsourcing mainly focuses on the label inference problem under the categorization setting. However, inferring the true label requires sophisticated aggregation models that usually can only perform well under certain assumptions. Meanwhile, no matter how complicated the aggregation model is, the true model that generated the crowd labels remains unknown. Therefore, the label inference problem can never infer the ground truth perfectly. Based on the fact that the crowdsourcing labels are abundant and utilizing aggregation will lose such kind of rich annotation information (e.g., which worker provided which labels), we believe that it is critical to take the diverse labeling abilities of the crowdsourcing workers as well as their correlations into consideration. To address the above challenge, we propose to tackle three research problems, namely inference, learning, and teaching.	0,0,0,1,0,0
Machine Learning Approach to RF Transmitter Identification	With the development and widespread use of wireless devices in recent years (mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has become extremely crowded. In order to counter security threats posed by rogue or unknown transmitters, it is important to identify RF transmitters not by the data content of the transmissions but based on the intrinsic physical characteristics of the transmitters. RF waveforms represent a particular challenge because of the extremely high data rates involved and the potentially large number of transmitters present in a given location. These factors outline the need for rapid fingerprinting and identification methods that go beyond the traditional hand-engineered approaches. In this study, we investigate the use of machine learning (ML) strategies to the classification and identification problems, and the use of wavelets to reduce the amount of data required. Four different ML strategies are evaluated: deep neural nets (DNN), convolutional neural nets (CNN), support vector machines (SVM), and multi-stage training (MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method preconditioned by wavelets was by far the most accurate, achieving 100% classification accuracy of transmitters, as tested using data originating from 12 different transmitters. We discuss strategies for extension of MST to a much larger number of transmitters.	1,0,0,1,0,0
The phase transitions between $Z_n\times Z_n$ bosonic topological phases in 1+1 D, and a constraint on the central charge for the critical points between bosonic symmetry protected topological phases	The study of continuous phase transitions triggered by spontaneous symmetry breaking has brought revolutionary ideas to physics. Recently, through the discovery of symmetry protected topological phases, it is realized that continuous quantum phase transition can also occur between states with the same symmetry but different topology. Here we study a specific class of such phase transitions in 1+1 dimensions -- the phase transition between bosonic topological phases protected by $Z_n\times Z_n$. We find in all cases the critical point possesses two gap opening relevant operators: one leads to a Landau-forbidden symmetry breaking phase transition and the other to the topological phase transition. We also obtained a constraint on the central charge for general phase transitions between symmetry protected bosonic topological phases in 1+1D.	0,1,0,0,0,0
Variegation and space weathering on asteroid 21 Lutetia	During the flyby in 2010, the OSIRIS camera on-board Rosetta acquired hundreds of high-resolution images of asteroid Lutetia's surface through a range of narrow-band filters. While Lutetia appears very bland in the visible wavelength range, Magrin et al. (2012) tentatively identified UV color variations in the Baetica cluster, a group of relatively young craters close to the north pole. As Lutetia remains a poorly understood asteroid, such color variations may provide clues to the nature of its surface. We take the color analysis one step further. First we orthorectify the images using a shape model and improved camera pointing, then apply a variety of techniques (photometric correction, principal component analysis) to the resulting color cubes. We characterize variegation in the Baetica crater cluster at high spatial resolution, identifying crater rays and small, fresh impact craters. We argue that at least some of the color variation is due to space weathering, which makes Lutetia's regolith redder and brighter.	0,1,0,0,0,0
Dynamics of higher-order rational solitons for the nonlocal nonlinear Schrodinger equation with the self-induced parity-time-symmetric potential	The integrable nonlocal nonlinear Schrodinger (NNLS) equation with the self-induced parity-time-symmetric potential [Phys. Rev. Lett. 110 (2013) 064105] is investigated, which is an integrable extension of the standard NLS equation. Its novel higher-order rational solitons are found using the nonlocal version of the generalized perturbation (1, N-1)-fold Darboux transformation. These rational solitons illustrate abundant wave structures for the distinct choices of parameters (e.g., the strong and weak interactions of bright and dark rational solitons). Moreover, we also explore the dynamical behaviors of these higher-order rational solitons with some small noises on the basis of numerical simulations.	0,1,1,0,0,0
Multi-SpaM: a Maximum-Likelihood approach to Phylogeny reconstruction based on Multiple Spaced-Word Matches	Motivation: Word-based or `alignment-free' methods for phylogeny reconstruction are much faster than traditional approaches, but they are generally less accurate. Most of these methods calculate pairwise distances for a set of input sequences, for example from word frequencies, from so-called spaced-word matches or from the average length of common substrings. Results: In this paper, we propose the first word-based approach to tree reconstruction that is based on multiple sequence comparison and Maximum Likelihood. Our algorithm first samples small, gap-free alignments involving four taxa each. For each of these alignments, it then calculates a quartet tree and, finally, the program Quartet MaxCut is used to infer a super tree topology for the full set of input taxa from the calculated quartet trees. Experimental results show that trees calculated with our approach are of high quality. Availability: The source code of the program is available at this https URL Contact: thomas.dencker@stud.uni-goettingen.de	0,0,0,0,1,0
Approximation Dynamics	We describe the approximation of a continuous dynamical system on a p. l. manifold or Cantor set by a tractable system. A system is tractable when it has a finite number of chain components and, with respect to a given full background measure, almost every point is generic for one of a finite number of ergodic invariant measures with non-overlapping supports. The approximations use non-degenerate simplicial dynamical systems for p. l. manifolds and shift-like dynamical systems for Cantor Sets.	0,0,1,0,0,0
A Loop-Based Methodology for Reducing Computational Redundancy in Workload Sets	The design of general purpose processors relies heavily on a workload gathering step in which representative programs are collected from various application domains. Processor performance, when running the workload set, is profiled using simulators that model the targeted processor architecture. However, simulating the entire workload set is prohibitively time-consuming, which precludes considering a large number of programs. To reduce simulation time, several techniques in the literature have exploited the internal program repetitiveness to extract and execute only representative code segments. Existing so- lutions are based on reducing cross-program computational redundancy or on eliminating internal-program redundancy to decrease execution time. In this work, we propose an orthogonal and complementary loop- centric methodology that targets loop-dominant programs by exploiting internal-program characteristics to reduce cross-program computational redundancy. The approach employs a newly developed framework that extracts and analyzes core loops within workloads. The collected characteristics model memory behavior, computational complexity, and data structures of a program, and are used to construct a signature vector for each program. From these vectors, cross-workload similarity metrics are extracted, which are processed by a novel heuristic to exclude similar programs and reduce redundancy within the set. Finally, a reverse engineering approach that synthesizes executable micro-benchmarks having the same instruction mix as the loops in the original workload is introduced. A tool that automates the flow steps of the proposed methodology is developed. Simulation results demonstrate that applying the proposed methodology to a set of workloads reduces the set size by half, while preserving the main characterizations of the initial workloads.	1,0,0,0,0,0
What Propels Celebrity Follower Counts? Language Use or Social Connectivity	Follower count is a factor that quantifies the popularity of celebrities. It is a reflection of their power, prestige and overall social reach. In this paper we investigate whether the social connectivity or the language choice is more correlated to the future follower count of a celebrity. We collect data about tweets, retweets and mentions of 471 Indian celebrities with verified Twitter accounts. We build two novel networks to approximate social connectivity of the celebrities. We study various structural properties of these two networks and observe their correlations with future follower counts. In parallel, we analyze the linguistic structure of the tweets (LIWC features, syntax and sentiment features and style and readability features) and observe the correlations of each of these with the future follower count of a celebrity. As a final step we use there features to classify a celebrity in a specific bucket of future follower count (HIGH, MID or LOW). We observe that the network features alone achieve an accuracy of 0.52 while the linguistic features alone achieve an accuracy of 0.69 grossly outperforming the network features. The network and linguistic features in conjunction produce an accuracy of 0.76. We also discuss some final insights that we obtain from further data analysis celebrities with larger follower counts post tweets that have (i) more words from friend and family LIWC categories, (ii) more positive sentiment laden words, (iii) have better language constructs and are (iv) more readable.	1,0,0,0,0,0
Solving Partial Differential Equations on Manifolds From Incomplete Inter-Point Distance	Solutions of partial differential equations (PDEs) on manifolds have provided important applications in different fields in science and engineering. Existing methods are majorly based on discretization of manifolds as implicit functions, triangle meshes, or point clouds, where the manifold structure is approximated by either zero level set of an implicit function or a set of points. In many applications, manifolds might be only provided as an inter-point distance matrix with possible missing values. This paper discusses a framework to discretize PDEs on manifolds represented as incomplete inter-point distance information. Without conducting a time-consuming global coordinates reconstruction, we propose a more efficient strategy by discretizing differential operators only based on point-wisely local reconstruction. Our local reconstruction model is based on the recent advances of low-rank matrix completion theory, where only a very small random portion of distance information is required. This method enables us to conduct analyses of incomplete distance data using solutions of special designed PDEs such as the Laplace-Beltrami (LB) eigen-system. As an application, we demonstrate a new way of manifold reconstruction from an incomplete distance by stitching patches using the spectrum of the LB operator. Intensive numerical experiments demonstrate the effectiveness of the proposed methods.	1,0,1,0,0,0
The Gaia-ESO Survey: Dynamical models of flattened, rotating globular clusters	We present a family of self-consistent axisymmetric rotating globular cluster models which are fitted to spectroscopic data for NGC 362, NGC 1851, NGC 2808, NGC 4372, NGC 5927 and NGC 6752 to provide constraints on their physical and kinematic properties, including their rotation signals. They are constructed by flattening Modified Plummer profiles, which have the same asymptotic behaviour as classical Plummer models, but can provide better fits to young clusters due to a slower turnover in the density profile. The models are in dynamical equilibrium as they depend solely on the action variables. We employ a fully Bayesian scheme to investigate the uncertainty in our model parameters (including mass-to-light ratios and inclination angles) and evaluate the Bayesian evidence ratio for rotating to non-rotating models. We find convincing levels of rotation only in NGC 2808. In the other clusters, there is just a hint of rotation (in particular, NGC 4372 and NGC 5927), as the data quality does not allow us to draw strong conclusions. Where rotation is present, we find that it is confined to the central regions, within radii of $R \leq 2 r_h$. As part of this work, we have developed a novel q-Gaussian basis expansion of the line-of-sight velocity distributions, from which general models can be constructed via interpolation on the basis coefficients.	0,1,0,0,0,0
Smooth positon solutions of the focusing modified Korteweg-de Vries equation	The $n$-fold Darboux transformation $T_{n}$ of the focusing real mo\-di\-fied Kor\-te\-weg-de Vries (mKdV) equation is expressed in terms of the determinant representation. Using this representation, the $n$-soliton solutions of the mKdV equation are also expressed by determinants whose elements consist of the eigenvalues $\lambda_{j}$ and the corresponding eigenfunctions of the associated Lax equation. The nonsingular $n$-positon solutions of the focusing mKdV equation are obtained in the special limit $\lambda_{j}\rightarrow\lambda_{1}$, from the corresponding $n$-soliton solutions and by using the associated higher-order Taylor expansion. Furthermore, the decomposition method of the $n$-positon solution into $n$ single-soliton solutions, the trajectories, and the corresponding "phase shifts" of the multi-positons are also investigated.	0,1,0,0,0,0
MHD Turbulence in spin-down flows of liquid metals	Intense spin-down flows allow one to reach high Rm in relatively small laboratory setups using moderate mass of liquid metals. The spin-down flow in toroidal channels was the first flow configuration used for studying dynamo effects in non-stationary flows. In this paper, we estimate the effect of small-scale dynamo in liquid metal spin-down flows realized in laboratory experiments. Our simulations have confirmed the conclusion that the dynamo effects observed in the experiments done on gallium are weak -- a slight burst of small-scale magnetic energy arises only at the highest available rotation velocity of the channel. In sodium flows, the induction effects are quite strong -- an essential part of kinetic energy of sodium spin-down flows is converted into magnetic energy and dissipates because of Joule heat losses. We have extended our simulations beyond the capabilities of existing laboratory facilities and examined the spin-down flows at the channel rotation velocity Omega>> 50 rps. It has been found that $\Omega\approx 100$rps is enough to reach the equipartition of magnetic and kinetic spectral power density at the lowest wave numbers (largest scales), whereas at Omega >= 200rps the intensity of the magnetic field becomes comparable to the intensity of velocity field fluctuations. We have also studied the influence of the Pm on the efficiency of small-scale dynamo in spin-down flows. In the experimental spin-down flows, the small-scale dynamo remains in a quasi-kinematic regime, and magnetic energy is mainly dissipated at the same scale, wherein it is converted from kinetic energy. The real small-scale dynamo starts to operate at Pm>10^{-4}, and the inertial range of the magnetic energy spectrum appears. Thereupon the energy dissipation is postponed to a later time and smaller scales, and the peak of turbulent energy (both kinetic and magnetic) slightly increases with Pm.	0,1,0,0,0,0
History-aware Autonomous Exploration in Confined Environments using MAVs	Many scenarios require a robot to be able to explore its 3D environment online without human supervision. This is especially relevant for inspection tasks and search and rescue missions. To solve this high-dimensional path planning problem, sampling-based exploration algorithms have proven successful. However, these do not necessarily scale well to larger environments or spaces with narrow openings. This paper presents a 3D exploration planner based on the principles of Next-Best Views (NBVs). In this approach, a Micro-Aerial Vehicle (MAV) equipped with a limited field-of-view depth sensor randomly samples its configuration space to find promising future viewpoints. In order to obtain high sampling efficiency, our planner maintains and uses a history of visited places, and locally optimizes the robot's orientation with respect to unobserved space. We evaluate our method in several simulated scenarios, and compare it against a state-of-the-art exploration algorithm. The experiments show substantial improvements in exploration time ($2\times$ faster), computation time, and path length, and advantages in handling difficult situations such as escaping dead-ends (up to $20\times$ faster). Finally, we validate the on-line capability of our algorithm on a computational constrained real world MAV.	1,0,0,0,0,0
The Supernova -- Supernova Remnant Connection	Many aspects of the progenitor systems, environments, and explosion dynamics of the various subtypes of supernovae are difficult to investigate at extragalactic distances where they are observed as unresolved sources. Alternatively, young supernova remnants in our own galaxy and in the Large and Small Magellanic Clouds offer opportunities to resolve, measure, and track expanding stellar ejecta in fine detail, but the handful that are known exhibit widely different properties that reflect the diversity of their parent explosions and local circumstellar and interstellar environments. A way of complementing both supernova and supernova remnant research is to establish strong empirical links between the two separate stages of stellar explosions. Here we briefly review recent progress in the development of supernova---supernova remnant connections, paying special attention to connections made through the study of "middle-aged" (10-100 yr) supernovae and young (< 1000 yr) supernova remnants. We highlight how this approach can uniquely inform several key areas of supernova research, including the origins of explosive mixing, high-velocity jets, and the formation of dust in the ejecta.	0,1,0,0,0,0
Transfer Learning for Brain-Computer Interfaces: An Euclidean Space Data Alignment Approach	Almost all EEG-based brain-computer interfaces (BCIs) need some labeled subject-specific data to calibrate a new subject, as neural responses are different across subjects to even the same stimulus. So, a major challenge in developing high-performance and user-friendly BCIs is to cope with such individual differences so that the calibration can be reduced or even completely eliminated. This paper focuses on the latter. More specifically, we consider an offline application scenario, in which we have unlabeled EEG trials from a new subject, and would like to accurately label them by leveraging auxiliary labeled EEG trials from other subjects in the same task. To accommodate the individual differences, we propose a novel unsupervised approach to align the EEG trials from different subjects in the Euclidean space to make them more consistent. It has three desirable properties: 1) the aligned trial lie in the Euclidean space, which can be used by any Euclidean space signal processing and machine learning approach; 2) it can be computed very efficiently; and, 3) it does not need any labeled trials from the new subject. Experiments on motor imagery and event-related potentials demonstrated the effectiveness and efficiency of our approach.	0,0,0,1,1,0
Social Robots for People with Developmental Disabilities: A User Study on Design Features of a Graphical User Interface	Social robots, also known as service or assistant robots, have been developed to improve the quality of human life in recent years. The design of socially capable and intelligent robots can vary, depending on the target user groups. In this work, we assess the effect of social robots' roles, functions, and communication approaches in the context of a social agent providing service or entertainment to users with developmental disabilities. In this paper, we describe an exploratory study of interface design for a social robot that assists people suffering from developmental disabilities. We developed series of prototypes and tested one in a user study that included three residents with various function levels. This entire study had been recorded for the following qualitative data analysis. Results show that each design factor played a different role in delivering information and in increasing engagement. We also note that some of the fundamental design principles that would work for ordinary users did not apply to our target user group. We conclude that social robots could benefit our target users, and acknowledge that these robots were not suitable for certain scenarios based on the feedback from our users.	1,0,0,0,0,0
Integrated Modeling of Second Phase Precipitation in Cold-Worked 316 Stainless Steels under Irradiation	The current work combines the Cluster Dynamics (CD) technique and CALPHAD-based precipitation modeling to address the second phase precipitation in cold-worked (CW) 316 stainless steels (SS) under irradiation at 300-400 C. CD provides the radiation enhanced diffusion and dislocation evolution as inputs for the precipitation model. The CALPHAD-based precipitation model treats the nucleation, growth and coarsening of precipitation processes based on classical nucleation theory and evolution equations, and simulates the composition, size and size distribution of precipitate phases. We benchmark the model against available experimental data at fast reactor conditions (9.4 x 10^-7 dpa/s and 390 C) and then use the model to predict the phase instability of CW 316 SS under light water reactor (LWR) extended life conditions (7 x 10^-8 dpa/s and 275 C). The model accurately predicts the gamma-prime (Ni3Si) precipitation evolution under fast reactor conditions and that the formation of this phase is dominated by radiation enhanced segregation. The model also predicts a carbide volume fraction that agrees well with available experimental data from a PWR reactor but is much higher than the volume fraction observed in fast reactors. We propose that radiation enhanced dissolution and/or carbon depletion at sinks that occurs at high flux could be the main sources of this inconsistency. The integrated model predicts ~1.2% volume fraction for carbide and ~3.0% volume fraction for gamma-prime for typical CW 316 SS (with 0.054 wt.% carbon) under LWR extended life conditions. This work provides valuable insights into the magnitudes and mechanisms of precipitation in irradiated CW 316 SS for nuclear applications.	0,1,0,0,0,0
Estimating linear functionals of a sparse family of Poisson means	Assume that we observe a sample of size n composed of p-dimensional signals, each signal having independent entries drawn from a scaled Poisson distribution with an unknown intensity. We are interested in estimating the sum of the n unknown intensity vectors, under the assumption that most of them coincide with a given 'background' signal. The number s of p-dimensional signals different from the background signal plays the role of sparsity and the goal is to leverage this sparsity assumption in order to improve the quality of estimation as compared to the naive estimator that computes the sum of the observed signals. We first introduce the group hard thresholding estimator and analyze its mean squared error measured by the squared Euclidean norm. We establish a nonasymptotic upper bound showing that the risk is at most of the order of {\sigma}^2(sp + s^2sqrt(p)) log^3/2(np). We then establish lower bounds on the minimax risk over a properly defined class of collections of s-sparse signals. These lower bounds match with the upper bound, up to logarithmic terms, when the dimension p is fixed or of larger order than s^2. In the case where the dimension p increases but remains of smaller order than s^2, our results show a gap between the lower and the upper bounds, which can be up to order sqrt(p).	0,0,1,1,0,0
Inverse dispersion method for calculation of complex photonic band diagram and $\cal{PT}$-symmetry	We suggest an inverse dispersion method for calculating photonic band diagram for materials with arbitrary frequency-dependent dielectric functions. The method is able to calculate the complex wave vector for a given frequency by solving the eigenvalue problem with a non-Hermitian operator. The analogy with $\cal{PT}$-symmetric Hamiltonians reveals that the operator corresponds to the momentum as a physical quantity and the singularities at the band edges are related to the branch points and responses for the features on the band edges. The method is realized using plane wave expansion technique for two-dimensional periodical structure in the case of TE- and TM-polarization. We illustrate the applicability of the method by calculation of the photonic band diagrams of an infinite two-dimension square lattice composed of dielectric cylinders using the measured frequency dependent dielectric functions of different materials (amorphous hydrogenated carbon, silicon, and chalcogenide glass). We show that the method allows to distinguish unambiguously between Bragg and Mie gaps in the spectra.	0,1,0,0,0,0
Comments on the National Toxicology Program Report on Cancer, Rats and Cell Phone Radiation	With the National Toxicology Program issuing its final report on cancer, rats and cell phone radiation, one can draw the following conclusions from their data. There is a roughly linear relationship between gliomas (brain cancers) and schwannomas (cancers of the nerve sheaths around the heart) with increased absorption of 900 MHz radiofrequency radiation for male rats. The rate of these cancers in female rats is about one third the rate in male rats; the rate of gliomas in female humans is about two thirds the rate in male humans. Both of these observations can be explained by a decrease in sensitivity to chemical carcinogenesis in both female rats and female humans. The increase in male rat life spans with increased radiofrequency absorption is due to a reduction in kidney failure from a decrease in food intake. No such similar increase in the life span of humans who use cell phones is expected.	0,0,0,0,1,0
On a generalization of Lie($k$): a CataLAnKe theorem	We define a generalization of the free Lie algebra based on an $n$-ary commutator and call it the free LAnKe. We show that the action of the symmetric group $S_{2n-1}$ on the multilinear component with $2n-1$ generators is given by the representation $S^{2^{n-1}1}$, whose dimension is the $n$th Catalan number. An application involving Specht modules of staircase shape is presented. We also introduce a conjecture that extends the relation between the Whitehouse representation and Lie($k$).	0,0,1,0,0,0
Shift-Coupling of Random Rooted Graphs and Networks	In this paper, we present a result similar to the shift-coupling result of Thorisson (1996) in the context of random graphs and networks. The result is that a given random rooted network can be obtained by changing the root of another given one if and only if the distributions of the two agree on the invariant sigma-field. Several applications of the result are presented for the case of unimodular networks. In particular, it is shown that the distribution of a unimodular network is uniquely determined by its restriction to the invariant sigma-filed. Also, the theorem is applied to the existence of an invariant transport kernel that balances between two given (discrete) measures on the vertices. An application is the existence of a so called extra head scheme for the Bernoulli process on an infinite unimodular graph. Moreover, a construction is presented for balancing transport kernels that is a generalization of the Gale-Shapley stable matching algorithm in bipartite graphs. Another application is on a general method that covers the situations where some vertices and edges are added to a unimodular network and then, to make it unimodular, the probability measure is biased and then a new root is selected. It is proved that this method provides all possible unimodularizations in these situations. Finally, analogous existing results for stationary point processes and unimodular networks are discussed in detail.	0,0,1,0,0,0
Small Telescope Exoplanet Transit Surveys: XO	The XO project aims at detecting transiting exoplanets around bright stars from the ground using small telescopes. The original configuration of XO (McCullough et al. 2005) has been changed and extended as described here. The instrumental setup consists of three identical units located at different sites, each composed of two lenses equipped with CCD cameras mounted on the same mount. We observed two strips of the sky covering an area of 520 deg$^2$ for twice nine months. We build lightcurves for ~20,000 stars up to magnitude R~12.5 using a custom-made photometric data reduction pipeline. The photometric precision is around 1-2% for most stars, and the large quantity of data allows us to reach a millimagnitude precision when folding the lightcurves on timescales that are relevant to exoplanetary transits. We search for periodic signals and identify several hundreds of variable stars and a few tens of transiting planet candidates. Follow-up observations are underway to confirm or reject these candidates. We found two close-in gas giant planets so far, in line with the expected yield.	0,1,0,0,0,0
Motion Planning for a UAV with a Straight or Kinked Tether	This paper develops and compares two motion planning algorithms for a tethered UAV with and without the possibility of the tether contacting the confined and cluttered environment. Tethered aerial vehicles have been studied due to their advantages such as power duration, stability, and safety. However, the disadvantages brought in by the extra tether have not been well investigated by the robotic locomotion community, especially when the tethered agent is locomoting in a non-free space occupied with obstacles. In this work, we propose two motion planning frameworks that (1) reduce the reachable configuration space by taking into account the tether and (2) deliberately plan (and relax) the contact point(s) of the tether with the environment and enable an equivalent reachable configuration space as the non-tethered counterpart would have. Both methods are tested on a physical robot, Fotokite Pro. With our approaches, tethered aerial vehicles could find their applications in confined and cluttered environments with obstacles as opposed to ideal free space, while still maintaining the advantages from the usage of a tether. The motion planning strategies are particularly suitable for marsupial heterogeneous robotic teams, such as visual servoing/assisting for another mobile, tele-operated primary robot.	1,0,0,0,0,0
Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification	Deep neural networks (DNNs) have transformed several artificial intelligence research areas including computer vision, speech recognition, and natural language processing. However, recent studies demonstrated that DNNs are vulnerable to adversarial manipulations at testing time. Specifically, suppose we have a testing example, whose label can be correctly predicted by a DNN classifier. An attacker can add a small carefully crafted noise to the testing example such that the DNN classifier predicts an incorrect label, where the crafted testing example is called adversarial example. Such attacks are called evasion attacks. Evasion attacks are one of the biggest challenges for deploying DNNs in safety and security critical applications such as self-driving cars. In this work, we develop new methods to defend against evasion attacks. Our key observation is that adversarial examples are close to the classification boundary. Therefore, we propose region-based classification to be robust to adversarial examples. For a benign/adversarial testing example, we ensemble information in a hypercube centered at the example to predict its label. In contrast, traditional classifiers are point-based classification, i.e., given a testing example, the classifier predicts its label based on the testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets demonstrate that our region-based classification can significantly mitigate evasion attacks without sacrificing classification accuracy on benign examples. Specifically, our region-based classification achieves the same classification accuracy on testing benign examples as point-based classification, but our region-based classification is significantly more robust than point-based classification to various evasion attacks.	1,0,0,1,0,0
A Topologist's View of Kinematic Maps and Manipulation Complexity	In this paper we combine a survey of the most important topological properties of kinematic maps that appear in robotics, with the exposition of some basic results regarding the topological complexity of a map. In particular, we discuss mechanical devices that consist of rigid parts connected by joints and show how the geometry of the joints determines the forward kinematic map that relates the configuration of joints with the pose of the end-effector of the device. We explain how to compute the dimension of the joint space and describe topological obstructions for a kinematic map to be a fibration or to admit a continuous section. In the second part of the paper we define the complexity of a continuous map and show how the concept can be viewed as a measure of the difficulty to find a robust manipulation plan for a given mechanical device. We also derive some basic estimates for the complexity and relate it to the degree of instability of a manipulation plan.	1,0,1,0,0,0
GIER: A Danish computer from 1961 with a role in the modern revolution of astronomy	A Danish computer, GIER, from 1961 played a vital role in the development of a new method for astrometric measurement. This method, photon counting astrometry, ultimately led to two satellites with a significant role in the modern revolution of astronomy. A GIER was installed at the Hamburg Observatory in 1964 where it was used to implement the entirely new method for the measurement of stellar positions by means of a meridian circle, then the fundamental instrument of astrometry. An expedition to Perth in Western Australia with the instrument and the computer was a success. This method was also implemented in space in the first ever astrometric satellite Hipparcos launched by ESA in 1989. The Hipparcos results published in 1997 revolutionized astrometry with an impact in all branches of astronomy from the solar system and stellar structure to cosmic distances and the dynamics of the Milky Way. In turn, the results paved the way for a successor, the one million times more powerful Gaia astrometry satellite launched by ESA in 2013. Preparations for a Gaia successor in twenty years are making progress.	0,1,0,0,0,0
Kulish-Sklyanin type models: integrability and reductions	We start with a Riemann-Hilbert problem (RHP) related to a BD.I-type symmetric spaces $SO(2r+1)/S(O(2r-2s +1)\otimes O(2s))$, $s\geq 1$. We consider two Riemann-Hilbert problems: the first formulated on the real axis $\mathbb{R}$ in the complex $\lambda$-plane; the second one is formulated on $\mathbb{R} \oplus i\mathbb{R}$. The first RHP for $s=1$ allows one to solve the Kulish-Sklyanin (KS) model; the second RHP is relevant for a new type of KS model. An important example for nontrivial deep reductions of KS model is given. Its effect on the scattering matrix is formulated. In particular we obtain new 2-component NLS equations. Finally, using the Wronskian relations we demonstrate that the inverse scattering method for KS models may be understood as a generalized Fourier transforms. Thus we have a tool to derive all their fundamental properties, including the hierarchy of equations and the hierarchy of their Hamiltonian structures.	0,1,0,0,0,0
High-speed X-ray imaging spectroscopy system with Zynq SoC for solar observations	We have developed a system combining a back-illuminated Complementary-Metal-Oxide-Semiconductor (CMOS) imaging sensor and Xilinx Zynq System-on-Chip (SoC) device for a soft X-ray (0.5-10 keV) imaging spectroscopy observation of the Sun to investigate the dynamics of the solar corona. Because typical timescales of energy release phenomena in the corona span a few minutes at most, we aim to obtain the corresponding energy spectra and derive the physical parameters, i.e., temperature and emission measure, every few tens of seconds or less for future solar X-ray observations. An X-ray photon-counting technique, with a frame rate of a few hundred frames per second or more, can achieve such results. We used the Zynq SoC device to achieve the requirements. Zynq contains an ARM processor core, which is also known as the Processing System (PS) part, and a Programmable Logic (PL) part in a single chip. We use the PL and PS to control the sensor and seamless recording of data to a storage system, respectively. We aim to use the system for the third flight of the Focusing Optics Solar X-ray Imager (FOXSI-3) sounding rocket experiment for the first photon-counting X-ray imaging and spectroscopy of the Sun.	0,1,0,0,0,0
Composite Rational Functions and Arithmetic Progressions	In this paper we deal with composite rational functions having zeros and poles forming consecutive elements of an arithmetic progression. We also correct a result published earlier related to composite rational functions having a fixed number of zeros and poles.	0,0,1,0,0,0
Belief Propagation, Bethe Approximation and Polynomials	Factor graphs are important models for succinctly representing probability distributions in machine learning, coding theory, and statistical physics. Several computational problems, such as computing marginals and partition functions, arise naturally when working with factor graphs. Belief propagation is a widely deployed iterative method for solving these problems. However, despite its significant empirical success, not much is known about the correctness and efficiency of belief propagation. Bethe approximation is an optimization-based framework for approximating partition functions. While it is known that the stationary points of the Bethe approximation coincide with the fixed points of belief propagation, in general, the relation between the Bethe approximation and the partition function is not well understood. It has been observed that for a few classes of factor graphs, the Bethe approximation always gives a lower bound to the partition function, which distinguishes them from the general case, where neither a lower bound, nor an upper bound holds universally. This has been rigorously proved for permanents and for attractive graphical models. Here we consider bipartite normal factor graphs and show that if the local constraints satisfy a certain analytic property, the Bethe approximation is a lower bound to the partition function. We arrive at this result by viewing factor graphs through the lens of polynomials. In this process, we reformulate the Bethe approximation as a polynomial optimization problem. Our sufficient condition for the lower bound property to hold is inspired by recent developments in the theory of real stable polynomials. We believe that this way of viewing factor graphs and its connection to real stability might lead to a better understanding of belief propagation and factor graphs in general.	1,0,0,1,0,0
User Donations in a Crowdsourced Video System	Crowdsourced video systems like YouTube and Twitch.tv have been a major internet phenomenon and are nowadays entertaining over a billion users. In addition to video sharing and viewing, over the years they have developed new features to boost the community engagement and some managed to attract users to donate, to the community as well as to other users. User donation directly reflects and influences user engagement in the community, and has a great impact on the success of such systems. Nevertheless, user donations in crowdsourced video systems remain trade secrets for most companies and to date are still unexplored. In this work, we attempt to fill this gap, and we obtain and provide a publicly available dataset on user donations in one crowdsourced video system named BiliBili. Based on information on nearly 40 thousand donators, we examine the dynamics of user donations and their social relationships, we quantitively reveal the factors that potentially impact user donation, and we adopt machine-learned classifiers and network representation learning models to timely and accurately predict the destinations of the majority and the individual donations.	1,0,0,0,0,0
Grand Fujii-Fujii-Nakamoto operator inequality dealing with operator order and operator chaotic order	In this paper, we shall prove that a grand Fujii-Fujii-Nakamoto operator inequality implies operator order and operator chaotic order under different conditions.	0,0,1,0,0,0
Nonlinear Modulational Instability of Dispersive PDE Models	We prove nonlinear modulational instability for both periodic and localized perturbations of periodic traveling waves for several dispersive PDEs, including the KDV type equations (e.g. the Whitham equation, the generalized KDV equation, the Benjamin-Ono equation), the nonlinear Schrödinger equation and the BBM equation. First, the semigroup estimates required for the nonlinear proof are obtained by using the Hamiltonian structures of the linearized PDEs; Second, for KDV type equations the loss of derivative in the nonlinear term is overcome in two complementary cases: (1) for smooth nonlinear terms and general dispersive operators, we construct higher order approximation solutions and then use energy type estimates; (2) for nonlinear terms of low regularity, with some additional assumption on the dispersive operator, we use a bootstrap argument to overcome the loss of derivative.	0,0,1,0,0,0
Gradient-enhanced kriging for high-dimensional problems	Surrogate models provide a low computational cost alternative to evaluating expensive functions. The construction of accurate surrogate models with large numbers of independent variables is currently prohibitive because it requires a large number of function evaluations. Gradient-enhanced kriging has the potential to reduce the number of function evaluations for the desired accuracy when efficient gradient computation, such as an adjoint method, is available. However, current gradient-enhanced kriging methods do not scale well with the number of sampling points due to the rapid growth in the size of the correlation matrix where new information is added for each sampling point in each direction of the design space. They do not scale well with the number of independent variables either due to the increase in the number of hyperparameters that needs to be estimated. To address this issue, we develop a new gradient-enhanced surrogate model approach that drastically reduced the number of hyperparameters through the use of the partial-least squares method that maintains accuracy. In addition, this method is able to control the size of the correlation matrix by adding only relevant points defined through the information provided by the partial-least squares method. To validate our method, we compare the global accuracy of the proposed method with conventional kriging surrogate models on two analytic functions with up to 100 dimensions, as well as engineering problems of varied complexity with up to 15 dimensions. We show that the proposed method requires fewer sampling points than conventional methods to obtain the desired accuracy, or provides more accuracy for a fixed budget of sampling points. In some cases, we get over 3 times more accurate models than a bench of surrogate models from the literature, and also over 3200 times faster than standard gradient-enhanced kriging models.	1,0,0,1,0,0
A Data-driven Approach Towards Human-robot Collaborative Problem Solving in a Shared Space	We are developing a system for human-robot communication that enables people to communicate with robots in a natural way and is focused on solving problems in a shared space. Our strategy for developing this system is fundamentally data-driven: we use data from multiple input sources and train key components with various machine learning techniques. We developed a web application that is collecting data on how two humans communicate to accomplish a task, as well as a mobile laboratory that is instrumented to collect data on how two humans communicate to accomplish a task in a physically shared space. The data from these systems will be used to train and fine-tune the second stage of our system, in which the robot will be simulated through software. A physical robot will be used in the final stage of our project. We describe these instruments, a test-suite and performance metrics designed to evaluate and automate the data gathering process as well as evaluate an initial data set.	1,0,0,0,0,0
On singular limit equations for incompressible fluids in moving thin domains	We consider the incompressible Euler and Navier-Stokes equations in a three-dimensional moving thin domain. Under the assumption that the moving thin domain degenerates into a two-dimensional moving closed surface as the width of the thin domain goes to zero, we give a heuristic derivation of singular limit equations on the degenerate moving surface of the Euler and Navier-Stokes equations in the moving thin domain and investigate relations between their energy structures. We also compare the limit equations with the Euler and Navier-Stokes equations on a stationary manifold, which are described in terms of the Levi-Civita connection.	0,1,1,0,0,0
Dark matter in dwarf galaxies	Although the cusp-core controversy for dwarf galaxies is seen as a problem, I argue that the cored central profiles can be explained by flattened cusps because they suffer from conflicting measurements and poor statistics and because there is a large number of conventional processes that could have flattened them since their creation, none of which requires new physics. Other problems, such as "too big to fail", are not discussed.	0,1,0,0,0,0
COLA: Decentralized Linear Learning	Decentralized machine learning is a promising emerging paradigm in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run on-device, on an arbitrary communication network, without a central coordinator. We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our framework overcomes many limitations of existing methods, and achieves communication efficiency, scalability, elasticity as well as resilience to changes in data and participating devices.	0,0,0,1,0,0
Diffraction-limited plenoptic imaging with correlated light	Traditional optical imaging faces an unavoidable trade-off between resolution and depth of field (DOF). To increase resolution, high numerical apertures (NA) are needed, but the associated large angular uncertainty results in a limited range of depths that can be put in sharp focus. Plenoptic imaging was introduced a few years ago to remedy this trade off. To this aim, plenoptic imaging reconstructs the path of light rays from the lens to the sensor. However, the improvement offered by standard plenoptic imaging is practical and not fundamental: the increased DOF leads to a proportional reduction of the resolution well above the diffraction limit imposed by the lens NA. In this paper, we demonstrate that correlation measurements enable pushing plenoptic imaging to its fundamental limits of both resolution and DOF. Namely, we demonstrate to maintain the imaging resolution at the diffraction limit while increasing the depth of field by a factor of 7. Our results represent the theoretical and experimental basis for the effective development of the promising applications of plenoptic imaging.	0,1,0,0,0,0
Optimal Decentralized Economical-sharing Criterion and Scheme for Microgrid	In order to address the economical dispatch problem in islanded microgrid, this letter proposes an optimal criterion and two decentralized economical-sharing schemes. The criterion is to judge whether global optimal economical-sharing can be realized via a decentralized manner. On the one hand, if the system cost functions meet this criterion, the corresponding decentralized droop method is proposed to achieve the global optimal dispatch. Otherwise, if the system does not meet this criterion, a modified method to achieve suboptimal dispatch is presented. The advantages of these methods are convenient,effective and communication-less.	1,0,0,0,0,0
Multi-Agent Coverage Control with Energy Depletion and Repletion	We develop a hybrid system model to describe the behavior of multiple agents cooperatively solving an optimal coverage problem under energy depletion and repletion constraints. The model captures the controlled switching of agents between coverage (when energy is depleted) and battery charging (when energy is replenished) modes. It guarantees the feasibility of the coverage problem by defining a guard function on each agent's battery level to prevent it from dying on its way to a charging station. The charging station plays the role of a centralized scheduler to solve the contention problem of agents competing for the only charging resource in the mission space. The optimal coverage problem is transformed into a parametric optimization problem to determine an optimal recharging policy. This problem is solved through the use of Infinitesimal Perturbation Analysis (IPA), with simulation results showing that a full recharging policy is optimal.	1,0,0,0,0,0
Conditional Variance Penalties and Domain Shift Robustness	When training a deep network for image classification, one can broadly distinguish between two types of latent features of images that will drive the classification. Following the notation of Gong et al. (2016), we can divide latent features into (i) "core" features $X^\text{core}$ whose distribution $X^\text{core}\vert Y$ does not change substantially across domains and (ii) "style" features $X^{\text{style}}$ whose distribution $X^{\text{style}}\vert Y$ can change substantially across domains. These latter orthogonal features would generally include features such as rotation, image quality or brightness but also more complex ones like hair color or posture for images of persons. Guarding against future adversarial domain shifts implies that the influence of the second type of style features in the prediction has to be limited. We assume that the domain itself is not observed and hence a latent variable. We do assume, however, that we can sometimes observe a typically discrete identifier or $\mathrm{ID}$ variable. We know in some applications, for example, that two images show the same person, and $\mathrm{ID}$ then refers to the identity of the person. The method requires only a small fraction of images to have an $\mathrm{ID}$ variable. We group data samples if they share the same class and identifier $(Y,\mathrm{ID})=(y,\mathrm{id})$ and penalize the conditional variance of the prediction if we condition on $(Y,\mathrm{ID})$. Using this approach is shown to protect against shifts in the distribution of the style variables for both regression and classification models. Specifically, the conditional variance penalty CoRe is shown to be equivalent to minimizing the risk under noise interventions in a regression setting and is shown to lead to adversarial risk consistency in a partially linear classification setting.	1,0,0,1,0,0
Tailoring Heterovalent Interface Formation with Light	Integrating different semiconductor materials into an epitaxial device structure offers additional degrees of freedom to select for optimal material properties in each layer. However, interface between materials with different valences (i.e. III-V, II-VI and IV semiconductors) can be difficult to form with high quality. Using ZnSe/GaAs as a model system, we explore the use of UV illumination during heterovalent interface growth by molecular beam epitaxy as a way to modify the interface properties. We find that UV illumination alters the mixture of chemical bonds at the interface, permitting the formation of Ga-Se bonds that help to passivate the underlying GaAs layer. Illumination also helps to reduce defects in the ZnSe epilayer. These results suggest that moderate UV illumination during growth may be used as a way to improve the optical properties of both the GaAs and ZnSe layers on either side of the interface.	0,1,0,0,0,0
The tumbling rotational state of 1I/`Oumuamua	The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a planetesimal born in another planetary system. This interloper exhibits a variable colour within a range that is broadly consistent with local small bodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited Kuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an axial ratio exceeding 5:1. Rotation period estimates are inconsistent and varied, with reported values between 6.9 and 8.3 hours. Here we analyse all available optical photometry reported to date. No single rotation period can explain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be in an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or tumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135 and 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the available data are insufficient to uniquely constrain the true frequencies and shape. Assuming a body that responds to NPA rotation in a similar manner to Solar System asteroids and comets, the timescale to damp 1I/'Oumuamua's tumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling within its parent planetary system, and will remain tumbling well after it has left ours.	0,1,0,0,0,0
Explaining Transition Systems through Program Induction	Explaining and reasoning about processes which underlie observed black-box phenomena enables the discovery of causal mechanisms, derivation of suitable abstract representations and the formulation of more robust predictions. We propose to learn high level functional programs in order to represent abstract models which capture the invariant structure in the observed data. We introduce the $\pi$-machine (program-induction machine) -- an architecture able to induce interpretable LISP-like programs from observed data traces. We propose an optimisation procedure for program learning based on backpropagation, gradient descent and A* search. We apply the proposed method to three problems: system identification of dynamical systems, explaining the behaviour of a DQN agent and learning by demonstration in a human-robot interaction scenario. Our experimental results show that the $\pi$-machine can efficiently induce interpretable programs from individual data traces.	1,0,0,0,0,0
Modelling the descent of nitric oxide during the elevated stratopause event of January 2013	Using simulations with a whole-atmosphere chemistry-climate model nudged by meteorological analyses, global satellite observations of nitrogen oxide (NO) and water vapour by the Sub-Millimetre Radiometer instrument (SMR), of temperature by the Microwave Limb Sounder (MLS), as well as local radar observations, this study examines the recent major stratospheric sudden warming accompanied by an elevated stratopause event (ESE) that occurred in January 2013. We examine dynamical processes during the ESE, including the role of planetary wave, gravity wave and tidal forcing on the initiation of the descent in the mesosphere-lower thermosphere (MLT) and its continuation throughout the mesosphere and stratosphere, as well as the impact of model eddy diffusion. We analyse the transport of NO and find the model underestimates the large descent of NO compared to SMR observations. We demonstrate that the discrepancy arises abruptly in the MLT region at a time when the resolved wave forcing and the planetary wave activity increase, just before the elevated stratopause reforms. The discrepancy persists despite doubling the model eddy diffusion. While the simulations reproduce an enhancement of the semi-diurnal tide following the onset of the 2013 SSW, corroborating new meteor radar observations at high northern latitudes over Trondheim (63.4$^{\circ}$N), the modelled tidal contribution to the forcing of the mean meridional circulation and to the descent is a small portion of the resolved wave forcing, and lags it by about ten days.	0,1,0,0,0,0
Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net	Models applied on real time response task, like click-through rate (CTR) prediction model, require high accuracy and rigorous response time. Therefore, top-performing deep models of high depth and complexity are not well suited for these applications with the limitations on the inference time. In order to further improve the neural networks' performance given the time and computational limitations, we propose an approach that exploits a cumbersome net to help train the lightweight net for prediction. We dub the whole process rocket launching, where the cumbersome booster net is used to guide the learning of the target light net throughout the whole training process. We analyze different loss functions aiming at pushing the light net to behave similarly to the booster net, and adopt the loss with best performance in our experiments. We use one technique called gradient block to improve the performance of the light net and booster net further. Experiments on benchmark datasets and real-life industrial advertisement data present that our light model can get performance only previously achievable with more complex models.	1,0,0,1,0,0
Adaptive Submodular Influence Maximization with Myopic Feedback	This paper examines the problem of adaptive influence maximization in social networks. As adaptive decision making is a time-critical task, a realistic feedback model has been considered, called myopic. In this direction, we propose the myopic adaptive greedy policy that is guaranteed to provide a (1 - 1/e)-approximation of the optimal policy under a variant of the independent cascade diffusion model. This strategy maximizes an alternative utility function that has been proven to be adaptive monotone and adaptive submodular. The proposed utility function considers the cumulative number of active nodes through the time, instead of the total number of the active nodes at the end of the diffusion. Our empirical analysis on real-world social networks reveals the benefits of the proposed myopic strategy, validating our theoretical results.	1,0,0,0,0,0
On the approximation by convolution type double singular integral operators	In this paper, we prove the pointwise convergence and the rate of pointwise convergence for a family of singular integral operators in two-dimensional setting in the following form: \begin{equation*} L_{\lambda }\left( f;x,y\right) =\underset{D}{\iint }f\left( t,s\right) K_{\lambda }\left( t-x,s-y\right) dsdt,\text{ }\left( x,y\right) \in D, \end{equation*} where $D=\left \langle a,b\right \rangle \times \left \langle c,d\right \rangle $ is an arbitrary closed, semi-closed or open rectangle in $\mathbb{R}^{2}$ and $% \lambda \in \Lambda ,$ $\Lambda $ is a set of non-negative indices with accumulation point $\lambda_{0}$. Also, we provide an example to support these theoretical results. In contrast to previous works, the kernel function $K_{\lambda }\left( t,s\right) $ does not have to be even, positive or 2$\pi -$periodic.	0,0,1,0,0,0
Nonlinear photoionization of transparent solids: a nonperturbative theory obeying selection rules	We provide a nonperturbative theory for photoionization of transparent solids. By applying a particular steepest-descent method, we derive analytical expressions for the photoionization rate within the two-band structure model, which consistently account for the $selection$ $rules$ related to the parity of the number of absorbed photons ($odd$ or $even$). We demonstrate the crucial role of the interference of the transition amplitudes (saddle-points), which in the semi-classical limit, can be interpreted in terms of interfering quantum trajectories. Keldysh's foundational work of laser physics [Sov. Phys. JETP 20, 1307 (1965)] disregarded this interference, resulting in the violation of $selection$ $rules$. We provide an improved Keldysh photoionization theory and show its excellent agreement with measurements for the frequency dependence of the two-photon absorption and nonlinear refractive index coefficients in dielectrics.	0,1,0,0,0,0
A Semantics for Probabilistic Control-Flow Graphs	This article develops a novel operational semantics for probabilistic control-flow graphs (pCFGs) of probabilistic imperative programs with random assignment and "observe" (or conditioning) statements. The semantics transforms probability distributions (on stores) as control moves from one node to another in pCFGs. We relate this semantics to a standard, expectation-transforming, denotational semantics of structured probabilistic imperative programs, by translating structured programs into (unstructured) pCFGs, and proving adequacy of the translation. This shows that the operational semantics can be used without loss of information, and is faithful to the "intended" semantics and hence can be used to reason about, for example, the correctness of transformations (as we do in a companion article).	1,0,0,0,0,0
Towards Open Data for the Citation Content Analysis	The paper presents first results of the CitEcCyr project funded by RANEPA. The project aims to create a source of open citation data for research papers written in Russian. Compared to existing sources of citation data, CitEcCyr is working to provide the following added values: a) a transparent and distributed architecture of a technology that generates the citation data; b) an openness of all built/used software and created citation data; c) an extended set of citation data sufficient for the citation content analysis; d) services for public control over a quality of the citation data and a citing activity of researchers.	1,0,0,0,0,0
Functional Dynamical Structures in Complex Systems: an Information-Theoretic Approach	Understanding the dynamical behavior of complex systems is of exceptional relevance in everyday life, from biology to economy. In order to describe the dynamical organization of complex systems, existing methods require the knowledge of the network topology. By contrast, in this thesis we develop a new method based on Information Theory which does not require any topological knowledge. We introduce the Dynamical Cluster Index to detect those groups of system elements which have strong mutual interactions, named as Relevant Subsets. Among them, we identify those which exchange most information with the rest of the system, thus being the most influential for its dynamics. In order to detect such Functional Dynamical Structures, we introduce another information theoretic measure, called D-index. The experimental results make us confident that our method can be effectively used to study both artificial and natural complex systems.	0,1,0,0,0,0
A New Approximation Guarantee for Monotone Submodular Function Maximization via Discrete Convexity	In monotone submodular function maximization, approximation guarantees based on the curvature of the objective function have been extensively studied in the literature. However, the notion of curvature is often pessimistic, and we rarely obtain improved approximation guarantees, even for very simple objective functions. In this paper, we provide a novel approximation guarantee by extracting an M$^\natural$-concave function $h:2^E \to \mathbb R_+$, a notion in discrete convex analysis, from the objective function $f:2^E \to \mathbb R_+$. We introduce the notion of $h$-curvature, which measures how much $f$ deviates from $h$, and show that we can obtain a $(1-\gamma/e-\epsilon)$-approximation to the problem of maximizing $f$ under a cardinality constraint in polynomial time for any constant $\epsilon > 0$. Then, we show that we can obtain nontrivial approximation guarantees for various problems by applying the proposed algorithm.	1,0,0,0,0,0
Thickening and sickening the SYK model	We discuss higher dimensional generalizations of the 0+1-dimensional Sachdev-Ye-Kitaev (SYK) model that has recently become the focus of intensive interdisciplinary studies by, both, the condensed matter and field-theoretical communities. Unlike the previous constructions where multiple SYK copies would be coupled to each other and/or hybridized with itinerant fermions via spatially short-ranged random hopping processes, we study algebraically varying long-range (spatially and/or temporally) correlated random couplings in the general d+1 dimensions. Such pertinent topics as translationally-invariant strong-coupling solutions, emergent reparametrization symmetry, effective action for fluctuations, chaotic behavior, and diffusive transport (or a lack thereof) are all addressed. We find that the most appealing properties of the original SYK model that suggest the existence of its 1+1-dimensional holographic gravity dual do not survive the aforementioned generalizations, thus lending no additional support to the hypothetical broad (including 'non-AdS/non-CFT') holographic correspondence.	0,1,0,0,0,0
Effect of ion motion on relativistic electron beam driven wakefield in a cold plasma	Excitation of relativistic electron beam driven wakefield in a cold plasma is studied using 1-D fluid simulation techniques where the effect of ion motion is included. We have excited the wakefield using a ultra-relativistic, homogeneous, rigid electron beam with different beam densities and mass-ratios (ratio of electron's to ion's mass). We have shown that the numerically excited wakefield is in a good agreement with the analytical results of Rosenzweig et al. \textcolor{blue}{[Physical Review A. 40, 9, (1989)]} for several plasma periods. It is shown here that the excited wake wave is equivalent to the corresponding "Khachatryan mode" \textcolor{blue}{[Physical Review E. 58, 6, (1998)]}. After several plasma periods, it is found that the excited wake wave gradually modifies and finally breaks, exhibiting sharp spikes in density and sawtooth like structure in electric field profile. It is shown here that the excited wake wave breaks much below the Khachatryan's wave breaking limit.	0,1,0,0,0,0
Dynamical system analysis of dark energy models in scalar coupled metric-torsion theories	We study the phase space dynamics of cosmological models in the theoretical formulations of non-minimal metric-torsion couplings with a scalar field, and investigate in particular the critical points which yield stable solutions exhibiting cosmic acceleration driven by the {\em dark energy}. The latter is defined in a way that it effectively has no direct interaction with the cosmological fluid, although in an equivalent scalar-tensor cosmological setup the scalar field interacts with the fluid (which we consider to be the pressureless dust). Determining the conditions for the existence of the stable critical points we check their physical viability, in both Einstein and Jordan frames. We also verify that in either of these frames, the evolution of the universe at the corresponding stable points matches with that given by the respective exact solutions we have found in an earlier work (arXiv: 1611.00654 [gr-qc]). We not only examine the regions of physical relevance for the trajectories in the phase space when the coupling parameter is varied, but also demonstrate the evolution profiles of the cosmological parameters of interest along fiducial trajectories in the effectively non-interacting scenarios, in both Einstein and Jordan frames.	0,1,0,0,0,0
An integral formula for the $Q$-prime curvature in 3-dimensional CR geometry	We give an integral formula for the total $Q^\prime$-curvature of a three-dimensional CR manifold with positive CR Yamabe constant and nonnegative Paneitz operator. Our derivation includes a relationship between the Green's functions of the CR Laplacian and the $P^\prime$-operator.	0,0,1,0,0,0
Construction and Encoding of QC-LDPC Codes Using Group Rings	Quasi-cyclic (QC) low-density parity-check (LDPC) codes which are known as QC-LDPC codes, have many applications due to their simple encoding implementation by means of cyclic shift registers. In this paper, we construct QC-LDPC codes from group rings. A group ring is a free module (at the same time a ring) constructed in a natural way from any given ring and any given group. We present a structure based on the elements of a group ring for constructing QC-LDPC codes. Some of the previously addressed methods for constructing QC-LDPC codes based on finite fields are special cases of the proposed construction method. The constructed QC-LDPC codes perform very well over the additive white Gaussian noise (AWGN) channel with iterative decoding in terms of bit-error probability and block-error probability. Simulation results demonstrate that the proposed codes have competitive performance in comparison with the similar existing LDPC codes. Finally, we propose a new encoding method for the proposed group ring based QC-LDPC codes that can be implemented faster than the current encoding methods. The encoding complexity of the proposed method is analyzed mathematically, and indicates a significate reduction in the required number of operations, even when compared to the available efficient encoding methods that have linear time and space complexities.	1,0,1,0,0,0
Multimodal Affect Analysis for Product Feedback Assessment	Consumers often react expressively to products such as food samples, perfume, jewelry, sunglasses, and clothing accessories. This research discusses a multimodal affect recognition system developed to classify whether a consumer likes or dislikes a product tested at a counter or kiosk, by analyzing the consumer's facial expression, body posture, hand gestures, and voice after testing the product. A depth-capable camera and microphone system - Kinect for Windows - is utilized. An emotion identification engine has been developed to analyze the images and voice to determine affective state of the customer. The image is segmented using skin color and adaptive threshold. Face, body and hands are detected using the Haar cascade classifier. Canny edges are identified and the lip, body and hand contours are extracted using spatial filtering. Edge count and orientation around the mouth, cheeks, eyes, shoulders, fingers and the location of the edges are used as features. Classification is done by an emotion template mapping algorithm and training a classifier using support vector machines. The real-time performance, accuracy and feasibility for multimodal affect recognition in feedback assessment are evaluated.	1,0,0,0,0,0
Unveiling Swarm Intelligence with Network Science$-$the Metaphor Explained	Self-organization is a natural phenomenon that emerges in systems with a large number of interacting components. Self-organized systems show robustness, scalability, and flexibility, which are essential properties when handling real-world problems. Swarm intelligence seeks to design nature-inspired algorithms with a high degree of self-organization. Yet, we do not know why swarm-based algorithms work well and neither we can compare the different approaches in the literature. The lack of a common framework capable of characterizing these several swarm-based algorithms, transcending their particularities, has led to a stream of publications inspired by different aspects of nature without much regard as to whether they are similar to already existing approaches. We address this gap by introducing a network-based framework$-$the interaction network$-$to examine computational swarm-based systems via the optics of social dynamics. We discuss the social dimension of several swarm classes and provide a case study of the Particle Swarm Optimization. The interaction network enables a better understanding of the plethora of approaches currently available by looking at them from a general perspective focusing on the structure of the social interactions.	1,0,0,0,0,0
LinXGBoost: Extension of XGBoost to Generalized Local Linear Models	XGBoost is often presented as the algorithm that wins every ML competition. Surprisingly, this is true even though predictions are piecewise constant. This might be justified in high dimensional input spaces, but when the number of features is low, a piecewise linear model is likely to perform better. XGBoost was extended into LinXGBoost that stores at each leaf a linear model. This extension, equivalent to piecewise regularized least-squares, is particularly attractive for regression of functions that exhibits jumps or discontinuities. Those functions are notoriously hard to regress. Our extension is compared to the vanilla XGBoost and Random Forest in experiments on both synthetic and real-world data sets.	1,0,0,1,0,0
Quantifying tidal stream disruption in a simulated Milky Way	Simulations of tidal streams show that close encounters with dark matter subhalos induce density gaps and distortions in on-sky path along the streams. Accordingly, observing disrupted streams in the Galactic halo would substantiate the hypothesis that dark matter substructure exists there, while in contrast, observing collimated streams with smoothly varying density profiles would place strong upper limits on the number density and mass spectrum of subhalos. Here, we examine several measures of stream "disruption" and their power to distinguish between halo potentials with and without substructure and with different global shapes. We create and evolve a population of 1280 streams on a range of orbits in the Via Lactea II simulation of a Milky Way-like halo, replete with a full mass range of {\Lambda}CDM subhalos, and compare it to two control stream populations evolved in smooth spherical and smooth triaxial potentials, respectively. We find that the number of gaps observed in a stellar stream is a poor indicator of the halo potential, but that (i) the thinness of the stream on-sky, (ii) the symmetry of the leading and trailing tails, and (iii) the deviation of the tails from a low-order polynomial path on-sky ("path regularity") distinguish between the three potentials more effectively. We find that globular cluster streams on low-eccentricity orbits far from the galactic center (apocentric radius ~ 30-80 kpc) are most powerful in distinguishing between the three potentials. If they exist, such streams will shortly be discoverable and mapped in high dimensions with near-future photometric and spectroscopic surveys.	0,1,0,0,0,0
Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks	Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.	1,0,0,1,0,0
Confidence Intervals for Quantiles from Histograms and Other Grouped Data	Interval estimation of quantiles has been treated by many in the literature. However, to the best of our knowledge there has been no consideration for interval estimation when the data are available in grouped format. Motivated by this, we introduce several methods to obtain confidence intervals for quantiles when only grouped data is available. Our preferred method for interval estimation is to approximate the underlying density using the Generalized Lambda Distribution (GLD) to both estimate the quantiles and variance of the quantile estimators. We compare the GLD method with some other methods that we also introduce which are based on a frequency approximation approach and a linear interpolation approximation of the density. Our methods are strongly supported by simulations showing that excellent coverage can be achieved for a wide number of distributions. These distributions include highly-skewed distributions such as the log-normal, Dagum and Singh-Maddala distributions. We also apply our methods to real data and show that inference can be carried out on published outcomes that have been summarized only by a histogram. Our methods are therefore useful for a broad range of applications. We have also created a web application that can be used to conveniently calculate the estimators.	0,0,0,1,0,0
Maximality of Galois actions for abelian varieties	Let $\{\rho_\ell\}_\ell$ be the system of $\ell$-adic representations arising from the $i$th $\ell$-adic cohomology of a complete smooth variety $X$ defined over a number field $K$. Denote the image of $\rho_\ell$ by $\Gamma_\ell$ and its Zariski closure, which is a linear algebraic group over $\mathbb{Q}_\ell$, by $\mathbf{G}_\ell$. We prove that $\mathbf{G}_\ell^{red}$, the quotient of $\mathbf{G}_\ell^\circ$ by its unipotent radical, is unramified over a totally ramified extension of $\mathbb{Q}_\ell$ for all sufficiently large $\ell$. We give a sufficient condition on $\{\rho_\ell\}_\ell$ such that for all sufficiently large $\ell$, $\Gamma_\ell$ is in some sense maximal compact in $\mathbf{G}_\ell(\mathbb{Q}_\ell)$. Since the condition is satisfied when $X$ is an abelian variety by the Tate conjecture, we obtain maximality of Galois actions for abelian varieties.	0,0,1,0,0,0
Stable Signatures for Dynamic Graphs and Dynamic Metric Spaces via Zigzag Persistence	When studying flocking/swarming behaviors in animals one is interested in quantifying and comparing the dynamics of the clustering induced by the coalescence and disbanding of animals in different groups. In a similar vein, studying the dynamics of social networks leads to the problem of characterizing groups/communities as they form and disperse throughout time. Motivated by this, we study the problem of obtaining persistent homology based summaries of time-dependent data. Given a finite dynamic graph (DG), we first construct a zigzag persistence module arising from linearizing the dynamic transitive graph naturally induced from the input DG. Based on standard results, we then obtain a persistence diagram or barcode from this zigzag persistence module. We prove that these barcodes are stable under perturbations in the input DG under a suitable distance between DGs that we identify. More precisely, our stability theorem can be interpreted as providing a lower bound for the distance between DGs. Since it relies on barcodes, and their bottleneck distance, this lower bound can be computed in polynomial time from the DG inputs. Since DGs can be given rise by applying the Rips functor (with a fixed threshold) to dynamic metric spaces, we are also able to derive related stable invariants for these richer class of dynamic objects. Along the way, we propose a summarization of dynamic graphs that captures their time-dependent clustering features which we call formigrams. These set-valued functions generalize the notion of dendrogram, a prevalent tool for hierarchical clustering. In order to elucidate the relationship between our distance between two DGs and the bottleneck distance between their associated barcodes, we exploit recent advances in the stability of zigzag persistence due to Botnan and Lesnick, and to Bjerkevik.	0,0,1,0,0,0
Phase reduction and synchronization of a network of coupled dynamical elements exhibiting collective oscillations	A general phase reduction method for a network of coupled dynamical elements exhibiting collective oscillations, which is applicable to arbitrary networks of heterogeneous dynamical elements, is developed. A set of coupled adjoint equations for phase sensitivity functions, which characterize phase response of the collective oscillation to small perturbations applied to individual elements, is derived. Using the phase sensitivity functions, collective oscillation of the network under weak perturbation can be described approximately by a one-dimensional phase equation. As an example, mutual synchronization between a pair of collectively oscillating networks of excitable and oscillatory FitzHugh-Nagumo elements with random coupling is studied.	0,1,0,0,0,0
Representing the Deligne-Hinich-Getzler $\infty$-groupoid	The goal of the present paper is to introduce a smaller, but equivalent version of the Deligne-Hinich-Getzler $\infty$-groupoid associated to a homotopy Lie algebra. In the case of differential graded Lie algebras, we represent it by a universal cosimplicial object.	0,0,1,0,0,0
Cross-lingual Distillation for Text Classification	Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.	1,0,0,0,0,0
A Stochastic Formulation of the Resolution of Identity: Application to Second Order Møller-Plesset Perturbation Theory	A stochastic orbital approach to the resolution of identity (RI) approximation for 4-index 2-electron electron repulsion integrals (ERIs) is presented. The stochastic RI-ERIs are then applied to M\o ller-Plesset perturbation theory (MP2) utilizing a \textit{multiple stochastic orbital approach}. The introduction of multiple stochastic orbitals results in an $N^3$ scaling for both the stochastic RI-ERIs and stochastic RI-MP2. We demonstrate that this method exhibits a small prefactor and an observed scaling of $N^{2.4}$ for a range of water clusters, already outperforming MP2 for clusters with as few as 21 water molecules.	0,1,0,0,0,0
Twin Learning for Similarity and Clustering: A Unified Kernel Approach	Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method.	1,0,0,1,0,0
A unified treatment of multiple testing with prior knowledge using the p-filter	There is a significant literature on methods for incorporating knowledge into multiple testing procedures so as to improve their power and precision. Some common forms of prior knowledge include (a) beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) multiple arbitrary partitions of the hypotheses into (possibly overlapping) groups; and (d) knowledge of independence, positive or arbitrary dependence between hypotheses or groups, suggesting the use of more aggressive or conservative procedures. We present a unified algorithmic framework called p-filter for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously, recovering a variety of known algorithms as special cases.	0,0,1,1,0,0
Learning Program Component Order	Successful programs are written to be maintained. One aspect to this is that programmers order the components in the code files in a particular way. This is part of programming style. While the conventions for ordering are sometimes given as part of a style guideline, such guidelines are often incomplete and programmers tend to have their own more comprehensive orderings in mind. This paper defines a model for ordering program components and shows how this model can be learned from sample code. Such a model is a useful tool for a programming environment in that it can be used to find the proper location for inserting new components or for reordering files to better meet the needs of the programmer. The model is designed so that it can be fine- tuned by the programmer. The learning framework is evaluated both by looking at code with known style guidelines and by testing whether it inserts existing components into a file correctly.	1,0,0,0,0,0
Connecting dissipation and phase slips in a Josephson junction between fermionic superfluids	We study the emergence of dissipation in an atomic Josephson junction between weakly-coupled superfluid Fermi gases. We find that vortex-induced phase slippage is the dominant microscopic source of dissipation across the BEC-BCS crossover. We explore different dynamical regimes by tuning the bias chemical potential between the two superfluid reservoirs. For small excitations, we observe dissipation and phase coherence to coexist, with a resistive current followed by well-defined Josephson oscillations. We link the junction transport properties to the phase-slippage mechanism, finding that vortex nucleation is primarily responsible for the observed trends of conductance and critical current. For large excitations, we observe the irreversible loss of coherence between the two superfluids, and transport cannot be described only within an uncorrelated phase-slip picture. Our findings open new directions for investigating the interplay between dissipative and superfluid transport in strongly correlated Fermi systems, and general concepts in out-of-equlibrium quantum systems.	0,1,0,0,0,0
Predicting Adolescent Suicide Attempts with Neural Networks	Though suicide is a major public health problem in the US, machine learning methods are not commonly used to predict an individual's risk of attempting/committing suicide. In the present work, starting with an anonymized collection of electronic health records for 522,056 unique, California-resident adolescents, we develop neural network models to predict suicide attempts. We frame the problem as a binary classification problem in which we use a patient's data from 2006-2009 to predict either the presence (1) or absence (0) of a suicide attempt in 2010. After addressing issues such as severely imbalanced classes and the variable length of a patient's history, we build neural networks with depths varying from two to eight hidden layers. For test set observations where we have at least five ED/hospital visits' worth of data on a patient, our depth-4 model achieves a sensitivity of 0.703, specificity of 0.980, and AUC of 0.958.	1,0,0,1,0,0
Weighted Tensor Decomposition for Learning Latent Variables with Partial Data	Tensor decomposition methods are popular tools for learning latent variables given only lower-order moments of the data. However, the standard assumption is that we have sufficient data to estimate these moments to high accuracy. In this work, we consider the case in which certain dimensions of the data are not always observed---common in applied settings, where not all measurements may be taken for all observations---resulting in moment estimates of varying quality. We derive a weighted tensor decomposition approach that is computationally as efficient as the non-weighted approach, and demonstrate that it outperforms methods that do not appropriately leverage these less-observed dimensions.	0,0,0,1,0,0
BindsNET: A machine learning-oriented spiking neural networks library in Python	The development of spiking neural network simulation software is a critical component enabling the modeling of neural systems and the development of biologically inspired algorithms. Existing software frameworks support a wide range of neural functionality, software abstraction levels, and hardware devices, yet are typically not suitable for rapid prototyping or application to problems in the domain of machine learning. In this paper, we describe a new Python package for the simulation of spiking neural networks, specifically geared towards machine learning and reinforcement learning. Our software, called BindsNET, enables rapid building and simulation of spiking networks and features user-friendly, concise syntax. BindsNET is built on top of the PyTorch deep neural networks library, enabling fast CPU and GPU computation for large spiking networks. The BindsNET framework can be adjusted to meet the needs of other existing computing and hardware environments, e.g., TensorFlow. We also provide an interface into the OpenAI gym library, allowing for training and evaluation of spiking networks on reinforcement learning problems. We argue that this package facilitates the use of spiking networks for large-scale machine learning experimentation, and show some simple examples of how we envision BindsNET can be used in practice. BindsNET code is available at this https URL	0,0,0,0,1,0
Vision-based Obstacle Removal System for Autonomous Ground Vehicles Using a Robotic Arm	Over the past few years, the use of camera-equipped robotic platforms for data collection and visually monitoring applications has exponentially grown. Cluttered construction sites with many objects (e.g., bricks, pipes, etc.) on the ground are challenging environments for a mobile unmanned ground vehicle (UGV) to navigate. To address this issue, this study presents a mobile UGV equipped with a stereo camera and a robotic arm that can remove obstacles along the UGV's path. To achieve this objective, the surrounding environment is captured by the stereo camera and obstacles are detected. The obstacle's relative location to the UGV is sent to the robotic arm module through Robot Operating System (ROS). Then, the robotic arm picks up and removes the obstacle. The proposed method will greatly enhance the degree of automation and the frequency of data collection for construction monitoring. The proposed system is validated through two case studies. The results successfully demonstrate the detection and removal of obstacles, serving as one of the enabling factors for developing an autonomous UGV with various construction operating applications.	1,0,0,0,0,0
Diagnosing added value of convection-permitting regional models using precipitation event identification and tracking	Dynamical downscaling with high-resolution regional climate models may offer the possibility of realistically reproducing precipitation and weather events in climate simulations. As resolutions fall to order kilometers, the use of explicit rather than parametrized convection may offer even greater fidelity. However, these increased model resolutions both allow and require increasingly complex diagnostics for evaluating model fidelity. In this study we use a suite of dynamically downscaled simulations of the summertime U.S. (WRF driven by NCEP) with systematic variations in parameters and treatment of convection as a test case for evaluation of model precipitation. In particular, we use a novel rainstorm identification and tracking algorithm that allocates essentially all rainfall to individual precipitation events (Chang et al. 2016). This approach allows multiple insights, including that, at least in these runs, model wet bias is driven by excessive areal extent of precipitating events. Biases are time-dependent, producing excessive diurnal cycle amplitude. We show that this effect is produced not by new production of events but by excessive enlargement of long-lived precipitation events during daytime, and that in the domain average, precipitation biases appear best represented as additive offsets. Of all model configurations evaluated, convection-permitting simulations most consistently reduced biases in precipitation event characteristics.	0,1,0,1,0,0
Hardy Spaces over Half-strip Domains	We define Hardy spaces $H^p(\Omega_\pm)$ on half-strip domain~$\Omega_+$ and $\Omega_-= \mathbb{C}\setminus\overline{\Omega_+}$, where $0<p<\infty$, and prove that functions in $H^p(\Omega_\pm)$ has non-tangential boundary limit a.e. on $\Gamma$, the common boundary of $\Omega_\pm$. We then prove that Cauchy integral of functions in $L^p(\Gamma)$ are in $H^p(\Omega_\pm)$, where $1<p<\infty$, that is, Cauchy transform is bounded. Besides, if $1\leqslant p<\infty$, then $H^p(\Omega_\pm)$ functions are the Cauchy integral of their non-tangential boundary limits. We also establish an isomorphism between $H^p(\Omega_\pm)$ and $H^p(\mathbb{C}_\pm)$, the classical Hardy spaces over upper and lower half complex planes.	0,0,1,0,0,0
Optimal Nonparametric Inference under Quantization	Statistical inference based on lossy or incomplete samples is of fundamental importance in research areas such as signal/image processing, medical image storage, remote sensing, signal transmission. In this paper, we propose a nonparametric testing procedure based on quantized samples. In contrast to the classic nonparametric approach, our method lives on a coarse grid of sample information and are simple-to-use. Under mild technical conditions, we establish the asymptotic properties of the proposed procedures including asymptotic null distribution of the quantization test statistic as well as its minimax power optimality. Concrete quantizers are constructed for achieving the minimax optimality in practical use. Simulation results and a real data analysis are provided to demonstrate the validity and effectiveness of the proposed test. Our work bridges the classical nonparametric inference to modern lossy data setting.	1,0,1,1,0,0
Robust Cooperative Manipulation without Force/Torque Measurements: Control Design and Experiments	This paper presents two novel control methodologies for the cooperative manipulation of an object by N robotic agents. Firstly, we design an adaptive control protocol which employs quaternion feedback for the object orientation to avoid potential representation singularities. Secondly, we propose a control protocol that guarantees predefined transient and steady-state performance for the object trajectory. Both methodologies are decentralized, since the agents calculate their own signals without communicating with each other, as well as robust to external disturbances and model uncertainties. Moreover, we consider that the grasping points are rigid, and avoid the need for force/torque measurements. Load distribution is also included via a grasp matrix pseudo-inverse to account for potential differences in the agents' power capabilities. Finally, simulation and experimental results with two robotic arms verify the theoretical findings.	1,0,0,0,0,0
On van Kampen-Flores, Conway-Gordon-Sachs and Radon theorems	We exhibit relations between van Kampen-Flores, Conway-Gordon-Sachs and Radon theorems, by presenting direct proofs of some implications between them. The key idea is an interesting relation between the van Kampen and the Conway-Gordon-Sachs numbers for restrictions of a map of $(d+2)$-simplex to $\mathbb R^d$ to the $(d+1)$-face and to the $[d/2]$-skeleton.	1,0,1,0,0,0
Sorting sums of binary decision summands	A sum where each of the $N$ summands can be independently chosen from two choices yields $2^N$ possible summation outcomes. There is an $\mathcal{O}(K^2)$-algorithm that finds the $K$ smallest/largest of these sums by evading the enumeration of all sums.	1,0,0,0,0,0
Effective One-Dimensional Coupling in the Highly-Frustrated Square-Lattice Itinerant Magnet CaCo$_{\mathrm{2}-y}$As$_{2}$	Inelastic neutron scattering measurements on the itinerant antiferromagnet (AFM) CaCo$_{\mathrm{2}-y}$As$_{2}$ at a temperature of 8 K reveal two orthogonal planes of scattering perpendicular to the Co square lattice in reciprocal space, demonstrating the presence of effective one-dimensional spin interactions. These results are shown to arise from near-perfect bond frustration within the $J_1$-$J_2$ Heisenberg model on a square lattice with ferromagnetic $J_1$, and hence indicate that the extensive previous experimental and theoretical study of the $J_1$-$J_2$ Heisenberg model on local-moment square spin lattices should be expanded to include itinerant spin systems.	0,1,0,0,0,0
Attacking Binarized Neural Networks	Neural networks with low-precision weights and activations offer compelling efficiency advantages over their full-precision equivalents. The two most frequently discussed benefits of quantization are reduced memory consumption, and a faster forward pass when implemented with efficient bitwise operations. We propose a third benefit of very low-precision neural networks: improved robustness against some adversarial attacks, and in the worst case, performance that is on par with full-precision models. We focus on the very low-precision case where weights and activations are both quantized to $\pm$1, and note that stochastically quantizing weights in just one layer can sharply reduce the impact of iterative attacks. We observe that non-scaled binary neural networks exhibit a similar effect to the original defensive distillation procedure that led to gradient masking, and a false notion of security. We address this by conducting both black-box and white-box experiments with binary models that do not artificially mask gradients.	1,0,0,1,0,0
Nonlinear Acceleration of Stochastic Algorithms	Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here, we study extrapolation methods in a stochastic setting, where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary, potentially biased perturbations, then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally, we apply this acceleration technique to stochastic algorithms such as SGD, SAGA, SVRG and Katyusha in different settings, and show significant performance gains.	0,0,1,0,0,0
A copula approach for dependence modeling in multivariate nonparametric time series	This paper is concerned with modeling the dependence structure of two (or more) time-series in the presence of a (possible multivariate) covariate which may include past values of the time series. We assume that the covariate influences only the conditional mean and the conditional variance of each of the time series but the distribution of the standardized innovations is not influenced by the covariate and is stable in time. The joint distribution of the time series is then determined by the conditional means, the conditional variances and the marginal distributions of the innovations, which we estimate nonparametrically, and the copula of the innovations, which represents the dependency structure. We consider a nonparametric as well as a semiparametric estimator based on the estimated residuals. We show that under suitable assumptions these copula estimators are asymptotically equivalent to estimators that would be based on the unobserved innovations. The theoretical results are illustrated by simulations and a real data example.	0,0,1,1,0,0
Thickness-dependent electronic and magnetic properties of $γ'$-Fe$_{\mathrm 4}$N atomic layers on Cu(001)	Growth, electronic and magnetic properties of $\gamma'$-Fe$_{4}$N atomic layers on Cu(001) are studied by scanning tunneling microscopy/spectroscopy and x-ray absorption spectroscopy/magnetic circular dichroism. A continuous film of ordered trilayer $\gamma'$-Fe$_{4}$N is obtained by Fe deposition under N$_{2}$ atmosphere onto monolayer Fe$_{2}$N/Cu(001), while the repetition of a bombardment with 0.5 keV N$^{+}$ ions during growth cycles results in imperfect bilayer $\gamma'$-Fe$_{4}$N. The increase in the sample thickness causes the change of the surface electronic structure, as well as the enhancement in the spin magnetic moment of Fe atoms reaching $\sim$ 1.4 $\mu_{\mathrm B}$/atom in the trilayer sample. The observed thickness-dependent properties of the system are well interpreted by layer-resolved density of states calculated using first principles, which demonstrates the strongly layer-dependent electronic states within each surface, subsurface, and interfacial plane of the $\gamma'$-Fe$_{4}$N atomic layers on Cu(001).	0,1,0,0,0,0
Experimental evidence for Glycolaldehyde and Ethylene Glycol formation by surface hydrogenation of CO molecules under dense molecular cloud conditions	This study focuses on the formation of two molecules of astrobiological importance - glycolaldehyde (HC(O)CH2OH) and ethylene glycol (H2C(OH)CH2OH) - by surface hydrogenation of CO molecules. Our experiments aim at simulating the CO freeze-out stage in interstellar dark cloud regions, well before thermal and energetic processing become dominant. It is shown that along with the formation of H2CO and CH3OH - two well established products of CO hydrogenation - also molecules with more than one carbon atom form. The key step in this process is believed to be the recombination of two HCO radicals followed by the formation of a C-C bond. The experimentally established reaction pathways are implemented into a continuous-time random-walk Monte Carlo model, previously used to model the formation of CH3OH on astrochemical time-scales, to study their impact on the solid-state abundances in dense interstellar clouds of glycolaldehyde and ethylene glycol.	0,1,0,0,0,0
First Discoveries of z>6 Quasars with the DECam Legacy Survey and UKIRT Hemisphere Survey	We present the first discoveries from a survey of $z\gtrsim6$ quasars using imaging data from the DECam Legacy Survey (DECaLS) in the optical, the UKIRT Deep Infrared Sky Survey (UKIDSS) and a preliminary version of the UKIRT Hemisphere Survey (UHS) in the near-IR, and ALLWISE in the mid-IR. DECaLS will image 9000 deg$^2$ of sky down to $z_{\rm AB}\sim23.0$, and UKIDSS and UHS, which will map the northern sky at $0<DEC<+60^{\circ}$, reaching $J_{\rm VEGA}\sim19.6$ (5-$\sigma$). The combination of these datasets allows us to discover quasars at redshift $z\gtrsim7$ and to conduct a complete census of the faint quasar population at $z\gtrsim6$. In this paper, we report on the selection method of our search, and on the initial discoveries of two new, faint $z\gtrsim6$ quasars and one new $z=6.63$ quasar in our pilot spectroscopic observations. The two new $z\sim6$ quasars are at $z=6.07$ and $z=6.17$ with absolute magnitudes at rest-frame wavelength 1450 \AA\ being $M_{1450}=-25.83$ and $M_{1450}=-25.76$, respectively. These discoveries suggest that we can find quasars close to or fainter than the break magnitude of the Quasar Luminosity Function (QLF) at $z\gtrsim6$. The new $z=6.63$ quasar has an absolute magnitude of $M_{1450}=-25.95$. This demonstrates the potential of using the combined DECaLS and UKIDSS/UHS datasets to find $z\gtrsim7$ quasars. Extrapolating from previous QLF measurements, we predict that these combined datasets will yield $\sim200$ $z\sim6$ quasars to $z_{\rm AB} < 21.5$, $\sim1{,}000$ $z\sim6$ quasars to $z_{\rm AB}<23$, and $\sim 30$ quasars at $z>6.5$ to $J_{\rm VEGA}<19.5$.	0,1,0,0,0,0
Learning End-to-end Autonomous Driving using Guided Auxiliary Supervision	Learning to drive faithfully in highly stochastic urban settings remains an open problem. To that end, we propose a Multi-task Learning from Demonstration (MT-LfD) framework which uses supervised auxiliary task prediction to guide the main task of predicting the driving commands. Our framework involves an end-to-end trainable network for imitating the expert demonstrator's driving commands. The network intermediately predicts visual affordances and action primitives through direct supervision which provide the aforementioned auxiliary supervised guidance. We demonstrate that such joint learning and supervised guidance facilitates hierarchical task decomposition, assisting the agent to learn faster, achieve better driving performance and increases transparency of the otherwise black-box end-to-end network. We run our experiments to validate the MT-LfD framework in CARLA, an open-source urban driving simulator. We introduce multiple non-player agents in CARLA and induce temporal noise in them for realistic stochasticity.	1,0,0,1,0,0
Canonical correlation coefficients of high-dimensional Gaussian vectors: finite rank case	Consider a Gaussian vector $\mathbf{z}=(\mathbf{x}',\mathbf{y}')'$, consisting of two sub-vectors $\mathbf{x}$ and $\mathbf{y}$ with dimensions $p$ and $q$ respectively, where both $p$ and $q$ are proportional to the sample size $n$. Denote by $\Sigma_{\mathbf{u}\mathbf{v}}$ the population cross-covariance matrix of random vectors $\mathbf{u}$ and $\mathbf{v}$, and denote by $S_{\mathbf{u}\mathbf{v}}$ the sample counterpart. The canonical correlation coefficients between $\mathbf{x}$ and $\mathbf{y}$ are known as the square roots of the nonzero eigenvalues of the canonical correlation matrix $\Sigma_{\mathbf{x}\mathbf{x}}^{-1}\Sigma_{\mathbf{x}\mathbf{y}}\Sigma_{\mathbf{y}\mathbf{y}}^{-1}\Sigma_{\mathbf{y}\mathbf{x}}$. In this paper, we focus on the case that $\Sigma_{\mathbf{x}\mathbf{y}}$ is of finite rank $k$, i.e. there are $k$ nonzero canonical correlation coefficients, whose squares are denoted by $r_1\geq\cdots\geq r_k>0$. We study the sample counterparts of $r_i,i=1,\ldots,k$, i.e. the largest $k$ eigenvalues of the sample canonical correlation matrix $§_{\mathbf{x}\mathbf{x}}^{-1}§_{\mathbf{x}\mathbf{y}}§_{\mathbf{y}\mathbf{y}}^{-1}§_{\mathbf{y}\mathbf{x}}$, denoted by $\lambda_1\geq\cdots\geq \lambda_k$. We show that there exists a threshold $r_c\in(0,1)$, such that for each $i\in\{1,\ldots,k\}$, when $r_i\leq r_c$, $\lambda_i$ converges almost surely to the right edge of the limiting spectral distribution of the sample canonical correlation matrix, denoted by $d_{+}$. When $r_i>r_c$, $\lambda_i$ possesses an almost sure limit in $(d_{+},1]$. We also obtain the limiting distribution of $\lambda_i$'s under appropriate normalization. Specifically, $\lambda_i$ possesses Gaussian type fluctuation if $r_i>r_c$, and follows Tracy-Widom distribution if $r_i<r_c$. Some applications of our results are also discussed.	0,0,1,1,0,0
ForestClaw: A parallel algorithm for patch-based adaptive mesh refinement on a forest of quadtrees	We describe a parallel, adaptive, multi-block algorithm for explicit integration of time dependent partial differential equations on two-dimensional Cartesian grids. The grid layout we consider consists of a nested hierarchy of fixed size, non-overlapping, logically Cartesian grids stored as leaves in a quadtree. Dynamic grid refinement and parallel partitioning of the grids is done through the use of the highly scalable quadtree/octree library p4est. Because our concept is multi-block, we are able to easily solve on a variety of geometries including the cubed sphere. In this paper, we pay special attention to providing details of the parallel ghost-filling algorithm needed to ensure that both corner and edge ghost regions around each grid hold valid values. We have implemented this algorithm in the ForestClaw code using single-grid solvers from ClawPack, a software package for solving hyperbolic PDEs using finite volumes methods. We show weak and strong scalability results for scalar advection problems on two-dimensional manifold domains on 1 to 64Ki MPI processes, demonstrating neglible regridding overhead.	1,0,0,0,0,0
Stability and Grothendieck	This note is a commentary on the model-theoretic interpretation of Grothendieck's double limit characterization of weak relative compactness.	0,0,1,0,0,0
Decomposition theorems for asymptotic property C and property A	We combine aspects of the notions of finite decomposition complexity and asymptotic property C into a notion that we call finite APC-decomposition complexity. Any space with finite decomposition complexity has finite APC-decomposition complexity and any space with asymptotic property C has finite APC-decomposition complexity. Moreover, finite APC-decomposition complexity implies property A for metric spaces. We also show that finite APC-decomposition complexity is preserved by direct products of groups and spaces, amalgamated products of groups, and group extensions, among other constructions.	0,0,1,0,0,0
Planning with Multiple Biases	Recent work has considered theoretical models for the behavior of agents with specific behavioral biases: rather than making decisions that optimize a given payoff function, the agent behaves inefficiently because its decisions suffer from an underlying bias. These approaches have generally considered an agent who experiences a single behavioral bias, studying the effect of this bias on the outcome. In general, however, decision-making can and will be affected by multiple biases operating at the same time. How do multiple biases interact to produce the overall outcome? Here we consider decisions in the presence of a pair of biases exhibiting an intuitively natural interaction: present bias -- the tendency to value costs incurred in the present too highly -- and sunk-cost bias -- the tendency to incorporate costs experienced in the past into one's plans for the future. We propose a theoretical model for planning with this pair of biases, and we show how certain natural behavioral phenomena can arise in our model only when agents exhibit both biases. As part of our model we differentiate between agents that are aware of their biases (sophisticated) and agents that are unaware of them (naive). Interestingly, we show that the interaction between the two biases is quite complex: in some cases, they mitigate each other's effects while in other cases they might amplify each other. We obtain a number of further results as well, including the fact that the planning problem in our model for an agent experiencing and aware of both biases is computationally hard in general, though tractable under more relaxed assumptions.	1,1,0,0,0,0
QCD-Aware Recursive Neural Networks for Jet Physics	Recent progress in applying machine learning for jet physics has been built upon an analogy between calorimeters and images. In this work, we present a novel class of recursive neural networks built instead upon an analogy between QCD and natural languages. In the analogy, four-momenta are like words and the clustering history of sequential recombination jet algorithms is like the parsing of a sentence. Our approach works directly with the four-momenta of a variable-length set of particles, and the jet-based tree structure varies on an event-by-event basis. Our experiments highlight the flexibility of our method for building task-specific jet embeddings and show that recursive architectures are significantly more accurate and data efficient than previous image-based networks. We extend the analogy from individual jets (sentences) to full events (paragraphs), and show for the first time an event-level classifier operating on all the stable particles produced in an LHC event.	0,1,0,1,0,0
Temporal Justification Logic	Justification logics are modal-like logics with the additional capability of recording the reason, or justification, for modalities in syntactic structures, called justification terms. Justification logics can be seen as explicit counterparts to modal logics. The behavior and interaction of agents in distributed system is often modeled using logics of knowledge and time. In this paper, we sketch some preliminary ideas on how the modal knowledge part of such logics of knowledge and time could be replaced with an appropriate justification logic.	1,0,0,0,0,0
Safe Semi-Supervised Learning of Sum-Product Networks	In several domains obtaining class annotations is expensive while at the same time unlabelled data are abundant. While most semi-supervised approaches enforce restrictive assumptions on the data distribution, recent work has managed to learn semi-supervised models in a non-restrictive regime. However, so far such approaches have only been proposed for linear models. In this work, we introduce semi-supervised parameter learning for Sum-Product Networks (SPNs). SPNs are deep probabilistic models admitting inference in linear time in number of network edges. Our approach has several advantages, as it (1) allows generative and discriminative semi-supervised learning, (2) guarantees that adding unlabelled data can increase, but not degrade, the performance (safe), and (3) is computationally efficient and does not enforce restrictive assumptions on the data distribution. We show on a variety of data sets that safe semi-supervised learning with SPNs is competitive compared to state-of-the-art and can lead to a better generative and discriminative objective value than a purely supervised approach.	1,0,0,1,0,0
The impact of neutral impurity concentration on charge drift mobility in n-type germanium	The impact of neutral impurity scattering of electrons on the charge drift mobility in high purity n-type germanium crystals at 77 Kelvin is investigated. We calculated the contributions from ionized impurity scattering, lattice scattering, and neutral impurity scattering to the total charge drift mobility using theoretical models. The experimental data such as charge carrier concentration, mobility and resistivity are measured by Hall Effect system at 77 Kelvin. The neutral impurity concentration is derived from the Matthiessen's rule using the measured Hall mobility and ionized impurity concentration. The radial distribution of the neutral impurity concentration in the self-grown crystals is determined. Consequently, we demonstrated that neutral impurity scattering is a significant contribution to the charge drift mobility, which has a dependence on the concentration of neutral impurities in high purity n-type germanium crystal.	0,1,0,0,0,0
Explicit three dimensional topology optimization via Moving Morphable Void (MMV) approach	Three dimensional (3D) topology optimization problems always involve huge numbers of Degrees of Freedom (DOFs) in finite element analysis (FEA) and design variables in numerical optimization, respectively. This will inevitably lead to large computational efforts in the solution process. In the present paper, an efficient and explicit topology optimization approach which can reduce not only the number of design variables but also the number of degrees of freedom in FEA is proposed based on the Moving Morphable Voids (MMVs) solution framework. This is achieved by introducing a set of geometry parameters (e.g., control points of B-spline surfaces) to describe the boundary of a structure explicitly and removing the unnecessary DOFs from the FE model at every step of numerical optimization. Numerical examples demonstrate that the proposed approach does can overcome the bottleneck problems associated with a 3D topology optimization problem in a straightforward way and enhance the solution efficiency significantly.	0,1,0,0,0,0
Room temperature line lists for CO\2 symmetric isotopologues with \textit{ab initio} computed intensities	Remote sensing experiments require high-accuracy, preferably sub-percent, line intensities and in response to this need we present computed room temperature line lists for six symmetric isotopologues of carbon dioxide: $^{13}$C$^{16}$O$_2$, $^{14}$C$^{16}$O$_2$, $^{12}$C$^{17}$O$_2$, $^{12}$C$^{18}$O$_2$, $^{13}$C$^{17}$O$_2$ and $^{13}$C$^{18}$O$_2$, covering the range 0-8000 \cm. Our calculation scheme is based on variational nuclear motion calculations and on a reliability analysis of the generated line intensities. Rotation-vibration wavefunctions and energy levels are computed using the DVR3D software suite and a high quality semi-empirical potential energy surface (PES), followed by computation of intensities using an \abinitio\ dipole moment surface (DMS). Four line lists are computed for each isotopologue to quantify sensitivity to minor distortions of the PES/DMS. Reliable lines are benchmarked against recent state-of-the-art measurements and against the HITRAN2012 database, supporting the claim that the majority of line intensities for strong bands are predicted with sub-percent accuracy. Accurate line positions are generated using an effective Hamiltonian. We recommend the use of these line lists for future remote sensing studies and their inclusion in databases.	0,1,0,0,0,0
Tidal synchronization of an anelastic multi-layered body: Titan's synchronous rotation	This paper presents one analytical tidal theory for a viscoelastic multi-layered body with an arbitrary number of homogeneous layers. Starting with the static equilibrium figure, modified to include tide and differential rotation, and using the Newtonian creep approach, we find the dynamical equilibrium figure of the deformed body, which allows us to calculate the tidal potential and the forces acting on the tide generating body, as well as the rotation and orbital elements variations. In the particular case of the two-layer model, we study the tidal synchronization when the gravitational coupling and the friction in the interface between the layers is added. For high relaxation factors (low viscosity), the stationary solution of each layer is synchronous with the orbital mean motion (n) when the orbit is circular, but the spin rates increase if the orbital eccentricity increases. For low relaxation factors (high viscosity), as in planetary satellites, if friction remains low, each layer can be trapped in different spin-orbit resonances with frequencies n/2,n,3n/2,... . We apply the theory to Titan. The main results are: i) the rotational constraint does not allow us confirm or reject the existence of a subsurface ocean in Titan; and ii) the crust-atmosphere exchange of angular momentum can be neglected. Using the rotation estimate based on Cassini's observation, we limit the possible value of the shell relaxation factor, when a subsurface ocean is assumed, to 10^-9 Hz, which correspond to a shell's viscosity 10^18 Pa s, depending on the ocean's thickness and viscosity values. In the case in which the ocean does not exist, the maximum shell relaxation factor is one order of magnitude smaller and the corresponding minimum shell's viscosity is one order higher.	0,1,0,0,0,0
Sampling and Reconstruction of Graph Signals via Weak Submodularity and Semidefinite Relaxation	We study the problem of sampling a bandlimited graph signal in the presence of noise, where the objective is to select a node subset of prescribed cardinality that minimizes the signal reconstruction mean squared error (MSE). To that end, we formulate the task at hand as the minimization of MSE subject to binary constraints, and approximate the resulting NP-hard problem via semidefinite programming (SDP) relaxation. Moreover, we provide an alternative formulation based on maximizing a monotone weak submodular function and propose a randomized-greedy algorithm to find a sub-optimal subset. We then derive a worst-case performance guarantee on the MSE returned by the randomized greedy algorithm for general non-stationary graph signals. The efficacy of the proposed methods is illustrated through numerical simulations on synthetic and real-world graphs. Notably, the randomized greedy algorithm yields an order-of-magnitude speedup over state-of-the-art greedy sampling schemes, while incurring only a marginal MSE performance loss.	1,0,0,1,0,0
Certifying coloring algorithms for graphs without long induced paths	Let $P_k$ be a path, $C_k$ a cycle on $k$ vertices, and $K_{k,k}$ a complete bipartite graph with $k$ vertices on each side of the bipartition. We prove that (1) for any integers $k, t>0$ and a graph $H$ there are finitely many subgraph minimal graphs with no induced $P_k$ and $K_{t,t}$ that are not $H$-colorable and (2) for any integer $k>4$ there are finitely many subgraph minimal graphs with no induced $P_k$ that are not $C_{k-2}$-colorable. The former generalizes the result of Hell and Huang [Complexity of coloring graphs without paths and cycles, Discrete Appl. Math. 216: 211--232 (2017)] and the latter extends a result of Bruce, Hoang, and Sawada [A certifying algorithm for 3-colorability of $P_5$-Free Graphs, ISAAC 2009: 594--604]. Both our results lead to polynomial-time certifying algorithms for the corresponding coloring problems.	1,0,0,0,0,0
Ergodicity analysis and antithetic integral control of a class of stochastic reaction networks with delays	Delays are an important phenomenon arising in a wide variety of real world systems. They occur in biological models because of diffusion effects or as simplifying modeling elements. We propose here to consider delayed stochastic reaction networks. The difficulty here lies in the fact that the state-space of a delayed reaction network is infinite-dimensional, which makes their analysis more involved. We demonstrate here that a particular class of stochastic time-varying delays, namely those that follow a phase-type distribution, can be exactly implemented in terms of a chemical reaction network. Hence, any delay-free network can be augmented to incorporate those delays through the addition of delay-species and delay-reactions. Hence, for this class of stochastic delays, which can be used to approximate any delay distribution arbitrarily accurately, the state-space remains finite-dimensional and, therefore, standard tools developed for standard reaction network still apply. In particular, we demonstrate that for unimolecular mass-action reaction networks that the delayed stochastic reaction network is ergodic if and only if the non-delayed network is ergodic as well. Bimolecular reactions are more difficult to consider but an analogous result is also obtained. These results tell us that delays that are phase-type distributed, regardless of their distribution, are not harmful to the ergodicity property of reaction networks. We also prove that the presence of those delays adds convolution terms in the moment equation but does not change the value of the stationary means compared to the delay-free case. Finally, the control of a certain class of delayed stochastic reaction network using a delayed antithetic integral controller is considered. It is proven that this controller achieves its goal provided that the delay-free network satisfy the conditions of ergodicity and output-controllability.	0,0,0,0,1,0
Temperley-Lieb and Birman-Murakami-Wenzl like relations from multiplicity free semi-simple tensor system	In this article we consider conditions under which projection operators in multiplicity free semi-simple tensor categories satisfy Temperley-Lieb like relations. This is then used as a stepping stone to prove sufficient conditions for obtaining a representation of the Birman-Murakami-Wenzl algebra from a braided multiplicity free semi-simple tensor category. The results are found by utalising the data of the categories. There is considerable overlap with the results found in arXiv:1607.08908, where proofs are shown by manipulating diagrams.	0,0,1,0,0,0
Posterior contraction rates for support boundary recovery	Given a sample of a Poisson point process with intensity $\lambda_f(x,y) = n \mathbf{1}(f(x) \leq y),$ we study recovery of the boundary function $f$ from a nonparametric Bayes perspective. Because of the irregularity of this model, the analysis is non-standard. We establish a general result for the posterior contraction rate with respect to the $L^1$-norm based on entropy and one-sided small probability bounds. From this, specific posterior contraction results are derived for Gaussian process priors and priors based on random wavelet series.	0,0,1,1,0,0
Automatic sequences and generalised polynomials	We conjecture that bounded generalised polynomial functions cannot be generated by finite automata, except for the trivial case when they are ultimately periodic. Using methods from ergodic theory, we are able to partially resolve this conjecture, proving that any hypothetical counterexample is periodic away from a very sparse and structured set. In particular, we show that for a polynomial $p(n)$ with at least one irrational coefficient (except for the constant one) and integer $m\geq 2$, the sequence $\lfloor p(n) \rfloor \bmod{m}$ is never automatic. We also prove that the conjecture is equivalent to the claim that the set of powers of an integer $k\geq 2$ is not given by a generalised polynomial.	1,0,1,0,0,0
Quasimomentum of an elementary excitation for a system of point bosons with zero boundary conditions	As is known, an elementary excitation of a many-particle system with boundaries is not characterized by a definite momentum. Here, we obtain the formula for the quasimomentum of an elementary excitation for a one-dimensional system of $N$ spinless point bosons with zero boundary conditions (BCs). We also find that the dispersion law $E(p)$ of the system with zero BCs coincides with that of a system with periodic BCs. The elementary excitations are defined within a new approach proposed earlier by the author. This approach is mathematically equivalent to the traditional approach by Lieb, but differs from it by a simpler way of enumeration of excited states and leads to a single dispersion law (instead of two ones in the Lieb's approach).	0,1,0,0,0,0
Separator Reconnection at Earth's Dayside Magnetopause: MMS Observations Compared to Global Simulations	We compare a global high resolution resistive magnetohydrodynamics (MHD) simulation of Earth's magnetosphere with observations from the Magnetospheric Multiscale (MMS) constellation for a southward IMF magnetopause crossing during October 16, 2015 that was previously identified as an electron diffusion region (EDR) event. The simulation predicts a complex time-dependent magnetic topology consisting of multiple separators and flux ropes. Despite the topological complexity, the predicted distance between MMS and the primary separator is less than 0.5 Earth radii. These results suggest that global magnetic topology, rather than local magnetic geometry alone, determines the location of the electron diffusion region at the dayside magnetopause.	0,1,0,0,0,0
Bayesian Recurrent Neural Networks	In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks. Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\%. Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks. We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.	1,0,0,1,0,0
Improved Accounting for Differentially Private Learning	We consider the problem of differential privacy accounting, i.e. estimation of privacy loss bounds, in machine learning in a broad sense. We propose two versions of a generic privacy accountant suitable for a wide range of learning algorithms. Both versions are derived in a simple and principled way using well-known tools from probability theory, such as concentration inequalities. We demonstrate that our privacy accountant is able to achieve state-of-the-art estimates of DP guarantees and can be applied to new areas like variational inference. Moreover, we show that the latter enjoys differential privacy at minor cost.	1,0,0,1,0,0
A Syllable-based Technique for Word Embeddings of Korean Words	Word embedding has become a fundamental component to many NLP tasks such as named entity recognition and machine translation. However, popular models that learn such embeddings are unaware of the morphology of words, so it is not directly applicable to highly agglutinative languages such as Korean. We propose a syllable-based learning model for Korean using a convolutional neural network, in which word representation is composed of trained syllable vectors. Our model successfully produces morphologically meaningful representation of Korean words compared to the original Skip-gram embeddings. The results also show that it is quite robust to the Out-of-Vocabulary problem.	1,0,0,0,0,0
On the higher Cheeger problem	We develop the notion of higher Cheeger constants for a measurable set $\Omega \subset \mathbb{R}^N$. By the $k$-th Cheeger constant we mean the value \[h_k(\Omega) = \inf \max \{h_1(E_1), \dots, h_1(E_k)\},\] where the infimum is taken over all $k$-tuples of mutually disjoint subsets of $\Omega$, and $h_1(E_i)$ is the classical Cheeger constant of $E_i$. We prove the existence of minimizers satisfying additional "adjustment" conditions and study their properties. A relation between $h_k(\Omega)$ and spectral minimal $k$-partitions of $\Omega$ associated with the first eigenvalues of the $p$-Laplacian under homogeneous Dirichlet boundary conditions is stated. The results are applied to determine the second Cheeger constant of some planar domains.	0,0,1,0,0,0
Synthesis and electronic properties of Ruddlesden-Popper strontium iridate epitaxial thin films stabilized by control of growth kinetics	We report on the selective fabrication of high-quality Sr$_2$IrO$_4$ and SrIrO$_3$ epitaxial thin films from a single polycrystalline Sr$_2$IrO$_4$ target by pulsed laser deposition. Using a combination of X-ray diffraction and photoemission spectroscopy characterizations, we discover that within a relatively narrow range of substrate temperature, the oxygen partial pressure plays a critical role in the cation stoichiometric ratio of the films, and triggers the stabilization of different Ruddlesden-Popper (RP) phases. Resonant X-ray absorption spectroscopy measurements taken at the Ir $L$-edge and the O $K$-edge demonstrate the presence of strong spin-orbit coupling, and reveal the electronic and orbital structures of both compounds. These results suggest that in addition to the conventional thermodynamics consideration, higher members of the Sr$_{n+1}$Ir$_n$O$_{3n+1}$ series can possibly be achieved by kinetic control away from the thermodynamic limit. These findings offer a new approach to the synthesis of ultra-thin films of the RP series of iridates and can be extended to other complex oxides with layered structure.	0,1,0,0,0,0
Nonconvection and uniqueness in Navier-Stokes equation	In the presence of a certain class of functions we show that there exists a smooth solution to Navier-Stokes equation. This solution entertains the property of being nonconvective. We introduce a definition for any possible solution to the problem with minimum assumptions on the existence and the regularity of such solution. Then we prove that the proposed class of functions represents the unique solution to the problem and consequently we conclude that there exists no convective solutions to the problem in the sense of the given definition.	0,0,1,0,0,0
Spectral sets for numerical range	We define and study a numerical-range analogue of the notion of spectral set. Among the results obtained are a positivity criterion and a dilation theorem, analogous to those already known for spectral sets. An important difference from the classical definition is the role played in the new definition by the base point. We present some examples to illustrate this aspect.	0,0,1,0,0,0
DLBI: Deep learning guided Bayesian inference for structure reconstruction of super-resolution fluorescence microscopy	Super-resolution fluorescence microscopy, with a resolution beyond the diffraction limit of light, has become an indispensable tool to directly visualize biological structures in living cells at a nanometer-scale resolution. Despite advances in high-density super-resolution fluorescent techniques, existing methods still have bottlenecks, including extremely long execution time, artificial thinning and thickening of structures, and lack of ability to capture latent structures. Here we propose a novel deep learning guided Bayesian inference approach, DLBI, for the time-series analysis of high-density fluorescent images. Our method combines the strength of deep learning and statistical inference, where deep learning captures the underlying distribution of the fluorophores that are consistent with the observed time-series fluorescent images by exploring local features and correlation along time-axis, and statistical inference further refines the ultrastructure extracted by deep learning and endues physical meaning to the final image. Comprehensive experimental results on both real and simulated datasets demonstrate that our method provides more accurate and realistic local patch and large-field reconstruction than the state-of-the-art method, the 3B analysis, while our method is more than two orders of magnitude faster. The main program is available at this https URL	0,0,0,1,0,0
A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics	Deep Neural Networks are increasingly being used in a variety of machine learning applications applied to user data on the cloud. However, this approach introduces a number of privacy and efficiency challenges, as the cloud operator can perform secondary inferences on the available data. Recently, advances in edge processing have paved the way for more efficient, and private, data processing at the source for simple tasks and lighter models, though they remain a challenge for larger, and more complicated models. In this paper, we present a hybrid approach for breaking down large, complex deep models for cooperative, privacy-preserving analytics. We do this by breaking down the popular deep architectures and fine-tune them in a suitable way. We then evaluate the privacy benefits of this approach based on the information exposed to the cloud service. We also assess the local inference cost of different layers on a modern handset for mobile applications. Our evaluations show that by using certain kind of fine-tuning and embedding techniques and at a small processing cost, we can greatly reduce the level of information available to unintended tasks applied to the data features on the cloud, and hence achieving the desired tradeoff between privacy and performance.	1,0,0,0,0,0
The agreement distance of rooted phylogenetic networks	The minimal number of rooted subtree prune and regraft (rSPR) operations needed to transform one phylogenetic tree into another one induces a metric on phylogenetic trees - the rSPR-distance. The rSPR-distance between two phylogenetic trees $T$ and $T'$ can be characterised by a maximum agreement forest; a forest with a minimal number of components that covers both $T$ and $T'$. The rSPR operation has recently been generalised to phylogenetic networks with, among others, the subnetwork prune and regraft (SNPR) operation. Here, we introduce maximum agreement graphs as an explicit representations of differences of two phylogenetic networks, thus generalising maximum agreement forests. We show that maximum agreement graphs induce a metric on phylogenetic networks - the agreement distance. While this metric does not characterise the distances induced by SNPR and other generalisations of rSPR, we prove that it still bounds these distances with constant factors.	0,0,0,0,1,0
Laser annealing heals radiation damage in avalanche photodiodes	Avalanche photodiodes (APDs) are a practical option for space-based quantum communications requiring single-photon detection. However, radiation damage to APDs significantly increases their dark count rates and reduces their useful lifetimes in orbit. We show that high-power laser annealing of irradiated APDs of three different models (Excelitas C30902SH, Excelitas SLiK, and Laser Components SAP500S2) heals the radiation damage and substantially restores low dark count rates. Of nine samples, the maximum dark count rate reduction factor varies between 5.3 and 758 when operating at minus 80 degrees Celsius. The illumination power to reach these reduction factors ranges from 0.8 to 1.6 W. Other photon detection characteristics, such as photon detection efficiency, timing jitter, and afterpulsing probability, remain mostly unaffected. These results herald a promising method to extend the lifetime of a quantum satellite equipped with APDs.	0,1,0,0,0,0
Simulation Methods for Stochastic Storage Problems: A Statistical Learning Perspective	We consider solution of stochastic storage problems through regression Monte Carlo (RMC) methods. Taking a statistical learning perspective, we develop the dynamic emulation algorithm (DEA) that unifies the different existing approaches in a single modular template. We then investigate the two central aspects of regression architecture and experimental design that constitute DEA. For the regression piece, we discuss various non-parametric approaches, in particular introducing the use of Gaussian process regression in the context of stochastic storage. For simulation design, we compare the performance of traditional design (grid discretization), against space-filling, and several adaptive alternatives. The overall DEA template is illustrated with multiple examples drawing from natural gas storage valuation and optimal control of back-up generator in a microgrid.	0,0,0,0,0,1
A Compressive Sensing Approach to Community Detection with Applications	The community detection problem for graphs asks one to partition the n vertices V of a graph G into k communities, or clusters, such that there are many intracluster edges and few intercluster edges. Of course this is equivalent to finding a permutation matrix P such that, if A denotes the adjacency matrix of G, then PAP^T is approximately block diagonal. As there are k^n possible partitions of n vertices into k subsets, directly determining the optimal clustering is clearly infeasible. Instead one seeks to solve a more tractable approximation to the clustering problem. In this paper we reformulate the community detection problem via sparse solution of a linear system associated with the Laplacian of a graph G and then develop a two-stage approach based on a thresholding technique and a compressive sensing algorithm to find a sparse solution which corresponds to the community containing a vertex of interest in G. Crucially, our approach results in an algorithm which is able to find a single cluster of size n_0 in O(nlog(n)n_0) operations and all k clusters in fewer than O(n^2ln(n)) operations. This is a marked improvement over the classic spectral clustering algorithm, which is unable to find a single cluster at a time and takes approximately O(n^3) operations to find all k clusters. Moreover, we are able to provide robust guarantees of success for the case where G is drawn at random from the Stochastic Block Model, a popular model for graphs with clusters. Extensive numerical results are also provided, showing the efficacy of our algorithm on both synthetic and real-world data sets.	1,0,0,1,0,0
Extending holomorphic motions and monodromy	Let $E$ be a closed set in the Riemann sphere $\widehat{\mathbb{C}}$. We consider a holomorphic motion $\phi$ of $E$ over a complex manifold $M$, that is, a holomorphic family of injections on $E$ parametrized by $M$. It is known that if $M$ is the unit disk $\Delta$ in the complex plane, then any holomorphic motion of $E$ over $\Delta$ can be extended to a holomorphic motion of the Riemann sphere over $\Delta$. In this paper, we consider conditions under which a holomorphic motion of $E$ over a non-simply connected Riemann surface $X$ can be extended to a holomorphic motion of $\widehat{\mathbb{C}}$ over $X$. Our main result shows that a topological condition, the triviality of the monodromy, gives a necessary and sufficient condition for a holomorphic motion of $E$ over $X$ to be extended to a holomorphic motion of $\widehat{\mathbb{C}}$ over $X$. We give topological and geometric conditions for a holomorphic motion over a Riemann surface to be extended. We also apply our result to a lifting problem for holomorphic maps to Teichmüller spaces.	0,0,1,0,0,0
Transition from Weak Wave Turbulence to Soliton-Gas	We report an experimental investigation of the effect of finite depth on the statistical properties of wave turbulence at the surface of water in the gravity-capillary range. We tune the wave dispersion and the level of nonlinearity by modifying the depth of water and the forcing respectively. We use space-time resolved profilometry to reconstruct the deformed surface of water. When decreasing the water depth, we observe a drastic transition between weak turbulence at the weakest forcing and a solitonic regime at stronger forcing. We characterize the transition between both states by studying their Fourier Spectra. We also study the efficiency of energy transfer in the weak turbulence regime. We report a loss of efficiency of angular transfer as the dispersion of the wave is reduced until the system bifurcates into the solitonic regime.	0,1,0,0,0,0
Quantum-continuum calculation of the surface states and electrical response of silicon in solution	A wide range of electrochemical reactions of practical importance occur at the interface between a semiconductor and an electrolyte. We present an embedded density-functional theory method using the recently released self-consistent continuum solvation (SCCS) approach to study these interfaces. In this model, a quantum description of the surface is incorporated into a continuum representation of the bending of the bands within the electrode. The model is applied to understand the electrical response of silicon electrodes in solution, providing microscopic insights into the low-voltage region, where surface states determine the electrification of the semiconductor electrode.	0,1,0,0,0,0
First-Principles Many-Body Investigation of Correlated Oxide Heterostructures: Few-Layer-Doped SmTiO$_3$	Correlated oxide heterostructures pose a challenging problem in condensed matter research due to their structural complexity interweaved with demanding electron states beyond the effective single-particle picture. By exploring the correlated electronic structure of SmTiO$_3$ doped with few layers of SrO, we provide an insight into the complexity of such systems. Furthermore, it is shown how the advanced combination of band theory on the level of Kohn-Sham density functional theory with explicit many-body theory on the level of dynamical mean-field theory provides an adequate tool to cope with the problem. Coexistence of band-insulating, metallic and Mott-critical electronic regions is revealed in individual heterostructures with multi-orbital manifolds. Intriguing orbital polarizations, that qualitatively vary between the metallic and the Mott layers are also encountered.	0,1,0,0,0,0
Big Data Model Simulation on a Graph Database for Surveillance in Wireless Multimedia Sensor Networks	Sensors are present in various forms all around the world such as mobile phones, surveillance cameras, smart televisions, intelligent refrigerators and blood pressure monitors. Usually, most of the sensors are a part of some other system with similar sensors that compose a network. One of such networks is composed of millions of sensors connect to the Internet which is called Internet of things (IoT). With the advances in wireless communication technologies, multimedia sensors and their networks are expected to be major components in IoT. Many studies have already been done on wireless multimedia sensor networks in diverse domains like fire detection, city surveillance, early warning systems, etc. All those applications position sensor nodes and collect their data for a long time period with real-time data flow, which is considered as big data. Big data may be structured or unstructured and needs to be stored for further processing and analyzing. Analyzing multimedia big data is a challenging task requiring a high-level modeling to efficiently extract valuable information/knowledge from data. In this study, we propose a big database model based on graph database model for handling data generated by wireless multimedia sensor networks. We introduce a simulator to generate synthetic data and store and query big data using graph model as a big database. For this purpose, we evaluate the well-known graph-based NoSQL databases, Neo4j and OrientDB, and a relational database, MySQL.We have run a number of query experiments on our implemented simulator to show that which database system(s) for surveillance in wireless multimedia sensor networks is efficient and scalable.	1,0,0,0,0,0
CSGNet: Neural Shape Parser for Constructive Solid Geometry	We present a neural architecture that takes as input a 2D or 3D shape and outputs a program that generates the shape. The instructions in our program are based on constructive solid geometry principles, i.e., a set of boolean operations on shape primitives defined recursively. Bottom-up techniques for this shape parsing task rely on primitive detection and are inherently slow since the search space over possible primitive combinations is large. In contrast, our model uses a recurrent neural network that parses the input shape in a top-down manner, which is significantly faster and yields a compact and easy-to-interpret sequence of modeling instructions. Our model is also more effective as a shape detector compared to existing state-of-the-art detection techniques. We finally demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques.	1,0,0,0,0,0
On the structure of Hausdorff moment sequences of complex matrices	The paper treats several aspects of the truncated matricial $[\alpha,\beta]$-Hausdorff type moment problems. It is shown that each $[\alpha,\beta]$-Hausdorff moment sequence has a particular intrinsic structure. More precisely, each element of this sequence varies within a closed bounded matricial interval. The case that the corresponding moment coincides with one of the endpoints of the interval plays a particular important role. This leads to distinguished molecular solutions of the truncated matricial $[\alpha,\beta]$-Hausdorff moment problem, which satisfy some extremality properties. The proofs are mainly of algebraic character. The use of the parallel sum of matrices is an essential tool in the proofs.	0,0,1,0,0,0
Legendrian ribbons and strongly quasipositive links in an open book	We show that a link in an open book can be realized as a strongly quasipositive braid if and only if it bounds a Legendrian ribbon with respect to the associated contact structure. This generalizes a result due to Baader and Ishikawa for links in the three-sphere. We highlight some related techniques for determining whether or not a link is strongly quasipositive, emphasizing applications to fibered links and satellites.	0,0,1,0,0,0
Products of topological groups in which all closed subgroups are separable	We prove that if $H$ is a topological group such that all closed subgroups of $H$ are separable, then the product $G\times H$ has the same property for every separable compact group $G$. Let $c$ be the cardinality of the continuum. Assuming $2^{\omega_1} = c$, we show that there exist: (1) pseudocompact topological abelian groups $G$ and $H$ such that all closed subgroups of $G$ and $H$ are separable, but the product $G\times H$ contains a closed non-separable $\sigma$-compact subgroup; (2) pseudocomplete locally convex vector spaces $K$ and $L$ such that all closed vector subspaces of $K$ and $L$ are separable, but the product $K\times L$ contains a closed non-separable $\sigma$-compact vector subspace.	0,0,1,0,0,0
Synthesis of Optimal Resilient Control Strategies	Repair mechanisms are important within resilient systems to maintain the system in an operational state after an error occurred. Usually, constraints on the repair mechanisms are imposed, e.g., concerning the time or resources required (such as energy consumption or other kinds of costs). For systems modeled by Markov decision processes (MDPs), we introduce the concept of resilient schedulers, which represent control strategies guaranteeing that these constraints are always met within some given probability. Assigning rewards to the operational states of the system, we then aim towards resilient schedulers which maximize the long-run average reward, i.e., the expected mean payoff. We present a pseudo-polynomial algorithm that decides whether a resilient scheduler exists and if so, yields an optimal resilient scheduler. We show also that already the decision problem asking whether there exists a resilient scheduler is PSPACE-hard.	1,0,0,0,0,0
Learning Sparse Representations in Reinforcement Learning with Sparse Coding	A variety of representation learning approaches have been investigated for reinforcement learning; much less attention, however, has been given to investigating the utility of sparse coding. Outside of reinforcement learning, sparse coding representations have been widely used, with non-convex objectives that result in discriminative representations. In this work, we develop a supervised sparse coding objective for policy evaluation. Despite the non-convexity of this objective, we prove that all local minima are global minima, making the approach amenable to simple optimization strategies. We empirically show that it is key to use a supervised objective, rather than the more straightforward unsupervised sparse coding approach. We compare the learned representations to a canonical fixed sparse representation, called tile-coding, demonstrating that the sparse coding representation outperforms a wide variety of tilecoding representations.	1,0,0,1,0,0
Degrees of Freedom in Cached MIMO Relay Networks With Multiple Base Stations	The ability of physical layer relay caching to increase the degrees of freedom (DoF) of a single cell was recently illustrated. In this paper, we extend this result to the case of multiple cells in which a caching relay is shared among multiple non-cooperative base stations (BSs). In particular, we show that a large DoF gain can be achieved by exploiting the benefits of having a shared relay that cooperates with the BSs. We first propose a cache-assisted relaying protocol that improves the cooperation opportunity between the BSs and the relay. Next, we consider the cache content placement problem that aims to design the cache content at the relay such that the DoF gain is maximized. We propose an optimal algorithm and a near-optimal low-complexity algorithm for the cache content placement problem. Simulation results show significant improvement in the DoF gain using the proposed relay-caching protocol.	1,0,0,0,0,0
Representation of I(1) and I(2) autoregressive Hilbertian processes	We extend the Granger-Johansen representation theorems for I(1) and I(2) vector autoregressive processes to accommodate processes that take values in an arbitrary complex separable Hilbert space. This more general setting is of central relevance for statistical applications involving functional time series. We first obtain a range of necessary and sufficient conditions for a pole in the inverse of a holomorphic index-zero Fredholm operator pencil to be of first or second order. Those conditions form the basis for our development of I(1) and I(2) representations of autoregressive Hilbertian processes. Cointegrating and attractor subspaces are characterized in terms of the behavior of the autoregressive operator pencil in a neighborhood of one.	0,0,1,1,0,0
Unsupervised learning of object frames by dense equivariant image labelling	One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.	1,0,0,1,0,0
Developing a Method to Determine Electrical Conductivity in Meteoritic Materials with Applications to Induction Heating Theory (2008 Student Thesis)	Magnetic induction was first proposed as a planetary heating mechanism by Sonett and Colburn in 1968, in recent years this theory has lost favor as a plausible source of heating in the early solar system. However, new models of proto-planetary disk evolution suggest that magnetic fields play an important role in solar system formation. In particular, the magneto-hydrodynamic behavior of proto-planetary disks is believed to be responsible for the net outward flow of angular momentum in the solar system. It is important to re-evaluate the plausibility of magnetic induction based on the intense magnetic field environments described by the most recent models of proto-planetary disk evolution. In order to re-evaluate electromagnetic induction theory the electrical conductivity of meteorites must be determined. To develop a technique capable of making these measurements, a time-varying magnetic field was generated to inductively heat metallic control samples. The thermal response of each sample, which depends on electrical conductivity, was monitored until a thermal steady state was achieved. The relationship between conductivity and thermal response can be exploited to estimate the electrical conductivity of unknown samples. After applying the technique to various metals it was recognized that this method is not capable of making precise electrical conductivity measurements. However, this method can constrain the product of the electrical conductivity and the square of the magnetic permeability, or ${\sigma}{{\mu}^2}$, for meteoritic and metallic samples alike. The results also illustrate that along with electrical conductivity {\sigma}, the magnetic permeability {\mu} of a substance has an important effect on induction heating phenomena for paramagnetic ({\mu}/{\mu}0 > 1) and especially ferromagnetic materials ({\mu}/{\mu}0 >> 1).	0,1,0,0,0,0
Automata-Guided Hierarchical Reinforcement Learning for Skill Composition	Skills learned through (deep) reinforcement learning often generalizes poorly across domains and re-training is necessary when presented with a new task. We present a framework that combines techniques in \textit{formal methods} with \textit{reinforcement learning} (RL). The methods we provide allows for convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards, and construct new skills from existing ones with little to no additional exploration. We evaluate the proposed methods in a simple grid world simulation as well as a more complicated kitchen environment in AI2Thor	1,0,0,0,0,0
Self-exciting Point Processes: Infections and Implementations	This is a comment on Reinhart's "Review of Self-Exciting Spatio-Temporal Point Processes and Their Applications" (arXiv:1708.02647v1). I contribute some experiences from modelling the spread of infectious diseases. Furthermore, I try to complement the review with regard to the availability of software for the described models, which I think is essential in "paving the way for new uses".	0,0,0,1,0,0
The geometry of hypothesis testing over convex cones: Generalized likelihood tests and minimax radii	We consider a compound testing problem within the Gaussian sequence model in which the null and alternative are specified by a pair of closed, convex cones. Such cone testing problem arise in various applications, including detection of treatment effects, trend detection in econometrics, signal detection in radar processing, and shape-constrained inference in non-parametric statistics. We provide a sharp characterization of the GLRT testing radius up to a universal multiplicative constant in terms of the geometric structure of the underlying convex cones. When applied to concrete examples, this result reveals some interesting phenomena that do not arise in the analogous problems of estimation under convex constraints. In particular, in contrast to estimation error, the testing error no longer depends purely on the problem complexity via a volume-based measure (such as metric entropy or Gaussian complexity), other geometric properties of the cones also play an important role. To address the issue of optimality, we prove information-theoretic lower bounds for minimax testing radius again in terms of geometric quantities. Our general theorems are illustrated by examples including the cases of monotone and orthant cones, and involve some results of independent interest.	1,0,1,1,0,0
A resource-frugal probabilistic dictionary and applications in bioinformatics	Indexing massive data sets is extremely expensive for large scale problems. In many fields, huge amounts of data are currently generated, however extracting meaningful information from voluminous data sets, such as computing similarity between elements, is far from being trivial. It remains nonetheless a fundamental need. This work proposes a probabilistic data structure based on a minimal perfect hash function for indexing large sets of keys. Our structure out-compete the hash table for construction, query times and for memory usage, in the case of the indexation of a static set. To illustrate the impact of algorithms performances, we provide two applications based on similarity computation between collections of sequences, and for which this calculation is an expensive but required operation. In particular, we show a practical case in which other bioinformatics tools fail to scale up the tested data set or provide lower recall quality results.	1,0,0,0,0,0
Photometric Stereo by Hemispherical Metric Embedding	Photometric Stereo methods seek to reconstruct the 3d shape of an object from motionless images obtained with varying illumination. Most existing methods solve a restricted problem where the physical reflectance model, such as Lambertian reflectance, is known in advance. In contrast, we do not restrict ourselves to a specific reflectance model. Instead, we offer a method that works on a wide variety of reflectances. Our approach uses a simple yet uncommonly used property of the problem - the sought after normals are points on a unit hemisphere. We present a novel embedding method that maps pixels to normals on the unit hemisphere. Our experiments demonstrate that this approach outperforms existing manifold learning methods for the task of hemisphere embedding. We further show successful reconstructions of objects from a wide variety of reflectances including smooth, rough, diffuse and specular surfaces, even in the presence of significant attached shadows. Finally, we empirically prove that under these challenging settings we obtain more accurate shape reconstructions than existing methods.	1,0,0,0,0,0
An overview of knot Floer homology	Knot Floer homology is an invariant for knots discovered by the authors and, independently, Jacob Rasmussen. The discovery of this invariant grew naturally out of studying how a certain three-manifold invariant, Heegaard Floer homology, changes as the three-manifold undergoes Dehn surgery along a knot. Since its original definition, thanks to the contributions of many researchers, knot Floer homology has emerged as a useful tool for studying knots in its own right. We give here a few selected highlights of this theory, and then move on to some new algebraic developments in the computation of knot Floer homology.	0,0,1,0,0,0
Bayesian inference in Y-linked two-sex branching processes with mutations: ABC approach	A Y-linked two-sex branching process with mutations and blind choice of males is a suitable model for analyzing the evolution of the number of carriers of an allele and its mutations of a Y-linked gene. Considering a two-sex monogamous population, in this model each female chooses her partner from among the male population without caring about his type (i.e., the allele he carries). In this work, we deal with the problem of estimating the main parameters of such model developing the Bayesian inference in a parametric framework. Firstly, we consider, as sample scheme, the observation of the total number of females and males up to some generation as well as the number of males of each genotype at last generation. Later, we introduce the information of the mutated males only in the last generation obtaining in this way a second sample scheme. For both samples, we apply the Approximate Bayesian Computation (ABC) methodology to approximate the posterior distributions of the main parameters of this model. The accuracy of the procedure based on these samples is illustrated and discussed by way of simulated examples.	0,0,0,1,1,0
Local Gaussian Processes for Efficient Fine-Grained Traffic Speed Prediction	Traffic speed is a key indicator for the efficiency of an urban transportation system. Accurate modeling of the spatiotemporally varying traffic speed thus plays a crucial role in urban planning and development. This paper addresses the problem of efficient fine-grained traffic speed prediction using big traffic data obtained from static sensors. Gaussian processes (GPs) have been previously used to model various traffic phenomena, including flow and speed. However, GPs do not scale with big traffic data due to their cubic time complexity. In this work, we address their efficiency issues by proposing local GPs to learn from and make predictions for correlated subsets of data. The main idea is to quickly group speed variables in both spatial and temporal dimensions into a finite number of clusters, so that future and unobserved traffic speed queries can be heuristically mapped to one of such clusters. A local GP corresponding to that cluster can then be trained on the fly to make predictions in real-time. We call this method localization. We use non-negative matrix factorization for localization and propose simple heuristics for cluster mapping. We additionally leverage on the expressiveness of GP kernel functions to model road network topology and incorporate side information. Extensive experiments using real-world traffic data collected in the two U.S. cities of Pittsburgh and Washington, D.C., show that our proposed local GPs significantly improve both runtime performances and prediction accuracies compared to the baseline global and local GPs.	1,0,0,0,0,0
Temporal correlation detection using computational phase-change memory	For decades, conventional computers based on the von Neumann architecture have performed computation by repeatedly transferring data between their processing and their memory units, which are physically separated. As computation becomes increasingly data-centric and as the scalability limits in terms of performance and power are being reached, alternative computing paradigms are searched for in which computation and storage are collocated. A fascinating new approach is that of computational memory where the physics of nanoscale memory devices are used to perform certain computational tasks within the memory unit in a non-von Neumann manner. Here we present a large-scale experimental demonstration using one million phase-change memory devices organized to perform a high-level computational primitive by exploiting the crystallization dynamics. Also presented is an application of such a computational memory to process real-world data-sets. The results show that this co-existence of computation and storage at the nanometer scale could be the enabler for new, ultra-dense, low power, and massively parallel computing systems.	1,0,0,0,0,0
Stable Clustering Ansatz, Consistency Relations and Gravity Dual of Large-Scale Structure	Gravitational clustering in the nonlinear regime remains poorly understood. Gravity dual of gravitational clustering has recently been proposed as a means to study the nonlinear regime. The stable clustering ansatz remains a key ingredient to our understanding of gravitational clustering in the highly nonlinear regime. We study certain aspects of violation of the stable clustering ansatz in the gravity dual of Large Scale Structure (LSS). We extend the recent studies of gravitational clustering using AdS gravity dual to take into account possible departure from the stable clustering ansatz and to arbitrary dimensions. Next, we extend the recently introduced consistency relations to arbitrary dimensions. We use the consistency relations to test the commonly used models of gravitational clustering including the halo models and hierarchical ansätze. In particular we establish a tower of consistency relations for the hierarchical amplitudes: $Q, R_a, R_b, S_a,S_b,S_c$ etc. as a functions of the scaled peculiar velocity $h$. We also study the variants of popular halo models in this context. In contrast to recent claims, none of these models, in their simplest incarnation, seem to satisfy the consistency relations in the soft limit.	0,1,0,0,0,0
Geometry-Oblivious FMM for Compressing Dense SPD Matrices	We present GOFMM (geometry-oblivious FMM), a novel method that creates a hierarchical low-rank approximation, "compression," of an arbitrary dense symmetric positive definite (SPD) matrix. For many applications, GOFMM enables an approximate matrix-vector multiplication in $N \log N$ or even $N$ time, where $N$ is the matrix size. Compression requires $N \log N$ storage and work. In general, our scheme belongs to the family of hierarchical matrix approximation methods. In particular, it generalizes the fast multipole method (FMM) to a purely algebraic setting by only requiring the ability to sample matrix entries. Neither geometric information (i.e., point coordinates) nor knowledge of how the matrix entries have been generated is required, thus the term "geometry-oblivious." Also, we introduce a shared-memory parallel scheme for hierarchical matrix computations that reduces synchronization barriers. We present results on the Intel Knights Landing and Haswell architectures, and on the NVIDIA Pascal architecture for a variety of matrices.	1,0,0,0,0,0
Fast generation of isotropic Gaussian random fields on the sphere	The efficient simulation of isotropic Gaussian random fields on the unit sphere is a task encountered frequently in numerical applications. A fast algorithm based on Markov properties and fast Fourier Transforms in 1d is presented that generates samples on an n x n grid in O(n^2 log n). Furthermore, an efficient method to set up the necessary conditional covariance matrices is derived and simulations demonstrate the performance of the algorithm. An open source implementation of the code has been made available at this https URL .	0,0,1,1,0,0
Recommendations of the LHC Dark Matter Working Group: Comparing LHC searches for heavy mediators of dark matter production in visible and invisible decay channels	Weakly-coupled TeV-scale particles may mediate the interactions between normal matter and dark matter. If so, the LHC would produce dark matter through these mediators, leading to the familiar "mono-X" search signatures, but the mediators would also produce signals without missing momentum via the same vertices involved in their production. This document from the LHC Dark Matter Working Group suggests how to compare searches for these two types of signals in case of vector and axial-vector mediators, based on a workshop that took place on September 19/20, 2016 and subsequent discussions. These suggestions include how to extend the spin-1 mediated simplified models already in widespread use to include lepton couplings. This document also provides analytic calculations of the relic density in the simplified models and reports an issue that arose when ATLAS and CMS first began to use preliminary numerical calculations of the dark matter relic density in these models.	0,1,0,0,0,0
The study on quantum material WTe2	WTe2 and its sister alloys have attracted tremendous attentions recent years due to the large non-saturating magnetoresistance and topological non-trivial properties. Herein, we briefly review the electrical property studies on this new quantum material.	0,1,0,0,0,0
A Unified Approach to Nonlinear Transformation Materials	The advances in geometric approaches to optical devices due to transformation optics has led to the development of cloaks, concentrators, and other devices. It has also been shown that transformation optics can be used to gravitational fields from general relativity. However, the technique is currently constrained to linear devices, as a consistent approach to nonlinearity (including both the case of a nonlinear background medium and a nonlinear transformation) remains an open question. Here we show that nonlinearity can be incorporated into transformation optics in a consistent way. We use this to illustrate a number of novel effects, including cloaking an optical soliton, modeling nonlinear solutions to Einstein's field equations, controlling transport in a Debye solid, and developing a set of constitutive to relations for relativistic cloaks in arbitrary nonlinear backgrounds.	0,1,0,0,0,0
The Combinatorics of Weighted Vector Compositions	A vector composition of a vector $\mathbf{\ell}$ is a matrix $\mathbf{A}$ whose rows sum to $\mathbf{\ell}$. We define a weighted vector composition as a vector composition in which the column values of $\mathbf{A}$ may appear in different colors. We study vector compositions from different viewpoints: (1) We show how they are related to sums of random vectors and (2) how they allow to derive formulas for partial derivatives of composite functions. (3) We study congruence properties of the number of weighted vector compositions, for fixed and arbitrary number of parts, many of which are analogous to those of ordinary binomial coefficients and related quantities. Via the Central Limit Theorem and their multivariate generating functions, (4) we also investigate the asymptotic behavior of several special cases of numbers of weighted vector compositions. Finally, (5) we conjecture an extension of a primality criterion due to Mann and Shanks in the context of weighted vector compositions.	1,0,1,0,0,0
Global Orientifolded Quivers with Inflation	We describe global embeddings of fractional D3 branes at orientifolded singularities in type IIB flux compactifications. We present an explicit Calabi-Yau example where the chiral visible sector lives on a local orientifolded quiver while non-perturbative effects, $\alpha'$ corrections and a T-brane hidden sector lead to full closed string moduli stabilisation in a de Sitter vacuum. The same model can also successfully give rise to inflation driven by a del Pezzo divisor. Our model represents the first explicit Calabi-Yau example featuring both an inflationary and a chiral visible sector.	0,1,0,0,0,0
Distributed Deep Transfer Learning by Basic Probability Assignment	Transfer learning is a popular practice in deep neural networks, but fine-tuning of large number of parameters is a hard task due to the complex wiring of neurons between splitting layers and imbalance distributions of data in pretrained and transferred domains. The reconstruction of the original wiring for the target domain is a heavy burden due to the size of interconnections across neurons. We propose a distributed scheme that tunes the convolutional filters individually while backpropagates them jointly by means of basic probability assignment. Some of the most recent advances in evidence theory show that in a vast variety of the imbalanced regimes, optimizing of some proper objective functions derived from contingency matrices prevents biases towards high-prior class distributions. Therefore, the original filters get gradually transferred based on individual contributions to overall performance of the target domain. This largely reduces the expected complexity of transfer learning whilst highly improves precision. Our experiments on standard benchmarks and scenarios confirm the consistent improvement of our distributed deep transfer learning strategy.	1,0,0,1,0,0
The same strain of Piscine orthoreovirus (PRV-1) is involved with the development of different, but related, diseases in Atlantic and Pacific Salmon in British Columbia	Piscine orthoreovirus Strain PRV-1 is the causative agent of heart and skeletal muscle inflammation (HSMI) in Atlantic salmon (Salmo salar). Given its high prevalence in net pen salmon, debate has arisen on whether PRV poses a risk to migratory salmon, especially in British Columbia (BC) where commercially important wild Pacific salmon are in decline. Various strains of PRV have been associated with diseases in Pacific salmon, including erythrocytic inclusion body syndrome (EIBS), HSMI-like disease, and jaundice/anemia in Japan, Norway, Chile and Canada. We examine the developmental pathway of HSMI and jaundice/anemia associated with PRV-1 in farmed Atlantic and Chinook (Oncorhynchus tshawytscha) salmon in BC, respectively. In situ hybridization localized PRV-1 within developing lesions in both diseases. The two diseases showed dissimilar pathological pathways, with inflammatory lesions in heart and skeletal muscle in Atlantic salmon, and degenerative-necrotic lesions in kidney and liver in Chinook salmon, plausibly explained by differences in PRV load tolerance in red blood cells. Viral genome sequencing revealed no consistent differences in PRV-1 variants intimately involved in the development of both diseases, suggesting that migratory Chinook salmon may be at more than a minimal risk of disease from exposure to the high levels of PRV occurring on salmon farms.	0,0,0,0,1,0
Constrained empirical Bayes priors on regression coefficients	Under model uncertainty, empirical Bayes (EB) procedures can have undesirable properties such as extreme estimates of inclusion probabilities (Scott & Berger, 2010) or inconsistency under the null model (Liang et al., 2008). To avoid these issues, we define empirical Bayes priors with constraints that ensure that the estimates of the hyperparameters are at least as "vague" as those of proper default priors. In our examples, we observe that constrained EB procedures are better behaved than their unconstrained counterparts and that the Bayesian Information Criterion (BIC) is similar to an intuitively appealing constrained EB procedure.	0,0,1,1,0,0
Controlling the thermoelectric effect by mechanical manipulation of the electron's quantum phase in atomic junctions	The thermoelectric voltage developed across an atomic metal junction (i.e., a nanostructure in which one or a few atoms connect two metal electrodes) in response to a temperature difference between the electrodes, results from the quantum interference of electrons that pass through the junction multiple times after being scattered by the surrounding defects. Here we report successfully tuning this quantum interference and thus controlling the magnitude and sign of the thermoelectric voltage by applying a mechanical force that deforms the junction. The observed switching of the thermoelectric voltage is reversible and can be cycled many times. Our ab initio and semi-empirical calculations elucidate the detailed mechanism by which the quantum interference is tuned. We show that the applied strain alters the quantum phases of electrons passing through the narrowest part of the junction and hence modifies the electronic quantum interference in the device. Tuning the quantum interference causes the energies of electronic transport resonances to shift, which affects the thermoelectric voltage. These experimental and theoretical studies reveal that Au atomic junctions can be made to exhibit both positive and negative thermoelectric voltages on demand, and demonstrate the importance and tunability of the quantum interference effect in the atomic-scale metal nanostructures.	0,1,0,0,0,0
Classification in biological networks with hypergraphlet kernels	Biological and cellular systems are often modeled as graphs in which vertices represent objects of interest (genes, proteins, drugs) and edges represent relational ties among these objects (binds-to, interacts-with, regulates). This approach has been highly successful owing to the theory, methodology and software that support analysis and learning on graphs. Graphs, however, often suffer from information loss when modeling physical systems due to their inability to accurately represent multiobject relationships. Hypergraphs, a generalization of graphs, provide a framework to mitigate information loss and unify disparate graph-based methodologies. In this paper, we present a hypergraph-based approach for modeling physical systems and formulate vertex classification, edge classification and link prediction problems on (hyper)graphs as instances of vertex classification on (extended, dual) hypergraphs in a semi-supervised setting. We introduce a novel kernel method on vertex- and edge-labeled (colored) hypergraphs for analysis and learning. The method is based on exact and inexact (via hypergraph edit distances) enumeration of small simple hypergraphs, referred to as hypergraphlets, rooted at a vertex of interest. We extensively evaluate this method and show its potential use in a positive-unlabeled setting to estimate the number of missing and false positive links in protein-protein interaction networks.	1,0,0,1,0,0
Spectral Theory of Infinite Quantum Graphs	We investigate quantum graphs with infinitely many vertices and edges without the common restriction on the geometry of the underlying metric graph that there is a positive lower bound on the lengths of its edges. Our central result is a close connection between spectral properties of a quantum graph and the corresponding properties of a certain weighted discrete Laplacian on the underlying discrete graph. Using this connection together with spectral theory of (unbounded) discrete Laplacians on infinite graphs, we prove a number of new results on spectral properties of quantum graphs. Namely, we prove several self-adjointness results including a Gaffney type theorem. We investigate the problem of lower semiboundedness, prove several spectral estimates (bounds for the bottom of spectra and essential spectra of quantum graphs, CLR-type estimates) and study spectral types.	0,0,1,0,0,0
Disorder-protected topological entropy after a quantum quench	Topological phases of matter are considered the bedrock of novel quantum materials as well as ideal candidates for quantum computers that possess robustness at the physical level. The robustness of the topological phase at finite temperature or away from equilibrium is therefore a very desirable feature. Disorder can improve the lifetime of the encoded topological qubits. Here we tackle the problem of the survival of the topological phase as detected by topological entropy, after a sudden quantum quench. We introduce a method to study analytically the time evolution of the system after a quantum quench and show that disorder in the couplings of the Hamiltonian of the toric code and the resulting Anderson localization can make the topological entropy resilient.	0,1,0,0,0,0
Counting the number of distinct distances of elements in valued field extensions	The defect of valued field extensions is a major obstacle in open problems in resolution of singularities and in the model theory of valued fields, whenever positive characteristic is involved. We continue the detailed study of defect extensions through the tool of distances, which measure how well an element in an immediate extension can be approximated by elements from the base field. We show that in several situations the number of essentially distinct distances in fixed extensions, or even just over a fixed base field, is finite, and we compute upper bounds. We apply this to the special case of valued functions fields over perfect base fields. This provides important information used in forthcoming research on relative resolution problems.	0,0,1,0,0,0
Cosmological solutions in generalized hybrid metric-Palatini gravity	We construct exact solutions representing a Friedmann-Lemaître-Robsertson-Walker (FLRW) universe in a generalized hybrid metric-Palatini theory. By writing the gravitational action in a scalar-tensor representation, the new solutions are obtained by either making an ansatz on the scale factor or on the effective potential. Among other relevant results, we show that it is possible to obtain exponentially expanding solutions for flat universes even when the cosmology is not purely vacuum. We then derive the classes of actions for the original theory which generate these solutions.	0,1,0,0,0,0
Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks	We consider the problem of learning function classes computed by neural networks with various activations (e.g. ReLU or Sigmoid), a task believed to be computationally intractable in the worst-case. A major open problem is to understand the minimal assumptions under which these classes admit provably efficient algorithms. In this work we show that a natural distributional assumption corresponding to {\em eigenvalue decay} of the Gram matrix yields polynomial-time algorithms in the non-realizable setting for expressive classes of networks (e.g. feed-forward networks of ReLUs). We make no assumptions on the structure of the network or the labels. Given sufficiently-strong polynomial eigenvalue decay, we obtain {\em fully}-polynomial time algorithms in {\em all} the relevant parameters with respect to square-loss. Milder decay assumptions also lead to improved algorithms. This is the first purely distributional assumption that leads to polynomial-time algorithms for networks of ReLUs, even with one hidden layer. Further, unlike prior distributional assumptions (e.g., the marginal distribution is Gaussian), eigenvalue decay has been observed in practice on common data sets.	1,0,0,0,0,0
A recipe for topological observables of density matrices	Meaningful topological invariants for mixed quantum states are challenging to identify as there is no unique way to define them, and most choices do not directly relate to physical observables. Here, we propose a simple pragmatic approach to construct topological invariants of mixed states while preserving a connection to physical observables, by continuously deforming known topological invariants for pure (ground) states. Our approach relies on expectation values of many-body operators, with no reference to single-particle (e.g., Bloch) wavefunctions. To illustrate it, we examine extensions to mixed states of $U(1)$ geometric (Berry) phases and their corresponding topological invariant (winding or Chern number). We discuss measurement schemes, and provide a detailed construction of invariants for thermal or more general mixed states of quantum systems with (at least) $U(1)$ charge-conservation symmetry, such as quantum Hall insulators.	0,1,0,0,0,0
Discovering Bayesian Market Views for Intelligent Asset Allocation	Along with the advance of opinion mining techniques, public mood has been found to be a key element for stock market prediction. However, how market participants' behavior is affected by public mood has been rarely discussed. Consequently, there has been little progress in leveraging public mood for the asset allocation problem, which is preferred in a trusted and interpretable way. In order to address the issue of incorporating public mood analyzed from social media, we propose to formalize public mood into market views, because market views can be integrated into the modern portfolio theory. In our framework, the optimal market views will maximize returns in each period with a Bayesian asset allocation model. We train two neural models to generate the market views, and benchmark the model performance on other popular asset allocation strategies. Our experimental results suggest that the formalization of market views significantly increases the profitability (5% to 10% annually) of the simulated portfolio at a given risk level.	0,0,0,0,0,1
Bergman kernel estimates and Toeplitz operators on holomorphic line bundles	We characterize operator-theoretic properties (boundedness, compactness, and Schatten class membership) of Toeplitz operators with positive measure symbols on Bergman spaces of holomorphic hermitian line bundles over Kähler Cartan-Hadamard manifolds in terms of geometric or operator-theoretic properties of measures.	0,0,1,0,0,0
A 3D MHD simulation of SN 1006: a polarized emission study for the turbulent case	Three dimensional magnetohydrodynamical simulations were carried out in order to perform a new polarization study of the radio emission of the supernova remnant SN 1006. These simulations consider that the remnant expands into a turbulent interstellar medium (including both magnetic field and density perturbations). Based on the referenced-polar angle technique, a statistical study was done on observational and numerical magnetic field position-angle distributions. Our results show that a turbulent medium with an adiabatic index of 1.3 can reproduce the polarization properties of the SN 1006 remnant. This statistical study reveals itself as a useful tool for obtaining the orientation of the ambient magnetic field, previous to be swept up by the main supernova remnant shock.	0,1,0,0,0,0
On the second Feng-Rao distance of Algebraic Geometry codes related to Arf semigroups	We describe the second (generalized) Feng-Rao distance for elements in an Arf numerical semigroup that are greater than or equal to the conductor of the semigroup. This provides a lower bound for the second Hamming weight for one point AG codes. In particular, we can obtain the second Feng-Rao distance for the codes defined by asymptotically good towers of function fields whose Weierstrass semigroups are inductive. In addition, we compute the second Feng-Rao number, and provide some examples and comparisons with previous results on this topic. These calculations rely on Apéry sets, and thus several results concerning Apéry sets of Arf semigroups are presented.	1,0,0,0,0,0
On minimum distance of locally repairable codes	Distributed and cloud storage systems are used to reliably store large-scale data. Erasure codes have been recently proposed and used in real-world distributed and cloud storage systems such as Google File System, Microsoft Azure Storage, and Facebook HDFS-RAID, to enhance the reliability. In order to decrease the repair bandwidth and disk I/O, a class of erasure codes called locally repairable codes (LRCs) have been proposed which have small locality compare to other erasure codes. Although LRCs have small locality, they have lower minimum distance compare to the Singleton bound. Hence, seeking the largest possible minimum distance for LRCs have been the topic of many recent studies. In this paper, we study the largest possible minimum distance of a class of LRCs and evaluate them in terms of achievability. Furthermore, we compare our results with the existence bounds in the literature.	1,0,0,0,0,0
Evaluating Quality of Chatbots and Intelligent Conversational Agents	Chatbots are one class of intelligent, conversational software agents activated by natural language input (which can be in the form of text, voice, or both). They provide conversational output in response, and if commanded, can sometimes also execute tasks. Although chatbot technologies have existed since the 1960s and have influenced user interface development in games since the early 1980s, chatbots are now easier to train and implement. This is due to plentiful open source code, widely available development platforms, and implementation options via Software as a Service (SaaS). In addition to enhancing customer experiences and supporting learning, chatbots can also be used to engineer social harm - that is, to spread rumors and misinformation, or attack people for posting their thoughts and opinions online. This paper presents a literature review of quality issues and attributes as they relate to the contemporary issue of chatbot development and implementation. Finally, quality assessment approaches are reviewed, and a quality assessment method based on these attributes and the Analytic Hierarchy Process (AHP) is proposed and examined.	1,0,0,0,0,0
DNA Base Pair Mismatches Induce Structural Changes and Alter the Free Energy Landscape of Base Flip	Double-stranded DNA may contain mismatched base pairs beyond the Watson-Crick pairs guanine-cytosine and adenine-thymine. Such mismatches bear adverse consequences for human health. We utilize molecular dynamics and metadynamics computer simulations to study the equilibrium structure and dynamics for both matched and mismatched base pairs. We discover significant differences between matched and mismatched pairs in structure, hydrogen bonding, and base flip work profiles. Mismatched pairs shift further in the plane normal to the DNA strand and are more likely to exhibit non-canonical structures, including the e-motif. We discuss potential implications on mismatch repair enzymes' detection of DNA mismatches.	0,0,0,0,1,0
Kohn anomalies in momentum dependence of magnetic susceptibility of some three-dimensional systems	We study a question of presence of Kohn points, yielding at low temperatures non-analytic momentum dependence of magnetic susceptibility near its maximum, in electronic spectum of some three-dimensional systems. In particular, we consider one-band model on face centered cubic lattice with hopping between nearest and next-nearest neighbors, which models some aspects of the dispersion of ZrZn$_2$, and the two-band model on body centered cubic lattice, modeling the dispersion of chromium. For the former model it is shown that Kohn points yielding maxima of susceptibility exist in a certain (sufficiently wide) region of electronic concentrations; the dependence of the wave vectors, corresponding to the maxima, on the chemical potential is investigated. For the two-band model we show existence of the lines of Kohn points, yielding maximum of the susceptibility, which position agrees with the results of band structure calculations and experimental data on the wave vector of antiferromagnetism of chromium.	0,1,0,0,0,0
Ore's theorem on subfactor planar algebras	This paper proves that an irreducible subfactor planar algebra with a distributive biprojection lattice admits a minimal 2-box projection generating the identity biprojection. It is a generalization of a theorem of Ore on intervals of finite groups, conjectured by the author since 2013. We deduce a link between combinatorics and representations in finite groups theory, related to an open problem of K.S. Brown in algebraic combinatorics.	0,0,1,0,0,0
Regional Multi-Armed Bandits	We consider a variant of the classic multi-armed bandit problem where the expected reward of each arm is a function of an unknown parameter. The arms are divided into different groups, each of which has a common parameter. Therefore, when the player selects an arm at each time slot, information of other arms in the same group is also revealed. This regional bandit model naturally bridges the non-informative bandit setting where the player can only learn the chosen arm, and the global bandit model where sampling one arms reveals information of all arms. We propose an efficient algorithm, UCB-g, that solves the regional bandit problem by combining the Upper Confidence Bound (UCB) and greedy principles. Both parameter-dependent and parameter-free regret upper bounds are derived. We also establish a matching lower bound, which proves the order-optimality of UCB-g. Moreover, we propose SW-UCB-g, which is an extension of UCB-g for a non-stationary environment where the parameters slowly vary over time.	0,0,0,1,0,0
Surrogate-Based Bayesian Inverse Modeling of the Hydrological System: An Adaptive Approach Considering Surrogate Approximation Erro	Bayesian inverse modeling is important for a better understanding of hydrological processes. However, this approach can be computationally demanding as it usually requires a large number of model evaluations. To address this issue, one can take advantage of surrogate modeling techniques. Nevertheless, when approximation error of the surrogate model is neglected in inverse modeling, the inversion result will be biased. In this paper, we develop a surrogate-based Bayesian inversion framework that explicitly quantifies and gradually reduces the approximation error of the surrogate. Specifically, two strategies are proposed and compared. The first strategy works by obtaining an ensemble of sparse polynomial chaos expansion (PCE) surrogates with Markov chain Monte Carlo sampling, while the second one uses Gaussian process (GP) to simulate the approximation error of a single sparse PCE surrogate. The two strategies can also be applied with other surrogates, thus they have general applicability. By adaptively refining the surrogate over the posterior distribution, we can gradually reduce the surrogate approximation error to a small level. Demonstrated with three case studies involving high-dimensionality, multi-modality and a real-world application, respectively, it is found that both strategies can reduce the bias introduced by surrogate modeling, while the second strategy has a better performance as it integrates two methods (i.e., sparse PCE and GP) that complement each other.	0,0,0,1,0,0
Nonsparse learning with latent variables	As a popular tool for producing meaningful and interpretable models, large-scale sparse learning works efficiently when the underlying structures are indeed or close to sparse. However, naively applying the existing regularization methods can result in misleading outcomes due to model misspecification. In particular, the direct sparsity assumption on coefficient vectors has been questioned in real applications. Therefore, we consider nonsparse learning with the conditional sparsity structure that the coefficient vector becomes sparse after taking out the impacts of certain unobservable latent variables. A new methodology of nonsparse learning with latent variables (NSL) is proposed to simultaneously recover the significant observable predictors and latent factors as well as their effects. We explore a common latent family incorporating population principal components and derive the convergence rates of both sample principal components and their score vectors that hold for a wide class of distributions. With the properly estimated latent variables, properties including model selection consistency and oracle inequalities under various prediction and estimation losses are established for the proposed methodology. Our new methodology and results are evidenced by simulation and real data examples.	0,0,1,1,0,0
Breaking through the bandwidth barrier in distributed fiber vibration sensing by sub-Nyquist randomized sampling	The round trip time of the light pulse limits the maximum detectable frequency response range of vibration in phase-sensitive optical time domain reflectometry ({\phi}-OTDR). We propose a method to break the frequency response range restriction of {\phi}-OTDR system by modulating the light pulse interval randomly which enables a random sampling for every vibration point in a long sensing fiber. This sub-Nyquist randomized sampling method is suits for detecting sparse-wideband-frequency vibration signals. Up to MHz resonance vibration signal with over dozens of frequency components and 1.153MHz single frequency vibration signal are clearly identified for a sensing range of 9.6km with 10kHz maximum sampling rate.	0,1,0,0,0,0
Differences in 1D electron plasma wake field acceleration in MeV versus GeV and linear versus blowout regimes	In some laboratory and most astrophysical situations plasma wake-field acceleration of electrons is one dimensional, i.e. variation transverse to the beam's motion can be ignored. Thus, one dimensional (1D), particle-in-cell (PIC), fully electromagnetic simulations of electron plasma wake field acceleration are conducted in order to study the differences in electron plasma wake field acceleration in MeV versus GeV and linear versus blowout regimes. First, we show that caution needs to be taken when using fluid simulations, as PIC simulations prove that an approximation for an electron bunch not to evolve in time for few hundred plasma periods only applies when it is sufficiently relativistic. This conclusion is true irrespective of the plasma temperature. We find that in the linear regime and GeV energies, the accelerating electric field generated by the plasma wake is similar to the linear and MeV regime. However, because GeV energy driving bunch stays intact for much longer time, the final acceleration energies are much larger in the GeV energies case. In the GeV energy range and blowout regime the wake's accelerating electric field is much larger in amplitude compared to the linear case and also plasma wake geometrical size is much larger. Thus, the correct positioning of the trailing bunch is needed to achieve the efficient acceleration. For the considered case, optimally there should be approximately $(90-100) c/\omega_{pe}$ distance between trailing and driving electron bunches in the GeV blowout regime.	0,1,0,0,0,0
Around power law for PageRank components in Buckley-Osthus model of web graph	In the paper we investigate power law for PageRank components for the Buckley-Osthus model for web graph. We compare different numerical methods for PageRank calculation. With the best method we do a lot of numerical experiments. These experiments confirm the hypothesis about power law. At the end we discuss real model of web-ranking based on the classical PageRank approach.	1,0,1,0,0,0
Symplectic resolutions for Higgs moduli spaces	In this paper, we study the algebraic symplectic geometry of the singular moduli spaces of Higgs bundles of degree $0$ and rank $n$ on a compact Riemann surface $X$ of genus $g$. In particular, we prove that such moduli spaces are symplectic singularities, in the sense of Beauville [Bea00], and admit a projective symplectic resolution if and only if $g=1$ or $(g, n)=(2,2)$. These results are an application of a recent paper by Bellamy and Schedler [BS16] via the so-called Isosingularity Theorem.	0,0,1,0,0,0
Block Mean Approximation for Efficient Second Order Optimization	Advanced optimization algorithms such as Newton method and AdaGrad benefit from second order derivative or second order statistics to achieve better descent directions and faster convergence rates. At their heart, such algorithms need to compute the inverse or inverse square root of a matrix whose size is quadratic of the dimensionality of the search space. For high dimensional search spaces, the matrix inversion or inversion of square root becomes overwhelming which in turn demands for approximate methods. In this work, we propose a new matrix approximation method which divides a matrix into blocks and represents each block by one or two numbers. The method allows efficient computation of matrix inverse and inverse square root. We apply our method to AdaGrad in training deep neural networks. Experiments show encouraging results compared to the diagonal approximation.	0,0,0,1,0,0
Vector-valued extensions of operators through multilinear limited range extrapolation	We give an extension of Rubio de Francia's extrapolation theorem for functions taking values in UMD Banach function spaces to the multilinear limited range setting. In particular we show how boundedness of an $m$-(sub)linear operator \[T:L^{p_1}(w_1^{p_1})\times\cdots\times L^{p_m}(w_m^{p_m})\to L^p(w^p) \] for a certain class of Muckenhoupt weights yields an extension of the operator to Bochner spaces $L^{p}(w^p;X)$ for a wide class of Banach function spaces $X$, which includes certain Lebesgue, Lorentz and Orlicz spaces. We apply the extrapolation result to various operators, which yields new vector-valued bounds. Our examples include the bilinear Hilbert transform, certain Fourier multipliers and various operators satisfying sparse domination results.	0,0,1,0,0,0
Do triangle-free planar graphs have exponentially many 3-colorings?	Thomassen conjectured that triangle-free planar graphs have an exponential number of $3$-colorings. We show this conjecture to be equivalent to the following statement: there exists a positive real $\alpha$ such that whenever $G$ is a planar graph and $A$ is a subset of its edges whose deletion makes $G$ triangle-free, there exists a subset $A'$ of $A$ of size at least $\alpha|A|$ such that $G-(A\setminus A')$ is $3$-colorable. This equivalence allows us to study restricted situations, where we can prove the statement to be true.	0,0,1,0,0,0
Waist size for cusps in hyperbolic 3-manifolds II	The waist size of a cusp in an orientable hyperbolic 3-manifold is the length of the shortest nontrivial curve generated by a parabolic isometry in the maximal cusp boundary. Previously, it was shown that the smallest possible waist size, which is 1, is realized only by the cusp in the figure-eight knot complement. In this paper, it is proved that the next two smallest waist sizes are realized uniquely for the cusps in the $5_2$ knot complement and the manifold obtained by (2,1)-surgery on the Whitehead link. One application is an improvement on the universal upper bound for the length of an unknotting tunnel in a 2-cusped hyperbolic 3-manifold.	0,0,1,0,0,0
Inter-site pair superconductivity: origins and recent validation experiments	The challenge of understanding high-temperature superconductivity has led to a plethora of ideas, but 30 years after its discovery in cuprates, very few have achieved convincing experimental validation. While Hubbard and t-J models were given a lot of attention, a number of recent experiments appear to give decisive support to the model of real-space inter-site pairing and percolative superconductivity in cuprates. Systematic measurements of the doping dependence of the superfluid density show a linear dependence on superfluid density - rather than doping - over the entire phase diagram, in accordance with the model's predictions. The doping-dependence of the anomalous lattice dynamics of in-plane Cu-O mode vibrations observed by inelastic neutron scattering, gives remarkable reciprocal space signature of the inter-site pairing interaction whose doping dependence closely follows the predicted pair density. Symmetry-specific time-domain spectroscopy shows carrier localization, polaron formation, pairing and superconductivity to be distinct processes occurring on distinct timescales throughout the entire superconducting phase diagram. The three diverse experimental results confirm non-trivial predictions made more than a decade ago by the inter-site pairing model in the cuprates, remarkably also confirming some of the fundamental notions mentioned in the seminal paper on the discovery of high-temperature superconductivity in cuprates.	0,1,0,0,0,0
Learning the Structure of Generative Models without Labeled Data	Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the $\ell_1$-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100$\times$ faster than a maximum likelihood approach and selects $1/4$ as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.	1,0,0,1,0,0
Dominant dimension and tilting modules	We study which algebras have tilting modules that are both generated and cogenerated by projective-injective modules. Crawley-Boevey and Sauter have shown that Auslander algebras have such tilting modules; and for algebras of global dimension $2$, Auslander algebras are classified by the existence of such tilting modules. In this paper, we show that the existence of such a tilting module is equivalent to the algebra having dominant dimension at least $2$, independent of its global dimension. In general such a tilting module is not necessarily cotilting. Here, we show that the algebras which have a tilting-cotilting module generated-cogenerated by projective-injective modules are precisely $1$-Auslander-Gorenstein algebras. When considering such a tilting module, without the assumption that it is cotilting, we study the global dimension of its endomorphism algebra, and discuss a connection with the Finitistic Dimension Conjecture. Furthermore, as special cases, we show that triangular matrix algebras obtained from Auslander algebras and certain injective modules, have such a tilting module. We also give a description of which Nakayama algebras have such a tilting module.	0,0,1,0,0,0
Stabilization of self-mode-locked quantum dash lasers by symmetric dual-loop optical feedback	We report experimental studies of the influence of symmetric dual-loop optical feedback on the RF linewidth and timing jitter of self-mode-locked two-section quantum dash lasers emitting at 1550 nm. Various feedback schemes were investigated and optimum levels determined for narrowest RF linewidth and low timing jitter, for single-loop and symmetric dual-loop feedback. Two symmetric dual-loop configurations, with balanced and unbalanced feedback ratios, were studied. We demonstrate that unbalanced symmetric dual loop feedback, with the inner cavity resonant and fine delay tuning of the outer loop, gives narrowest RF linewidth and reduced timing jitter over a wide range of delay, unlike single and balanced symmetric dual-loop configurations. This configuration with feedback lengths 80 and 140 m narrows the RF linewidth by 4-67x and 10-100x, respectively, across the widest delay range, compared to free-running. For symmetric dual-loop feedback, the influence of different power split ratios through the feedback loops was determined. Our results show that symmetric dual-loop feedback is markedly more effective than single-loop feedback in reducing RF linewidth and timing jitter, and is much less sensitive to delay phase, making this technique ideal for applications where robustness and alignment tolerance are essential.	0,1,0,0,0,0
Finding Root Causes of Floating Point Error with Herbgrind	Floating-point arithmetic plays a central role in science, engineering, and finance by enabling developers to approximate real arithmetic. To address numerical issues in large floating-point applications, developers must identify root causes, which is difficult because floating-point errors are generally non-local, non-compositional, and non-uniform. This paper presents Herbgrind, a tool to help developers identify and address root causes in numerical code written in low-level C/C++ and Fortran. Herbgrind dynamically tracks dependencies between operations and program outputs to avoid false positives and abstracts erroneous computations to a simplified program fragment whose improvement can reduce output error. We perform several case studies applying Herbgrind to large, expert-crafted numerical programs and show that it scales to applications spanning hundreds of thousands of lines, correctly handling the low-level details of modern floating point hardware and mathematical libraries, and tracking error across function boundaries and through the heap.	1,0,0,0,0,0
Graded Lie algebras and regular prehomogeneous vector spaces with one-dimensional scalar multiplication	The aim of this paper is to study relations between regular reductive PVs with one-dimensional scalar multiplication and the structure of graded Lie algebras. We will show that the regularity of such PVs is described by an $\mathfrak{sl}_2$-triplet of a graded Lie algebra.	0,0,1,0,0,0
Phase limitations of Zames-Falb multipliers	Phase limitations of both continuous-time and discrete-time Zames-Falb multipliers and their relation with the Kalman conjecture are analysed. A phase limitation for continuous-time multipliers given by Megretski is generalised and its applicability is clarified; its relation to the Kalman conjecture is illustrated with a classical example from the literature. It is demonstrated that there exist fourth-order plants where the existence of a suitable Zames-Falb multiplier can be discarded and for which simulations show unstable behavior. A novel phase-limitation for discrete-time Zames-Falb multipliers is developed. Its application is demonstrated with a second-order counterexample to the Kalman conjecture. Finally, the discrete-time limitation is used to show that there can be no direct counterpart of the off-axis circle criterion in the discrete-time domain.	1,0,1,0,0,0
Finding Large Primes	In this paper we present and expand upon procedures for obtaining large d digit prime number to an arbitrary probability. We use a layered approach. The first step is to limit the pool of random number to exclude numbers that are obviously composite. We first remove any number ending in 1,3,7 or 9. We then exclude numbers whose digital root is not 3, 6, or 9. This sharply reduces the probability of the random number being composite. We then use the Prime Number Theorem to find the probability that the selected number n is prime and use primality tests to increase the probability to an arbitrarily high degree that n is prime. We apply primality tests including Euler's test based on Fermat Little theorem and the Miller-Rabin test. We computed these conditional probabilities and implemented it using the GNU GMP library.	0,0,1,0,0,0
Unraveling the escape dynamics and the nature of the normally hyperbolic invariant manifolds in tidally limited star clusters	The escape mechanism of orbits in a star cluster rotating around its parent galaxy in a circular orbit is investigated. A three degrees of freedom model is used for describing the dynamical properties of the Hamiltonian system. The gravitational field of the star cluster is represented by a smooth and spherically symmetric Plummer potential. We distinguish between ordered and chaotic orbits as well as between trapped and escaping orbits, considering only unbounded motion for several energy levels. The Smaller Alignment Index (SALI) method is used for determining the regular or chaotic nature of the orbits. The basins of escape are located and they are also correlated with the corresponding escape time of the orbits. Areas of bounded regular or chaotic motion and basins of escape were found to coexist in the $(x,z)$ plane. The properties of the normally hyperbolic invariant manifolds (NHIMs), located in the vicinity of the index-1 Lagrange points $L_1$ and $L_2$, are also explored. These manifolds are of paramount importance as they control the flow of stars over the saddle points, while they also trigger the formation of tidal tails observed in star clusters. Bifurcation diagrams of the Lyapunov periodic orbits as well as restrictions of the Poincaré map to the NHIMs are deployed for elucidating the dynamics in the neighbourhood of the saddle points. The extended tidal tails, or tidal arms, formed by stars with low velocity which escape through the Lagrange points are monitored. The numerical results of this work are also compared with previous related work.	0,1,0,0,0,0
Few-Shot Learning with Metric-Agnostic Conditional Embeddings	Learning high quality class representations from few examples is a key problem in metric-learning approaches to few-shot learning. To accomplish this, we introduce a novel architecture where class representations are conditioned for each few-shot trial based on a target image. We also deviate from traditional metric-learning approaches by training a network to perform comparisons between classes rather than relying on a static metric comparison. This allows the network to decide what aspects of each class are important for the comparison at hand. We find that this flexible architecture works well in practice, achieving state-of-the-art performance on the Caltech-UCSD birds fine-grained classification task.	0,0,0,1,0,0
Lexical Features in Coreference Resolution: To be Used With Caution	Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.	1,0,0,0,0,0
Intuitionistic Layered Graph Logic: Semantics and Proof Theory	Models of complex systems are widely used in the physical and social sciences, and the concept of layering, typically building upon graph-theoretic structure, is a common feature. We describe an intuitionistic substructural logic called ILGL that gives an account of layering. The logic is a bunched system, combining the usual intuitionistic connectives, together with a non-commutative, non-associative conjunction (used to capture layering) and its associated implications. We give soundness and completeness theorems for a labelled tableaux system with respect to a Kripke semantics on graphs. We then give an equivalent relational semantics, itself proven equivalent to an algebraic semantics via a representation theorem. We utilise this result in two ways. First, we prove decidability of the logic by showing the finite embeddability property holds for the algebraic semantics. Second, we prove a Stone-type duality theorem for the logic. By introducing the notions of ILGL hyperdoctrine and indexed layered frame we are able to extend this result to a predicate version of the logic and prove soundness and completeness theorems for an extension of the layered graph semantics . We indicate the utility of predicate ILGL with a resource-labelled bigraph model.	1,0,0,0,0,0
Design and experimental test of an optical vortex coronagraph	The optical vortex coronagraph (OVC) is one of the promising ways for direct imaging exoplanets because of its small inner working angle and high throughput. This paper presents the design and laboratory demonstration performance at 633nm and 1520nm of the OVC based on liquid crystal polymers (LCP). Two LCPs has been manufactured in partnership with a commercial vendor. The OVC can deliver a good performance in laboratory test and achieve the contrast of the order 10^-6 at angular distance 3{\lambda}/D, which is able to image the giant exoplanets at a young stage in combination with extreme adaptive optics.	0,1,0,0,0,0
Lower bounds for weak approximation errors for spatial spectral Galerkin approximations of stochastic wave equations	Although for a number of semilinear stochastic wave equations existence and uniqueness results for corresponding solution processes are known from the literature, these solution processes are typically not explicitly known and numerical approximation methods are needed in order for mathematical modelling with stochastic wave equations to become relevant for real world applications. This, in turn, requires the numerical analysis of convergence rates for such numerical approximation processes. A recent article by the authors proves upper bounds for weak errors for spatial spectral Galerkin approximations of a class of semilinear stochastic wave equations. The findings there are complemented by the main result of this work, that provides lower bounds for weak errors which show that in the general framework considered the established upper bounds can essentially not be improved.	0,0,1,0,0,0
On Blocking Collisions between People, Objects and other Robots	Intentional or unintentional contacts are bound to occur increasingly more often due to the deployment of autonomous systems in human environments. In this paper, we devise methods to computationally predict imminent collisions between objects, robots and people, and use an upper-body humanoid robot to block them if they are likely to happen. We employ statistical methods for effective collision prediction followed by sensor-based trajectory generation and real-time control to attempt to stop the likely collisions using the most favorable part of the blocking robot. We thoroughly investigate collisions in various types of experimental setups involving objects, robots, and people. Overall, the main contribution of this paper is to devise sensor-based prediction, trajectory generation and control processes for highly articulated robots to prevent collisions against people, and conduct numerous experiments to validate this approach.	1,0,0,0,0,0
ISeeU: Visually interpretable deep learning for mortality prediction inside the ICU	To improve the performance of Intensive Care Units (ICUs), the field of bio-statistics has developed scores which try to predict the likelihood of negative outcomes. These help evaluate the effectiveness of treatments and clinical practice, and also help to identify patients with unexpected outcomes. However, they have been shown by several studies to offer sub-optimal performance. Alternatively, Deep Learning offers state of the art capabilities in certain prediction tasks and research suggests deep neural networks are able to outperform traditional techniques. Nevertheless, a main impediment for the adoption of Deep Learning in healthcare is its reduced interpretability, for in this field it is crucial to gain insight on the why of predictions, to assure that models are actually learning relevant features instead of spurious correlations. To address this, we propose a deep multi-scale convolutional architecture trained on the Medical Information Mart for Intensive Care III (MIMIC-III) for mortality prediction, and the use of concepts from coalitional game theory to construct visual explanations aimed to show how important these inputs are deemed by the network. Our results show our model attains state of the art performance while remaining interpretable. Supporting code can be found at this https URL.	1,0,0,1,0,0
Security for 4G and 5G Cellular Networks: A Survey of Existing Authentication and Privacy-preserving Schemes	This paper presents a comprehensive survey of existing authentication and privacy-preserving schemes for 4G and 5G cellular networks. We start by providing an overview of existing surveys that deal with 4G and 5G communications, applications, standardization, and security. Then, we give a classification of threat models in 4G and 5G cellular networks in four categories, including, attacks against privacy, attacks against integrity, attacks against availability, and attacks against authentication. We also provide a classification of countermeasures into three types of categories, including, cryptography methods, humans factors, and intrusion detection methods. The countermeasures and informal and formal security analysis techniques used by the authentication and privacy preserving schemes are summarized in form of tables. Based on the categorization of the authentication and privacy models, we classify these schemes in seven types, including, handover authentication with privacy, mutual authentication with privacy, RFID authentication with privacy, deniable authentication with privacy, authentication with mutual anonymity, authentication and key agreement with privacy, and three-factor authentication with privacy. In addition, we provide a taxonomy and comparison of authentication and privacy-preserving schemes for 4G and 5G cellular networks in form of tables. Based on the current survey, several recommendations for further research are discussed at the end of this paper.	1,0,0,0,0,0
Propagation of regularity in $L^p$-spaces for Kolmogorov type hypoelliptic operators	Consider the following Kolmogorov type hypoelliptic operator $$ \mathscr L_t:=\mbox{$\sum_{j=2}^n$}x_j\cdot\nabla_{x_{j-1}}+{\rm Tr} (a_t \cdot\nabla^2_{x_n}), $$ where $n\geq 2$, $x=(x_1,\cdots,x_n)\in(\mathbb R^d)^n =\mathbb R^{nd}$ and $a_t$ is a time-dependent constant symmetric $d\times d$-matrix that is uniformly elliptic and bounded.. Let $\{\mathcal T_{s,t}; t\geq s\}$ be the time-dependent semigroup associated with $\mathscr L_t$; that is, $\partial_s {\mathcal T}_{s, t} f = - {\mathscr L}_s {\mathcal T}_{s, t}f$. For any $p\in(1,\infty)$, we show that there is a constant $C=C(p,n,d)>0$ such that for any $f(t, x)\in L^p(\mathbb R \times \mathbb R^{nd})=L^p(\mathbb R^{1+nd})$ and every $\lambda \geq 0$, $$ \left\|\Delta_{x_j}^{{1}/{(1+2(n-j)})}\int^{\infty}_0 e^{-\lambda t} {\mathcal T}_{s, s+t }f(t+s, x)dt\right\|_p\leq C\|f\|_p,\quad j=1,\cdots, n, $$ where $\|\cdot\|_p$ is the usual $L^p$-norm in $L^p(\mathbb R^{1+nd}; d s\times d x)$. To show this type of estimates, we first study the propagation of regularity in $L^2$-space from variable $x_n$ to $x_1$ for the solution of the transport equation $\partial_t u+\sum_{j=2}^nx_j\cdot\nabla_{x_{j-1}} u=f$.	0,0,1,0,0,0
Enumeration of Tree-like Maps with Arbitrary Number of Vertices	This paper provides the generating series for the embedding of tree-like graphs of arbitrary number of vertices, accourding to their genus. It applies and extends the techniques of Chan, where it was used to give an alternate proof of the Goulden and Slofstra formula. Furthermore, this greatly generalizes the famous Harer-Zagier formula, which computes the Euler characteristic of the moduli space of curves, and is equivalent to the computation of one vertex maps.	0,0,1,0,0,0
Generation of High Dynamic Range Illumination from a Single Image for the Enhancement of Undesirably Illuminated Images	This paper presents an algorithm that enhances undesirably illuminated images by generating and fusing multi-level illuminations from a single image.The input image is first decomposed into illumination and reflectance components by using an edge-preserving smoothing filter. Then the reflectance component is scaled up to improve the image details in bright areas. The illumination component is scaled up and down to generate several illumination images that correspond to certain camera exposure values different from the original. The virtual multi-exposure illuminations are blended into an enhanced illumination, where we also propose a method to generate appropriate weight maps for the tone fusion. Finally, an enhanced image is obtained by multiplying the equalized illumination and enhanced reflectance. Experiments show that the proposed algorithm produces visually pleasing output and also yields comparable objective results to the conventional enhancement methods, while requiring modest computational loads.	1,0,0,0,0,0
Visualizing the Phase-Space Dynamics of an External Cavity Semiconductor Laser	We map the phase-space trajectories of an external-cavity semiconductor laser using phase portraits. This is both a visualization tool as well as a thoroughly quantitative approach enabling unprecedented insight into the dynamical regimes, from continuous-wave through coherence collapse as feedback is increased. Namely, the phase portraits in the intensity versus laser-diode terminal-voltage (serving as a surrogate for inversion) plane are mapped out. We observe a route to chaos interrupted by two types of limit cycles, a subharmonic regime and period-doubled dynamics at the edge of chaos. The transition of the dynamics are analyzed utilizing bifurcation diagrams for both the optical intensity and the laser-diode terminal voltage. These observations provide visual insight into the dynamics in these systems.	0,1,0,0,0,0
Beyond the Erdős Matching Conjecture	A family $\mathcal F\subset {[n]\choose k}$ is $U(s,q)$ of for any $F_1,\ldots, F_s\in \mathcal F$ we have $|F_1\cup\ldots\cup F_s|\le q$. This notion generalizes the property of a family to be $t$-intersecting and to have matching number smaller than $s$. In this paper, we find the maximum $|\mathcal F|$ for $\mathcal F$ that are $U(s,q)$, provided $n>C(s,q)k$ with moderate $C(s,q)$. In particular, we generalize the result of the first author on the Erdős Matching Conjecture and prove a generalization of the Erdős-Ko-Rado theorem, which states that for $n> s^2k$ the largest family $\mathcal F\subset {[n]\choose k}$ with property $U(s,s(k-1)+1)$ is the star and is in particular intersecting. (Conversely, it is easy to see that any intersecting family in ${[n]\choose k}$ is $U(s,s(k-1)+1)$.) We investigate the case $k=3$ more thoroughly, showing that, unlike in the case of the Erdős Matching Conjecture, in general there may be $3$ extremal families.	1,0,0,0,0,0
Ultra-Low Noise Amplifier Design for Magnetic Resonance Imaging systems	This paper demonstrates designing and developing of an Ultra-Low Noise Amplifier which should potentially increase the sensitivity of the existing Magnetic Resonance Imaging (MRI) systems. The Design of the LNA is fabricated and characterized including matching and input high power protection circuits. The estimate improvement of SNR of the LNA in comparison to room temperature operation is taken here. The Cascode amplifier topology is chosen to be investigated for high performance Low Noise amplifier design and for the fabrication. The fabricated PCB layout of the Cascode LNA is tested by using measurement instruments Spectrum Analyser and Vector Network analyzer. The measurements of fabricated PCB layout of the Cascode LNA at room temperature had the following performance, the operation frequency is 32 MHz, the noise figure is 0.45 dB at source impedance 50 {\Omega}, the gain is 11.6 dB, the output return loss is 21.1 dB, and the input return loss 0.12 dB and it is unconditionally stable for up to 6 GHz band. The goal of the research is achieved where the Cascode LNA had improvement of SNR.	0,1,0,0,0,0
Correcting rural building annotations in OpenStreetMap using convolutional neural networks	Rural building mapping is paramount to support demographic studies and plan actions in response to crisis that affect those areas. Rural building annotations exist in OpenStreetMap (OSM), but their quality and quantity are not sufficient for training models that can create accurate rural building maps. The problems with these annotations essentially fall into three categories: (i) most commonly, many annotations are geometrically misaligned with the updated imagery; (ii) some annotations do not correspond to buildings in the images (they are misannotations or the buildings have been destroyed); and (iii) some annotations are missing for buildings in the images (the buildings were never annotated or were built between subsequent image acquisitions). First, we propose a method based on Markov Random Field (MRF) to align the buildings with their annotations. The method maximizes the correlation between annotations and a building probability map while enforcing that nearby buildings have similar alignment vectors. Second, the annotations with no evidence in the building probability map are removed. Third, we present a method to detect non-annotated buildings with predefined shapes and add their annotation. The proposed methodology shows considerable improvement in accuracy of the OSM annotations for two regions of Tanzania and Zimbabwe, being more accurate than state-of-the-art baselines.	1,0,0,0,0,0
L1188: a promising candidate of cloud-cloud collision triggering the formation of the low- and intermediate-mass stars	We present a new large-scale (4 square degrees) simultaneous $^{12}$CO, $^{13}$CO, and C$^{18}$O ($J$=1$-$0) mapping of L1188 with the PMO 13.7-m telescope. Our observations have revealed that L1188 consists of two nearly orthogonal filamentary molecular clouds at two clearly separated velocities. Toward the intersection showing large velocity spreads, we find several bridging features connecting the two clouds in velocity, and an open arc structure which exhibits high excitation temperatures, enhanced $^{12}$CO and $^{13}$CO emission, and broad $^{12}$CO line wings. This agrees with the scenario that the two clouds are colliding with each other. The distribution of young stellar object (YSO) candidates implies an enhancement of star formation in the intersection of the two clouds. We suggest that a cloud-cloud collision happened in L1188 about 1~Myr ago, possibly triggering the formation of low- and intermediate-mass YSOs in the intersection.	0,1,0,0,0,0
Analytical methods for vacuum simulations in high energy accelerators for future machines based on the LHC performance	The Future Circular Collider (FCC), currently in the design phase, will address many outstanding questions in particle physics. The technology to succeed in this 100 km circumference collider goes beyond present limits. Ultra-high vacuum conditions in the beam pipe is one essential requirement to provide a smooth operation. Different physics phenomena as photon-, ion- and electron- induced desorption and thermal outgassing of the chamber walls challenge this requirement. This paper presents an analytical model and a computer code PyVASCO that supports the design of a stable vacuum system by providing an overview of all the gas dynamics happening inside the beam pipes. A mass balance equation system describes the density distribution of the four dominating gas species $\text{H}_2, \text{CH}_4$, $\text{CO}$ and $\text{CO}_2$. An appropriate solving algorithm is discussed in detail and a validation of the model including a comparison of the output to the readings of LHC gauges is presented. This enables the evaluation of different designs for the FCC.	0,1,0,0,0,0
Abelian varieties isogenous to a power of an elliptic curve over a Galois extension	Given an elliptic curve $E/k$ and a Galois extension $k'/k$, we construct an exact functor from torsion-free modules over the endomorphism ring ${\rm End}(E_{k'})$ with a semilinear ${\rm Gal}(k'/k)$ action to abelian varieties over $k$ that are $k'$-isogenous to a power of $E$. As an application, we show that every elliptic curve with complex multiplication geometrically is isogenous over the ground field to one with complex multiplication by a maximal order.	0,0,1,0,0,0
Constraining black hole spins with low-frequency quasi-periodic oscillations in soft states	Black hole X-ray transients show a variety of state transitions during their outburst phases, characterized by changes in their spectral and timing properties. In particular, power density spectra (PDS) show quasi periodic oscillations (QPOs) that can be related to the accretion regime of the source. We looked for type-C QPOs in the disc-dominated state (i.e. the high soft state) and in the ultra-luminous state in the RXTE archival data of 12 transient black hole X-ray binaries known to show QPOs during their outbursts. We detected 6 significant QPOs in the soft state that can be classified as type-C QPOs. Under the assumption that the accretion disc in disc-dominated states extends down or close to the innermost stable circular orbit (ISCO) and that type-C QPOs would arise at the inner edge of the accretion flow, we use the relativistic precession model (RPM) to place constraints on the black hole spin. We were able to place lower limits on the spin value for all the 12 sources of our sample while we could place also an upper limit on the spin for 5 sources.	0,1,0,0,0,0
Extending applicability of bimetric theory: chameleon bigravity	This article extends bimetric formulations of massive gravity to make the mass of the graviton to depend on its environment. This minimal extension offers a novel way to reconcile massive gravity with local tests of general relativity without invoking the Vainshtein mechanism. On cosmological scales, it is argued that the model is stable and that it circumvents the Higuchi bound, hence relaxing the constraints on the parameter space. Moreover, with this extension the strong coupling scale is also environmentally dependent in such a way that it is kept sufficiently higher than the expansion rate all the way up to the very early universe, while the present graviton mass is low enough to be phenomenologically interesting. In this sense the extended bigravity theory serves as a partial UV completion of the standard bigravity theory. This extension is very generic and robust and a simple specific example is described.	0,1,0,0,0,0
Structure-Based Subspace Method for Multi-Channel Blind System Identification	In this work, a novel subspace-based method for blind identification of multichannel finite impulse response (FIR) systems is presented. Here, we exploit directly the impeded Toeplitz channel structure in the signal linear model to build a quadratic form whose minimization leads to the desired channel estimation up to a scalar factor. This method can be extended to estimate any predefined linear structure, e.g. Hankel, that is usually encountered in linear systems. Simulation findings are provided to highlight the appealing advantages of the new structure-based subspace (SSS) method over the standard subspace (SS) method in certain adverse identification scenarii.	1,0,0,1,0,0
Comparison results for first order linear operators with reflection and periodic boundary value conditions	This work is devoted to the study of the first order operator $x'(t)+m\,x(-t)$ coupled with periodic boundary value conditions. We describe the eigenvalues of the operator and obtain the expression of its related Green's function in the non resonant case. We also obtain the range of the values of the real parameter $m$ for which the integral kernel, which provides the unique solution, has constant sign. In this way, we automatically establish maximum and anti-maximum principles for the equation. Some applications to the existence of nonlinear periodic boundary value problems are showed.	0,0,1,0,0,0
Inferencing into the void: problems with implicit populations Comments on `Empirical software engineering experts on the use of students and professionals in experiments'	I welcome the contribution from Falessi et al. [1] hereafter referred to as F++ , and the ensuing debate. Experimentation is an important tool within empirical software engineering, so how we select participants is clearly a relevant question. Moreover as F++ point out, the question is considerably more nuanced than the simple dichotomy it might appear to be at first sight. This commentary is structured as follows. In Section 2 I briefly summarise the arguments of F++ and comment on their approach. Next, in Section 3, I take a step back to consider the nature of representativeness in inferential arguments and the need for careful definition. Then I give three examples of using different types of participant to consider impact. I conclude by arguing, largely in agreement with F++, that the question of whether student participants are representative or not depends on the target population. However, we need to give careful consideration to defining that population and, in particular, not to overlook the representativeness of tasks and environment. This is facilitated by explicit description of the target populations.	1,0,0,0,0,0
Bayesian model checking: A comparison of tests	Two procedures for checking Bayesian models are compared using a simple test problem based on the local Hubble expansion. Over four orders of magnitude, p-values derived from a global goodness-of-fit criterion for posterior probability density functions (Lucy 2017) agree closely with posterior predictive p-values. The former can therefore serve as an effective proxy for the difficult-to-calculate posterior predictive p-values.	0,1,0,1,0,0
Elliptic curves maximal over extensions of finite base fields	Given an elliptic curve $E$ over a finite field $\mathbb{F}_q$ we study the finite extensions $\mathbb{F}_{q^n}$ of $\mathbb{F}_q$ such that the number of $\mathbb{F}_{q^n}$-rational points on $E$ attains the Hasse upper bound. We obtain an upper bound on the degree $n$ for $E$ ordinary using an estimate for linear forms in logarithms, which allows us to compute the pairs of isogeny classes of such curves and degree $n$ for small $q$. Using a consequence of Schmidt's Subspace Theorem, we improve the upper bound to $n\leq 11$ for sufficiently large $q$. We also show that there are infinitely many isogeny classes of ordinary elliptic curves with $n=3$.	0,0,1,0,0,0
Vocabulary-informed Extreme Value Learning	The novel unseen classes can be formulated as the extreme values of known classes. This inspired the recent works on open-set recognition \cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no way of naming the novel unseen classes. To solve this problem, we propose the Extreme Value Learning (EVL) formulation to learn the mapping from visual feature to semantic space. To model the margin and coverage distributions of each class, the Vocabulary-informed Learning (ViL) is adopted by using vast open vocabulary in the semantic space. Essentially, by incorporating the EVL and ViL, we for the first time propose a novel semantic embedding paradigm -- Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual features into semantic space in a probabilistic way. The learned embedding can be directly used to solve supervised learning, zero-shot and open set recognition simultaneously. Experiments on two benchmark datasets demonstrate the effectiveness of proposed frameworks.	1,0,1,1,0,0
Approximate String Matching: Theory and Applications (La Recherche Approchée de Motifs : Théorie et Applications)	The approximate string matching is a fundamental and recurrent problem that arises in most computer science fields. This problem can be defined as follows: Let $D=\{x_1,x_2,\ldots x_d\}$ be a set of $d$ words defined on an alphabet $\Sigma$, let $q$ be a query defined also on $\Sigma$, and let $k$ be a positive integer. We want to build a data structure on $D$ capable of answering the following query: find all words in $D$ that are at most different from the query word $q$ with $k$ errors. In this thesis, we study the approximate string matching methods in dictionaries, texts, and indexes, to propose practical methods that solve this problem efficiently. We explore this problem in three complementary directions: 1) The approximate string matching in the dictionary. We propose two solutions to this problem, the first one uses hash tables for $k \geq 2$, the second uses the Trie and reverse Trie, and it is restricted to (k = 1). The two solutions are adaptable, without loss of performance, to the approximate string matching in a text. 2) The approximate string matching for \textit{autocompletion}, which is, find all suffixes of a given prefix that may contain errors. We give a new solution better in practice than all the previous proposed solutions. 3) The problem of the alignment of biological sequences can be interpreted as an approximate string matching problem. We propose a solution for peers and multiple sequences alignment. \medskip All the results obtained showed that our algorithms, give the best performance on sets of practical data (benchmark from the real world). All our methods are proposed as libraries, and they are published online.	1,0,0,0,0,0
Learning Depthwise Separable Graph Convolution from Data Manifold	Convolution Neural Network (CNN) has gained tremendous success in computer vision tasks with its outstanding ability to capture the local latent features. Recently, there has been an increasing interest in extending convolution operations to the non-Euclidean geometry. Although various types of convolution operations have been proposed for graphs or manifolds, their connections with traditional convolution over grid-structured data are not well-understood. In this paper, we show that depthwise separable convolution can be successfully generalized for the unification of both graph-based and grid-based convolution methods. Based on this insight we propose a novel Depthwise Separable Graph Convolution (DSGC) approach which is compatible with the tradition convolution network and subsumes existing convolution methods as special cases. It is equipped with the combined strengths in model expressiveness, compatibility (relatively small number of parameters), modularity and computational efficiency in training. Extensive experiments show the outstanding performance of DSGC in comparison with strong baselines on multi-domain benchmark datasets.	1,0,0,1,0,0
SMAGEXP: a galaxy tool suite for transcriptomics data meta-analysis	Bakground: With the proliferation of available microarray and high throughput sequencing experiments in the public domain, the use of meta-analysis methods increases. In these experiments, where the sample size is often limited, meta-analysis offers the possibility to considerably enhance the statistical power and give more accurate results. For those purposes, it combines either effect sizes or results of single studies in a appropriate manner. R packages metaMA and metaRNASeq perform meta-analysis on microarray and NGS data, respectively. They are not interchangeable as they rely on statistical modeling specific to each technology. Results: SMAGEXP (Statistical Meta-Analysis for Gene EXPression) integrates metaMA and metaRNAseq packages into Galaxy. We aim to propose a unified way to carry out meta-analysis of gene expression data, while taking care of their specificities. We have developed this tool suite to analyse microarray data from Gene Expression Omnibus (GEO) database or custom data from affymetrix microarrays. These data are then combined to carry out meta-analysis using metaMA package. SMAGEXP also offers to combine raw read counts from Next Generation Sequencing (NGS) experiments using DESeq2 and metaRNASeq package. In both cases, key values, independent from the technology type, are reported to judge the quality of the meta-analysis. These tools are available on the Galaxy main tool shed. Source code, help and installation instructions are available on github. Conclusion: The use of Galaxy offers an easy-to-use gene expression meta-analysis tool suite based on the metaMA and metaRNASeq packages.	0,0,0,1,1,0
Privacy Preserving and Collusion Resistant Energy Sharing	Energy has been increasingly generated or collected by different entities on the power grid (e.g., universities, hospitals and householdes) via solar panels, wind turbines or local generators in the past decade. With local energy, such electricity consumers can be considered as "microgrids" which can simulataneously generate and consume energy. Some microgrids may have excessive energy that can be shared to other power consumers on the grid. To this end, all the entities have to share their local private information (e.g., their local demand, local supply and power quality data) to each other or a third-party to find and implement the optimal energy sharing solution. However, such process is constrained by privacy concerns raised by the microgrids. In this paper, we propose a privacy preserving scheme for all the microgrids which can securely implement their energy sharing against both semi-honest and colluding adversaries. The proposed approach includes two secure communication protocols that can ensure quantified privacy leakage and handle collusions.	1,0,0,0,0,0
Quasi-two-dimensional Fermi surfaces with localized $f$ electrons in the layered heavy-fermion compound CePt$_2$In$_7$	We report measurements of the de Haas-van Alphen effect in the layered heavy-fermion compound CePt$_2$In$_7$ in high magnetic fields up to 35 T. Above an angle-dependent threshold field, we observed several de Haas-van Alphen frequencies originating from almost ideally two-dimensional Fermi surfaces. The frequencies are similar to those previously observed to develop only above a much higher field of 45 T, where a clear anomaly was detected and proposed to originate from a change in the electronic structure [M. M. Altarawneh et al., Phys. Rev. B 83, 081103 (2011)]. Our experimental results are compared with band structure calculations performed for both CePt$_2$In$_7$ and LaPt$_2$In$_7$, and the comparison suggests localized $f$ electrons in CePt$_2$In$_7$. This conclusion is further supported by comparing experimentally observed Fermi surfaces in CePt$_2$In$_7$ and PrPt$_2$In$_7$, which are found to be almost identical. The measured effective masses in CePt$_2$In$_7$ are only moderately enhanced above the bare electron mass $m_0$, from 2$m_0$ to 6$m_0$.	0,1,0,0,0,0
Development of verification system of socio-demographic data of virtual community member	The important task of developing verification system of data of virtual community member on the basis of computer-linguistic analysis of the content of a large sample of Ukrainian virtual communities is solved. The subject of research is methods and tools for verification of web-members socio-demographic characteristics based on computer-linguistic analysis of their communicative interaction results. The aim of paper is to verifying web-user personal data on the basis of computer-linguistic analysis of web-members information tracks. The structure of verification software for web-user profile is designed for a practical implementation of assigned tasks. The method of personal data verification of web-members by analyzing information track of virtual community member is conducted. For the first time the method for checking the authenticity of web members personal data, which helped to design of verification tool for socio-demographic characteristics of web-member is developed. The verification system of data of web-members, which forms the verified socio-demographic profiles of web-members, is developed as a result of conducted experiments. Also the user interface of the developed verification system web-members data is presented. Effectiveness and efficiency of use of the developed methods and means for solving tasks in web-communities administration is proved by their approbation. The number of false results of verification system is 18%.	1,0,0,0,0,0
The reparameterization trick for acquisition functions	Bayesian optimization is a sample-efficient approach to solving global optimization problems. Along with a surrogate model, this approach relies on theoretically motivated value heuristics (acquisition functions) to guide the search process. Maximizing acquisition functions yields the best performance; unfortunately, this ideal is difficult to achieve since optimizing acquisition functions per se is frequently non-trivial. This statement is especially true in the parallel setting, where acquisition functions are routinely non-convex, high-dimensional, and intractable. Here, we demonstrate how many popular acquisition functions can be formulated as Gaussian integrals amenable to the reparameterization trick and, ensuingly, gradient-based optimization. Further, we use this reparameterized representation to derive an efficient Monte Carlo estimator for the upper confidence bound acquisition function in the context of parallel selection.	1,0,0,1,0,0
Mean-Field Games with Differing Beliefs for Algorithmic Trading	Even when confronted with the same data, agents often disagree on a model of the real-world. Here, we address the question of how interacting heterogenous agents, who disagree on what model the real-world follows, optimize their trading actions. The market has latent factors that drive prices, and agents account for the permanent impact they have on prices. This leads to a large stochastic game, where each agents' performance criteria is computed under a different probability measure. We analyse the mean-field game (MFG) limit of the stochastic game and show that the Nash equilibria is given by the solution to a non-standard vector-valued forward-backward stochastic differential equation. Under some mild assumptions, we construct the solution in terms of expectations of the filtered states. We prove the MFG strategy forms an \epsilon-Nash equilibrium for the finite player game. Lastly, we present a least-squares Monte Carlo based algorithm for computing the optimal control and illustrate the results through simulation in market where agents disagree on the model.	0,0,0,0,0,1
State Sum Invariants of Three Manifolds from Spherical Multi-fusion Categories	We define a family of quantum invariants of closed oriented $3$-manifolds using spherical multi-fusion categories. The state sum nature of this invariant leads directly to $(2+1)$-dimensional topological quantum field theories ($\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury ($\text{TVBW}$) $\text{TQFT}$s from spherical fusion categories. The invariant is given as a state sum over labeled triangulations, which is mostly parallel to, but richer than the $\text{TVBW}$ approach in that here the labels live not only on $1$-simplices but also on $0$-simplices. It is shown that a multi-fusion category in general cannot be a spherical fusion category in the usual sense. Thus we introduce the concept of a spherical multi-fusion category by imposing a weakened version of sphericity. Besides containing the $\text{TVBW}$ theory, our construction also includes the recent higher gauge theory $(2+1)$-$\text{TQFT}$s given by Kapustin and Thorngren, which was not known to have a categorical origin before.	0,1,1,0,0,0
Profit Maximization for Online Advertising Demand-Side Platforms	We develop an optimization model and corresponding algorithm for the management of a demand-side platform (DSP), whereby the DSP aims to maximize its own profit while acquiring valuable impressions for its advertiser clients. We formulate the problem of profit maximization for a DSP interacting with ad exchanges in a real-time bidding environment in a cost-per-click/cost-per-action pricing model. Our proposed formulation leads to a nonconvex optimization problem due to the joint optimization over both impression allocation and bid price decisions. We use Lagrangian relaxation to develop a tractable convex dual problem, which, due to the properties of second-price auctions, may be solved efficiently with subgradient methods. We propose a two-phase solution procedure, whereby in the first phase we solve the convex dual problem using a subgradient algorithm, and in the second phase we use the previously computed dual solution to set bid prices and then solve a linear optimization problem to obtain the allocation probability variables. On several synthetic examples, we demonstrate that our proposed solution approach leads to superior performance over a baseline method that is used in practice.	1,0,1,0,0,0
Preliminary Experiments using Subjective Logic for the Polyrepresentation of Information Needs	According to the principle of polyrepresentation, retrieval accuracy may improve through the combination of multiple and diverse information object representations about e.g. the context of the user, the information sought, or the retrieval system. Recently, the principle of polyrepresentation was mathematically expressed using subjective logic, where the potential suitability of each representation for improving retrieval performance was formalised through degrees of belief and uncertainty. No experimental evidence or practical application has so far validated this model. We extend the work of Lioma et al. (2010), by providing a practical application and analysis of the model. We show how to map the abstract notions of belief and uncertainty to real-life evidence drawn from a retrieval dataset. We also show how to estimate two different types of polyrepresentation assuming either (a) independence or (b) dependence between the information objects that are combined. We focus on the polyrepresentation of different types of context relating to user information needs (i.e. work task, user background knowledge, ideal answer) and show that the subjective logic model can predict their optimal combination prior and independently to the retrieval process.	1,0,0,0,0,0
Mean square in the prime geodesic theorem	We prove upper bounds for the mean square of the remainder in the prime geodesic theorem, for every cofinite Fuchsian group, which improve on average on the best known pointwise bounds. The proof relies on the Selberg trace formula. For the modular group we prove a refined upper bound by using the Kuznetsov trace formula.	0,0,1,0,0,0
Robust Shape Estimation for 3D Deformable Object Manipulation	Existing shape estimation methods for deformable object manipulation suffer from the drawbacks of being off-line, model dependent, noise-sensitive or occlusion-sensitive, and thus are not appropriate for manipulation tasks requiring high precision. In this paper, we present a real-time shape estimation approach for autonomous robotic manipulation of 3D deformable objects. Our method fulfills all the requirements necessary for the high-quality deformable object manipulation in terms of being real-time, model-free and robust to noise and occlusion. These advantages are accomplished using a joint tracking and reconstruction framework, in which we track the object deformation by aligning a reference shape model with the stream input from the RGB-D camera, and simultaneously upgrade the reference shape model according to the newly captured RGB-D data. We have evaluated the quality and robustness of our real-time shape estimation pipeline on a set of deformable manipulation tasks implemented on physical robots. Videos are available at this https URL	1,0,0,0,0,0
Reply to comment on `Poynting flux in the neighbourhood of a point charge in arbitrary motion and the radiative power losses'	Doubts have been expressed in a comment (Eur. J. Phys., 39, 018001, 2018), about the tenability of the formulation for radiative losses in our recent published work (Eur. J. Phys., 37, 045210, 2016). We provide our reply to the comment. In particular, it is pointed out that one need to clearly distinguish between the rate of the energy-momentum being carried by the electromagnetic radiation to far-off space, and that of the mechanical energy-momentum losses being incurred by the radiating charge. It is also demonstrated that while the Poynting flux is always positive through a spherical surface centred on the retarded position of the charge, it could surprisingly be negative through a surface centred on the "present" position of the charge. It is further shown that the mysterious Schott term, hitherto thought in literature to arise from some acceleration-dependent energy in fields, is actually nothing but the difference in rate of change of energy in self-fields of the charge between the retarded and present times.	0,1,0,0,0,0
POMDP Structural Results for Controlled Sensing	This article provides a short review of some structural results in controlled sensing when the problem is formulated as a partially observed Markov decision process. In particular, monotone value functions, Blackwell dominance and quickest detection are described.	1,0,0,0,0,0
It Takes Two to Tango: Towards Theory of AI's Mind	Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one's own. Theory of Mind is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, there has been much emphasis on research to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds. In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. We further evaluate the role existing explanation (or interpretability) modalities play in helping humans build ToAIM. Explainable AI has received considerable scientific and popular attention in recent times. Surprisingly, we find that having access to the model's internal states - its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image - do not help people better predict its behavior.	1,0,0,0,0,0
Design of the Artificial: lessons from the biological roots of general intelligence	Our desire and fascination with intelligent machines dates back to the antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought (syllogism) and Heron of Alexandria's mechanical machines and automata. However, the quest for Artificial General Intelligence (AGI) is troubled with repeated failures of strategies and approaches throughout the history. This decade has seen a shift in interest towards bio-inspired software and hardware, with the assumption that such mimicry entails intelligence. Though these steps are fruitful in certain directions and have advanced automation, their singular design focus renders them highly inefficient in achieving AGI. Which set of requirements have to be met in the design of AGI? What are the limits in the design of the artificial? Here, a careful examination of computation in biological systems hints that evolutionary tinkering of contextual processing of information enabled by a hierarchical architecture is the key to build AGI.	1,1,0,0,0,0
Quasiparticles and charge transfer at the two surfaces of the honeycomb iridate Na$_2$IrO$_3$	Direct experimental investigations of the low-energy electronic structure of the Na$_2$IrO$_3$ iridate insulator are sparse and draw two conflicting pictures. One relies on flat bands and a clear gap, the other involves dispersive states approaching the Fermi level, pointing to surface metallicity. Here, by a combination of angle-resolved photoemission, photoemission electron microscopy, and x-ray absorption, we show that the correct picture is more complex and involves an anomalous band, arising from charge transfer from Na atoms to Ir-derived states. Bulk quasiparticles do exist, but in one of the two possible surface terminations the charge transfer is smaller and they remain elusive.	0,1,0,0,0,0
Considering Multiple Uncertainties in Stochastic Security-Constrained Unit Commitment Using Point Estimation Method	Security-Constrained Unit Commitment (SCUC) is one of the most significant problems in secure and optimal operation of modern electricity markets. New sources of uncertainties such as wind speed volatility and price-sensitive loads impose additional challenges to this large-scale problem. This paper proposes a new Stochastic SCUC using point estimation method to model the power system uncertainties more efficiently. Conventional scenario-based Stochastic SCUC approaches consider the Mont Carlo method; which presents additional computational burdens to this large-scale problem. In this paper we use point estimation instead of scenario generating to detract computational burdens of the problem. The proposed approach is implemented on a six-bus system and on a modified IEEE 118-bus system with 94 uncertain variables. The efficacy of proposed algorithm is confirmed, especially in the last case with notable reduction in computational burden without considerable loss of precision.	0,1,1,0,0,0
Construction of Directed 2K Graphs	We study the problem of constructing synthetic graphs that resemble real-world directed graphs in terms of their degree correlations. We define the problem of directed 2K construction (D2K) that takes as input the directed degree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to capture degree correlation specifically in directed graphs. We provide necessary and sufficient conditions to decide whether a target D2K is realizable, and we design an efficient algorithm that creates realizations with that target D2K. We evaluate our algorithm in creating synthetic graphs that target real-world directed graphs (such as Twitter) and we show that it brings significant benefits compared to state-of-the-art approaches.	1,0,0,0,0,0
Weighted Surface Algebras	A finite-dimensional algebra $A$ over an algebraically closed field $K$ is called periodic if it is periodic under the action of the syzygy operator in the category of $A-A-$ bimodules. The periodic algebras are self-injective and occur naturally in the study of tame blocks of group algebras, actions of finite groups on spheres, hypersurface singularities of finite Cohen-Macaulay type, and Jacobian algebras of quivers with potentials. Recently, the tame periodic algebras of polynomial growth have been classified and it is natural to attempt to classify all tame periodic algebras. We introduce the weighted surface algebras of triangulated surfaces with arbitrarily oriented triangles and describe their basic properties. In particular, we prove that all these algebras, except the singular tetrahedral algebras, are symmetric tame periodic algebras of period $4$. Moreover, we describe the socle deformations of the weighted surface algebras and prove that all these algebras are symmetric tame periodic algebras of period $4$. The main results of the paper form an important step towards a classification of all periodic symmetric tame algebras of non-polynomial growth, and lead to a complete description of all algebras of generalized quaternion type. Further, the orbit closures of the weighted surface algebras (and their socle deformations) in the affine varieties of associative $K$-algebra structures contain wide classes of tame symmetric algebras related to algebras of dihedral and semidihedral types, which occur in the study of blocks of group algebras with dihedral and semidihedral defect groups.	0,0,1,0,0,0
Electron-Hole Symmetry Breaking in Charge Transport in Nitrogen-Doped Graphene	Graphitic nitrogen-doped graphene is an excellent platform to study scattering processes of massless Dirac fermions by charged impurities, in which high mobility can be preserved due to the absence of lattice defects through direct substitution of carbon atoms in the graphene lattice by nitrogen atoms. In this work, we report on electrical and magnetotransport measurements of high-quality graphitic nitrogen-doped graphene. We show that the substitutional nitrogen dopants in graphene introduce atomically sharp scatters for electrons but long-range Coulomb scatters for holes and, thus, graphitic nitrogen-doped graphene exhibits clear electron-hole asymmetry in transport properties. Dominant scattering processes of charge carriers in graphitic nitrogen-doped graphene are analyzed. It is shown that the electron-hole asymmetry originates from a distinct difference in intervalley scattering of electrons and holes. We have also carried out the magnetotransport measurements of graphitic nitrogen-doped graphene at different temperatures and the temperature dependences of intervalley scattering, intravalley scattering and phase coherent scattering rates are extracted and discussed. Our results provide an evidence for the electron-hole asymmetry in the intervalley scattering induced by substitutional nitrogen dopants in graphene and shine a light on versatile and potential applications of graphitic nitrogen-doped graphene in electronic and valleytronic devices.	0,1,0,0,0,0
Ginzburg-Landau equations on Riemann surfaces of higher genus	We study the Ginzburg-Landau equations on Riemann surfaces of arbitrary genus. In particular: we explicitly construct the (local moduli space of gauge-equivalent) solutions in a neighbourhood of a constant curvature branch of solutions; in linearizing the problem, we find a relation with de Rham cohomology groups of the surface; we classify holomorphic structures on line bundles arising as solutions to the equations in terms of the degree, the Abel-Jacobi map, and symmetric products of the surface; we construct explicitly the automorphy factors and the equivariant connection on the trivial bundle over the Poincaré upper complex half plane.	0,0,1,0,0,0
Small Boxes Big Data: A Deep Learning Approach to Optimize Variable Sized Bin Packing	Bin Packing problems have been widely studied because of their broad applications in different domains. Known as a set of NP-hard problems, they have different vari- ations and many heuristics have been proposed for obtaining approximate solutions. Specifically, for the 1D variable sized bin packing problem, the two key sets of optimization heuristics are the bin assignment and the bin allocation. Usually the performance of a single static optimization heuristic can not beat that of a dynamic one which is tailored for each bin packing instance. Building such an adaptive system requires modeling the relationship between bin features and packing perform profiles. The primary drawbacks of traditional AI machine learnings for this task are the natural limitations of feature engineering, such as the curse of dimensionality and feature selection quality. We introduce a deep learning approach to overcome the drawbacks by applying a large training data set, auto feature selection and fast, accurate labeling. We show in this paper how to build such a system by both theoretical formulation and engineering practices. Our prediction system achieves up to 89% training accuracy and 72% validation accuracy to select the best heuristic that can generate a better quality bin packing solution.	1,0,0,1,0,0
Self corrective Perturbations for Semantic Segmentation and Classification	Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.	1,0,0,1,0,0
Virtual Crystals and Nakajima Monomials	An explicit description of the virtualization map for the (modified) Nakajima monomial model for crystals is given. We give an explicit description of the Lusztig data for modified Nakajima monomials in type $A_n$.	0,0,1,0,0,0
Fermionic projected entangled-pair states and topological phases	We study fermionic matrix product operator algebras and identify the associated algebraic data. Using this algebraic data we construct fermionic tensor network states in two dimensions that have non-trivial symmetry-protected or intrinsic topological order. The tensor network states allow us to relate physical properties of the topological phases to the underlying algebraic data. We illustrate this by calculating defect properties and modular matrices of supercohomology phases. Our formalism also captures Majorana defects as we show explicitly for a class of $\mathbb{Z}_2$ symmetry-protected and intrinsic topological phases. The tensor networks states presented here are well-suited for numerical applications and hence open up new possibilities for studying interacting fermionic topological phases.	0,1,0,0,0,0
Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation	Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.	0,0,0,1,0,0
Fractional quantum Hall systems near nematicity: bimetric theory, composite fermions, and Dirac brackets	We perform a detailed comparison of the Dirac composite fermion and the recently proposed bimetric theory for a quantum Hall Jain states near half filling. By tuning the composite Fermi liquid to the vicinity of a nematic phase transition, we find that the two theories are equivalent to each other. We verify that the single mode approximation for the response functions and the static structure factor becomes reliable near the phase transition. We show that the dispersion relation of the nematic mode near the phase transition can be obtained from the Dirac brackets between the components of the nematic order parameter. The dispersion is quadratic at low momenta and has a magnetoroton minimum at a finite momentum, which is not related to any nearby inhomogeneous phase.	0,1,0,0,0,0
Secure communications with cooperative jamming: Optimal power allocation and secrecy outage analysis	This paper studies the secrecy rate maximization problem of a secure wireless communication system, in the presence of multiple eavesdroppers. The security of the communication link is enhanced through cooperative jamming, with the help of multiple jammers. First, a feasibility condition is derived to achieve a positive secrecy rate at the destination. Then, we solve the original secrecy rate maximization problem, which is not convex in terms of power allocation at the jammers. To circumvent this non-convexity, the achievable secrecy rate is approximated for a given power allocation at the jammers and the approximated problem is formulated into a geometric programming one. Based on this approximation, an iterative algorithm has been developed to obtain the optimal power allocation at the jammers. Next, we provide a bisection approach, based on one-dimensional search, to validate the optimality of the proposed algorithm. In addition, by assuming Rayleigh fading, the secrecy outage probability (SOP) of the proposed cooperative jamming scheme is analyzed. More specifically, a single-integral form expression for SOP is derived for the most general case as well as a closed-form expression for the special case of two cooperative jammers and one eavesdropper. Simulation results have been provided to validate the convergence and the optimality of the proposed algorithm as well as the theoretical derivations of the presented SOP analysis.	1,0,1,0,0,0
Predicting shim gaps in aircraft assembly with machine learning and sparse sensing	A modern aircraft may require on the order of thousands of custom shims to fill gaps between structural components in the airframe that arise due to manufacturing tolerances adding up across large structures. These shims are necessary to eliminate gaps, maintain structural performance, and minimize pull-down forces required to bring the aircraft into engineering nominal configuration for peak aerodynamic efficiency. Gap filling is a time-consuming process, involving either expensive by-hand inspection or computations on vast quantities of measurement data from increasingly sophisticated metrology equipment. Either case amounts to significant delays in production, with much of the time spent in the critical path of aircraft assembly. This work presents an alternative strategy for predictive shimming, based on machine learning and sparse sensing to first learn gap distributions from historical data, and then design optimized sparse sensing strategies to streamline data collection and processing. This new approach is based on the assumption that patterns exist in shim distributions across aircraft, which may be mined and used to reduce the burden of data collection and processing in future aircraft. Specifically, robust principal component analysis is used to extract low-dimensional patterns in the gap measurements while rejecting outliers. Next, optimized sparse sensors are obtained that are most informative about the dimensions of a new aircraft in these low-dimensional principal components. We demonstrate the success of the proposed approach, called PIXel Identification Despite Uncertainty in Sensor Technology (PIXI-DUST), on historical production data from 54 representative Boeing commercial aircraft. Our algorithm successfully predicts $99\%$ of shim gaps within the desired measurement tolerance using $3\%$ of the laser scan points typically required; all results are cross-validated.	0,0,0,1,0,0
A Liouville theorem for the Euler equations in the plane	This paper is concerned with qualitative properties of bounded steady flows of an ideal incompressible fluid with no stagnation point in the two-dimensional plane R^2. We show that any such flow is a shear flow, that is, it is parallel to some constant vector. The proof of this Liouville-type result is firstly based on the study of the geometric properties of the level curves of the stream function and secondly on the derivation of some estimates on the at most logarithmic growth of the argument of the flow. These estimates lead to the conclusion that the streamlines of the flow are all parallel lines.	0,0,1,0,0,0
A Local Prime Factor Decomposition Algorithm for Strong Product Graphs	This work is concerned with the prime factor decomposition (PFD) of strong product graphs. A new quasi-linear time algorithm for the PFD with respect to the strong product for arbitrary, finite, connected, undirected graphs is derived. Moreover, since most graphs are prime although they can have a product-like structure, also known as approximate graph products, the practical application of the well-known "classical" prime factorization algorithm is strictly limited. This new PFD algorithm is based on a local approach that covers a graph by small factorizable subgraphs and then utilizes this information to derive the global factors. Therefore, we can take advantage of this approach and derive in addition a method for the recognition of approximate graph products.	1,0,0,0,0,0
Symplectic integrators for second-order linear non-autonomous equations	Two families of symplectic methods specially designed for second-order time-dependent linear systems are presented. Both are obtained from the Magnus expansion of the corresponding first-order equation, but otherwise they differ in significant aspects. The first family is addressed to problems with low to moderate dimension, whereas the second is more appropriate when the dimension is large, in particular when the system corresponds to a linear wave equation previously discretised in space. Several numerical experiments illustrate the main features of the new schemes.	0,0,1,0,0,0
Error Analysis of the Stochastic Linear Feedback Particle Filter	This paper is concerned with the convergence and long-term stability analysis of the feedback particle filter (FPF) algorithm. The FPF is an interacting system of $N$ particles where the interaction is designed such that the empirical distribution of the particles approximates the posterior distribution. It is known that in the mean-field limit ($N=\infty$), the distribution of the particles is equal to the posterior distribution. However little is known about the convergence to the mean-field limit. In this paper, we consider the FPF algorithm for the linear Gaussian setting. In this setting, the algorithm is similar to the ensemble Kalman-Bucy filter algorithm. Although these algorithms have been numerically evaluated and widely used in applications, their convergence and long-term stability analysis remains an active area of research. In this paper, we show that, (i) the mean-field limit is well-defined with a unique strong solution; (ii) the mean-field process is stable with respect to the initial condition; (iii) we provide conditions such that the finite-$N$ system is long term stable and we obtain some mean-squared error estimates that are uniform in time.	1,0,0,0,0,0
Ultra-Wideband Aided Fast Localization and Mapping System	This paper proposes an ultra-wideband (UWB) aided localization and mapping system that leverages on inertial sensor and depth camera. Inspired by the fact that visual odometry (VO) system, regardless of its accuracy in the short term, still faces challenges with accumulated errors in the long run or under unfavourable environments, the UWB ranging measurements are fused to remove the visual drift and improve the robustness. A general framework is developed which consists of three parallel threads, two of which carry out the visual-inertial odometry (VIO) and UWB localization respectively. The other mapping thread integrates visual tracking constraints into a pose graph with the proposed smooth and virtual range constraints, such that an optimization is performed to provide robust trajectory estimation. Experiments show that the proposed system is able to create dense drift-free maps in real-time even running on an ultra-low power processor in featureless environments.	1,0,0,0,0,0
Cross-validation in high-dimensional spaces: a lifeline for least-squares models and multi-class LDA	Least-squares models such as linear regression and Linear Discriminant Analysis (LDA) are amongst the most popular statistical learning techniques. However, since their computation time increases cubically with the number of features, they are inefficient in high-dimensional neuroimaging datasets. Fortunately, for k-fold cross-validation, an analytical approach has been developed that yields the exact cross-validated predictions in least-squares models without explicitly training the model. Its computation time grows with the number of test samples. Here, this approach is systematically investigated in the context of cross-validation and permutation testing. LDA is used exemplarily but results hold for all other least-squares methods. Furthermore, a non-trivial extension to multi-class LDA is formally derived. The analytical approach is evaluated using complexity calculations, simulations, and permutation testing of an EEG/MEG dataset. Depending on the ratio between features and samples, the analytical approach is up to 10,000x faster than the standard approach (retraining the model on each training set). This allows for a fast cross-validation of least-squares models and multi-class LDA in high-dimensional data, with obvious applications in multi-dimensional datasets, Representational Similarity Analysis, and permutation testing.	0,0,0,1,0,0
Deformable Generator Network: Unsupervised Disentanglement of Appearance and Geometry	We propose a deformable generator model to disentangle the appearance and geometric information from images into two independent latent vectors. The appearance generator produces the appearance information, including color, illumination, identity or category, of an image. The geometric generator produces displacement of the coordinates of each pixel and performs geometric warping, such as stretching and rotation, on the appearance generator to obtain the final synthesized image. The proposed model can learn both representations from image data in an unsupervised manner. The learned geometric generator can be conveniently transferred to the other image datasets to facilitate downstream AI tasks.	0,0,0,1,0,0
A Proximity-Aware Hierarchical Clustering of Faces	In this paper, we propose an unsupervised face clustering algorithm called "Proximity-Aware Hierarchical Clustering" (PAHC) that exploits the local structure of deep representations. In the proposed method, a similarity measure between deep features is computed by evaluating linear SVM margins. SVMs are trained using nearest neighbors of sample data, and thus do not require any external training data. Clusters are then formed by thresholding the similarity scores. We evaluate the clustering performance using three challenging unconstrained face datasets, including Celebrity in Frontal-Profile (CFP), IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3) datasets. Experimental results demonstrate that the proposed approach can achieve significant improvements over state-of-the-art methods. Moreover, we also show that the proposed clustering algorithm can be applied to curate a set of large-scale and noisy training dataset while maintaining sufficient amount of images and their variations due to nuisance factors. The face verification performance on JANUS CS3 improves significantly by finetuning a DCNN model with the curated MS-Celeb-1M dataset which contains over three million face images.	1,0,0,0,0,0
Mining Significant Microblogs for Misinformation Identification: An Attention-based Approach	With the rapid growth of social media, massive misinformation is also spreading widely on social media, such as microblog, and bring negative effects to human life. Nowadays, automatic misinformation identification has drawn attention from academic and industrial communities. For an event on social media usually consists of multiple microblogs, current methods are mainly based on global statistical features. However, information on social media is full of noisy and outliers, which should be alleviated. Moreover, most of microblogs about an event have little contribution to the identification of misinformation, where useful information can be easily overwhelmed by useless information. Thus, it is important to mine significant microblogs for a reliable misinformation identification method. In this paper, we propose an Attention-based approach for Identification of Misinformation (AIM). Based on the attention mechanism, AIM can select microblogs with largest attention values for misinformation identification. The attention mechanism in AIM contains two parts: content attention and dynamic attention. Content attention is calculated based textual features of each microblog. Dynamic attention is related to the time interval between the posting time of a microblog and the beginning of the event. To evaluate AIM, we conduct a series of experiments on the Weibo dataset and the Twitter dataset, and the experimental results show that the proposed AIM model outperforms the state-of-the-art methods.	1,0,0,0,0,0
A Hardware Platform for Efficient Multi-Modal Sensing with Adaptive Approximation	We present Warp, a hardware platform to support research in approximate computing, sensor energy optimization, and energy-scavenged systems. Warp incorporates 11 state-of-the-art sensor integrated circuits, computation, and an energy-scavenged power supply, all within a miniature system that is just 3.6 cm x 3.3 cm x 0.5 cm. Warp's sensor integrated circuits together contain a total of 21 sensors with a range of precisions and accuracies for measuring eight sensing modalities of acceleration, angular rate, magnetic flux density (compass heading), humidity, atmospheric pressure (elevation), infrared radiation, ambient temperature, and color. Warp uses a combination of analog circuits and digital control to facilitate further tradeoffs between sensor and communication accuracy, energy efficiency, and performance. This article presents the design of Warp and presents an evaluation of our hardware implementation. The results show how Warp's design enables performance and energy efficiency versus ac- curacy tradeoffs.	1,0,0,0,0,0
Integrating Runtime Values with Source Code to Facilitate Program Comprehension	An inherently abstract nature of source code makes programs difficult to understand. In our research, we designed three techniques utilizing concrete values of variables and other expressions during program execution. RuntimeSearch is a debugger extension searching for a given string in all expressions at runtime. DynamiDoc generates documentation sentences containing examples of arguments, return values and state changes. RuntimeSamp augments source code lines in the IDE (integrated development environment) with sample variable values. In this post-doctoral article, we briefly describe these three approaches and related motivational studies, surveys and evaluations. We also reflect on the PhD study, providing advice for current students. Finally, short-term and long-term future work is described.	1,0,0,0,0,0
Distance covariance for stochastic processes	The distance covariance of two random vectors is a measure of their dependence. The empirical distance covariance and correlation can be used as statistical tools for testing whether two random vectors are independent. We propose an analogs of the distance covariance for two stochastic processes defined on some interval. Their empirical analogs can be used to test the independence of two processes.	0,0,1,1,0,0
Contraction par Frobenius et modules de Steinberg	For a reductive group G defined over an algebraically closed field of positive characteristic, we show that the Frobenius contraction functor of G-modules is right adjoint to the Frobenius twist of the modules tensored with the Steinberg module twice. It follows that the Frobenius contraction functor preserves injectivity, good filtrations, but not semisiplicity.	0,0,1,0,0,0
Improved Bounds for Online Dominating Sets of Trees	The online dominating set problem is an online variant of the minimum dominating set problem, which is one of the most important NP-hard problems on graphs. This problem is defined as follows: Given an undirected graph $G = (V, E)$, in which $V$ is a set of vertices and $E$ is a set of edges. We say that a set $D \subseteq V$ of vertices is a {\em dominating set} of $G$ if for each $v \in V \setminus D$, there exists a vertex $u \in D$ such that $\{ u, v \} \in E$. The vertices are revealed to an online algorithm one by one over time. When a vertex is revealed, edges between the vertex and vertices revealed in the past are also revealed. A revelaed subtree is connected at any time. Immediately after the revelation of each vertex, an online algorithm can choose vertices which were already revealed irrevocably and must maintain a dominating set of a graph revealed so far. The cost of an algorithm on a given tree is the number of vertices chosen by it, and its objective is to minimize the cost. Eidenbenz (Technical report, Institute of Theoretical Computer Science, ETH Zürich, 2002) and Boyar et al.\ (SWAT 2016) studied the case in which given graphs are trees. They designed a deterministic online algorithm whose competitive ratio is at most three, and proved that a lower bound on the competitive ratio of any deterministic algorithm is two. In this paper, we also focus on trees. We establish a matching lower bound for any deterministic algorithm. Moreover, we design a randomized online algorithm whose competitive ratio is at most $5/2 = 2.5$, and show that the competitive ratio of any randomized algorithm is at least $4/3 \approx 1.333$.	1,0,0,0,0,0
Modularity of complex networks models	Modularity is designed to measure the strength of division of a network into clusters (known also as communities). Networks with high modularity have dense connections between the vertices within clusters but sparse connections between vertices of different clusters. As a result, modularity is often used in optimization methods for detecting community structure in networks, and so it is an important graph parameter from a practical point of view. Unfortunately, many existing non-spatial models of complex networks do not generate graphs with high modularity; on the other hand, spatial models naturally create clusters. We investigate this phenomenon by considering a few examples from both sub-classes. We prove precise theoretical results for the classical model of random d-regular graphs as well as the preferential attachment model, and contrast these results with the ones for the spatial preferential attachment (SPA) model that is a model for complex networks in which vertices are embedded in a metric space, and each vertex has a sphere of influence whose size increases if the vertex gains an in-link, and otherwise decreases with time. The results obtained in this paper can be used for developing statistical tests for models selection and to measure statistical significance of clusters observed in complex networks.	0,0,1,0,0,0
A Dynamic Model of Central Counterparty Risk	We introduce a dynamic model of the default waterfall of derivatives CCPs and propose a risk sensitive method for sizing the initial margin (IM), and the default fund (DF) and its allocation among clearing members. Using a Markovian structure model of joint credit migrations, our evaluation of DF takes into account the joint credit quality of clearing members as they evolve over time. Another important aspect of the proposed methodology is the use of the time consistent dynamic risk measures for computation of IM and DF. We carry out a comprehensive numerical study, where, in particular, we analyze the advantages of the proposed methodology and its comparison with the currently prevailing methods used in industry.	0,0,0,0,0,1
A Survey of Model Compression and Acceleration for Deep Neural Networks	Deep convolutional neural networks (CNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past few years, tremendous progress has been made in this area. In this paper, we survey the recent advanced techniques for compacting and accelerating CNNs model developed. These techniques are roughly categorized into four schemes: parameter pruning and sharing, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and sharing will be described at the beginning, after that the other techniques will be introduced. For each scheme, we provide insightful analysis regarding the performance, related applications, advantages, and drawbacks etc. Then we will go through a few very recent additional successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrix, the main datasets used for evaluating the model performance and recent benchmarking efforts. Finally, we conclude this paper, discuss remaining challenges and possible directions on this topic.	1,0,0,0,0,0
Uniformly recurrent subgroups and the ideal structure of reduced crossed products	We study the ideal structure of reduced crossed product of topological dynamical systems of a countable discrete group. More concretely, for a compact Hausdorff space $X$ with an action of a countable discrete group $\Gamma$, we consider the absence of a non-zero ideals in the reduced crossed product $C(X) \rtimes_r \Gamma$ which has a zero intersection with $C(X)$. We characterize this condition by a property for amenable subgroups of the stabilizer subgroups of $X$ in terms of the Chabauty space of $\Gamma$. This generalizes Kennedy's algebraic characterization of the simplicity for a reduced group $\mathrm{C}^{*}$-algebra of a countable discrete group.	0,0,1,0,0,0
On-the-fly Operation Batching in Dynamic Computation Graphs	Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.	1,0,0,1,0,0
Theoretical investigation of excitonic magnetism in LaSrCoO$_{4}$	We use the LDA+U approach to search for possible ordered ground states of LaSrCoO$_4$. We find a staggered arrangement of magnetic multipoles to be stable over a broad range of Co $3d$ interaction parameters. This ordered state can be described as a spin-denity-wave-type condensate of $d_{xy} \otimes d_{x^2-y^2}$ excitons carrying spin $S=1$. Further, we construct an effective strong-coupling model, calculate the exciton dispersion and investigate closing of the exciton gap, which marks the exciton condensation instability. Comparing the layered LaSrCoO$_4$ with its pseudo cubic analog LaCoO$_3$, we find that for the same interaction parameters the excitonic gap is smaller (possibly vanishing) in the layered cobaltite.	0,1,0,0,0,0
The OGLE Collection of Variable Stars. Over 450 000 Eclipsing and Ellipsoidal Binary Systems Toward the Galactic Bulge	We present a collection of 450 598 eclipsing and ellipsoidal binary systems detected in the OGLE fields toward the Galactic bulge. The collection consists of binary systems of all types: detached, semi-detached, and contact eclipsing binaries, RS CVn stars, cataclysmic variables, HW Vir binaries, double periodic variables, and even planetary transits. For all stars we provide the I- and V-band time-series photometry obtained during the OGLE-II, OGLE-III, and OGLE-IV surveys. We discuss methods used to identify binary systems in the OGLE data and present several objects of particular interest.	0,1,0,0,0,0
Periodic fourth-order cubic NLS: Local well-posedness and Non-squeezing property	In this paper, we consider the cubic fourth-order nonlinear Schrödinger equation (4NLS) under the periodic boundary condition. We prove two results. One is the local well-posedness in $H^s$ with $-1/3 \le s < 0$ for the Cauchy problem of the Wick ordered 4NLS. The other one is the non-squeezing property for the flow map of 4NLS in the symplectic phase space $L^2(\mathbb{T})$. To prove the former we used the ideas introduced in [Takaoka and Tsutsumi 2004] and [Nakanish et al 2010], and to prove the latter we used the ideas in [Colliander et al 2005].	0,0,1,0,0,0
Projection Theorems of Divergences and Likelihood Maximization Methods	Projection theorems of divergences enable us to find reverse projection of a divergence on a specific statistical model as a forward projection of the divergence on a different but rather "simpler" statistical model, which, in turn, results in solving a system of linear equations. Reverse projection of divergences are closely related to various estimation methods such as the maximum likelihood estimation or its variants in robust statistics. We consider projection theorems of three parametric families of divergences that are widely used in robust statistics, namely the Rényi divergences (or the Cressie-Reed power divergences), density power divergences, and the relative $\alpha$-entropy (or the logarithmic density power divergences). We explore these projection theorems from the usual likelihood maximization approach and from the principle of sufficiency. In particular, we show the equivalence of solving the estimation problems by the projection theorems of the respective divergences and by directly solving the corresponding estimating equations. We also derive the projection theorem for the density power divergences.	0,0,1,1,0,0
A computer simulation of the Volga River hydrological regime: a problem of water-retaining dam optimal location	We investigate of a special dam optimal location at the Volga river in area of the Akhtuba left sleeve beginning (7 \, km to the south of the Volga Hydroelectric Power Station dam). We claim that a new water-retaining dam can resolve the key problem of the Volga-Akhtuba floodplain related to insufficient water amount during the spring flooding due to the overregulation of the Lower Volga. By using a numerical integration of Saint-Vacant equations we study the water dynamics across the northern part of the Volga-Akhtuba floodplain with taking into account its actual topography. As the result we found an amount of water $V_A$ passing to the Akhtuba during spring period for a given water flow through the Volga Hydroelectric Power Station (so-called hydrograph which characterises the water flow per unit of time). By varying the location of the water-retaining dam $ x_d, y_d $ we obtained various values of $V_A (x_d, y_d) $ as well as various flow spatial structure on the territory during the flood period. Gradient descent method provide us the dam coordinated with the maximum value of ${V_A}$. Such approach to the dam location choice let us to find the best solution, that the value $V_A$ increases by a factor of 2. Our analysis demonstrate a good potential of the numerical simulations in the field of hydraulic works.	1,0,0,0,0,0
Unbalancing Sets and an Almost Quadratic Lower Bound for Syntactically Multilinear Arithmetic Circuits	We prove a lower bound of $\Omega(n^2/\log^2 n)$ on the size of any syntactically multilinear arithmetic circuit computing some explicit multilinear polynomial $f(x_1, \ldots, x_n)$. Our approach expands and improves upon a result of Raz, Shpilka and Yehudayoff ([RSY08]), who proved a lower bound of $\Omega(n^{4/3}/\log^2 n)$ for the same polynomial. Our improvement follows from an asymptotically optimal lower bound for a generalized version of Galvin's problem in extremal set theory.	1,0,0,0,0,0
Accurate Pouring with an Autonomous Robot Using an RGB-D Camera	Robotic assistants in a home environment are expected to perform various complex tasks for their users. One particularly challenging task is pouring drinks into cups, which for successful completion, requires the detection and tracking of the liquid level during a pour to determine when to stop. In this paper, we present a novel approach to autonomous pouring that tracks the liquid level using an RGB-D camera and adapts the rate of pouring based on the liquid level feedback. We thoroughly evaluate our system on various types of liquids and under different conditions, conducting over 250 pours with a PR2 robot. The results demonstrate that our approach is able to pour liquids to a target height with an accuracy of a few millimeters.	1,0,0,0,0,0
Understanding Web Archiving Services and Their (Mis)Use on Social Media	Web archiving services play an increasingly important role in today's information ecosystem, by ensuring the continuing availability of information, or by deliberately caching content that might get deleted or removed. Among these, the Wayback Machine has been proactively archiving, since 2001, versions of a large number of Web pages, while newer services like archive.is allow users to create on-demand snapshots of specific Web pages, which serve as time capsules that can be shared across the Web. In this paper, we present a large-scale analysis of Web archiving services and their use on social media, shedding light on the actors involved in this ecosystem, the content that gets archived, and how it is shared. We crawl and study: 1) 21M URLs from archive.is, spanning almost two years, and 2) 356K archive.is plus 391K Wayback Machine URLs that were shared on four social networks: Reddit, Twitter, Gab, and 4chan's Politically Incorrect board (/pol/) over 14 months. We observe that news and social media posts are the most common types of content archived, likely due to their perceived ephemeral and/or controversial nature. Moreover, URLs of archiving services are extensively shared on "fringe" communities within Reddit and 4chan to preserve possibly contentious content. Lastly, we find evidence of moderators nudging or even forcing users to use archives, instead of direct links, for news sources with opposing ideologies, potentially depriving them of ad revenue.	1,0,0,0,0,0
Rgtsvm: Support Vector Machines on a GPU in R	Rgtsvm provides a fast and flexible support vector machine (SVM) implementation for the R language. The distinguishing feature of Rgtsvm is that support vector classification and support vector regression tasks are implemented on a graphical processing unit (GPU), allowing the libraries to scale to millions of examples with >100-fold improvement in performance over existing implementations. Nevertheless, Rgtsvm retains feature parity and has an interface that is compatible with the popular e1071 SVM package in R. Altogether, Rgtsvm enables large SVM models to be created by both experienced and novice practitioners.	1,0,0,1,0,0
Large Scale Empirical Risk Minimization via Truncated Adaptive Newton Method	We consider large scale empirical risk minimization (ERM) problems, where both the problem dimension and variable size is large. In these cases, most second order methods are infeasible due to the high cost in both computing the Hessian over all samples and computing its inverse in high dimensions. In this paper, we propose a novel adaptive sample size second-order method, which reduces the cost of computing the Hessian by solving a sequence of ERM problems corresponding to a subset of samples and lowers the cost of computing the Hessian inverse using a truncated eigenvalue decomposition. We show that while we geometrically increase the size of the training set at each stage, a single iteration of the truncated Newton method is sufficient to solve the new ERM within its statistical accuracy. Moreover, for a large number of samples we are allowed to double the size of the training set at each stage, and the proposed method subsequently reaches the statistical accuracy of the full training set approximately after two effective passes. In addition to this theoretical result, we show empirically on a number of well known data sets that the proposed truncated adaptive sample size algorithm outperforms stochastic alternatives for solving ERM problems.	1,0,1,1,0,0
Variations on a Visserian Theme	A first order theory T is said to be "tight" if for any two deductively closed extensions U and V of T (both of which are formulated in the language of T), U and V are bi-interpretable iff U = V. By a theorem of Visser, PA (Peano Arithmetic) is tight. Here we show that Z_2 (second order arithmetic), ZF (Zermelo-Fraenkel set theory), and KM (Kelley-Morse theory of classes) are also tight theories.	0,0,1,0,0,0
Map Memorization and Forgetting in the IARA Autonomous Car	In this work, we present a novel strategy for correcting imperfections in occupancy grid maps called map decay. The objective of map decay is to correct invalid occupancy probabilities of map cells that are unobservable by sensors. The strategy was inspired by an analogy between the memory architecture believed to exist in the human brain and the maps maintained by an autonomous vehicle. It consists in merging sensory information obtained during runtime (online) with a priori data from a high-precision map constructed offline. In map decay, cells observed by sensors are updated using traditional occupancy grid mapping techniques and unobserved cells are adjusted so that their occupancy probabilities tend to the values found in the offline map. This strategy is grounded in the idea that the most precise information available about an unobservable cell is the value found in the high-precision offline map. Map decay was successfully tested and is still in use in the IARA autonomous vehicle from Universidade Federal do Espírito Santo.	1,0,0,0,0,0
A study of sliding motion of a solid body on a rough surface with asymmetric friction	Recent studies show interest in materials with asymmetric friction forces. We investigate terminal motion of a solid body with circular contact area. We assume that friction forces are asymmetric orthotropic. Two cases of pressure distribution are analyzed: Hertz and Boussinesq laws. Equations for friction force and moment are formulated and solved for these cases. Numer- ical results show significant impact of the asymmetry of friction on the motion. Our results can be used for more accurate prediction of contact behavior of bodies made from new materials with asymmetric surface textures.	0,1,0,0,0,0
Node classification for signed networks using diffuse interface methods	Signed networks are a crucial tool when modeling friend and foe relationships. In contrast to classical undirected, weighted graphs, the edge weights for signed graphs are positive and negative. Crucial network properties are often derived from the study of the associated graph Laplacians. We here study several different signed network Laplacians with a focus on the task of classifying the nodes of the graph. We here extend a recently introduced technique based on a partial differential equation defined on the signed network, namely the Allen-Cahn-equation, to classify the nodes into two or more classes. We illustrate the performance of this approach on several real-world networks.	1,0,0,1,0,0
Order-disorder transitions in lattice gases with annealed reactive constraints	We study equilibrium properties of catalytically-activated $A + A \to \oslash$ reactions taking place on a lattice of adsorption sites. The particles undergo continuous exchanges with a reservoir maintained at a constant chemical potential $\mu$ and react when they appear at the neighbouring sites, provided that some reactive conditions are fulfilled. We model the latter in two different ways: In the Model I some fraction $p$ of the {\em bonds} connecting neighbouring sites possesses special catalytic properties such that any two $A$s appearing on the sites connected by such a bond instantaneously react and desorb. In the Model II some fraction $p$ of the adsorption {\em sites} possesses such properties and neighbouring particles react if at least one of them resides on a catalytic site. For the case of \textit{annealed} disorder in the distribution of the catalyst, which is tantamount to the situation when the reaction may take place at any point on the lattice but happens with a finite probability $p$, we provide an exact solution for both models for the interior of an infinitely large Cayley tree - the so-called Bethe lattice. We show that both models exhibit a rich critical behaviour: For the annealed Model I it is characterised by a transition into an ordered state and a re-entrant transition into a disordered phase, which both are continuous. For the annealed Model II, which represents a rather exotic model of statistical mechanics in which interactions of any particle with its environment have a peculiar Boolean form, the transition to an ordered state is always continuous, while the re-entrant transition into the disordered phase may be either continuous or discontinuous, depending on the value of $p$.	0,1,0,0,0,0
Tight contact structures on Seifert surface complements	We consider complements of standard Seifert surfaces of special alternating links. On these handlebodies, we use Honda's method to enumerate those tight contact structures whose dividing sets are isotopic to the link, and find their number to be the leading coefficient of the Alexander polynomial. The Euler classes of the contact structures are identified with hypertrees in a certain hypergraph. Using earlier work, this establishes a connection between contact topology and the Homfly polynomial. We also show that the contact invariants of our tight contact structures form a basis for sutured Floer homology. Finally, we relate our methods and results to Kauffman's formal knot theory.	0,0,1,0,0,0
Rapid Near-Neighbor Interaction of High-dimensional Data via Hierarchical Clustering	Calculation of near-neighbor interactions among high dimensional, irregularly distributed data points is a fundamental task to many graph-based or kernel-based machine learning algorithms and applications. Such calculations, involving large, sparse interaction matrices, expose the limitation of conventional data-and-computation reordering techniques for improving space and time locality on modern computer memory hierarchies. We introduce a novel method for obtaining a matrix permutation that renders a desirable sparsity profile. The method is distinguished by the guiding principle to obtain a profile that is block-sparse with dense blocks. Our profile model and measure capture the essential properties affecting space and time locality, and permit variation in sparsity profile without imposing a restriction to a fixed pattern. The second distinction lies in an efficient algorithm for obtaining a desirable profile, via exploring and exploiting multi-scale cluster structure hidden in but intrinsic to the data. The algorithm accomplishes its task with key components for lower-dimensional embedding with data-specific principal feature axes, hierarchical data clustering, multi-level matrix compression storage, and multi-level interaction computations. We provide experimental results from case studies with two important data analysis algorithms. The resulting performance is remarkably comparable to the BLAS performance for the best-case interaction governed by a regularly banded matrix with the same sparsity.	1,0,0,0,0,0
Uniform cohomological expansion of uniformly quasiregular mappings	Let $f\colon M \to M$ be a uniformly quasiregular self-mapping of a compact, connected, and oriented Riemannian $n$-manifold $M$ without boundary, $n\ge 2$. We show that, for $k \in \{0,\ldots, n\}$, the induced homomorphism $f^* \colon H^k(M;\mathbb{R}) \to H^k(M;\mathbb{R})$, where $H^k(M;\mathbb{R})$ is the $k$:th singular cohomology of $M$, is complex diagonalizable and the eigenvalues of $f^*$ have modulus $(\mathrm{deg}\ f)^{k/n}$. As an application, we obtain a degree restriction for uniformly quasiregular self-mappings of closed manifolds. In the proof of the main theorem, we use a Sobolev--de Rham cohomology based on conformally invariant differential forms and an induced push-forward operator.	0,0,1,0,0,0
Intermodulation distortion of actuated MEMS capacitive switches	For the first time, intermodulation distortion of micro-electromechanical capacitive switches in the actuated state was analyzed both theoretically and experimentally. The distortion, although higher than that of switches in the suspended state, was found to decrease with increasing bias voltage but to depend weakly on modulation frequencies between 55 kHz and 1.1 MHz. This dependence could be explained by the orders-of-magnitude increase of the spring constant when the switches were actuated. Additionally, the analysis suggested that increasing the spring constant and decreasing the contact roughness could improve the linearity of actuated switches. These results are critical to micro-electromechanical capacitive switches used in tuners, filters, phase shifters, etc. where the linearity of both suspended and actuated states are critical.	0,1,0,0,0,0
Artificial Intelligence Based Malware Analysis	Artificial intelligence methods have often been applied to perform specific functions or tasks in the cyber-defense realm. However, as adversary methods become more complex and difficult to divine, piecemeal efforts to understand cyber-attacks, and malware-based attacks in particular, are not providing sufficient means for malware analysts to understand the past, present and future characteristics of malware. In this paper, we present the Malware Analysis and Attributed using Genetic Information (MAAGI) system. The underlying idea behind the MAAGI system is that there are strong similarities between malware behavior and biological organism behavior, and applying biologically inspired methods to corpora of malware can help analysts better understand the ecosystem of malware attacks. Due to the sophistication of the malware and the analysis, the MAAGI system relies heavily on artificial intelligence techniques to provide this capability. It has already yielded promising results over its development life, and will hopefully inspire more integration between the artificial intelligence and cyber--defense communities.	1,0,0,0,0,0
Survey on Models and Techniques for Root-Cause Analysis	Automation and computer intelligence to support complex human decisions becomes essential to manage large and distributed systems in the Cloud and IoT era. Understanding the root cause of an observed symptom in a complex system has been a major problem for decades. As industry dives into the IoT world and the amount of data generated per year grows at an amazing speed, an important question is how to find appropriate mechanisms to determine root causes that can handle huge amounts of data or may provide valuable feedback in real-time. While many survey papers aim at summarizing the landscape of techniques for modelling system behavior and infering the root cause of a problem based in the resulting models, none of those focuses on analyzing how the different techniques in the literature fit growing requirements in terms of performance and scalability. In this survey, we provide a review of root-cause analysis, focusing on these particular aspects. We also provide guidance to choose the best root-cause analysis strategy depending on the requirements of a particular system and application.	1,0,0,0,0,0
Can Boltzmann Machines Discover Cluster Updates ?	Boltzmann machines are physics informed generative models with wide applications in machine learning. They can learn the probability distribution from an input dataset and generate new samples accordingly. Applying them back to physics, the Boltzmann machines are ideal recommender systems to accelerate Monte Carlo simulation of physical systems due to their flexibility and effectiveness. More intriguingly, we show that the generative sampling of the Boltzmann Machines can even discover unknown cluster Monte Carlo algorithms. The creative power comes from the latent representation of the Boltzmann machines, which learn to mediate complex interactions and identify clusters of the physical system. We demonstrate these findings with concrete examples of the classical Ising model with and without four spin plaquette interactions. Our results endorse a fresh research paradigm where intelligent machines are designed to create or inspire human discovery of innovative algorithms.	0,1,0,1,0,0
Measurement of the Planck constant at the National Institute of Standards and Technology from 2015 to 2017	Researchers at the National Institute of Standards and Technology(NIST) have measured the value of the Planck constant to be $h =6.626\,069\,934(89)\times 10^{-34}\,$J$\,$s (relative standard uncertainty $13\times 10^{-9}$). The result is based on over 10$\,$000 weighings of masses with nominal values ranging from 0.5$\,$kg to 2$\,$kg with the Kibble balance NIST-4. The uncertainty has been reduced by more than twofold relative to a previous determination because of three factors: (1) a much larger data set than previously available, allowing a more realistic, and smaller, Type A evaluation; (2) a more comprehensive measurement of the back action of the weighing current on the magnet by weighing masses up to 2$\,$kg, decreasing the uncertainty associated with magnet non-linearity; (3) a rigorous investigation of the dependence of the geometric factor on the coil velocity reducing the uncertainty assigned to time-dependent leakage of current in the coil.	0,1,0,0,0,0
Adaptive Algebraic Multiscale Solver for Compressible Flow in Heterogeneous Porous Media	This paper presents the development of an Adaptive Algebraic Multiscale Solver for Compressible flow (C-AMS) in heterogeneous porous media. Similar to the recently developed AMS for incompressible (linear) flows [Wang et al., JCP, 2014], C-AMS operates by defining primal and dual-coarse blocks on top of the fine-scale grid. These coarse grids facilitate the construction of a conservative (finite volume) coarse-scale system and the computation of local basis functions, respectively. However, unlike the incompressible (elliptic) case, the choice of equations to solve for basis functions in compressible problems is not trivial. Therefore, several basis function formulations (incompressible and compressible, with and without accumulation) are considered in order to construct an efficient multiscale prolongation operator. As for the restriction operator, C-AMS allows for both multiscale finite volume (MSFV) and finite element (MSFE) methods. Finally, in order to resolve high-frequency errors, fine-scale (pre- and post-) smoother stages are employed. In order to reduce computational expense, the C-AMS operators (prolongation, restriction, and smoothers) are updated adaptively. In addition to this, the linear system in the Newton-Raphson loop is infrequently updated. Systematic numerical experiments are performed to determine the effect of the various options, outlined above, on the C-AMS convergence behaviour. An efficient C-AMS strategy for heterogeneous 3D compressible problems is developed based on overall CPU times. Finally, C-AMS is compared against an industrial-grade Algebraic MultiGrid (AMG) solver. Results of this comparison illustrate that the C-AMS is quite efficient as a nonlinear solver, even when iterated to machine accuracy.	1,1,0,0,0,0
Testing High-dimensional Covariance Matrices under the Elliptical Distribution and Beyond	We study testing high-dimensional covariance matrices under a generalized elliptical model. The model accommodates several stylized facts of real data including heteroskedasticity, heavy-tailedness, asymmetry, etc. We consider the high-dimensional setting where the dimension $p$ and the sample size $n$ grow to infinity proportionally, and establish a central limit theorem for the {linear spectral statistic} of the sample covariance matrix based on self-normalized observations. The central limit theorem is different from the existing ones for the linear spectral statistic of the usual sample covariance matrix. Our tests based on the new central limit theorem neither assume a specific parametric distribution nor involve the kurtosis of data. Simulation studies show that our tests work well even when the fourth moment does not exist. Empirically, we analyze the idiosyncratic returns under the Fama-French three-factor model for S\&P 500 Financials sector stocks, and our tests reject the hypothesis that the idiosyncratic returns are uncorrelated.	0,0,1,1,0,0
Up-down colorings of virtual-link diagrams and the necessity of Reidemeister moves of type II	We introduce an up-down coloring of a virtual-link diagram. The colorabilities give a lower bound of the minimum number of Reidemeister moves of type II which are needed between two 2-component virtual-link diagrams. By using the notion of a quandle cocycle invariant, we determine the necessity of Reidemeister moves of type II for a pair of diagrams of the trivial virtual-knot. This implies that for any virtual-knot diagram $D$, there exists a diagram $D'$ representing the same virtual-knot such that any sequence of generalized Reidemeister moves between them includes at least one Reidemeister move of type II.	0,0,1,0,0,0
Function space analysis of deep learning representation layers	In this paper we propose a function space approach to Representation Learning and the analysis of the representation layers in deep learning architectures. We show how to compute a weak-type Besov smoothness index that quantifies the geometry of the clustering in the feature space. This approach was already applied successfully to improve the performance of machine learning algorithms such as the Random Forest and tree-based Gradient Boosting. Our experiments demonstrate that in well-known and well-performing trained networks, the Besov smoothness of the training set, measured in the corresponding hidden layer feature map representation, increases from layer to layer. We also contribute to the understanding of generalization by showing how the Besov smoothness of the representations, decreases as we add more mis-labeling to the training data. We hope this approach will contribute to the de-mystification of some aspects of deep learning.	1,0,0,1,0,0
Structured low rank decomposition of multivariate Hankel matrices	We study the decomposition of a multivariate Hankel matrix H\_$\sigma$ as a sum of Hankel matrices of small rank in correlation with the decomposition of its symbol $\sigma$ as a sum of polynomial-exponential series. We present a new algorithm to compute the low rank decomposition of the Hankel operator and the decomposition of its symbol exploiting the properties of the associated Artinian Gorenstein quotient algebra A\_$\sigma$. A basis of A\_$\sigma$ is computed from the Singular Value Decomposition of a sub-matrix of the Hankel matrix H\_$\sigma$. The frequencies and the weights are deduced from the generalized eigenvectors of pencils of shifted sub-matrices of H $\sigma$. Explicit formula for the weights in terms of the eigenvectors avoid us to solve a Vandermonde system. This new method is a multivariate generalization of the so-called Pencil method for solving Prony-type decomposition problems. We analyse its numerical behaviour in the presence of noisy input moments, and describe a rescaling technique which improves the numerical quality of the reconstruction for frequencies of high amplitudes. We also present a new Newton iteration, which converges locally to the closest multivariate Hankel matrix of low rank and show its impact for correcting errors on input moments.	0,0,1,0,0,0
Likelihood ratio test for variance components in nonlinear mixed effects models	Mixed effects models are widely used to describe heterogeneity in a population. A crucial issue when adjusting such a model to data consists in identifying fixed and random effects. From a statistical point of view, it remains to test the nullity of the variances of a given subset of random effects. Some authors have proposed to use the likelihood ratio test and have established its asymptotic distribution in some particular cases. Nevertheless, to the best of our knowledge, no general variance components testing procedure has been fully investigated yet. In this paper, we study the likelihood ratio test properties to test that the variances of a general subset of the random effects are equal to zero in both linear and nonlinear mixed effects model, extending the existing results. We prove that the asymptotic distribution of the test is a chi-bar-square distribution, that is to say a mixture of chi-square distributions, and we identify the corresponding weights. We highlight in particular that the limiting distribution depends on the presence of correlations between the random effects but not on the linear or nonlinear structure of the mixed effects model. We illustrate the finite sample size properties of the test procedure through simulation studies and apply the test procedure to two real datasets of dental growth and of coucal growth.	0,0,0,1,0,0
Impact Ionization in $β-Ga_2O_3$	A theoretical investigation of extremely high field transport in an emerging wide-bandgap material $\beta-Ga_2O_3$ is reported from first principles. The signature high-field effect explored here is impact ionization. Interaction between a valence-band electron and an excited electron is computed from the matrix elements of a screened Coulomb operator. Maximally localized Wannier functions (MLWF) are utilized in computing the impact ionization rates. A full-band Monte Carlo (FBMC) simulation is carried out incorporating the impact ionization rates, and electron-phonon scattering rates. This work brings out valuable insights on the impact ionization coefficient (IIC) of electrons in $\beta-Ga_2O_3$. The isolation of the $\Gamma$ point conduction band minimum by a significantly high energy from other satellite band pockets play a vital role in determining ionization co-efficients. IICs are calculated for electric fields ranging up to 8 MV/cm for different crystal directions. A Chynoweth fitting of the computed IICs is done to calibrate ionization models in device simulators.	0,1,0,0,0,0
Beyond perturbation 1: de Rham spaces	It is shown that if one uses the notion of infinity nilpotent elements due to Moerdijk and Reyes, instead of the usual definition of nilpotents to define reduced $C^\infty$-schemes, the resulting de Rham spaces are given as quotients by actions of germs of diagonals, instead of the formal neighbourhoods of the diagonals.	0,0,1,0,0,0
Unit circle rectification of the MVDR beamformer	The sample matrix inversion (SMI) beamformer implements Capon's minimum variance distortionless (MVDR) beamforming using the sample covariance matrix (SCM). In a snapshot limited environment, the SCM is poorly conditioned resulting in a suboptimal performance from the SMI beamformer. Imposing structural constraints on the SCM estimate to satisfy known theoretical properties of the ensemble MVDR beamformer mitigates the impact of limited snapshots on the SMI beamformer performance. Toeplitz rectification and bounding the norm of weight vector are common approaches for such constrains. This paper proposes the unit circle rectification technique which constraints the SMI beamformer to satisfy a property of the ensemble MVDR beamformer: for narrowband planewave beamforming on a uniform linear array, the zeros of the MVDR weight array polynomial must fall on the unit circle. Numerical simulations show that the resulting unit circle MVDR (UC MVDR) beamformer frequently improves the suppression of both discrete interferers and white background noise compared to the classic SMI beamformer. Moreover, the UC MVDR beamformer is shown to suppress discrete interferers better than the MVDR beamformer diagonally loaded to maximize the SINR.	1,0,0,0,0,0
Non-wetting drops at liquid interfaces: From liquid marbles to Leidenfrost drops	We consider the flotation of deformable, non-wetting drops on a liquid interface. We consider the deflection of both the liquid interface and the droplet itself in response to the buoyancy forces, density difference and the various surface tensions within the system. Our results suggest new insight into a range of phenomena in which such drops occur, including Leidenfrost droplets and floating liquid marbles. In particular, we show that the floating state of liquid marbles is very sensitive to the tension of the particle-covered interface and suggest that this sensitivity may make such experiments a useful assay of the properties of these complex interfaces.	0,1,0,0,0,0
Locally free actions of groupoids and proper topological correspondences	Let $(G,\alpha)$ and $(H,\beta)$ be locally compact Hausdorff groupoids with Haar systems, and let $(X,\lambda)$ be a topological correspondence from $(G,\alpha)$ to $(H,\beta)$ which induce the ${C}^*$-correspondence $\mathcal{H}(X)\colon {C}^*(G,\alpha)\to {C}^*(H,\beta)$. We give sufficient topological conditions which when satisfied the ${C}^*$-correspondence $\mathcal{H}(X)$ is proper, that is, the ${C}^*$-algebra ${C}^*(G,\alpha)$ acts on the Hilbert ${C}^*(H,\beta)$-module ${H}(X)$ via the comapct operators. Thus a proper topological correspondence produces an element in ${KK}({C}^*(G,\alpha),{C}^*(H,\beta))$.	0,0,1,0,0,0
Imbalanced Malware Images Classification: a CNN based Approach	Deep convolutional neural networks (CNNs) can be applied to malware binary detection through images classification. The performance, however, is degraded due to the imbalance of malware families (classes). To mitigate this issue, we propose a simple yet effective weighted softmax loss which can be employed as the final layer of deep CNNs. The original softmax loss is weighted, and the weight value can be determined according to class size. A scaling parameter is also included in computing the weight. Proper selection of this parameter has been studied and an empirical option is given. The weighted loss aims at alleviating the impact of data imbalance in an end-to-end learning fashion. To validate the efficacy, we deploy the proposed weighted loss in a pre-trained deep CNN model and fine-tune it to achieve promising results on malware images classification. Extensive experiments also indicate that the new loss function can fit other typical CNNs with an improved classification performance.	1,0,0,1,0,0
Parametric Analysis of Cherenkov Light LDF from EAS for High Energy Gamma Rays and Nuclei: Ways of Practical Application	In this paper we propose a 'knee-like' approximation of the lateral distribution of the Cherenkov light from extensive air showers in the energy range 30-3000 TeV and study a possibility of its practical application in high energy ground-based gamma-ray astronomy experiments (in particular, in TAIGA-HiSCORE). The approximation has a very good accuracy for individual showers and can be easily simplified for practical application in the HiSCORE wide angle timing array in the condition of a limited number of triggered stations.	0,1,0,0,0,0
Bases of standard modules for affine Lie algebras of type $C_\ell^{(1)}$	Feigin-Stoyanovsky's type subspaces for affine Lie algebras of type $C_\ell^{(1)}$ have monomial bases with a nice combinatorial description. We describe bases of whole standard modules in terms of semi-infinite monomials obtained as "a limit of translations" of bases for Feigin-Stoyanovsky's type subspaces.	0,0,1,0,0,0
Some remarkable infinite product identities involving Fibonacci and Lucas numbers	By applying the classic telescoping summation formula and its variants to identities involving inverse hyperbolic tangent functions having inverse powers of the golden ratio as arguments and employing subtle properties of the Fibonacci and Lucas numbers, we derive interesting general infinite product identities involving these numbers.	0,0,1,0,0,0
Topological Sieving of Rings According to their Rigidity	We present a novel mechanism for resolving the mechanical rigidity of nanoscopic circular polymers that flow in a complex environment. The emergence of a regime of negative differential mobility induced by topological interactions between the rings and the substrate is the key mechanism for selective sieving of circular polymers with distinct flexibilities. A simple model accurately describes the sieving process observed in molecular dynamics simulations and yields experimentally verifiable analytical predictions, which can be used as a reference guide for improving filtration procedures of circular filaments. The topological sieving mechanism we propose ought to be relevant also in probing the microscopic details of complex substrates.	0,0,0,0,1,0
Kernel Robust Bias-Aware Prediction under Covariate Shift	Under covariate shift, training (source) data and testing (target) data differ in input space distribution, but share the same conditional label distribution. This poses a challenging machine learning task. Robust Bias-Aware (RBA) prediction provides the conditional label distribution that is robust to the worstcase logarithmic loss for the target distribution while matching feature expectation constraints from the source distribution. However, employing RBA with insufficient feature constraints may result in high certainty predictions for much of the source data, while leaving too much uncertainty for target data predictions. To overcome this issue, we extend the representer theorem to the RBA setting, enabling minimization of regularized expected target risk by a reweighted kernel expectation under the source distribution. By applying kernel methods, we establish consistency guarantees and demonstrate better performance of the RBA classifier than competing methods on synthetically biased UCI datasets as well as datasets that have natural covariate shift.	1,0,0,1,0,0
A Privacy-preserving Community-based P2P OSNs Using Broadcast Encryption Supporting Recommendation Mechanism	Online Social Networks (OSNs) have become one of the most important activities on the Internet, such as Facebook and Google+. However, security and privacy have become major concerns in existing C/S based OSNs. In this paper, we propose a novel scheme called a Privacy-preserving Community-based P2P OSNs Using Broadcast Encryption Supporting Recommendation Mechanism (PCBE) that supports cross-platform availability in stringent privacy requirements. For the first time, we introduce recommendation mechanism into a privacy-preserving P2P based OSNs, in which we firstly employ the Open Directory Project to generate user interest model. We firstly introduce broadcast encryption into P2P community-based social networks together with reputation mechanism to decrease the system overhead. We formulate the security requirements and design goals for privacy- preserving P2P based OSNs supporting recommendation mechanism. The RESTful web-services help to ensure cross-platform availability and transmission security. As a result, thorough security analysis and performance evaluation on experiments demonstrate that the PCBE scheme indeed accords with our proposed design goals.	1,0,0,0,0,0
Empirical analysis of non-linear activation functions for Deep Neural Networks in classification tasks	We provide an overview of several non-linear activation functions in a neural network architecture that have proven successful in many machine learning applications. We conduct an empirical analysis on the effectiveness of using these function on the MNIST classification task, with the aim of clarifying which functions produce the best results overall. Based on this first set of results, we examine the effects of building deeper architectures with an increasing number of hidden layers. We also survey the impact of using, on the same task, different initialisation schemes for the weights of our neural network. Using these sets of experiments as a base, we conclude by providing a optimal neural network architecture that yields impressive results in accuracy on the MNIST classification task.	1,0,0,1,0,0
Magnetization dynamics of weakly interacting sub-100 nm square artificial spin ices	Artificial Spin Ice (ASI), consisting of a two dimensional array of nanoscale magnetic elements, provides a fascinating opportunity to observe the physics of out of equilibrium systems. Initial studies concentrated on the static, frozen state, whilst more recent studies have accessed the out-of-equilibrium dynamic, fluctuating state. This opens up exciting possibilities such as the observation of systems exploring their energy landscape through monopole quasiparticle creation, potentially leading to ASI magnetricity, and to directly observe unconventional phase transitions. In this work we have measured and analysed the magnetic relaxation of thermally active ASI systems by means of SQUID magnetometry. We have investigated the effect of the interaction strength on the magnetization dynamics at different temperatures in the range where the nanomagnets are thermally active and have observed that they follow an Arrhenius-type Néel-Brown behaviour. An unexpected negative correlation of the average blocking temperature with the interaction strength is also observed, which is supported by Monte Carlo simulations. The magnetization relaxation measurements show faster relaxation for more strongly coupled nanoelements with similar dimensions. The analysis of the stretching exponents obtained from the measurements suggest 1-D chain-like magnetization dynamics. This indicates that the nature of the interactions between nanoelements lowers the dimensionality of the ASI from 2-D to 1-D. Finally, we present a way to quantify the effective interaction energy of a square ASI system, and compare it to the interaction energy calculated from a simple dipole model and also to the magnetostatic energy computed with micromagnetic simulations.	0,1,0,0,0,0
Subject Selection on a Riemannian Manifold for Unsupervised Cross-subject Seizure Detection	Inter-subject variability between individuals poses a challenge in inter-subject brain signal analysis problems. A new algorithm for subject-selection based on clustering covariance matrices on a Riemannian manifold is proposed. After unsupervised selection of the subsets of relevant subjects, data in a cluster is mapped to a tangent space at the mean point of covariance matrices in that cluster and an SVM classifier on labeled data from relevant subjects is trained. Experiment on an EEG seizure database shows that the proposed method increases the accuracy over state-of-the-art from 86.83% to 89.84% and specificity from 87.38% to 89.64% while reducing the false positive rate/hour from 0.8/hour to 0.77/hour.	1,0,0,1,0,0
The stratified micro-randomized trial design: sample size considerations for testing nested causal effects of time-varying treatments	Technological advancements in the field of mobile devices and wearable sensors have helped overcome obstacles in the delivery of care, making it possible to deliver behavioral treatments anytime and anywhere. Increasingly the delivery of these treatments is triggered by predictions of risk or engagement which may have been impacted by prior treatments. Furthermore the treatments are often designed to have an impact on individuals over a span of time during which subsequent treatments may be provided. Here we discuss our work on the design of a mobile health smoking cessation experimental study in which two challenges arose. First the randomizations to treatment should occur at times of stress and second the outcome of interest accrues over a period that may include subsequent treatment. To address these challenges we develop the "stratified micro-randomized trial," in which each individual is randomized among treatments at times determined by predictions constructed from outcomes to prior treatment and with randomization probabilities depending on these outcomes. We define both conditional and marginal proximal treatment effects. Depending on the scientific goal these effects may be defined over a period of time during which subsequent treatments may be provided. We develop a primary analysis method and associated sample size formulae for testing these effects.	0,0,0,1,0,0
An analytical Model which Determines the Apparent T1 for Modified Look-Locker Inversion Recovery (MOLLI) -- Analysis of the Longitudinal Relaxation under the Influence of Discontinuous Balanced and Spoiled Gradient Echo Readouts	Quantitative nuclear magnetic resonance imaging (MRI) shifts more and more into the focus of clinical research. Especially determination of relaxation times without/and with contrast agents becomes the foundation of tissue characterization, e.g. in cardiac MRI for myocardial fibrosis. Techniques which assess longitudinal relaxation times rely on repetitive application of readout modules, which are interrupted by free relaxation periods, e.g. the Modified Look-Locker Inversion Recovery = MOLLI sequence. These discontinuous sequences reveal an apparent relaxation time, and, by techniques extrapolated from continuous readout sequences, the real T1 is determined. What is missing is a rigorous analysis of the dependence of the apparent relaxation time on its real partner, readout sequence parameters and biological parameters as heart rate. This is provided in this paper for the discontinuous balanced steady state free precession (bSSFP) and spoiled gradient echo readouts. It turns out that the apparente longitudinal relaxation rate is the time average of the relaxation rates during the readout module, and free relaxation period. Knowing the heart rate our results vice versa allow to determine the real T1 from its measured apparent partner.	0,1,0,0,0,0
Turing Completeness of Finite, Epistemic Programs	In this note, we show the class of finite, epistemic programs to be Turing complete. Epistemic programs is a widely used update mechanism used in epistemic logic, where it such are a special type of action models: One which does not contain postconditions.	1,0,0,0,0,0
Efficient Reinforcement Learning via Initial Pure Exploration	In several realistic situations, an interactive learning agent can practice and refine its strategy before going on to be evaluated. For instance, consider a student preparing for a series of tests. She would typically take a few practice tests to know which areas she needs to improve upon. Based of the scores she obtains in these practice tests, she would formulate a strategy for maximizing her scores in the actual tests. We treat this scenario in the context of an agent exploring a fixed-horizon episodic Markov Decision Process (MDP), where the agent can practice on the MDP for some number of episodes (not necessarily known in advance) before starting to incur regret for its actions. During practice, the agent's goal must be to maximize the probability of following an optimal policy. This is akin to the problem of Pure Exploration (PE). We extend the PE problem of Multi Armed Bandits (MAB) to MDPs and propose a Bayesian algorithm called Posterior Sampling for Pure Exploration (PSPE), which is similar to its bandit counterpart. We show that the Bayesian simple regret converges at an optimal exponential rate when using PSPE. When the agent starts being evaluated, its goal would be to minimize the cumulative regret incurred. This is akin to the problem of Reinforcement Learning (RL). The agent uses the Posterior Sampling for Reinforcement Learning algorithm (PSRL) initialized with the posteriors of the practice phase. We hypothesize that this PSPE + PSRL combination is an optimal strategy for minimizing regret in RL problems with an initial practice phase. We show empirical results which prove that having a lower simple regret at the end of the practice phase results in having lower cumulative regret during evaluation.	1,0,0,1,0,0
Advanced Steel Microstructural Classification by Deep Learning Methods	The inner structure of a material is called microstructure. It stores the genesis of a material and determines all its physical and chemical properties. While microstructural characterization is widely spread and well known, the microstructural classification is mostly done manually by human experts, which gives rise to uncertainties due to subjectivity. Since the microstructure could be a combination of different phases or constituents with complex substructures its automatic classification is very challenging and only a few prior studies exist. Prior works focused on designed and engineered features by experts and classified microstructures separately from the feature extraction step. Recently, Deep Learning methods have shown strong performance in vision applications by learning the features from data together with the classification step. In this work, we propose a Deep Learning method for microstructural classification in the examples of certain microstructural constituents of low carbon steel. This novel method employs pixel-wise segmentation via Fully Convolutional Neural Networks (FCNN) accompanied by a max-voting scheme. Our system achieves 93.94% classification accuracy, drastically outperforming the state-of-the-art method of 48.89% accuracy. Beyond the strong performance of our method, this line of research offers a more robust and first of all objective way for the difficult task of steel quality appreciation.	1,1,0,0,0,0
Cosmological searches for a non-cold dark matter component	We explore an extended cosmological scenario where the dark matter is an admixture of cold and additional non-cold species. The mass and temperature of the non-cold dark matter particles are extracted from a number of cosmological measurements. Among others, we consider tomographic weak lensing data and Milky Way dwarf satellite galaxy counts. We also study the potential of these scenarios in alleviating the existing tensions between local measurements and Cosmic Microwave Background (CMB) estimates of the $S_8$ parameter, with $S_8=\sigma_8\sqrt{\Omega_m}$, and of the Hubble constant $H_0$. In principle, a sub-dominant, non-cold dark matter particle with a mass $m_X\sim$~keV, could achieve the goals above. However, the preferred ranges for its temperature and its mass are different when extracted from weak lensing observations and from Milky Way dwarf satellite galaxy counts, since these two measurements require suppressions of the matter power spectrum at different scales. Therefore, solving simultaneously the CMB-weak lensing tensions and the small scale crisis in the standard cold dark matter picture via only one non-cold dark matter component seems to be challenging.	0,1,0,0,0,0
Index of Dirac operators and classification of topological insulators	Real and complex Clifford bundles and Dirac operators defined on them are considered. By using the index theorems of Dirac operators, table of topological invariants is constructed from the Clifford chessboard. Through the relations between K-theory groups, Grothendieck groups and symmetric spaces, the periodic table of topological insulators and superconductors is obtained. This gives the result that the periodic table of real and complex topological phases is originated from the Clifford chessboard and index theorems.	0,1,0,0,0,0
Phase Congruency Parameter Optimization for Enhanced Detection of Image Features for both Natural and Medical Applications	Following the presentation and proof of the hypothesis that image features are particularly perceived at points where the Fourier components are maximally in phase, the concept of phase congruency (PC) is introduced. Subsequently, a two-dimensional multi-scale phase congruency (2D-MSPC) is developed, which has been an important tool for detecting and evaluation of image features. However, the 2D-MSPC requires many parameters to be appropriately tuned for optimal image features detection. In this paper, we defined a criterion for parameter optimization of the 2D-MSPC, which is a function of its maximum and minimum moments. We formulated the problem in various optimal and suboptimal frameworks, and discussed the conditions and features of the suboptimal solutions. The effectiveness of the proposed method was verified through several examples, ranging from natural objects to medical images from patients with a neurological disease, multiple sclerosis.	1,0,1,0,0,0
Linear-Size Hopsets with Small Hopbound, and Distributed Routing with Low Memory	For a positive parameter $\beta$, the $\beta$-bounded distance between a pair of vertices $u,v$ in a weighted undirected graph $G = (V,E,\omega)$ is the length of the shortest $u-v$ path in $G$ with at most $\beta$ edges, aka {\em hops}. For $\beta$ as above and $\epsilon>0$, a {\em $(\beta,\epsilon)$-hopset} of $G = (V,E,\omega)$ is a graph $G' =(V,H,\omega_H)$ on the same vertex set, such that all distances in $G$ are $(1+\epsilon)$-approximated by $\beta$-bounded distances in $G\cup G'$. Hopsets are a fundamental graph-theoretic and graph-algorithmic construct, and they are widely used for distance-related problems in a variety of computational settings. Currently existing constructions of hopsets produce hopsets either with $\Omega(n \log n)$ edges, or with a hopbound $n^{\Omega(1)}$. In this paper we devise a construction of {\em linear-size} hopsets with hopbound $(\log n)^{\log^{(3)}n+O(1)}$. This improves the previous bound almost exponentially. We also devise efficient implementations of our construction in PRAM and distributed settings. The only existing PRAM algorithm \cite{EN16} for computing hopsets with a constant (i.e., independent of $n$) hopbound requires $n^{\Omega(1)}$ time. We devise a PRAM algorithm with polylogarithmic running time for computing hopsets with a constant hopbound, i.e., our running time is exponentially better than the previous one. Moreover, these hopsets are also significantly sparser than their counterparts from \cite{EN16}. We use our hopsets to devise a distributed routing scheme that exhibits near-optimal tradeoff between individual memory requirement $\tilde{O}(n^{1/k})$ of vertices throughout preprocessing and routing phases of the algorithm, and stretch $O(k)$, along with a near-optimal construction time $\approx D + n^{1/2 + 1/k}$, where $D$ is the hop-diameter of the input graph.	1,0,0,0,0,0
A Conjoint Application of Data Mining Techniques for Analysis of Global Terrorist Attacks -- Prevention and Prediction for Combating Terrorism	Terrorism has become one of the most tedious problems to deal with and a prominent threat to mankind. To enhance counter-terrorism, several research works are developing efficient and precise systems, data mining is not an exception. Immense data is floating in our lives, though the scarce availability of authentic terrorist attack data in the public domain makes it complicated to fight terrorism. This manuscript focuses on data mining classification techniques and discusses the role of United Nations in counter-terrorism. It analyzes the performance of classifiers such as Lazy Tree, Multilayer Perceptron, Multiclass and Naïve Bayes classifiers for observing the trends for terrorist attacks around the world. The database for experiment purpose is created from different public and open access sources for years 1970-2015 comprising of 156,772 reported attacks causing massive losses of lives and property. This work enumerates the losses occurred, trends in attack frequency and places more prone to it, by considering the attack responsibilities taken as evaluation class.	1,0,0,1,0,0
A Hybrid Approach using Ontology Similarity and Fuzzy Logic for Semantic Question Answering	One of the challenges in information retrieval is providing accurate answers to a user's question often expressed as uncertainty words. Most answers are based on a Syntactic approach rather than a Semantic analysis of the query. In this paper, our objective is to present a hybrid approach for a Semantic question answering retrieval system using Ontology Similarity and Fuzzy logic. We use a Fuzzy Co-clustering algorithm to retrieve the collection of documents based on Ontology Similarity. The Fuzzy Scale uses Fuzzy type-1 for documents and Fuzzy type-2 for words to prioritize answers. The objective of this work is to provide retrieval system with more accurate answers than non-fuzzy Semantic Ontology approach.	1,0,0,0,0,0
A compilation of LEGO Technic parts to support learning experiments on linkages	We present a compilation of LEGO Technic parts to provide easy-to-build constructions of basic planar linkages. Some technical issues and their possible solutions are discussed. To solve questions on fine details---like deciding whether the motion is an exactly straight line or not---we refer to the dynamic mathematics software tool GeoGebra.	0,0,1,0,0,0
On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis	Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.	1,0,0,0,0,0
Single Element Nonlinear Chimney Model	We generalize the chimney model by introducing nonlinear restoring and gravitational forces for the purpose of modeling swaying of trees at high wind speeds. Here we have restricted to the simplest case of a single element and the governing equation we arrive at has not been studied so far. We study the onset of fractal basin boundary of the two fixed points and also observe the chaotic solutions. We also examine the need for considering the full sine term in the gravitational force.	0,1,0,0,0,0
The Road to Success: Assessing the Fate of Linguistic Innovations in Online Communities	We investigate the birth and diffusion of lexical innovations in a large dataset of online social communities. We build on sociolinguistic theories and focus on the relation between the spread of a novel term and the social role of the individuals who use it, uncovering characteristics of innovators and adopters. Finally, we perform a prediction task that allows us to anticipate whether an innovation will successfully spread within a community.	1,0,0,0,0,0
Model Selection Confidence Sets by Likelihood Ratio Testing	The traditional activity of model selection aims at discovering a single model superior to other candidate models. In the presence of pronounced noise, however, multiple models are often found to explain the same data equally well. To resolve this model selection ambiguity, we introduce the general approach of model selection confidence sets (MSCSs) based on likelihood ratio testing. A MSCS is defined as a list of models statistically indistinguishable from the true model at a user-specified level of confidence, which extends the familiar notion of confidence intervals to the model-selection framework. Our approach guarantees asymptotically correct coverage probability of the true model when both sample size and model dimension increase. We derive conditions under which the MSCS contains all the relevant information about the true model structure. In addition, we propose natural statistics based on the MSCS to measure importance of variables in a principled way that accounts for the overall model uncertainty. When the space of feasible models is large, MSCS is implemented by an adaptive stochastic search algorithm which samples MSCS models with high probability. The MSCS methodology is illustrated through numerical experiments on synthetic data and real data examples.	0,0,1,1,0,0
Revisiting Frequency Reuse towards Supporting Ultra-Reliable Ubiquitous-Rate Communication	One of the goals of 5G wireless systems stated by the NGMN alliance is to provide moderate rates (50+ Mbps) everywhere and with very high reliability. We term this service Ultra-Reliable Ubiquitous-Rate Communication (UR2C). This paper investigates the role of frequency reuse in supporting UR2C in the downlink. To this end, two frequency reuse schemes are considered: user-specific frequency reuse (FRu) and BS-specific frequency reuse (FRb). For a given unit frequency channel, FRu reduces the number of serving user equipments (UEs), whereas FRb directly decreases the number of interfering base stations (BSs). This increases the distance from the interfering BSs and the signal-to-interference ratio (SIR) attains ultra-reliability, e.g. 99% SIR coverage at a randomly picked UE. The ultra-reliability is, however, achieved at the cost of the reduced frequency allocation, which may degrade overall downlink rate. To fairly capture this reliability-rate tradeoff, we propose ubiquitous rate defined as the maximum downlink rate whose required SIR can be achieved with ultra-reliability. By using stochastic geometry, we derive closed-form ubiquitous rate as well as the optimal frequency reuse rules for UR2C.	1,0,0,0,0,0
A Domain Specific Language for Performance Portable Molecular Dynamics Algorithms	Developers of Molecular Dynamics (MD) codes face significant challenges when adapting existing simulation packages to new hardware. In a continuously diversifying hardware landscape it becomes increasingly difficult for scientists to be experts both in their own domain (physics/chemistry/biology) and specialists in the low level parallelisation and optimisation of their codes. To address this challenge, we describe a "Separation of Concerns" approach for the development of parallel and optimised MD codes: the science specialist writes code at a high abstraction level in a domain specific language (DSL), which is then translated into efficient computer code by a scientific programmer. In a related context, an abstraction for the solution of partial differential equations with grid based methods has recently been implemented in the (Py)OP2 library. Inspired by this approach, we develop a Python code generation system for molecular dynamics simulations on different parallel architectures, including massively parallel distributed memory systems and GPUs. We demonstrate the efficiency of the auto-generated code by studying its performance and scalability on different hardware and compare it to other state-of-the-art simulation packages. With growing data volumes the extraction of physically meaningful information from the simulation becomes increasingly challenging and requires equally efficient implementations. A particular advantage of our approach is the easy expression of such analysis algorithms. We consider two popular methods for deducing the crystalline structure of a material from the local environment of each atom, show how they can be expressed in our abstraction and implement them in the code generation framework.	1,1,0,0,0,0
Step bunching with both directions of the current: Vicinal W(110) surfaces versus atomistic scale model	We report for the first time the observation of bunching of monoatomic steps on vicinal W(110) surfaces induced by step up or step down currents across the steps. Measurements reveal that the size scaling exponent {\gamma}, connecting the maximal slope of a bunch with its height, differs depending on the current direction. We provide a numerical perspective by using an atomistic scale model with a conserved surface flux to mimic experimental conditions, and also for the first time show that there is an interval of parameters in which the vicinal surface is unstable against step bunching for both directions of the adatom drift.	0,1,0,0,0,0
Code-division multiplexed resistive pulse sensor networks for spatio-temporal detection of particles in microfluidic devices	Spatial separation of suspended particles based on contrast in their physical or chemical properties forms the basis of various biological assays performed on lab-on-achip devices. To electronically acquire this information, we have recently introduced a microfluidic sensing platform, called Microfluidic CODES, which combines the resistive pulse sensing with the code division multiple access in multiplexing a network of integrated electrical sensors. In this paper, we enhance the multiplexing capacity of the Microfluidic CODES by employing sensors that generate non-orthogonal code waveforms and a new decoding algorithm that combines machine learning techniques with minimum mean-squared error estimation. As a proof of principle, we fabricated a microfluidic device with a network of 10 code-multiplexed sensors and characterized it using cells suspended in phosphate buffer saline solution.	0,0,0,1,0,0
Mid-price estimation for European corporate bonds: a particle filtering approach	In most illiquid markets, there is no obvious proxy for the market price of an asset. The European corporate bond market is an archetypal example of such an illiquid market where mid-prices can only be estimated with a statistical model. In this OTC market, dealers / market makers only have access, indeed, to partial information about the market. In real-time, they know the price associated with their trades on the dealer-to-dealer (D2D) and dealer-to-client (D2C) markets, they know the result of the requests for quotes (RFQ) they answered, and they have access to composite prices (e.g., Bloomberg CBBT). This paper presents a Bayesian method for estimating the mid-price of corporate bonds by using the real-time information available to a dealer. This method relies on recent ideas coming from the particle filtering (PF) / sequential Monte-Carlo (SMC) literature.	0,0,0,0,0,1
TIP: Typifying the Interpretability of Procedures	We provide a novel notion of what it means to be interpretable, looking past the usual association with human understanding. Our key insight is that interpretability is not an absolute concept and so we define it relative to a target model, which may or may not be a human. We define a framework that allows for comparing interpretable procedures by linking them to important practical aspects such as accuracy and robustness. We characterize many of the current state-of-the-art interpretable methods in our framework portraying its general applicability. Finally, principled interpretable strategies are proposed and empirically evaluated on synthetic data, as well as on the largest public olfaction dataset that was made recently available \cite{olfs}. We also experiment on MNIST with a simple target model and different oracle models of varying complexity. This leads to the insight that the improvement in the target model is not only a function of the oracle model's performance, but also its relative complexity with respect to the target model. Further experiments on CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit of our methods over Knowledge Distillation when the target models are simple and the complex model is a neural network.	1,0,0,1,0,0
Deep Graphs	We propose an algorithm for deep learning on networks and graphs. It relies on the notion that many graph algorithms, such as PageRank, Weisfeiler-Lehman, or Message Passing can be expressed as iterative vertex updates. Unlike previous methods which rely on the ingenuity of the designer, Deep Graphs are adaptive to the estimation problem. Training and deployment are both efficient, since the cost is $O(|E| + |V|)$, where $E$ and $V$ are the sets of edges and vertices respectively. In short, we learn the recurrent update functions rather than positing their specific functional form. This yields an algorithm that achieves excellent accuracy on both graph labeling and regression tasks.	0,0,0,1,0,0
Hochschild cohomology for periodic algebras of polynomial growth	We describe the dimensions of low Hochschild cohomology spaces of exceptional periodic representation-infinite algebras of polynomial growth. As an application we obtain that an indecomposable non-standard periodic representation-infinite algebra of polynomial growth is not derived equivalent to a standard self-injective algebra.	0,0,1,0,0,0
Unsupervised Learning of Mixture Regression Models for Longitudinal Data	This paper is concerned with learning of mixture regression models for individuals that are measured repeatedly. The adjective "unsupervised" implies that the number of mixing components is unknown and has to be determined, ideally by data driven tools. For this purpose, a novel penalized method is proposed to simultaneously select the number of mixing components and to estimate the mixing proportions and unknown parameters in the models. The proposed method is capable of handling both continuous and discrete responses by only requiring the first two moment conditions of the model distribution. It is shown to be consistent in both selecting the number of components and estimating the mixing proportions and unknown regression parameters. Further, a modified EM algorithm is developed to seamlessly integrate model selection and estimation. Simulation studies are conducted to evaluate the finite sample performance of the proposed procedure. And it is further illustrated via an analysis of a primary biliary cirrhosis data set.	0,0,0,1,0,0
Large deformations of the Tracy-Widom distribution I. Non-oscillatory asymptotics	We analyze the left-tail asymptotics of deformed Tracy-Widom distribution functions describing the fluctuations of the largest eigenvalue in invariant random matrix ensembles after removing each soft edge eigenvalue independently with probability $1-\gamma\in[0,1]$. As $\gamma$ varies, a transition from Tracy-Widom statistics ($\gamma=1$) to classical Weibull statistics ($\gamma=0$) was observed in the physics literature by Bohigas, de Carvalho, and Pato \cite{BohigasCP:2009}. We provide a description of this transition by rigorously computing the leading-order left-tail asymptotics of the thinned GOE, GUE and GSE Tracy-Widom distributions. In this paper, we obtain the asymptotic behavior in the non-oscillatory region with $\gamma\in[0,1)$ fixed (for the GOE, GUE, and GSE distributions) and $\gamma\uparrow 1$ at a controlled rate (for the GUE distribution). This is the first step in an ongoing program to completely describe the transition between Tracy-Widom and Weibull statistics. As a corollary to our results, we obtain a new total-integral formula involving the Ablowitz-Segur solution to the second Painlevé equation.	0,1,1,0,0,0
Combinatorial Miller-Hagberg Algorithm for Randomization of Dense Networks	We propose a slightly revised Miller-Hagberg (MH) algorithm that efficiently generates a random network from a given expected degree sequence. The revision was to replace the approximated edge probability between a pair of nodes with a combinatorically calculated edge probability that better captures the likelihood of edge presence especially where edges are dense. The computational complexity of this combinatorial MH algorithm is still in the same order as the original one. We evaluated the proposed algorithm through several numerical experiments. The results demonstrated that the proposed algorithm was particularly good at accurately representing high-degree nodes in dense, heterogeneous networks. This algorithm may be a useful alternative of other more established network randomization methods, given that the data are increasingly becoming larger and denser in today's network science research.	1,0,0,0,0,0
Genuine equivariant operads	We build new algebraic structures, which we call genuine equivariant operads, which can be thought of as a hybrid between equivariant operads and coefficient systems. We then prove an Elmendorf-Piacenza type theorem stating that equivariant operads, with their graph model structure, are equivalent to genuine equivariant operads, with their projective model structure. As an application, we build explicit models for the $N_{\infty}$-operads of Blumberg and Hill.	0,0,1,0,0,0
Generalizing Distance Covariance to Measure and Test Multivariate Mutual Dependence	We propose three measures of mutual dependence between multiple random vectors. All the measures are zero if and only if the random vectors are mutually independent. The first measure generalizes distance covariance from pairwise dependence to mutual dependence, while the other two measures are sums of squared distance covariance. All the measures share similar properties and asymptotic distributions to distance covariance, and capture non-linear and non-monotone mutual dependence between the random vectors. Inspired by complete and incomplete V-statistics, we define the empirical measures and simplified empirical measures as a trade-off between the complexity and power when testing mutual independence. Implementation of the tests is demonstrated by both simulation results and real data examples.	0,0,1,1,0,0
Orbital Evolution, Activity, and Mass Loss of Comet C/1995 O1 (Hale-Bopp). I. Close Encounter with Jupiter in Third Millennium BCE and Effects of Outgassing on the Comet's Motion and Physical Properties	This comprehensive study of comet C/1995 O1 focuses first on investigating its orbital motion over a period of 17.6 yr (1993-2010). The comet is suggested to have approached Jupiter to 0.005 AU on -2251 November 7, in general conformity with Marsden's (1999) proposal of a Jovian encounter nearly 4300 yr ago. The variations of sizable nongravitational effects with heliocentric distance correlate with the evolution of outgassing, asymmetric relative to perihelion. The future orbital period will shorten to ~1000 yr because of orbital-cascade resonance effects. We find that the sublimation curves of parent molecules are fitted with the type of a law used for the nongravitational acceleration, determine their orbit-integrated mass loss, and conclude that the share of water ice was at most 57%, and possibly less than 50%, of the total outgassed mass. Even though organic parent molecules (many still unidentified) had very low abundances relative to water individually, their high molar mass and sheer number made them, summarily, important potential mass contributors to the total production of gas. The mass loss of dust per orbit exceeded that of water ice by a factor of ~12, a dust loading high enough to imply a major role for heavy organic molecules of low volatility in accelerating the minuscule dust particles in the expanding halos to terminal velocities as high as 0.7 km s^{-1}. In Part II, the comet's nucleus will be modeled as a compact cluster of massive fragments to conform to the integrated nongravitational effect.	0,1,0,0,0,0
Tunable low energy Ps beam for the anti-hydrogen free fall and for testing gravity with a Mach-Zehnder interferometer	The test of gravitational force on antimatter in the field of the matter gravitational field, produced by earth, can be done by a free fall experiment which involves only General Relativity, and with a Mach-Zehnder interferometer which involves Quantum Mechanics. This article presents a new method to produce a tunable low energy (Ps ) beam suitable for trapping the (Hbar + ) ion in a free fall experiment, and suitable for a gravity Mach-Zehnder interferometer with (Ps). The low energy (Ps) beam is tunable in the [10 eV, 100 eV] range.	0,1,0,0,0,0
Accurate Inference for Adaptive Linear Models	Estimators computed from adaptively collected data do not behave like their non-adaptive brethren. Rather, the sequential dependence of the collection policy can lead to severe distributional biases that persist even in the infinite data limit. We develop a general method -- $\mathbf{W}$-decorrelation -- for transforming the bias of adaptive linear regression estimators into variance. The method uses only coarse-grained information about the data collection policy and does not need access to propensity scores or exact knowledge of the policy. We bound the finite-sample bias and variance of the $\mathbf{W}$-estimator and develop asymptotically correct confidence intervals based on a novel martingale central limit theorem. We then demonstrate the empirical benefits of the generic $\mathbf{W}$-decorrelation procedure in two different adaptive data settings: the multi-armed bandit and the autoregressive time series.	1,0,0,1,0,0
Gradual Tuning: a better way of Fine Tuning the parameters of a Deep Neural Network	In this paper we present an alternative strategy for fine-tuning the parameters of a network. We named the technique Gradual Tuning. Once trained on a first task, the network is fine-tuned on a second task by modifying a progressively larger set of the network's parameters. We test Gradual Tuning on different transfer learning tasks, using networks of different sizes trained with different regularization techniques. The result shows that compared to the usual fine tuning, our approach significantly reduces catastrophic forgetting of the initial task, while still retaining comparable if not better performance on the new task.	1,0,0,0,0,0
3D Reconstruction & Assessment Framework based on affordable 2D Lidar	Lidar is extensively used in the industry and mass-market. Due to its measurement accuracy and insensitivity to illumination compared to cameras, It is applied onto a broad range of applications, like geodetic engineering, self driving cars or virtual reality. But the 3D Lidar with multi-beam is very expensive, and the massive measurements data can not be fully leveraged on some constrained platforms. The purpose of this paper is to explore the possibility of using cheap 2D Lidar off-the-shelf, to preform complex 3D Reconstruction, moreover, the generated 3D map quality is evaluated by our proposed metrics at the end. The 3D map is constructed in two ways, one way in which the scan is performed at known positions with an external rotary axis at another plane. The other way, in which the 2D Lidar for mapping and another 2D Lidar for localization are placed on a trolley, the trolley is pushed on the ground arbitrarily. The generated maps by different approaches are converted to octomaps uniformly before the evaluation. The similarity and difference between two maps will be evaluated by the proposed metrics thoroughly. The whole mapping system is composed of several modular components. A 3D bracket was made for assembling of the Lidar with a long range, the driver and the motor together. A cover platform made for the IMU and 2D Lidar with a shorter range but high accuracy. The software is stacked up in different ROS packages.	1,0,0,0,0,0
Constraints on the Intergalactic Magnetic Field from Bow Ties in the Gamma-ray Sky	Pair creation on the cosmic infrared background and subsequent inverse-Compton scattering on the CMB potentially reprocesses the TeV emission of blazars into faint GeV halos with structures sensitive to intergalactic magnetic fields (IGMF). We attempt to detect such halos exploiting their highly anisotropic shape. Their persistent nondetection excludes at greater than $3.9\sigma$ an IGMF with correlation lengths >100 Mpc and current-day strengths in the range $10^{-16}$ to $10^{-15}$ G, and at 2 sigma from $10^{-17}$ to $10^{-14}$ G, covering the range implied by gamma-ray spectra of nearby TeV emitters. Alternatively, plasma processes could pre-empt the inverse-Compton cascade.	0,1,0,0,0,0
A global sensitivity analysis and reduced order models for hydraulically-fractured horizontal wells	We present a systematic global sensitivity analysis using the Sobol method which can be utilized to rank the variables that affect two quantity of interests -- pore pressure depletion and stress change -- around a hydraulically-fractured horizontal well based on their degree of importance. These variables include rock properties and stimulation design variables. A fully-coupled poroelastic hydraulic fracture model is used to account for pore pressure and stress changes due to production. To ease the computational cost of a simulator, we also provide reduced order models (ROMs), which can be used to replace the complex numerical model with a rather simple analytical model, for calculating the pore pressure and stresses at different locations around hydraulic fractures. The main findings of this research are: (i) mobility, production pressure, and fracture half-length are the main contributors to the changes in the quantities of interest. The percentage of the contribution of each parameter depends on the location with respect to pre-existing hydraulic fractures and the quantity of interest. (ii) As the time progresses, the effect of mobility decreases and the effect of production pressure increases. (iii) These two variables are also dominant for horizontal stresses at large distances from hydraulic fractures. (iv) At zones close to hydraulic fracture tips or inside the spacing area, other parameters such as fracture spacing and half-length are the dominant factors that affect the minimum horizontal stress. The results of this study will provide useful guidelines for the stimulation design of legacy wells and secondary operations such as refracturing and infill drilling.	1,0,0,0,0,0
Parallel Concatenation of Bayesian Filters: Turbo Filtering	In this manuscript a method for developing novel filtering algorithms through the parallel concatenation of two Bayesian filters is illustrated. Our description of this method, called turbo filtering, is based on a new graphical model; this allows us to efficiently describe both the processing accomplished inside each of the constituent filter and the interactions between them. This model is exploited to develop two new filtering algorithms for conditionally linear Gaussian systems. Numerical results for a specific dynamic system evidence that such filters can achieve a better complexity-accuracy tradeoff than marginalized particle filtering.	0,0,0,1,0,0
Handling Incomplete Heterogeneous Data using VAEs	Variational autoencoders (VAEs), as well as other generative models, have been shown to be efficient and accurate to capture the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications. In this paper, we propose a general framework to design VAEs, suitable for fitting incomplete heterogenous data. The proposed HI-VAE includes likelihood models for real-valued, positive real valued, interval, categorical, ordinal and count data, and allows to estimate (and potentially impute) missing data accurately. Furthermore, HI-VAE presents competitive predictive performance in supervised tasks, outperforming super- vised models when trained on incomplete data	0,0,0,1,0,0
Estimating occupation time functionals	We study the estimation of integral type functionals $\int_{0}^{t}f(X_{r})dr$ for a function $f$ and a $d$-dimensional càdlàg process $X$ with respect to discrete observations by a Riemann-sum estimator. Based on novel semimartingale approximations in the Fourier domain, central limit theorems are proved for $L^{2}$-Sobolev functions $f$ with fractional smoothness and continuous Itô semimartingales $X$. General $L^{2}(\mathbb{P})$-upper bounds on the error for càdlàg processes are given under weak assumptions. These bounds combine and generalize all previously obtained results in the literature and apply also to non-Markovian processes. Several detailed examples are discussed. As application the approximation of local times for fractional Brownian motion is studied. The optimality of the $L^{2}(\mathbb{P})$-upper bounds is shown by proving the corresponding lower bounds in case of Brownian motion.	0,0,1,1,0,0
Superradiant Mott Transition	The combination of strong correlation and emergent lattice can be achieved when quantum gases are confined in a superradiant Fabry-Perot cavity. In addition to the discoveries of exotic phases, such as density wave ordered Mott insulator and superfluid, a surprising kink structure is found in the slope of the cavity strength as a function of the pumping strength. In this Letter, we show that the appearance of such a kink is a manifestation of a liquid-gas like transition between two superfluids with different densities. The slopes in the immediate neighborhood of the kink become divergent at the liquid-gas critical points and display a critical scaling law with a critical exponent 1 in the quantum critical region. Our predictions could be tested in current experimental set-up.	0,1,0,0,0,0
Four-dimensional Lens Space Index from Two-dimensional Chiral Algebra	We study the supersymmetric partition function on $S^1 \times L(r, 1)$, or the lens space index of four-dimensional $\mathcal{N}=2$ superconformal field theories and their connection to two-dimensional chiral algebras. We primarily focus on free theories as well as Argyres-Douglas theories of type $(A_1, A_k)$ and $(A_1, D_k)$. We observe that in specific limits, the lens space index is reproduced in terms of the (refined) character of an appropriately twisted module of the associated two-dimensional chiral algebra or a generalized vertex operator algebra. The particular twisted module is determined by the choice of discrete holonomies for the flavor symmetry in four-dimensions.	0,0,1,0,0,0
Means of infinite sets I	We open a new field on how one can define means on infinite sets. We investigate many different ways on how such means can be constructed. One method is based on sequences of ideals, other deals with accumulation points, one uses isolated points, other deals with average using integral, other with limit of average on surroundings and one deals with evenly distributed samples. We study various properties of such means and their relations to each other.	0,0,1,0,0,0
Large sums of Hecke eigenvalues of holomorphic cusp forms	Let $f$ be a Hecke cusp form of weight $k$ for the full modular group, and let $\{\lambda_f(n)\}_{n\geq 1}$ be the sequence of its normalized Fourier coefficients. Motivated by the problem of the first sign change of $\lambda_f(n)$, we investigate the range of $x$ (in terms of $k$) for which there are cancellations in the sum $S_f(x)=\sum_{n\leq x} \lambda_f(n)$. We first show that $S_f(x)=o(x\log x)$ implies that $\lambda_f(n)<0$ for some $n\leq x$. We also prove that $S_f(x)=o(x\log x)$ in the range $\log x/\log\log k\to \infty$ assuming the Riemann hypothesis for $L(s, f)$, and furthermore that this range is best possible unconditionally. More precisely, we establish the existence of many Hecke cusp forms $f$ of large weight $k$, for which $S_f(x)\gg_A x\log x$, when $x=(\log k)^A.$ Our results are $GL_2$ analogues of work of Granville and Soundararajan for character sums, and could also be generalized to other families of automorphic forms.	0,0,1,0,0,0
Global spectral graph wavelet signature for surface analysis of carpal bones	In this paper, we present a spectral graph wavelet approach for shape analysis of carpal bones of human wrist. We apply a metric called global spectral graph wavelet signature for representation of cortical surface of the carpal bone based on eigensystem of Laplace-Beltrami operator. Furthermore, we propose a heuristic and efficient way of aggregating local descriptors of a carpal bone surface to global descriptor. The resultant global descriptor is not only isometric invariant, but also much more efficient and requires less memory storage. We perform experiments on shape of the carpal bones of ten women and ten men from a publicly-available database. Experimental results show the excellency of the proposed GSGW compared to recent proposed GPS embedding approach for comparing shapes of the carpal bones across populations.	1,0,0,0,0,0
Frequent flaring in the TRAPPIST-1 system - unsuited for life?	We analyze short cadence K2 light curve of the TRAPPIST-1 system. Fourier analysis of the data suggests $P_\mathrm{rot}=3.295\pm0.003$ days. The light curve shows several flares, of which we analyzed 42 events, these have integrated flare energies of $1.26\times10^{30}-1.24\times10^{33}$ ergs. Approximately 12% of the flares were complex, multi-peaked eruptions. The flaring and the possible rotational modulation shows no obvious correlation. The flaring activity of TRAPPIST-1 probably continuously alters the atmospheres of the orbiting exoplanets, making these less favorable for hosting life.	0,1,0,0,0,0
The Urban Last Mile Problem: Autonomous Drone Delivery to Your Balcony	Drone delivery has been a hot topic in the industry in the past few years. However, existing approaches either focus on rural areas or rely on centralized drop-off locations from where the last mile delivery is performed. In this paper we tackle the problem of autonomous last mile delivery in urban environments using an off-the-shelf drone. We build a prototype system that is able to fly to the approximate delivery location using GPS and then find the exact drop-off location using visual navigation. The drop-off location could, e.g., be on a balcony or porch, and simply needs to be indicated by a visual marker on the wall or window. We test our system components in simulated environments, including the visual navigation and collision avoidance. Finally, we deploy our drone in a real-world environment and show how it can find the drop-off point on a balcony. To stimulate future research in this topic we open source our code.	1,0,0,0,0,0
The LOFAR window on star-forming galaxies and AGN - curved radio SEDs and IR-radio correlation at $0 < z < 2.5$	We present a study of the low-frequency radio properties of star forming (SF) galaxies and active galactic nuclei (AGN) up to redshift $z=2.5$. The new spectral window probed by the Low Frequency Array (LOFAR) allows us to reconstruct the radio continuum emission from 150 MHz to 1.4 GHz to an unprecedented depth for a radio-selected sample of $1542$ galaxies in $\sim 7~ \rm{deg}^2$ of the LOFAR Boötes field. Using the extensive multi-wavelength dataset available in Boötes and detailed modelling of the FIR to UV spectral energy distribution (SED), we are able to separate the star-formation (N=758) and the AGN (N=784) dominated populations. We study the shape of the radio SEDs and their evolution across cosmic time and find significant differences in the spectral curvature between the SF galaxy and AGN populations. While the radio spectra of SF galaxies exhibit a weak but statistically significant flattening, AGN SEDs show a clear trend to become steeper towards lower frequencies. No evolution of the spectral curvature as a function of redshift is found for SF galaxies or AGN. We investigate the redshift evolution of the infrared-radio correlation (IRC) for SF galaxies and find that the ratio of total infrared to 1.4 GHz radio luminosities decreases with increasing redshift: $ q_{\rm 1.4GHz} = (2.45 \pm 0.04) \times (1+z)^{-0.15 \pm 0.03} $. Similarly, $q_{\rm 150MHz}$ shows a redshift evolution following $ q_{\rm 150GHz} = (1.72 \pm 0.04) \times (1+z)^{-0.22 \pm 0.05}$. Calibration of the 150 MHz radio luminosity as a star formation rate tracer suggests that a single power-law extrapolation from $q_{\rm 1.4GHz}$ is not an accurate approximation at all redshifts.	0,1,0,0,0,0
Symmetry Enforced Stability of Interacting Weyl and Dirac Semimetals	The nodal and effectively relativistic dispersion featuring in a range of novel materials including two- dimensional graphene and three-dimensional Dirac and Weyl semimetals has attracted enormous interest during the past decade. Here, by studying the structure and symmetry of the diagrammatic expansion, we show that these nodal touching points are in fact perturbatively stable to all orders with respect to generic two-body interactions. For effective low-energy theories relevant for single and multilayer graphene, type-I and type-II Weyl and Dirac semimetals as well as Weyl points with higher topological charge, this stability is shown to be a direct consequence of a spatial symmetry that anti-commutes with the effective Hamiltonian while leaving the interaction invariant. A more refined argument is applied to the honeycomb lattice model of graphene showing that its Dirac points are also perturbatively stable to all orders. We also give examples of nodal Hamiltonians that acquire a gap from interactions as a consequence of symmetries different from those of Weyl and Dirac materials.	0,1,0,0,0,0
Towards Deep Learning Models for Psychological State Prediction using Smartphone Data: Challenges and Opportunities	There is an increasing interest in exploiting mobile sensing technologies and machine learning techniques for mental health monitoring and intervention. Researchers have effectively used contextual information, such as mobility, communication and mobile phone usage patterns for quantifying individuals' mood and wellbeing. In this paper, we investigate the effectiveness of neural network models for predicting users' level of stress by using the location information collected by smartphones. We characterize the mobility patterns of individuals using the GPS metrics presented in the literature and employ these metrics as input to the network. We evaluate our approach on the open-source StudentLife dataset. Moreover, we discuss the challenges and trade-offs involved in building machine learning models for digital mental health and highlight potential future work in this direction.	1,0,0,1,0,0
Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration	Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on a new analysis of Sinkhorn iteration, which also directly suggests a new greedy coordinate descent algorithm, Greenkhorn, with the same theoretical guarantees. Numerical simulations illustrate that Greenkhorn significantly outperforms the classical Sinkhorn algorithm in practice.	1,0,0,1,0,0
Large covers and sharp resonances of hyperbolic surfaces	Let $\Gamma$ be a convex co-compact discrete group of isometries of the hyperbolic plane $\mathbb{H}^2$, and $X=\Gamma\backslash \mathbb{H}^2$ the associated surface. In this paper we investigate the behaviour of resonances of the Laplacian for large degree covers of $X$ given by a finite index normal subgroup of $\Gamma$. Using various techniques of thermodynamical formalism and representation theory, we prove two new existence results of "sharp non-trivial resonances" close to $\Re(s)=\delta_\Gamma$, both in the large degree limit, for abelian covers and also infinite index congruence subgroups of $SL2(\mathbb{Z})$.	0,0,1,0,0,0
Gradient Normalization & Depth Based Decay For Deep Learning	In this paper we introduce a novel method of gradient normalization and decay with respect to depth. Our method leverages the simple concept of normalizing all gradients in a deep neural network, and then decaying said gradients with respect to their depth in the network. Our proposed normalization and decay techniques can be used in conjunction with most current state of the art optimizers and are a very simple addition to any network. This method, although simple, showed improvements in convergence time on state of the art networks such as DenseNet and ResNet on image classification tasks, as well as on an LSTM for natural language processing tasks.	1,0,0,1,0,0
Molecular semimetallic hydrogen	Establishing metallic hydrogen is a goal of intensive theoretical and experimental work since 1935 when Wigner and Hungtinton [1] predicted that insulating molecular hydrogen will dissociate at high pressures and transform to a metal. This metal is predicted to be a superconductor with very high critical temperature [2]. In another scenario, the metallization can be realized through overlapping of electronic bands in molecular hydrogen in the similar 400 - 500 GPa pressure range [3-5]. The calculations are not accurate enough to predict which option will be realized. Our data are consistent with transforms of hydrogen to semimetal by closing the indirect band gap in the molecular phase III at pressure ~ 360 GPa. Above this pressure, the metallic behaviour in the electrical conductivity appears, the reflection significantly increases. With pressure, the electrical conductivity strongly increases as measured up to 440 GPa. The Raman measurements evidence that hydrogen is in the molecular phase III at pressures at least up to 440 GPa. At higher pressures measured up to 480 GPa, the Raman signal gradually disappears indicating further transformation to a good molecular metal or to an atomic state.	0,1,0,0,0,0
High-dose-rate prostate brachytherapy inverse planning on dose-volume criteria by simulated annealing	High-dose-rate brachytherapy is a tumor treatment method where a highly radioactive source is brought in close proximity to the tumor. In this paper we develop a simulated annealing algorithm to optimize the dwell times at preselected dwell positions to maximize tumor coverage under dose-volume constraints on the organs at risk. Compared to existing algorithms, our algorithm has advantages in terms of speed and objective value and does not require an expensive general purpose solver. Its success mainly depends on exploiting the efficiency of matrix multiplication and a careful selection of the neighboring states. In this paper we outline its details and make an in-depth comparison with existing methods using real patient data.	0,1,0,0,0,0
Probing the accretion disc structure by the twin kHz QPOs and spins of neutron stars in LMXBs	We analyze the relation between the emission radii of twin kilohertz quasi-periodic oscillations (kHz QPOs) and the co-rotation radii of the 12 neutron star low mass X-ray binaries (NS-LMXBs) which are simultaneously detected with the twin kHz QPOs and NS spins. We find that the average co-rotation radius of these sources is r_co about 32 km, and all the emission positions of twin kHz QPOs lie inside the corotation radii, indicating that the twin kHz QPOs are formed in the spin-up process. It is noticed that the upper frequency of twin kHz QPOs is higher than NS spin frequency by > 10%, which may account for a critical velocity difference between the Keplerian motion of accretion matter and NS spin that is corresponding to the production of twin kHz QPOs. In addition, we also find that about 83% of twin kHz QPOs cluster around the radius range of 15-20 km, which may be affected by the hard surface or the local strong magnetic field of NS. As a special case, SAX J1808.4-3658 shows the larger emission radii of twin kHz QPOs of r about 21-24 km, which may be due to its low accretion rate or small measured NS mass (< 1.4 solar mass).	0,1,0,0,0,0
Explicit polynomial sequences with maximal spaces of partial derivatives and a question of K. Mulmuley	We answer a question of K. Mulmuley: In [Efremenko-Landsberg-Schenck-Weyman] it was shown that the method of shifted partial derivatives cannot be used to separate the padded permanent from the determinant. Mulmuley asked if this "no-go" result could be extended to a model without padding. We prove this is indeed the case using the iterated matrix multiplication polynomial. We also provide several examples of polynomials with maximal space of partial derivatives, including the complete symmetric polynomials. We apply Koszul flattenings to these polynomials to have the first explicit sequence of polynomials with symmetric border rank lower bounds higher than the bounds attainable via partial derivatives.	1,0,1,0,0,0
The Host Galaxy and Redshift of the Repeating Fast Radio Burst FRB 121102	The precise localization of the repeating fast radio burst (FRB 121102) has provided the first unambiguous association (chance coincidence probability $p\lesssim3\times10^{-4}$) of an FRB with an optical and persistent radio counterpart. We report on optical imaging and spectroscopy of the counterpart and find that it is an extended ($0.6^{\prime\prime}-0.8^{\prime\prime}$) object displaying prominent Balmer and [OIII] emission lines. Based on the spectrum and emission line ratios, we classify the counterpart as a low-metallicity, star-forming, $m_{r^\prime} = 25.1$ AB mag dwarf galaxy at a redshift of $z=0.19273(8)$, corresponding to a luminosity distance of 972 Mpc. From the angular size, the redshift, and luminosity, we estimate the host galaxy to have a diameter $\lesssim4$ kpc and a stellar mass of $M_*\sim4-7\times 10^{7}\,M_\odot$, assuming a mass-to-light ratio between 2 to 3$\,M_\odot\,L_\odot^{-1}$. Based on the H$\alpha$ flux, we estimate the star formation rate of the host to be $0.4\,M_\odot\,\mathrm{yr^{-1}}$ and a substantial host dispersion measure depth $\lesssim 324\,\mathrm{pc\,cm^{-3}}$. The net dispersion measure contribution of the host galaxy to FRB 121102 is likely to be lower than this value depending on geometrical factors. We show that the persistent radio source at FRB 121102's location reported by Marcote et al (2017) is offset from the galaxy's center of light by $\sim$200 mas and the host galaxy does not show optical signatures for AGN activity. If FRB 121102 is typical of the wider FRB population and if future interferometric localizations preferentially find them in dwarf galaxies with low metallicities and prominent emission lines, they would share such a preference with long gamma ray bursts and superluminous supernovae.	0,1,0,0,0,0
A Constrained Shortest Path Scheme for Virtual Network Service Management	Virtual network services that span multiple data centers are important to support emerging data-intensive applications in fields such as bioinformatics and retail analytics. Successful virtual network service composition and maintenance requires flexible and scalable 'constrained shortest path management' both in the management plane for virtual network embedding (VNE) or network function virtualization service chaining (NFV-SC), as well as in the data plane for traffic engineering (TE). In this paper, we show analytically and empirically that leveraging constrained shortest paths within recent VNE, NFV-SC and TE algorithms can lead to network utilization gains (of up to 50%) and higher energy efficiency. The management of complex VNE, NFV-SC and TE algorithms can be, however, intractable for large scale substrate networks due to the NP-hardness of the constrained shortest path problem. To address such scalability challenges, we propose a novel, exact constrained shortest path algorithm viz., 'Neighborhoods Method' (NM). Our NM uses novel search space reduction techniques and has a theoretical quadratic speed-up making it practically faster (by an order of magnitude) than recent branch-and-bound exhaustive search solutions. Finally, we detail our NM-based SDN controller implementation in a real-world testbed to further validate practical NM benefits for virtual network services.	1,0,0,0,0,0
Size Agnostic Change Point Detection Framework for Evolving Networks	Changes in the structure of observed social and complex networks' structure can indicate a significant underlying change in an organization, or reflect the response of the network to an external event. Automatic detection of change points in evolving networks is rudimentary to the research and the understanding of the effect of such events on networks. Here we present an easy-to-implement and fast framework for change point detection in temporal evolving networks. Unlike previous approaches, our method is size agnostic, and does not require either prior knowledge about the network's size and structure, nor does it require obtaining historical information or nodal identities over time. We use both synthetic data derived from dynamic models and two real datasets: Enron email exchange and Ask-Ubuntu forum. Our framework succeeds with both precision and recall and outperforms previous solutions	1,0,0,0,0,0
Bundle Optimization for Multi-aspect Embedding	Understanding semantic similarity among images is the core of a wide range of computer vision applications. An important step towards this goal is to collect and learn human perceptions. Interestingly, the semantic context of images is often ambiguous as images can be perceived with emphasis on different aspects, which may be contradictory to each other. In this paper, we present a method for learning the semantic similarity among images, inferring their latent aspects and embedding them into multi-spaces corresponding to their semantic aspects. We consider the multi-embedding problem as an optimization function that evaluates the embedded distances with respect to the qualitative clustering queries. The key idea of our approach is to collect and embed qualitative measures that share the same aspects in bundles. To ensure similarity aspect sharing among multiple measures, image classification queries are presented to, and solved by users. The collected image clusters are then converted into bundles of tuples, which are fed into our bundle optimization algorithm that jointly infers the aspect similarity and multi-aspect embedding. Extensive experimental results show that our approach significantly outperforms state-of-the-art multi-embedding approaches on various datasets, and scales well for large multi-aspect similarity measures.	1,0,0,0,0,0
The descriptive look at the size of subsets of groups	We explore the Borel complexity of some basic families of subsets of a countable group (large, small, thin, sparse and other) defined by the size of their elements. Applying the obtained results to the Stone-Čech compactification $\beta G$ of $G$, we prove, in particular, that the closure of the minimal ideal of $\beta G$ is of type $F_{\sigma\delta}$.	0,0,1,0,0,0
Towards a Bootstrap approach to higher orders of epsilon expansion	We employ a hybrid approach in determining the anomalous dimension and OPE coefficient of higher spin operators in the Wilson-Fisher theory. First we do a large spin analysis for CFT data where we use results obtained from the usual and the Mellin Bootstrap and also from Feynman diagram literature. This gives new predictions at $O(\epsilon^4)$ and $O(\epsilon^5)$ for anomalous dimensions and OPE coefficients, and also provides a cross-check for the results from Mellin Bootstrap. These higher orders get contributions from all higher spin operators in the crossed channel. We also use the Bootstrap in Mellin space method for $\phi^3$ in $d=6-\epsilon$ CFT where we calculate general higher spin OPE data. We demonstrate a higher loop order calculation in this approach by summing over contributions from higher spin operators of the crossed channel in the same spirit as before.	0,1,0,0,0,0
Representations associated to small nilpotent orbits for complex Spin groups	This paper provides a comparison between the $K$-structure of unipotent representations and regular sections of bundles on nilpotent orbits for complex groups of type $D$. Precisely, let $ G_ 0 =Spin(2n,\mathbb C)$ be the Spin complex group viewed as a real group, and $K\cong G_0$ be the complexification of the maximal compact subgroup of $G_0$. We compute $K$-spectra of the regular functions on some small nilpotent orbits $\mathcal O$ transforming according to characters $\psi$ of $C_{ K}(\mathcal O)$ trivial on the connected component of the identity $C_{ K}(\mathcal O)^0$. We then match them with the ${K}$-types of the genuine (i.e. representations which do not factor to $SO(2n,\mathbb C)$) unipotent representations attached to $\mathcal O$.	0,0,1,0,0,0
A revision of the subtract-with-borrow random number generators	The most popular and widely used subtract-with-borrow generator, also known as RANLUX, is reimplemented as a linear congruential generator using large integer arithmetic with the modulus size of 576 bits. Modern computers, as well as the specific structure of the modulus inferred from RANLUX, allow for the development of a fast modular multiplication -- the core of the procedure. This was previously believed to be slow and have too high cost in terms of computing resources. Our tests show a significant gain in generation speed which is comparable with other fast, high quality random number generators. An additional feature is the fast skipping of generator states leading to a seeding scheme which guarantees the uniqueness of random number sequences.	1,1,0,0,0,0
Entanglement spectroscopy on a quantum computer	We present a quantum algorithm to compute the entanglement spectrum of arbitrary quantum states. The interesting universal part of the entanglement spectrum is typically contained in the largest eigenvalues of the density matrix which can be obtained from the lower Renyi entropies through the Newton-Girard method. Obtaining the $p$ largest eigenvalues ($\lambda_1>\lambda_2\ldots>\lambda_p$) requires a parallel circuit depth of $\mathcal{O}(p(\lambda_1/\lambda_p)^p)$ and $\mathcal{O}(p\log(N))$ qubits where up to $p$ copies of the quantum state defined on a Hilbert space of size $N$ are needed as the input. We validate this procedure for the entanglement spectrum of the topologically-ordered Laughlin wave function corresponding to the quantum Hall state at filling factor $\nu=1/3$. Our scaling analysis exposes the tradeoffs between time and number of qubits for obtaining the entanglement spectrum in the thermodynamic limit using finite-size digital quantum computers. We also illustrate the utility of the second Renyi entropy in predicting a topological phase transition and in extracting the localization length in a many-body localized system.	0,1,0,0,0,0
GAMER-2: a GPU-accelerated adaptive mesh refinement code -- accuracy, performance, and scalability	We present GAMER-2, a GPU-accelerated adaptive mesh refinement (AMR) code for astrophysics. It provides a rich set of features, including adaptive time-stepping, several hydrodynamic schemes, magnetohydrodynamics, self-gravity, particles, star formation, chemistry and radiative processes with GRACKLE, data analysis with yt, and memory pool for efficient object allocation. GAMER-2 is fully bitwise reproducible. For the performance optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes overlapping CPU computation, GPU computation, and CPU-GPU communication. Load balancing is achieved using a Hilbert space-filling curve on a level-by-level basis without the need to duplicate the entire AMR hierarchy on each MPI process. To provide convincing demonstrations of the accuracy and performance of GAMER-2, we directly compare with Enzo on isolated disk galaxy simulations and with FLASH on galaxy cluster merger simulations. We show that the physical results obtained by different codes are in very good agreement, and GAMER-2 outperforms Enzo and FLASH by nearly one and two orders of magnitude, respectively, on the Blue Waters supercomputers using $1-256$ nodes. More importantly, GAMER-2 exhibits similar or even better parallel scalability compared to the other two codes. We also demonstrate good weak and strong scaling using up to 4096 GPUs and 65,536 CPU cores, and achieve a uniform resolution as high as $10{,}240^3$ cells. Furthermore, GAMER-2 can be adopted as an AMR+GPUs framework and has been extensively used for the wave dark matter ($\psi$DM) simulations. GAMER-2 is open source (available at this https URL) and new contributions are welcome.	0,1,0,0,0,0
The Minimum Euclidean-Norm Point on a Convex Polytope: Wolfe's Combinatorial Algorithm is Exponential	The complexity of Philip Wolfe's method for the minimum Euclidean-norm point problem over a convex polytope has remained unknown since he proposed the method in 1974. The method is important because it is used as a subroutine for one of the most practical algorithms for submodular function minimization. We present the first example that Wolfe's method takes exponential time. Additionally, we improve previous results to show that linear programming reduces in strongly-polynomial time to the minimum norm point problem over a simplex.	1,0,1,0,0,0
Optimal Scheduling of Electrolyzer in Power Market with Dynamic Prices	Optimal scheduling of hydrogen production in dynamic pricing power market can maximize the profit of hydrogen producer; however, it highly depends on the accurate forecast of hydrogen consumption. In this paper, we propose a deep leaning based forecasting approach for predicting hydrogen consumption of fuel cell vehicles in future taxi industry. The cost of hydrogen production is minimized by utilizing the proposed forecasting tool to reduce the hydrogen produced during high cost on-peak hours and guide hydrogen producer to store sufficient hydrogen during low cost off-peak hours.	0,0,0,1,0,0
Achieveing reliable UDP transmission at 10 Gb/s using BSD socket for data acquisition systems	User Datagram Protocol (UDP) is a commonly used protocol for data transmission in small embedded systems. UDP as such is unreliable and packet losses can occur. The achievable data rates can suffer if optimal packet sizes are not used. The alternative, Transmission Control Protocol (TCP) guarantees the ordered delivery of data and automatically adjusts transmission to match the capability of the transmission link. Nevertheless UDP is often favored over TCP due to its simplicity, small memory and instruction footprints. Both UDP and TCP are implemented in all larger operating systems and commercial embedded frameworks. In addition UDP also supported on a variety of small hardware platforms such as Digital Signal Processors (DSP) Field Programmable Gate Arrays (FPGA). This is not so common for TCP. This paper describes how high speed UDP based data transmission with very low packet error ratios was achieved. The near-reliable communications link is used in a data acquisition (DAQ) system for the next generation of extremely intense neutron source, European Spallation Source. This paper presents measurements of UDP performance and reliability as achieved by employing several optimizations. The measurements were performed on Xeon E5 based CentOS (Linux) servers. The measured data rates are very close to the 10 Gb/s line rate, and zero packet loss was achieved. The performance was obtained utilizing a single processor core as transmitter and a single core as receiver. The results show that support for transmitting large data packets is a key parameter for good performance. Optimizations for throughput are: MTU, packet sizes, tuning Linux kernel parameters, thread affinity, core locality and efficient timers.	1,1,0,0,0,0
Coherent structures and spectral energy transfer in turbulent plasma: a space-filter approach	Plasma turbulence at scales of the order of the ion inertial length is mediated by several mechanisms, including linear wave damping, magnetic reconnection, formation and dissipation of thin current sheets, stochastic heating. It is now understood that the presence of localized coherent structures enhances the dissipation channels and the kinetic features of the plasma. However, no formal way of quantifying the relationship between scale-to-scale energy transfer and the presence of spatial structures has so far been presented. In this letter we quantify such relationship analyzing the results of a two-dimensional high-resolution Hall-MHD simulation. In particular, we employ the technique of space-filtering to derive a spectral energy flux term which defines, in any point of the computational domain, the signed flux of spectral energy across a given wavenumber. The characterization of coherent structures is performed by means of a traditional two-dimensional wavelet transformation. By studying the correlation between the spectral energy flux and the wavelet amplitude, we demonstrate the strong relationship between scale-to-scale transfer and coherent structures. Furthermore, by conditioning one quantity with respect to the other, we are able for the first time to quantify the inhomogeneity of the turbulence cascade induced by topological structures in the magnetic field. Taking into account the low filling-factor of coherent structures (i.e. they cover a small portion of space), it emerges that 80% of the spectral energy transfer (both in the direct and inverse cascade directions) is localized in about 50% of space, and 50% of the energy transfer is localized in only 25% of space.	0,1,0,0,0,0
Central limit theorem for linear spectral statistics of general separable sample covariance matrices with applications	In this paper, we consider the separable covariance model, which plays an important role in wireless communications and spatio-temporal statistics and describes a process where the time correlation does not depend on the spatial location and the spatial correlation does not depend on time. We established a central limit theorem for linear spectral statistics of general separable sample covariance matrices in the form of $\mathbf S_n=\frac1n\mathbf T_{1n}\mathbf X_n\mathbf T_{2n}\mathbf X_n^*\mathbf T_{1n}^*$ where $\mathbf X_n=(x_{jk})$ is of $m_1\times m_2$ dimension, the entries $\{x_{jk}, j=1,...,m_1, k=1,...,m_2\}$ are independent and identically distributed complex variables with zero means and unit variances, $\mathbf T_{1n}$ is a $p\times m_1 $ complex matrix and $\mathbf T_{2n}$ is an $m_2\times m_2$ Hermitian matrix. We then apply this general central limit theorem to the problem of testing white noise in time series.	0,0,1,1,0,0
Lions' formula for RKHSs of real harmonic functions on Lipschitz domains	Let $ \Omega$ be a bounded Lipschitz domain of $ \mathbb{R}^{d}.$ The purpose of this paper is to establish Lions' formula for reproducing kernel Hilbert spaces $\mathcal H^s(\Omega)$ of real harmonic functions elements of the usual Sobolev space $H^s(\Omega)$ for $s\geq 0.$ To this end, we provide a functional characterization of $\mathcal H^s(\Omega)$ via some new families of positive self-adjoint operators, describe their trace data and discuss the values of $s$ for which they are RKHSs. Also a construction of an orthonormal basis of $\mathcal H^s(\Omega)$ is established.	0,0,1,0,0,0
A note on clustered cells	This note contains additions to the paper 'Clustered cell decomposition in P-minimal structures' (arXiv:1612.02683). We discuss a question which was raised in that paper, on the order of clustered cells. We also consider a notion of cells of minimal order, which is a slight optimalisation of the theorem from the original paper.	0,0,1,0,0,0
Tight Analysis for the 3-Majority Consensus Dynamics	We present a tight analysis for the well-studied randomized 3-majority dynamics of stabilizing consensus, hence answering the main open question of Becchetti et al. [SODA'16]. Consider a distributed system of n nodes, each initially holding an opinion in {1, 2, ..., k}. The system should converge to a setting where all (non-corrupted) nodes hold the same opinion. This consensus opinion should be \emph{valid}, meaning that it should be among the initially supported opinions, and the (fast) convergence should happen even in the presence of a malicious adversary who can corrupt a bounded number of nodes per round and in particular modify their opinions. A well-studied distributed algorithm for this problem is the 3-majority dynamics, which works as follows: per round, each node gathers three opinions --- say by taking its own and two of other nodes sampled at random --- and then it sets its opinion equal to the majority of this set; ties are broken arbitrarily, e.g., towards the node's own opinion. Becchetti et al. [SODA'16] showed that the 3-majority dynamics converges to consensus in O((k^2\sqrt{\log n} + k\log n)(k+\log n)) rounds, even in the presence of a limited adversary. We prove that, even with a stronger adversary, the convergence happens within O(k\log n) rounds. This bound is known to be optimal.	1,0,0,0,0,0
Full likelihood inference for max-stable data	We show how to perform full likelihood inference for max-stable multivariate distributions or processes based on a stochastic Expectation-Maximisation algorithm, which combines statistical and computational efficiency in high-dimensions. The good performance of this methodology is demonstrated by simulation based on the popular logistic and Brown--Resnick models, and it is shown to provide dramatic computational time improvements with respect to a direct computation of the likelihood. Strategies to further reduce the computational burden are also discussed.	0,0,0,1,0,0
Plausible Deniability for Privacy-Preserving Data Synthesis	Releasing full data records is one of the most challenging problems in data privacy. On the one hand, many of the popular techniques such as data de-identification are problematic because of their dependence on the background knowledge of adversaries. On the other hand, rigorous methods such as the exponential mechanism for differential privacy are often computationally impractical to use for releasing high dimensional data or cannot preserve high utility of original data due to their extensive data perturbation. This paper presents a criterion called plausible deniability that provides a formal privacy guarantee, notably for releasing sensitive datasets: an output record can be released only if a certain amount of input records are indistinguishable, up to a privacy parameter. This notion does not depend on the background knowledge of an adversary. Also, it can efficiently be checked by privacy tests. We present mechanisms to generate synthetic datasets with similar statistical properties to the input data and the same format. We study this technique both theoretically and experimentally. A key theoretical result shows that, with proper randomization, the plausible deniability mechanism generates differentially private synthetic data. We demonstrate the efficiency of this generative technique on a large dataset; it is shown to preserve the utility of original data with respect to various statistical analysis and machine learning measures.	1,0,0,1,0,0
Classifying and Qualifying GUI Defects	Graphical user interfaces (GUIs) are integral parts of software systems that require interactions from their users. Software testers have paid special attention to GUI testing in the last decade, and have devised techniques that are effective in finding several kinds of GUI errors. However, the introduction of new types of interactions in GUIs (e.g., direct manipulation) presents new kinds of errors that are not targeted by current testing techniques. We believe that to advance GUI testing, the community needs a comprehensive and high level GUI fault model, which incorporates all types of interactions. The work detailed in this paper establishes 4 contributions: 1) A GUI fault model designed to identify and classify GUI faults. 2) An empirical analysis for assessing the relevance of the proposed fault model against failures found in real GUIs. 3) An empirical assessment of two GUI testing tools (i.e. GUITAR and Jubula) against those failures. 4) GUI mutants we've developed according to our fault model. These mutants are freely available and can be reused by developers for benchmarking their GUI testing tools.	1,0,0,0,0,0
Proceedings 15th International Conference on Automata and Formal Languages	The 15th International Conference on Automata and Formal Languages (AFL 2017) was held in Debrecen, Hungary, from September 4 to 6, 2017. The conference was organized by the Faculty of Informatics of the University of Debrecen and the Faculty of Informatics of the Eötvös Loránd University of Budapest. Topics of interest covered all aspects of automata and formal languages, including theory and applications.	1,0,0,0,0,0
Haro 11: Where is the Lyman continuum source?	Identifying the mechanism by which high energy Lyman continuum (LyC) photons escaped from early galaxies is one of the most pressing questions in cosmic evolution. Haro 11 is the best known local LyC leaking galaxy, providing an important opportunity to test our understanding of LyC escape. The observed LyC emission in this galaxy presumably originates from one of the three bright, photoionizing knots known as A, B, and C. It is known that Knot C has strong Ly$\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray source, which may be a low-luminosity AGN. To clarify the LyC source, we carry out ionization-parameter mapping (IPM) by obtaining narrow-band imaging from the Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved ratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization structure of the interstellar medium and allows us to identify optically thin regions. To optimize the continuum subtraction, we introduce a new method for determining the best continuum scale factor derived from the mode of the continuum-subtracted, image flux distribution. We find no conclusive evidence of LyC escape from Knots B or C, but instead, we identify a high-ionization region extending over at least 1 kpc from Knot A. Knot A shows evidence of an extremely young age ($\lesssim 1$ Myr), perhaps containing very massive stars ($>100$ M$_\odot$). It is weak in Ly$\alpha$, so if it is confirmed as the LyC source, our results imply that LyC emission may be independent of Ly$\alpha$ emission.	0,1,0,0,0,0
Using English as Pivot to Extract Persian-Italian Parallel Sentences from Non-Parallel Corpora	The effectiveness of a statistical machine translation system (SMT) is very dependent upon the amount of parallel corpus used in the training phase. For low-resource language pairs there are not enough parallel corpora to build an accurate SMT. In this paper, a novel approach is presented to extract bilingual Persian-Italian parallel sentences from a non-parallel (comparable) corpus. In this study, English is used as the pivot language to compute the matching scores between source and target sentences and candidate selection phase. Additionally, a new monolingual sentence similarity metric, Normalized Google Distance (NGD) is proposed to improve the matching process. Moreover, some extensions of the baseline system are applied to improve the quality of extracted sentences measured with BLEU. Experimental results show that using the new pivot based extraction can increase the quality of bilingual corpus significantly and consequently improves the performance of the Persian-Italian SMT system.	1,0,0,0,0,0
Direct mapping of the temperature and velocity gradients in discs. Imaging the vertical CO snow line around IM Lupi	Accurate measurements of the physical structure of protoplanetary discs are critical inputs for planet formation models. These constraints are traditionally established via complex modelling of continuum and line observations. Instead, we present an empirical framework to locate the CO isotopologue emitting surfaces from high spectral and spatial resolution ALMA observations. We apply this framework to the disc surrounding IM Lupi, where we report the first direct, i.e. model independent, measurements of the radial and vertical gradients of temperature and velocity in a protoplanetary disc. The measured disc structure is consistent with an irradiated self-similar disc structure, where the temperature increases and the velocity decreases towards the disc surface. We also directly map the vertical CO snow line, which is located at about one gas scale height at radii between 150 and 300 au, with a CO freeze-out temperature of $21\pm2$ K. In the outer disc ($> 300$ au), where the gas surface density transitions from a power law to an exponential taper, the velocity rotation field becomes significantly sub-Keplerian, in agreement with the expected steeper pressure gradient. The sub-Keplerian velocities should result in a very efficient inward migration of large dust grains, explaining the lack of millimetre continuum emission outside of 300 au. The sub-Keplerian motions may also be the signature of the base of an externally irradiated photo-evaporative wind. In the same outer region, the measured CO temperature above the snow line decreases to $\approx$ 15 K because of the reduced gas density, which can result in a lower CO freeze-out temperature, photo-desorption, or deviations from local thermodynamic equilibrium.	0,1,0,0,0,0
A simple introduction to Karmarkar's Algorithm for Linear Programming	An extremely simple, description of Karmarkar's algorithm with very few technical terms is given.	1,0,0,0,0,0
Decentralized Random Walk-Based Data Collection in Networks	We analyze a decentralized random walk-based algorithm for data collection at the sink in a multi-hop sensor network. Our algorithm, Random-Collect, which involves data packets being passed to random neighbors in the network according to a random walk mechanism, requires no configuration and incurs no routing overhead. To analyze this method, we model the data generation process as independent Bernoulli arrivals at the source nodes. We analyze both latency and throughput in this setting, providing a theoretical lower bound for the throughput and a theoretical upper bound for the latency. The main contribution of our paper, however, is the throughput result: we present a general lower bound on the throughput achieved by our data collection method in terms of the underlying network parameters. In particular, we show that the rate at which our algorithm can collect data depends on the spectral gap of the given random walk's transition matrix and if the random walk is simple then it also depends on the maximum and minimum degrees of the graph modeling the network. For latency, we show that the time taken to collect data not only depends on the worst-case hitting time of the given random walk but also depends on the data arrival rate. In fact, our latency bound reflects the data rate-latency trade-off i.e., in order to achieve a higher data rate we need to compromise on latency and vice-versa. We also discuss some examples that demonstrate that our lower bound on the data rate is optimal up to constant factors, i.e., there exists a network topology and sink placement for which the maximum stable data rate is just a constant factor above our lower bound.	1,0,0,0,0,0
Criteria for strict monotonicity of the mixed volume of convex polytopes	Let $P_1,\dots, P_n$ and $Q_1,\dots, Q_n$ be convex polytopes in $\mathbb{R}^n$ such that $P_i\subset Q_i$. It is well-known that the mixed volume has the monotonicity property: $V(P_1,\dots,P_n)\leq V(Q_1,\dots,Q_n)$. We give two criteria for when this inequality is strict in terms of essential collections of faces as well as mixed polyhedral subdivisions. This geometric result allows us to characterize sparse polynomial systems with Newton polytopes $P_1,\dots,P_n$ whose number of isolated solutions equals the normalized volume of the convex hull of $P_1\cup\dots\cup P_n$. In addition, we obtain an analog of Cramer's rule for sparse polynomial systems.	0,0,1,0,0,0
A note on a separating system of rational invariants for finite dimensional generic algebras	The paper deals with a construction of a separating system of rational invariants for finite dimensional generic algebras. In the process of dealing an approach to a rough classification of finite dimensional algebras is offered by attaching them some quadratic forms.	0,0,1,0,0,0
pH dependence of charge multipole moments in proteins	Electrostatic interactions play a fundamental role in the structure and function of proteins. Due to ionizable amino acid residues present on the solvent-exposed surfaces of proteins, the protein charge is not constant but varies with the changes in the environment -- most notably, the pH of the surrounding solution. We study the effects of pH on the charge of four globular proteins by expanding their surface charge distributions in terms of multipoles. The detailed representation of the charges on the proteins is in this way replaced by the magnitudes and orientations of the multipole moments of varying order. Focusing on the three lowest-order multipoles -- the total charge, dipole, and quadrupole moment -- we show that the value of pH influences not only their magnitudes, but more notably and importantly also the spatial orientation of their principal axes. Our findings imply important consequences for the study of protein-protein interactions and the assembly of both proteinaceous shells and patchy colloids with dissociable charge groups.	0,1,0,0,0,0
Friction Variability in Planar Pushing Data: Anisotropic Friction and Data-collection Bias	Friction plays a key role in manipulating objects. Most of what we do with our hands, and most of what robots do with their grippers, is based on the ability to control frictional forces. This paper aims to better understand the variability and predictability of planar friction. In particular, we focus on the analysis of a recent dataset on planar pushing by Yu et al. [1] devised to create a data-driven footprint of planar friction. We show in this paper how we can explain a significant fraction of the observed unconventional phenomena, e.g., stochasticity and multi-modality, by combining the effects of material non-homogeneity, anisotropy of friction and biases due to data collection dynamics, hinting that the variability is explainable but inevitable in practice. We introduce an anisotropic friction model and conduct simulation experiments comparing with more standard isotropic friction models. The anisotropic friction between object and supporting surface results in convergence of initial condition during the automated data collection. Numerical results confirm that the anisotropic friction model explains the bias in the dataset and the apparent stochasticity in the outcome of a push. The fact that the data collection process itself can originate biases in the collected datasets, resulting in deterioration of trained models, calls attention to the data collection dynamics.	1,0,0,0,0,0
Large deviations of a tracer in the symmetric exclusion process	The one-dimensional symmetric exclusion process, the simplest interacting particle process, is a lattice-gas made of particles that hop symmetrically on a discrete line respecting hard-core exclusion. The system is prepared on the infinite lattice with a step initial profile with average densities $\rho_{+}$ and $\rho_{-}$ on the right and on the left of the origin. When $\rho_{+} = \rho_{-}$, the gas is at equilibrium and undergoes stationary fluctuations. When these densities are unequal, the gas is out of equilibrium and will remain so forever. A tracer, or a tagged particle, is initially located at the boundary between the two domains; its position $X_t$ is a random observable in time, that carries information on the non-equilibrium dynamics of the whole system. We derive an exact formula for the cumulant generating function and the large deviation function of $X_t$, in the long time limit, and deduce the full statistical properties of the tracer's position. The equilibrium fluctuations of the tracer's position, when the density is uniform, are obtained as an important special case.	0,1,0,0,0,0
Diffusive Tidal Evolution for Migrating hot Jupiters	I consider a Jovian planet on a highly eccentric orbit around its host star, a situation produced by secular interactions with its planetary or stellar companions. The tidal interactions at every periastron passage exchange energy between the orbit and the planet's degree-2 fundamental-mode. Starting from zero energy, the f-mode can diffusively grow to large amplitudes if its one-kick energy gain > 10^-5 of the orbital energy. This requires a pericentre distance of < 4 tidal radii (or 1.6 Roche radii). If the f-mode has a non-negligible initial energy, diffusive evolution can occur at a lower threshold. The first effect can stall the secular migration as the f-mode can absorb orbital energy and decouple the planet from its secular perturbers, parking all migrating jupiters safely outside the zone of tidal disruption. The second effect leads to rapid orbit circularization as it allows an excited f-mode to continuously absorb orbital energy as the orbit eccentricity decreases. So without any explicit dissipation, other than the fact that the f-mode will damp nonlinearly when its amplitude reaches unity, the planet can be transported from a few AU to ~ 0.2 AU in ~ 10^4 yrs. Such a rapid circularization is equivalent to a dissipation factor Q ~ 1, and it explains the observed deficit of super-eccentric Jovian planets. Lastly, the repeated f-mode breaking likely deposit energy and angular momentum in the outer envelope, and avoid thermally ablating the planet. Overall, this work boosts the case for forming hot Jupiters through high-eccentricity secular migration.	0,1,0,0,0,0
Getting around the Halting Problem	The Halting Theorem establishes that there is no program (or Turing machine) H that can decide in all cases if an arbitrary program n halts on input m. The conjecture of this paper is that nevertheless there exists a sound program H such that if it halts it answers either yes or no, and can also in a certain sense identify all the cases it is unable to decide. The Halting Theorem can be proved by constructing a counterexample, i.e. a program that attempts to assert that it itself does not halt. The thesis is that there exists a program that proves about itself that its own attempt to prove, that the counterexample does not halt, does not halt. This outcome can be interpreted as it is NOT TRUE that the counterexample does not halt as opposed to it is FALSE that the counterexample does not halt. This becomes possible when the Recursion Theorem is reinterpreted as mutual necessitation rather than equivalence.	1,0,0,0,0,0
Configuration Path Integral Monte Carlo Approach to the Static Density Response of the Warm Dense Electron Gas	Precise knowledge of the static density response function (SDRF) of the uniform electron gas (UEG) serves as key input for numerous applications, most importantly for density functional theory beyond generalized gradient approximations. Here we extend the configuration path integral Monte Carlo (CPIMC) formalism that was previously applied to the spatially uniform electron gas to the case of an inhomogeneous electron gas by adding a spatially periodic external potential. This procedure has recently been successfully used in permutation blocking path integral Monte Carlo simulations (PB-PIMC) of the warm dense electron gas [Dornheim \textit{et al.}, Phys. Rev. E in press, arXiv:1706.00315], but this method is restricted to low and moderate densities. Implementing this procedure into CPIMC allows us to obtain exact finite temperature results for the SDRF of the electron gas at \textit{high to moderate densities} closing the gap left open by the PB-PIMC data. In this paper we demonstrate how the CPIMC formalism can be efficiently extended to the spatially inhomogeneous electron gas and present the first data points. Finally, we discuss finite size errors involved in the quantum Monte Carlo results for the SDRF in detail and present a solution how to remove them that is based on a generalization of ground state techniques.	0,1,0,0,0,0
Large-sample approximations for variance-covariance matrices of high-dimensional time series	Distributional approximations of (bi--) linear functions of sample variance-covariance matrices play a critical role to analyze vector time series, as they are needed for various purposes, especially to draw inference on the dependence structure in terms of second moments and to analyze projections onto lower dimensional spaces as those generated by principal components. This particularly applies to the high-dimensional case, where the dimension $d$ is allowed to grow with the sample size $n$ and may even be larger than $n$. We establish large-sample approximations for such bilinear forms related to the sample variance-covariance matrix of a high-dimensional vector time series in terms of strong approximations by Brownian motions. The results cover weakly dependent as well as many long-range dependent linear processes and are valid for uniformly $ \ell_1 $-bounded projection vectors, which arise, either naturally or by construction, in many statistical problems extensively studied for high-dimensional series. Among those problems are sparse financial portfolio selection, sparse principal components, the LASSO, shrinkage estimation and change-point analysis for high--dimensional time series, which matter for the analysis of big data and are discussed in greater detail.	0,0,1,1,0,0
Non-convex Conditional Gradient Sliding	We investigate a projection free method, namely conditional gradient sliding on batched, stochastic and finite-sum non-convex problem. CGS is a smart combination of Nesterov's accelerated gradient method and Frank-Wolfe (FW) method, and outperforms FW in the convex setting by saving gradient computations. However, the study of CGS in the non-convex setting is limited. In this paper, we propose the non-convex conditional gradient sliding (NCGS) which surpasses the non-convex Frank-Wolfe method in batched, stochastic and finite-sum setting.	0,0,1,0,0,0
Compressed sensing and optimal denoising of monotone signals	We consider the problems of compressed sensing and optimal denoising for signals $\mathbf{x_0}\in\mathbb{R}^N$ that are monotone, i.e., $\mathbf{x_0}(i+1) \geq \mathbf{x_0}(i)$, and sparsely varying, i.e., $\mathbf{x_0}(i+1) > \mathbf{x_0}(i)$ only for a small number $k$ of indices $i$. We approach the compressed sensing problem by minimizing the total variation norm restricted to the class of monotone signals subject to equality constraints obtained from a number of measurements $A\mathbf{x_0}$. For random Gaussian sensing matrices $A\in\mathbb{R}^{m\times N}$ we derive a closed form expression for the number of measurements $m$ required for successful reconstruction with high probability. We show that the probability undergoes a phase transition as $m$ varies, and depends not only on the number of change points, but also on their location. For denoising we regularize with the same norm and derive a formula for the optimal regularizer weight that depends only mildly on $\mathbf{x_0}$. We obtain our results using the statistical dimension tool.	1,0,1,1,0,0
Inner-Scene Similarities as a Contextual Cue for Object Detection	Using image context is an effective approach for improving object detection. Previously proposed methods used contextual cues that rely on semantic or spatial information. In this work, we explore a different kind of contextual information: inner-scene similarity. We present the CISS (Context by Inner Scene Similarity) algorithm, which is based on the observation that two visually similar sub-image patches are likely to share semantic identities, especially when both appear in the same image. CISS uses base-scores provided by a base detector and performs as a post-detection stage. For each candidate sub-image (denoted anchor), the CISS algorithm finds a few similar sub-images (denoted supporters), and, using them, calculates a new enhanced score for the anchor. This is done by utilizing the base-scores of the supporters and a pre-trained dependency model. The new scores are modeled as a linear function of the base scores of the anchor and the supporters and is estimated using a minimum mean square error optimization. This approach results in: (a) improved detection of partly occluded objects (when there are similar non-occluded objects in the scene), and (b) fewer false alarms (when the base detector mistakenly classifies a background patch as an object). This work relates to Duncan and Humphreys' "similarity theory," a psychophysical study. which suggested that the human visual system perceptually groups similar image regions and that the classification of one region is affected by the estimated identity of the other. Experimental results demonstrate the enhancement of a base detector's scores on the PASCAL VOC dataset.	1,0,0,0,0,0
Numerical solutions of an unsteady 2-D incompressible flow with heat and mass transfer at low, moderate, and high Reynolds numbers	In this paper, we have proposed a modified Marker-And-Cell (MAC) method to investigate the problem of an unsteady 2-D incompressible flow with heat and mass transfer at low, moderate, and high Reynolds numbers with no-slip and slip boundary conditions. We have used this method to solve the governing equations along with the boundary conditions and thereby to compute the flow variables, viz. $u$-velocity, $v$-velocity, $P$, $T$, and $C$. We have used the staggered grid approach of this method to discretize the governing equations of the problem. A modified MAC algorithm was proposed and used to compute the numerical solutions of the flow variables for Reynolds numbers $Re = 10$, 500, and 50,000 in consonance with low, moderate, and high Reynolds numbers. We have also used appropriate Prandtl $(Pr)$ and Schmidt $(Sc)$ numbers in consistence with relevancy of the physical problem considered. We have executed this modified MAC algorithm with the aid of a computer program developed and run in C compiler. We have also computed numerical solutions of local Nusselt $(Nu)$ and Sherwood $(Sh)$ numbers along the horizontal line through the geometric center at low, moderate, and high Reynolds numbers for fixed $Pr = 6.62$ and $Sc = 340$ for two grid systems at time $t = 0.0001s$. Our numerical solutions for u and v velocities along the vertical and horizontal line through the geometric center of the square cavity for $Re = 100$ has been compared with benchmark solutions available in the literature and it has been found that they are in good agreement. The present numerical results indicate that, as we move along the horizontal line through the geometric center of the domain, we observed that, the heat and mass transfer decreases up to the geometric center. It, then, increases symmetrically.	0,1,0,0,0,0
Gaussian One-Armed Bandit and Optimization of Batch Data Processing	We consider the minimax setup for Gaussian one-armed bandit problem, i.e. the two-armed bandit problem with Gaussian distributions of incomes and known distribution corresponding to the first arm. This setup naturally arises when the optimization of batch data processing is considered and there are two alternative processing methods available with a priori known efficiency of the first method. One should estimate the efficiency of the second method and provide predominant usage of the most efficient of both them. According to the main theorem of the theory of games minimax strategy and minimax risk are searched for as Bayesian ones corresponding to the worst-case prior distribution. As a result, we obtain the recursive integro-difference equation and the second order partial differential equation in the limiting case as the number of batches goes to infinity. This makes it possible to determine minimax risk and minimax strategy by numerical methods. If the number of batches is large enough we show that batch data processing almost does not influence the control performance, i.e. the value of the minimax risk. Moreover, in case of Bernoulli incomes and large number of batches, batch data processing provides almost the same minimax risk as the optimal one-by-one data processing.	0,0,1,1,0,0
CO2 infrared emission as a diagnostic of planet-forming regions of disks	[Abridged] The infrared ro-vibrational emission lines from organic molecules in the inner regions of protoplanetary disks are unique probes of the physical and chemical structure of planet forming regions and the processes that shape them. The non-LTE excitation effects of carbon dioxide (CO2) are studied in a full disk model to evaluate: (i) what the emitting regions of the different CO2 ro-vibrational bands are; (ii) how the CO2 abundance can be best traced using CO2 ro-vibrational lines using future JWST data and; (iii) what the excitation and abundances tell us about the inner disk physics and chemistry. CO2 is a major ice component and its abundance can potentially test models with migrating icy pebbles across the iceline. A full non-LTE CO2 excitation model has been built. The characteristics of the model are tested using non-LTE slab models. Subsequently the CO2 line formation has been modelled using a two-dimensional disk model representative of T-Tauri disks. The CO2 gas that emits in the 15 $\mu$m and 4.5 $\mu$m regions of the spectrum is not in LTE and arises in the upper layers of disks, pumped by infrared radiation. The v$_2$ 15 $\mu$m feature is dominated by optically thick emission for most of the models that fit the observations and increases linearly with source luminosity. Its narrowness compared with that of other molecules stems from a combination of the low rotational excitation temperature (~250 K) and the inherently narrower feature for CO2. The inferred CO2 abundances derived for observed disks are more than two orders of magnitude lower than those in interstellar ices (~10$^5$), similar to earlier LTE disk estimates. Line-to-continuum ratios are low, of order a few %, thus high signal-to-noise (S/N > 300) observations are needed for individual line detections. Prospects of accurate abundance retreival with JWST-MIRI and JWST-NIRSpec are discussed.	0,1,0,0,0,0
Existence of Noise Induced Order, a Computer Aided Proof	We prove, by a computer aided proof, the existence of noise induced order in the model of chaotic chemical reactions where it was first discovered numerically by Matsumoto and Tsuda in 1983. We prove that in this random dynamical system the increase in amplitude of the noise causes the Lyapunov exponent to decrease from positive to negative, stabilizing the system. The method used is based on a certified approximation of the stationary measure in the $L^{1}$ norm. This is done by an efficient algorithm which is general enough to be adapted to any piecewise differentiable dynamical system on the interval with additive noise. We also prove that the stationary measure of the system and its Lyapunov exponent have a Lipschitz stability under several kinds of perturbation of the noise and of the system itself. The Lipschitz constants of this stability result are also estimated explicitly.	0,0,1,0,0,0
Monolayer FeSe on SrTiO$_3$	Epitaxial engineering of solid-state heterointerfaces is a leading avenue to realizing enhanced or novel electronic states of matter. As a recent example, bulk FeSe is an unconventional superconductor with a modest transition temperature ($T_c$) of 9 K. When a single atomic layer of FeSe is grown on SrTiO$_3$, however, its $T_c$ can skyrocket by an order of magnitude to 65 K or 109 K. Since this discovery in 2012, efforts to reproduce, understand, and extend these findings continue to draw both excitement and scrutiny. In this review, we first present a critical survey of experimental measurements performed using a wide range of techniques. We then turn to the open question of microscopic mechanisms of superconductivity. We examine contrasting indications for both phononic (conventional) and magnetic/orbital (unconventional) means of electron pairing, and speculations about whether they could work cooperatively to boost $T_c$ in a monolayer of FeSe.	0,1,0,0,0,0
Compositional Human Pose Regression	Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M and is competitive with state-of-the-art results on MPII.	1,0,0,0,0,0
The trouble with tensor ring decompositions	The tensor train decomposition decomposes a tensor into a "train" of 3-way tensors that are interconnected through the summation of auxiliary indices. The decomposition is stable, has a well-defined notion of rank and enables the user to perform various linear algebra operations on vectors and matrices of exponential size in a computationally efficient manner. The tensor ring decomposition replaces the train by a ring through the introduction of one additional auxiliary variable. This article discusses a major issue with the tensor ring decomposition: its inability to compute an exact minimal-rank decomposition from a decomposition with sub-optimal ranks. Both the contraction operation and Hadamard product are motivated from applications and it is shown through simple examples how the tensor ring-rounding procedure fails to retrieve minimal-rank decompositions with these operations. These observations, together with the already known issue of not being able to find a best low-rank tensor ring approximation to a given tensor indicate that the applicability of tensor rings is severely limited.	1,0,0,0,0,0
L-Graphs and Monotone L-Graphs	In an $\mathsf{L}$-embedding of a graph, each vertex is represented by an $\mathsf{L}$-segment, and two segments intersect each other if and only if the corresponding vertices are adjacent in the graph. If the corner of each $\mathsf{L}$-segment in an $\mathsf{L}$-embedding lies on a straight line, we call it a monotone $\mathsf{L}$-embedding. In this paper we give a full characterization of monotone $\mathsf{L}$-embeddings by introducing a new class of graphs which we call "non-jumping" graphs. We show that a graph admits a monotone $\mathsf{L}$-embedding if and only if the graph is a non-jumping graph. Further, we show that outerplanar graphs, convex bipartite graphs, interval graphs, 3-leaf power graphs, and complete graphs are subclasses of non-jumping graphs. Finally, we show that distance-hereditary graphs and $k$-leaf power graphs ($k\le 4$) admit $\mathsf{L}$-embeddings.	1,0,0,0,0,0
The OSIRIS-REx Visible and InfraRed Spectrometer (OVIRS): Spectral Maps of the Asteroid Bennu	The OSIRIS-REx Visible and Infrared Spectrometer (OVIRS) is a point spectrometer covering the spectral range of 0.4 to 4.3 microns (25,000-2300 cm-1). Its primary purpose is to map the surface composition of the asteroid Bennu, the target asteroid of the OSIRIS-REx asteroid sample return mission. The information it returns will help guide the selection of the sample site. It will also provide global context for the sample and high spatial resolution spectra that can be related to spatially unresolved terrestrial observations of asteroids. It is a compact, low-mass (17.8 kg), power efficient (8.8 W average), and robust instrument with the sensitivity needed to detect a 5% spectral absorption feature on a very dark surface (3% reflectance) in the inner solar system (0.89-1.35 AU). It, in combination with the other instruments on the OSIRIS-REx Mission, will provide an unprecedented view of an asteroid's surface.	0,1,0,0,0,0
ART: adaptive residual--time restarting for Krylov subspace matrix exponential evaluations	In this paper a new restarting method for Krylov subspace matrix exponential evaluations is proposed. Since our restarting technique essentially employs the residual, some convergence results for the residual are given. We also discuss how the restart length can be adjusted after each restart cycle, which leads to an adaptive restarting procedure. Numerical tests are presented to compare our restarting with three other restarting methods. Some of the algorithms described in this paper are a part of the Octave/Matlab package expmARPACK available at this http URL.	1,0,0,0,0,0
Towards personalized human AI interaction - adapting the behavior of AI agents using neural signatures of subjective interest	Reinforcement Learning AI commonly uses reward/penalty signals that are objective and explicit in an environment -- e.g. game score, completion time, etc. -- in order to learn the optimal strategy for task performance. However, Human-AI interaction for such AI agents should include additional reinforcement that is implicit and subjective -- e.g. human preferences for certain AI behavior -- in order to adapt the AI behavior to idiosyncratic human preferences. Such adaptations would mirror naturally occurring processes that increase trust and comfort during social interactions. Here, we show how a hybrid brain-computer-interface (hBCI), which detects an individual's level of interest in objects/events in a virtual environment, can be used to adapt the behavior of a Deep Reinforcement Learning AI agent that is controlling a virtual autonomous vehicle. Specifically, we show that the AI learns a driving strategy that maintains a safe distance from a lead vehicle, and most novelly, preferentially slows the vehicle when the human passengers of the vehicle encounter objects of interest. This adaptation affords an additional 20\% viewing time for subjectively interesting objects. This is the first demonstration of how an hBCI can be used to provide implicit reinforcement to an AI agent in a way that incorporates user preferences into the control system.	1,0,0,1,0,0
On Polymorphic Sessions and Functions: A Tale of Two (Fully Abstract) Encodings	This work exploits the logical foundation of session types to determine what kind of type discipline for the pi-calculus can exactly capture, and is captured by, lambda-calculus behaviours. Leveraging the proof theoretic content of the soundness and completeness of sequent calculus and natural deduction presentations of linear logic, we develop the first mutually inverse and fully abstract processes-as-functions and functions-as-processes encodings between a polymorphic session pi-calculus and a linear formulation of System F. We are then able to derive results of the session calculus from the theory of the lambda-calculus: (1) we obtain a characterisation of inductive and coinductive session types via their algebraic representations in System F; and (2) we extend our results to account for value and process passing, entailing strong normalisation.	1,0,0,0,0,0
On Compiling DNNFs without Determinism	State-of-the-art knowledge compilers generate deterministic subsets of DNNF, which have been recently shown to be exponentially less succinct than DNNF. In this paper, we propose a new method to compile DNNFs without enforcing determinism necessarily. Our approach is based on compiling deterministic DNNFs with the addition of auxiliary variables to the input formula. These variables are then existentially quantified from the deterministic structure in linear time, which would lead to a DNNF that is equivalent to the input formula and not necessarily deterministic. On the theoretical side, we show that the new method could generate exponentially smaller DNNFs than deterministic ones, even by adding a single auxiliary variable. Further, we show that various existing techniques that introduce auxiliary variables to the input formulas can be employed in our framework. On the practical side, we empirically demonstrate that our new method can significantly advance DNNF compilation on certain benchmarks.	1,0,0,0,0,0
A rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ with 432 symmetries	The recent discovery that the exponent of matrix multiplication is determined by the rank of the symmetrized matrix multiplication tensor has invigorated interest in better understanding symmetrized matrix multiplication. I present an explicit rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ and describe its symmetry group.	0,0,1,0,0,0
Neural networks for post-processing ensemble weather forecasts	Ensemble weather predictions require statistical post-processing of systematic errors to obtain reliable and accurate probabilistic forecasts. Traditionally, this is accomplished with distributional regression models in which the parameters of a predictive distribution are estimated from a training period. We propose a flexible alternative based on neural networks that can incorporate nonlinear relationships between arbitrary predictor variables and forecast distribution parameters that are automatically learned in a data-driven way rather than requiring pre-specified link functions. In a case study of 2-meter temperature forecasts at surface stations in Germany, the neural network approach significantly outperforms benchmark post-processing methods while being computationally more affordable. Key components to this improvement are the use of auxiliary predictor variables and station-specific information with the help of embeddings. Furthermore, the trained neural network can be used to gain insight into the importance of meteorological variables thereby challenging the notion of neural networks as uninterpretable black boxes. Our approach can easily be extended to other statistical post-processing and forecasting problems. We anticipate that recent advances in deep learning combined with the ever-increasing amounts of model and observation data will transform the post-processing of numerical weather forecasts in the coming decade.	0,0,0,1,0,0
New Bounds on the Field Size for Maximally Recoverable Codes Instantiating Grid-like Topologies	In recent years, the rapidly increasing amounts of data created and processed through the internet resulted in distributed storage systems employing erasure coding based schemes. Aiming to balance the tradeoff between data recovery for correlated failures and efficient encoding and decoding, distributed storage systems employing maximally recoverable codes came up. Unifying a number of topologies considered both in theory and practice, Gopalan \cite{Gopalan2017} initiated the study of maximally recoverable codes for grid-like topologies. In this paper, we focus on the maximally recoverable codes that instantiate grid-like topologies $T_{m\times n}(1,b,0)$. To characterize the property of codes for these topologies, we introduce the notion of \emph{pseudo-parity check matrix}. Then, using the hypergraph independent set approach, we establish the first polynomial upper bound on the field size needed for achieving the maximal recoverability in topologies $T_{m\times n}(1,b,0)$, when $n$ is large enough. And we further improve this general upper bound for topologies $T_{4\times n}(1,2,0)$ and $T_{3\times n}(1,3,0)$. By relating the problem to generalized \emph{Sidon sets} in $\mathbb{F}_q$, we also obtain non-trivial lower bounds on the field size for maximally recoverable codes that instantiate topologies $T_{4\times n}(1,2,0)$ and $T_{3\times n}(1,3,0)$.	1,0,0,0,0,0
On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests	The reproducing kernel Hilbert space (RKHS) embedding of distributions offers a general and flexible framework for testing problems in arbitrary domains and has attracted considerable amount of attention in recent years. To gain insights into their operating characteristics, we study here the statistical performance of such approaches within a minimax framework. Focusing on the case of goodness-of-fit tests, our analyses show that a vanilla version of the kernel-embedding based test could be suboptimal, and suggest a simple remedy by moderating the embedding. We prove that the moderated approach provides optimal tests for a wide range of deviations from the null and can also be made adaptive over a large collection of interpolation spaces. Numerical experiments are presented to further demonstrate the merits of our approach.	0,0,1,1,0,0
A Riemannian gossip approach to subspace learning on Grassmann manifold	In this paper, we focus on subspace learning problems on the Grassmann manifold. Interesting applications in this setting include low-rank matrix completion and low-dimensional multivariate regression, among others. Motivated by privacy concerns, we aim to solve such problems in a decentralized setting where multiple agents have access to (and solve) only a part of the whole optimization problem. The agents communicate with each other to arrive at a consensus, i.e., agree on a common quantity, via the gossip protocol. We propose a novel cost function for subspace learning on the Grassmann manifold, which is a weighted sum of several sub-problems (each solved by an agent) and the communication cost among the agents. The cost function has a finite sum structure. In the proposed modeling approach, different agents learn individual local subspace but they achieve asymptotic consensus on the global learned subspace. The approach is scalable and parallelizable. Numerical experiments show the efficacy of the proposed decentralized algorithms on various matrix completion and multivariate regression benchmarks.	1,0,1,0,0,0
Complete Analysis of a Random Forest Model	Random forests have become an important tool for improving accuracy in regression problems since their popularization by [Breiman, 2001] and others. In this paper, we revisit a random forest model originally proposed by [Breiman, 2004] and later studied by [Biau, 2012], where a feature is selected at random and the split occurs at the midpoint of the box containing the chosen feature. If the Lipschitz regression function is sparse and only depends on a small, unknown subset of $S$ out of $d$ features, we show that, given access to $n$ observations, this random forest model outputs a predictor that has a mean-squared prediction error $O((n(\sqrt{\log n})^{S-1})^{-\frac{1}{S\log2+1}})$. This positively answers an outstanding question of [Biau, 2012] about whether the rate of convergence therein could be improved. The second part of this article shows that the aforementioned prediction error cannot generally be improved, which we accomplish by characterizing the variance and by showing that the bias is tight for any linear model with nonzero parameter vector. As a striking consequence of our analysis, we show the variance of this forest is similar in form to the best-case variance lower bound of [Lin and Jeon, 2006] among all random forest models with nonadaptive splitting schemes (i.e., where the split protocol is independent of the training data).	0,0,0,1,0,0
The Correct Application of Variance Concept in Measurement Theory	The existing measurement theory interprets the variance as the dispersion of measured value, which is actually contrary to a general mathematical knowledge that the variance of a constant is 0. This paper will fully demonstrate that the variance in measurement theory is actually the evaluation of probability interval of an error instead of the dispersion of a measured value, point out the key point of mistake in the existing interpretation, and fully interpret a series of changes in conceptual logic and processing method brought about by this new concept.	0,0,1,1,0,0
Directional Statistics and Filtering Using libDirectional	In this paper, we present libDirectional, a MATLAB library for directional statistics and directional estimation. It supports a variety of commonly used distributions on the unit circle, such as the von Mises, wrapped normal, and wrapped Cauchy distributions. Furthermore, various distributions on higher-dimensional manifolds such as the unit hypersphere and the hypertorus are available. Based on these distributions, several recursive filtering algorithms in libDirectional allow estimation on these manifolds. The functionality is implemented in a clear, well-documented, and object-oriented structure that is both easy to use and easy to extend.	1,0,0,1,0,0
One Model To Learn Them All	Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.	1,0,0,1,0,0
A novel approach for fast mining frequent itemsets use N-list structure based on MapReduce	Frequent Pattern Mining is a one field of the most significant topics in data mining. In recent years, many algorithms have been proposed for mining frequent itemsets. A new algorithm has been presented for mining frequent itemsets based on N-list data structure called Prepost algorithm. The Prepost algorithm is enhanced by implementing compact PPC-tree with the general tree. Prepost algorithm can only find a frequent itemsets with required (pre-order and post-order) for each node. In this chapter, we improved prepost algorithm based on Hadoop platform (HPrepost), proposed using the Mapreduce programming model. The main goals of proposed method are efficient mining frequent itemsets requiring less running time and memory usage. We have conduct experiments for the proposed scheme to compare with another algorithms. With dense datasets, which have a large average length of transactions, HPrepost is more effective than frequent itemsets algorithms in terms of execution time and memory usage for all min-sup. Generally, our algorithm outperforms algorithms in terms of runtime and memory usage with small thresholds and large datasets.	1,0,0,0,0,0
Focusing on a Probability Element: Parameter Selection of Message Importance Measure in Big Data	Message importance measure (MIM) is applicable to characterize the importance of information in the scenario of big data, similar to entropy in information theory. In fact, MIM with a variable parameter can make an effect on the characterization of distribution. Furthermore, by choosing an appropriate parameter of MIM, it is possible to emphasize the message importance of a certain probability element in a distribution. Therefore, parametric MIM can play a vital role in anomaly detection of big data by focusing on probability of an anomalous event. In this paper, we propose a parameter selection method of MIM focusing on a probability element and then present its major properties. In addition, we discuss the parameter selection with prior probability, and investigate the availability in a statistical processing model of big data for anomaly detection problem.	1,0,1,0,0,0
Lensing and the Warm Hot Intergalactic Medium	The correlation of weak lensing and Cosmic Microwave Anisotropy (CMB) data traces the pressure distribution of the hot, ionized gas and the underlying matter density field. The measured correlation is dominated by baryons residing in halos. Detecting the contribution from unbound gas by measuring the residual cross-correlation after masking all known halos requires a theoretical understanding of this correlation and its dependence with model parameters. Our model assumes that the gas in filaments is well described by a log-normal probability distribution function, with temperatures $10^{5-7}$K and overdensities $\xi\le 100$. The lensing-comptonization cross-correlation is dominated by gas with overdensities in the range $\xi\approx[3-33]$; the signal is generated at redshifts $z\le 1$. If only 10\% of the measured cross-correlation is due to unbound gas, then the most recent measurements set an upper limit of $\bar{T}_e\lesssim 10^6$K on the mean temperature of Inter Galactic Medium. The amplitude is proportional to the baryon fraction stored in filaments. The lensing-comptonization power spectrum peaks at a different scale than the gas in halos making it possible to distinguish both contributions. To trace the distribution of the low density and low temperature plasma on cosmological scales, the effect of halos will have to be subtracted from the data, requiring observations with larger signal-to-noise ratio than currently available.	0,1,0,0,0,0
Finite-sample risk bounds for maximum likelihood estimation with arbitrary penalties	The MDL two-part coding $ \textit{index of resolvability} $ provides a finite-sample upper bound on the statistical risk of penalized likelihood estimators over countable models. However, the bound does not apply to unpenalized maximum likelihood estimation or procedures with exceedingly small penalties. In this paper, we point out a more general inequality that holds for arbitrary penalties. In addition, this approach makes it possible to derive exact risk bounds of order $1/n$ for iid parametric models, which improves on the order $(\log n)/n$ resolvability bounds. We conclude by discussing implications for adaptive estimation.	0,0,1,1,0,0
Smart grid modeling and simulation - Comparing GridLAB-D and RAPSim via two Case studies	One of the most important tools for the development of the smart grid is simulation. Therefore, analyzing, designing, modeling, and simulating the smart grid will allow to explore future scenarios and support decision making for the grid's development. In this paper, we compare two open source simulation tools for the smart grid, GridLAB-Distribution (GridLAB-D) and Renewable Alternative Power systems Simulation (RAPSim). The comparison is based on the implementation of two case studies related to a power flow problem and the integration of renewable energy resources to the grid. Results show that even for very simple case studies, specific properties such as weather simulation or load modeling are influencing the results in a way that they are not reproducible with a different simulator.	1,0,0,0,0,0
All or Nothing Caching Games with Bounded Queries	We determine the value of some search games where our goal is to find all of some hidden treasures using queries of bounded size. The answer to a query is either empty, in which case we lose, or a location, which contains a treasure. We prove that if we need to find $d$ treasures at $n$ possible locations with queries of size at most $k$, then our chance of winning is $\frac{k^d}{\binom nd}$ if each treasure is at a different location and $\frac{k^d}{\binom{n+d-1}d}$ if each location might hide several treasures for large enough $n$. Our work builds on some results by Csóka who has studied a continuous version of this problem, known as Alpern's Caching Game; we also prove that the value of Alpern's Caching Game is $\frac{k^d}{\binom{n+d-1}d}$ for integer $k$ and large enough $n$.	1,0,1,0,0,0
Behaviour of electron content in the ionospheric D-region during solar X-ray flares	One of the most important parameters in ionospheric plasma research also having a wide practical application in wireless satellite telecommunications is the total electron content (TEC) representing the columnal electron number density. The F region with high electron density provides the biggest contribution to TEC while the relatively weakly ionized plasma of the D region (60 km - 90 km above Earths surface) is often considered as a negligible cause of satellite signal disturbances. However, sudden intensive ionization processes like those induced by solar X ray flares can cause relative increases of electron density that are significantly larger in the D-region than in regions at higher altitudes. Therefore, one cannot exclude a priori the D region from investigations of ionospheric influences on propagation of electromagnetic signals emitted by satellites. We discuss here this problem which has not been sufficiently treated in literature so far. The obtained results are based on data collected from the D region monitoring by very low frequency radio waves and on vertical TEC calculations from the Global Navigation Satellite System (GNSS) signal analyses, and they show noticeable variations in the D region electron content (TECD) during activity of a solar X ray flare (it rises by a factor of 136 in the considered case) when TECD contribution to TEC can reach several percent and which cannot be neglected in practical applications like global positioning procedures by satellites.	0,1,0,0,0,0
Clustering and Labelling Auction Fraud Data	Although shill bidding is a common auction fraud, it is however very tough to detect. Due to the unavailability and lack of training data, in this study, we build a high-quality labeled shill bidding dataset based on recently collected auctions from eBay. Labeling shill biding instances with multidimensional features is a critical phase for the fraud classification task. For this purpose, we introduce a new approach to systematically label the fraud data with the help of the hierarchical clustering CURE that returns remarkable results as illustrated in the experiments.	0,0,0,1,0,0
Recognizing Union-Find trees built up using union-by-rank strategy is NP-complete	Disjoint-Set forests, consisting of Union-Find trees, are data structures having a widespread practical application due to their efficiency. Despite them being well-known, no exact structural characterization of these trees is known (such a characterization exists for Union trees which are constructed without using path compression) for the case assuming union-by-rank strategy for merging. In this paper we provide such a characterization by means of a simple push operation and show that the decision problem whether a given tree (along with the rank info of its nodes) is a Union-Find tree is NP-complete, complementing our earlier similar result for the union-by-size strategy.	1,0,0,0,0,0
A Robust Utility Learning Framework via Inverse Optimization	In many smart infrastructure applications flexibility in achieving sustainability goals can be gained by engaging end-users. However, these users often have heterogeneous preferences that are unknown to the decision-maker tasked with improving operational efficiency. Modeling user interaction as a continuous game between non-cooperative players, we propose a robust parametric utility learning framework that employs constrained feasible generalized least squares estimation with heteroskedastic inference. To improve forecasting performance, we extend the robust utility learning scheme by employing bootstrapping with bagging, bumping, and gradient boosting ensemble methods. Moreover, we estimate the noise covariance which provides approximated correlations between players which we leverage to develop a novel correlated utility learning framework. We apply the proposed methods both to a toy example arising from Bertrand-Nash competition between two firms as well as to data from a social game experiment designed to encourage energy efficient behavior amongst smart building occupants. Using occupant voting data for shared resources such as lighting, we simulate the game defined by the estimated utility functions to demonstrate the performance of the proposed methods.	1,0,1,0,0,0
Source Selection for Cluster Weak Lensing Measurements in the Hyper Suprime-Cam Survey	We present optimized source galaxy selection schemes for measuring cluster weak lensing (WL) mass profiles unaffected by cluster member dilution from the Subaru Hyper Suprime-Cam Strategic Survey Program (HSC-SSP). The ongoing HSC-SSP survey will uncover thousands of galaxy clusters to $z\lesssim1.5$. In deriving cluster masses via WL, a critical source of systematics is contamination and dilution of the lensing signal by cluster {members, and by foreground galaxies whose photometric redshifts are biased}. Using the first-year CAMIRA catalog of $\sim$900 clusters with richness larger than 20 found in $\sim$140 deg$^2$ of HSC-SSP data, we devise and compare several source selection methods, including selection in color-color space (CC-cut), and selection of robust photometric redshifts by applying constraints on their cumulative probability distribution function (PDF; P-cut). We examine the dependence of the contamination on the chosen limits adopted for each method. Using the proper limits, these methods give mass profiles with minimal dilution in agreement with one another. We find that not adopting either the CC-cut or P-cut methods results in an underestimation of the total cluster mass ($13\pm4\%$) and the concentration of the profile ($24\pm11\%$). The level of cluster contamination can reach as high as $\sim10\%$ at $R\approx 0.24$ Mpc/$h$ for low-z clusters without cuts, while employing either the P-cut or CC-cut results in cluster contamination consistent with zero to within the 0.5% uncertainties. Our robust methods yield a $\sim60\sigma$ detection of the stacked CAMIRA surface mass density profile, with a mean mass of $M_\mathrm{200c} = (1.67\pm0.05({\rm {stat}}))\times 10^{14}\,M_\odot/h$.	0,1,0,0,0,0
Convergence Rates of Latent Topic Models Under Relaxed Identifiability Conditions	In this paper we study the frequentist convergence rate for the Latent Dirichlet Allocation (Blei et al., 2003) topic models. We show that the maximum likelihood estimator converges to one of the finitely many equivalent parameters in Wasserstein's distance metric at a rate of $n^{-1/4}$ without assuming separability or non-degeneracy of the underlying topics and/or the existence of more than three words per document, thus generalizing the previous works of Anandkumar et al. (2012, 2014) from an information-theoretical perspective. We also show that the $n^{-1/4}$ convergence rate is optimal in the worst case.	1,0,0,1,0,0
Stable monoenergetic ion acceleration by a two color laser tweezer	In the past decades, the phenomenal progress in the development of ultraintense lasers has opened up many exciting new frontiers in laser matter physics, including laser plasma ion acceleration. Currently a major challenge in this frontier is to find simple methods to stably produce monoenergetic ion beams with sufficient charge for real applications. Here, we propose a novel scheme using a two color laser tweezer to fulfill this goal. In this scheme, two circularly polarized lasers with different wavelengths collide right on a thin nano-foil target containing mixed ion species. The radiation pressure of this laser pair acts like a tweezer to pinch and fully drag the electrons out, forming a stable uniform accelerating field for the ions. Scaling laws and three-dimensional particle-in-cell simulations confirm that high energy (10-1000 MeV) high charge ($\sim 10^{10}$) proton beams with narrow energy spread ($\sim4\%-20\%$) can be obtained by commercially available lasers. Such a scheme may open up a new route for compact high quality ion sources for various applications.	0,1,0,0,0,0
Optimization by gradient boosting	Gradient boosting is a state-of-the-art prediction technique that sequentially produces a model in the form of linear combinations of simple predictors---typically decision trees---by solving an infinite-dimensional convex optimization problem. We provide in the present paper a thorough analysis of two widespread versions of gradient boosting, and introduce a general framework for studying these algorithms from the point of view of functional optimization. We prove their convergence as the number of iterations tends to infinity and highlight the importance of having a strongly convex risk functional to minimize. We also present a reasonable statistical context ensuring consistency properties of the boosting predictors as the sample size grows. In our approach, the optimization procedures are run forever (that is, without resorting to an early stopping strategy), and statistical regularization is basically achieved via an appropriate $L^2$ penalization of the loss and strong convexity arguments.	1,0,1,1,0,0
Deuterium fractionation and H2D+ evolution in turbulent and magnetized cloud cores	High-mass stars are expected to form from dense prestellar cores. Their precise formation conditions are widely discussed, including their virial condition, which results in slow collapse for super-virial cores with strong support by turbulence or magnetic fields, or fast collapse for sub-virial sources. To disentangle their formation processes, measurements of the deuterium fractions are frequently employed to approximately estimate the ages of these cores and to obtain constraints on their dynamical evolution. We here present 3D magneto-hydrodynamical simulations including for the first time an accurate non-equilibrium chemical network with 21 gas-phase species plus dust grains and 213 reactions. With this network we model the deuteration process in fully depleted prestellar cores in great detail and determine its response to variations in the initial conditions. We explore the dependence on the initial gas column density, the turbulent Mach number, the mass-to-magnetic flux ratio and the distribution of the magnetic field, as well as the initial ortho-to-para ratio of H2. We find excellent agreement with recent observations of deuterium fractions in quiescent sources. Our results show that deuteration is rather efficient, even when assuming a conservative ortho-to-para ratio of 3 and highly sub-virial initial conditions, leading to large deuterium fractions already within roughly a free-fall time. We discuss the implications of our results and give an outlook to relevant future investigations.	0,1,0,0,0,0
Electromagnetic properties of terbium gallium garnet at millikelvin temperatures and single photon energy	Electromagnetic properties of single crystal terbium gallium garnet (TGG) are characterised from room down to millikelvin temperatures using the whispering gallery mode method. Microwave spectroscopy is performed at low powers equivalent to a few photons in energy and conducted as functions of the magnetic field and temperature. A phase transition is detected close to the temperature of 3.5 K. This is observed for multiple whispering gallery modes causing an abrupt negative frequency shift and a change in transmission due to extra losses in the new phase caused by a change in complex magnetic susceptibility.	0,1,0,0,0,0
An omnibus test for the global null hypothesis	Global hypothesis tests are a useful tool in the context of, e.g, clinical trials, genetic studies or meta analyses, when researchers are not interested in testing individual hypotheses, but in testing whether none of the hypotheses is false. There are several possibilities how to test the global null hypothesis when the individual null hypotheses are independent. If it is assumed that many of the individual null hypotheses are false, combinations tests have been recommended to maximise power. If, however, it is assumed that only one or a few null hypotheses are false, global tests based on individual test statistics are more powerful (e.g., Bonferroni or Simes test). However, usually there is no a-priori knowledge on the number of false individual null hypotheses. We therefore propose an omnibus test based on the combination of p-values. We show that this test yields an impressive overall performance. The proposed method is implemented in the R-package omnibus.	0,0,0,1,0,0
Affective Neural Response Generation	Existing neural conversational models process natural language primarily on a lexico-syntactic level, thereby ignoring one of the most crucial components of human-to-human dialogue: its affective content. We take a step in this direction by proposing three novel ways to incorporate affective/emotional aspects into long short term memory (LSTM) encoder-decoder neural conversation models: (1) affective word embeddings, which are cognitively engineered, (2) affect-based objective functions that augment the standard cross-entropy loss, and (3) affectively diverse beam search for decoding. Experiments show that these techniques improve the open-domain conversational prowess of encoder-decoder networks by enabling them to produce emotionally rich responses that are more interesting and natural.	1,0,0,0,0,0
Electrical Tuning of Polarizaion-state Using Graphene-Integrated Metasurfaces	Plasmonic metasurfaces have been employed for tuning and controlling light enabling various novel applications. Their appeal is enhanced with the incorporation of an active element with the metasurfaces paving the way for dynamic control. In this letter, we realize a dynamic polarization state generator using graphene-integrated anisotropic metasurface (GIAM), where a linear incidence polarization is controllably converted into an elliptical one. The anisotropic metasurface leads to an intrinsic polarization conversion when illuminated with non-orthogonal incident polarization. Additionally, the single-layer graphene allows us to tune the phase and intensity of the reflected light on the application of a gate voltage, enabling dynamic polarization control. The stokes polarization parameters of the reflected light are measured using rotating polarizer method and it is demonstrated that a large change in the ellipticity as well as orientation angle can be induced by this device. We also provide experimental evidence that the titl angle can change independent of the ellipticity going from positive values to nearly zero to negative values while ellipticity is constant.	0,1,0,0,0,0
Cluster-based Haldane state in edge-shared tetrahedral spin-cluster chain: Fedotovite K$_2$Cu$_3$O(SO$_4$)$_3$	Fedotovite K$_2$Cu$_3$O(SO$_4$)$_3$ is a candidate of new quantum spin systems, in which the edge-shared tetrahedral (EST) spin-clusters consisting of Cu$^{2+}$ are connected by weak inter-cluster couplings to from one-dimensional array. Comprehensive experimental studies by magnetic susceptibility, magnetization, heat capacity, and inelastic neutron scattering measurements reveal the presence of an effective $S$ = 1 Haldane state below $T \cong 4$ K. Rigorous theoretical studies provide an insight into the magnetic state of K$_2$Cu$_3$O(SO$_4$)$_3$: an EST cluster makes a triplet in the ground state and one-dimensional chain of the EST induces a cluster-based Haldane state. We predict that the cluster-based Haldene state emerges whenever the number of tetrahedra in the EST is $even$.	0,1,0,0,0,0
Inverse Kinematics for Control of Tensegrity Soft Robots: Existence and Optimality of Solutions	Tension-network (`tensegrity') robots encounter many control challenges as articulated soft robots, due to the structures' high-dimensional nonlinear dynamics. Control approaches have been developed which use the inverse kinematics of tensegrity structures, either for open-loop control or as equilibrium inputs for closed-loop controllers. However, current formulations of the tensegrity inverse kinematics problem are limited in robotics applications: first, they can lead to higher than needed cable tensions, and second, may lack solutions when applied to robots with high node-to-cable ratios. This work provides progress in both directions. To address the first limitation, the objective function for the inverse kinematics optimization problem is modified to produce cable tensions as low or lower than before, thus reducing the load on the robots' motors. For the second, a reformulation of the static equilibrium constraint is proposed, which produces solutions independent of the number of nodes within each rigid body. Simulation results using the second reformulation on a specific tensegrity spine robot show reasonable open-loop control results, whereas the previous formulation could not produce any solution.	1,0,0,0,0,0
OpenML: An R Package to Connect to the Machine Learning Platform OpenML	OpenML is an online machine learning platform where researchers can easily share data, machine learning tasks and experiments as well as organize them online to work and collaborate more efficiently. In this paper, we present an R package to interface with the OpenML platform and illustrate its usage in combination with the machine learning R package mlr. We show how the OpenML package allows R users to easily search, download and upload data sets and machine learning tasks. Furthermore, we also show how to upload results of experiments, share them with others and download results from other users. Beyond ensuring reproducibility of results, the OpenML platform automates much of the drudge work, speeds up research, facilitates collaboration and increases the users' visibility online.	1,0,0,1,0,0
Automorphisms of Partially Commutative Groups III: Inversions and Transvections	The structure of a certain subgroup $S$ of the automorphism group of a partially commutative group (RAAG) $G$ is described in detail: namely the subgroup generated by inversions and elementary transvections. We define admissible subsets of the generators of $G$, and show that $S$ is the subgroup of automorphisms which fix all subgroups $\langle Y\rangle$ of $G$, for all admissible subsets $Y$. A decomposition of $S$ as an iterated tower of semi-direct products in given and the structure of the factors of this decomposition described. The construction allows a presentation of $S$ to be computed, from the commutation graph of $G$.	0,0,1,0,0,0
A Practical Bandit Method with Advantages in Neural Network Tuning	Stochastic bandit algorithms can be used for challenging non-convex optimization problems. Hyperparameter tuning of neural networks is particularly challenging, necessitating new approaches. To this end, we present a method that adaptively partitions the combined space of hyperparameters, context, and training resources (e.g., total number of training iterations). By adaptively partitioning the space, the algorithm is able to focus on the portions of the hyperparameter search space that are most relevant in a practical way. By including the resources in the combined space, the method tends to use fewer training resources overall. Our experiments show that this method can surpass state-of-the-art methods in tuning neural networks on benchmark datasets. In some cases, our implementations can achieve the same levels of accuracy on benchmark datasets as existing state-of-the-art approaches while saving over 50% of our computational resources (e.g. time, training iterations).	1,0,0,1,0,0
ZOOpt: Toolbox for Derivative-Free Optimization	Recent advances of derivative-free optimization allow efficient approximating the global optimal solutions of sophisticated functions, such as functions with many local optima, non-differentiable and non-continuous functions. This article describes the ZOOpt (this https URL) toolbox that provides efficient derivative-free solvers and are designed easy to use. ZOOpt provides a Python package for single-thread optimization, and a light-weighted distributed version with the help of the Julia language for Python described functions. ZOOpt toolbox particularly focuses on optimization problems in machine learning, addressing high-dimensional, noisy, and large-scale problems. The toolbox is being maintained toward ready-to-use tool in real-world machine learning tasks.	0,0,0,1,0,0
Characterizing correlations and synchronization in collective dynamics	Synchronization, that occurs both for non-chaotic and chaotic systems, is a striking phenomenon with many practical implications in natural phenomena. However, even before synchronization, strong correlations occur in the collective dynamics of complex systems. To characterize their nature is essential for the understanding of phenomena in physical and social sciences. The emergence of strong correlations before synchronization is illustrated in a few piecewise linear models. They are shown to be associated to the behavior of ergodic parameters which may be exactly computed in some models. The models are also used as a testing ground to find general methods to characterize and parametrize the correlated nature of collective dynamics.	0,1,0,0,0,0
Performance of time delay estimation in a cognitive radar	A cognitive radar adapts the transmit waveform in response to changes in the radar and target environment. In this work, we analyze the recently proposed sub-Nyquist cognitive radar wherein the total transmit power in a multi-band cognitive waveform remains the same as its full-band conventional counterpart. For such a system, we derive lower bounds on the mean-squared-error (MSE) of a single-target time delay estimate. We formulate a procedure to select the optimal bands, and recommend distribution of the total power in different bands to enhance the accuracy of delay estimation. In particular, using Cramér-Rao bounds, we show that equi-width subbands in cognitive radar always have better delay estimation than the conventional radar. Further analysis using Ziv-Zakai bound reveals that cognitive radar performs well in low signal-to-noise (SNR) regions.	1,0,1,0,0,0
Time-Frequency Audio Features for Speech-Music Classification	Distinct striation patterns are observed in the spectrograms of speech and music. This motivated us to propose three novel time-frequency features for speech-music classification. These features are extracted in two stages. First, a preset number of prominent spectral peak locations are identified from the spectra of each frame. These important peak locations obtained from each frame are used to form Spectral peak sequences (SPS) for an audio interval. In second stage, these SPS are treated as time series data of frequency locations. The proposed features are extracted as periodicity, average frequency and statistical attributes of these spectral peak sequences. Speech-music categorization is performed by learning binary classifiers on these features. We have experimented with Gaussian mixture models, support vector machine and random forest classifiers. Our proposal is validated on four datasets and benchmarked against three baseline approaches. Experimental results establish the validity of our proposal.	1,0,0,0,0,0
PowerAlert: An Integrity Checker using Power Measurement	We propose PowerAlert, an efficient external integrity checker for untrusted hosts. Current attestation systems suffer from shortcomings in requiring complete checksum of the code segment, being static, use of timing information sourced from the untrusted machine, or use of timing information with high error (network round trip time). We address those shortcomings by (1) using power measurements from the host to ensure that the checking code is executed and (2) checking a subset of the kernel space over a long period of time. We compare the power measurement against a learned power model of the execution of the machine and validate that the execution was not tampered. Finally, power diversifies the integrity checking program to prevent the attacker from adapting. We implement a prototype of PowerAlert using Raspberry pi and evaluate the performance of the integrity checking program generation. We model the interaction between PowerAlert and an attacker as a game. We study the effectiveness of the random initiation strategy in deterring the attacker. The study shows that \power forces the attacker to trade-off stealthiness for the risk of detection, while still maintaining an acceptable probability of detection given the long lifespan of stealthy attacks.	1,0,0,0,0,0
Bayesian Gaussian models for interpolating large-dimensional data at misaligned areal units	Areal level spatial data are often large, sparse and may appear with geographical shapes that are regular or irregular (e.g., postcode). Moreover, sometimes it is important to obtain predictive inference in regular or irregular areal shapes that is misaligned with the observed spatial areal geographical boundary. For example, in a survey the respondents were asked about their postcode, however for policy making purposes, researchers are often interested to obtain information at the SA2. The statistical challenge is to obtain spatial prediction at the SA2s, where the SA2s may have overlapped geographical boundaries with postcodes. The study is motivated by a practical survey data obtained from the Australian National University (ANU) Poll. Here the main research question is to understand respondents' satisfaction level with the way Australia is heading. The data are observed at 1,944 postcodes among the 2,516 available postcodes across Australia, and prediction is obtained at the 2,196 SA2s. The proposed method also explored through a grid-based simulation study, where data have been observed in a regular grid and spatial prediction has been done in a regular grid that has a misaligned geographical boundary with the first regular grid-set. The real-life example with ANU Poll data addresses the situation of irregular geographical boundaries that are misaligned, i.e., model fitted with postcode data and hence obtained prediction at the SA2. A comparison study is also performed to validate the proposed method. In this paper, a Gaussian model is constructed under Bayesian hierarchy. The novelty lies in the development of the basis function that can address spatial sparsity and localised spatial structure. It can also address the large-dimensional spatial data modelling problem by constructing knot based reduced-dimensional basis functions.	0,0,0,1,0,0
Video Pandemics: Worldwide Viral Spreading of Psy's Gangnam Style Video	Viral videos can reach global penetration traveling through international channels of communication similarly to real diseases starting from a well-localized source. In past centuries, disease fronts propagated in a concentric spatial fashion from the the source of the outbreak via the short range human contact network. The emergence of long-distance air-travel changed these ancient patterns. However, recently, Brockmann and Helbing have shown that concentric propagation waves can be reinstated if propagation time and distance is measured in the flight-time and travel volume weighted underlying air-travel network. Here, we adopt this method for the analysis of viral meme propagation in Twitter messages, and define a similar weighted network distance in the communication network connecting countries and states of the World. We recover a wave-like behavior on average and assess the randomizing effect of non-locality of spreading. We show that similar result can be recovered from Google Trends data as well.	1,0,0,0,0,0
Consensus report on 25 years of searches for damped Ly$α$ galaxies in emission: Confirming their metallicity-luminosity relation at $z \gtrsim 2$	Starting from a summary of detection statistics of our recent X-shooter campaign, we review the major surveys, both space and ground based, for emission counterparts of high-redshift damped Ly$\alpha$ absorbers (DLAs) carried out since the first detection 25 years ago. We show that the detection rates of all surveys are precisely reproduced by a simple model in which the metallicity and luminosity of the galaxy associated to the DLA follow a relation of the form, ${\rm M_{UV}} = -5 \times \left(\,[{\rm M/H}] + 0.3\, \right) - 20.8$, and the DLA cross-section follows a relation of the form $\sigma_{DLA} \propto L^{0.8}$. Specifically, our spectroscopic campaign consists of 11 DLAs preselected based on their equivalent width of SiII $\lambda1526$ to have a metallicity higher than [Si/H] > -1. The targets have been observed with the X-shooter spectrograph at the Very Large Telescope to search for emission lines around the quasars. We observe a high detection rate of 64% (7/11), significantly higher than the typical $\sim$10% for random, HI-selected DLA samples. We use the aforementioned model, to simulate the results of our survey together with a range of previous surveys: spectral stacking, direct imaging (using the `double DLA' technique), long-slit spectroscopy, and integral field spectroscopy. Based on our model results, we are able to reconcile all results. Some tension is observed between model and data when looking at predictions of Ly$\alpha$ emission for individual targets. However, the object to object variations are most likely a result of the significant scatter in the underlying scaling relations as well as uncertainties in the amount of dust which affects the emission.	0,1,0,0,0,0
Scalable multimodal convolutional networks for brain tumour segmentation	Brain tumour segmentation plays a key role in computer-assisted surgery. Deep neural networks have increased the accuracy of automatic segmentation significantly, however these models tend to generalise poorly to different imaging modalities than those for which they have been designed, thereby limiting their applications. For example, a network architecture initially designed for brain parcellation of monomodal T1 MRI can not be easily translated into an efficient tumour segmentation network that jointly utilises T1, T1c, Flair and T2 MRI. To tackle this, we propose a novel scalable multimodal deep learning architecture using new nested structures that explicitly leverage deep features within or across modalities. This aims at making the early layers of the architecture structured and sparse so that the final architecture becomes scalable to the number of modalities. We evaluate the scalable architecture for brain tumour segmentation and give evidence of its regularisation effect compared to the conventional concatenation approach.	1,0,0,0,0,0
Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection	Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of data quality and tries to reduce the negative impact of data noise on MT training, in particular, neural MT (NMT) training. This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training. The proposed approach uses trusted data and a denoising curriculum realized by online data selection. Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise.	0,0,0,1,0,0
Computing Simple Multiple Zeros of Polynomial Systems	Given a polynomial system f associated with a simple multiple zero x of multiplicity {\mu}, we give a computable lower bound on the minimal distance between the simple multiple zero x and other zeros of f. If x is only given with limited accuracy, we propose a numerical criterion that f is certified to have {\mu} zeros (counting multiplicities) in a small ball around x. Furthermore, for simple double zeros and simple triple zeros whose Jacobian is of normalized form, we define modified Newton iterations and prove the quantified quadratic convergence when the starting point is close to the exact simple multiple zero. For simple multiple zeros of arbitrary multiplicity whose Jacobian matrix may not have a normalized form, we perform unitary transformations and modified Newton iterations, and prove its non-quantified quadratic convergence and its quantified convergence for simple triple zeros.	0,0,1,0,0,0
An analytic formulation for positive-unlabeled learning via weighted integral probability metric	We consider the problem of learning a binary classifier from only positive and unlabeled observations (PU learning). Although recent research in PU learning has succeeded in showing theoretical and empirical performance, most existing algorithms need to solve either a convex or a non-convex optimization problem and thus are not suitable for large-scale datasets. In this paper, we propose a simple yet theoretically grounded PU learning algorithm by extending the previous work proposed for supervised binary classification (Sriperumbudur et al., 2012). The proposed PU learning algorithm produces a closed-form classifier when the hypothesis space is a closed ball in reproducing kernel Hilbert space. In addition, we establish upper bounds of the estimation error and the excess risk. The obtained estimation error bound is sharper than existing results and the excess risk bound does not rely on an approximation error term. To the best of our knowledge, we are the first to explicitly derive the excess risk bound in the field of PU learning. Finally, we conduct extensive numerical experiments using both synthetic and real datasets, demonstrating improved accuracy, scalability, and robustness of the proposed algorithm.	1,0,0,1,0,0
Kinetic cascade in solar-wind turbulence: 3D3V hybrid-kinetic simulations with electron inertia	Understanding the nature of the turbulent fluctuations below the ion gyroradius in solar-wind turbulence is a great challenge. Recent studies have been mostly in favor of kinetic Alfvén wave (KAW) type of fluctuations, but other kinds of fluctuations with characteristics typical of magnetosonic, whistler and ion Bernstein modes, could also play a role depending on the plasma parameters. Here we investigate the properties of the sub-proton-scale cascade with high-resolution hybrid-kinetic simulations of freely-decaying turbulence in 3D3V phase space, including electron inertia effects. Two proton plasma beta are explored: the "intermediate" $\beta_p=1$ and "low" $\beta_p=0.2$ regimes, both typically observed in solar wind and corona. The magnetic energy spectum exhibits $k_\perp^{-8/3}$ and $k_\|^{-7/2}$ power laws at $\beta_p=1$, while they are slightly steeper at $\beta_p=0.2$. Nevertheless, both regimes develop a spectral anisotropy consistent with $k_\|\sim k_\perp^{2/3}$ at $k_\perp\rho_p>1$, and pronounced small-scale intermittency. In this context, we find that the kinetic-scale cascade is dominated by KAW-like fluctuations at $\beta_p=1$, whereas the low-$\beta$ case presents a more complex scenario suggesting the simultaneous presence of different types of fluctuations. In both regimes, however, a non-negligible role of ion Bernstein type of fluctuations at the smallest scales seems to emerge.	0,1,0,0,0,0
Energy dependent stereodynamics of the Ne($^3$P$_2$)+Ar reaction	The stereodynamics of the Ne($^3$P$_2$)+Ar Penning and Associative ionization reactions have been studied using a crossed molecular beam apparatus. The experiment uses a curved magnetic hexapole to polarise the Ne($^3$P$_2$) which is then oriented with a shaped magnetic field in the region where it intersects with a beam of Ar($^1$S). The ratios of Penning to associative ionization were recorded over a range of collision energies from 320 cm$^{-1}$ to 500 cm$^{-1}$ and the data was used to obtain $\Omega$ state dependent reactivities for the two reaction channels. These reactivities were found to compare favourably to those predicted in the theoretical work of Brumer et al.	0,1,0,0,0,0
Three-dimensional oscillatory magnetic reconnection	Here we detail the dynamic evolution of localised reconnection regions about three-dimensional (3D) magnetic null points by using numerical simulation. We demonstrate for the first time that reconnection triggered by the localised collapse of a 3D null point due to an external MHD wave involves a self-generated oscillation, whereby the current sheet and outflow jets undergo a reconnection reversal process during which back-pressure formation at the jet heads acts to prise open the collapsed field before overshooting the equilibrium into an opposite-polarity configuration. The discovery that reconnection at fully 3D nulls can proceed naturally in a time-dependent and periodic fashion is suggestive that oscillatory reconnection mechanisms may play a role in explaining periodicity in astrophysical phenomena associated with magnetic reconnection, such as the observed quasi-periodicity of solar and stellar flare emission. Furthermore, we find a consequence of oscillatory reconnection is the generation of a plethora of freely-propagating MHD waves which escape the vicinity of the reconnection region	0,1,0,0,0,0
Factors in Recommending Contrarian Content on Social Media	Polarization is a troubling phenomenon that can lead to societal divisions and hurt the democratic process. It is therefore important to develop methods to reduce it. We propose an algorithmic solution to the problem of reducing polarization. The core idea is to expose users to content that challenges their point of view, with the hope broadening their perspective, and thus reduce their polarity. Our method takes into account several aspects of the problem, such as the estimated polarity of the user, the probability of accepting the recommendation, the polarity of the content, and popularity of the content being recommended. We evaluate our recommendations via a large-scale user study on Twitter users that were actively involved in the discussion of the US elections results. Results shows that, in most cases, the factors taken into account in the recommendation affect the users as expected, and thus capture the essential features of the problem.	1,0,0,0,0,0
Signal coupling to embedded pitch adapters in silicon sensors	We have examined the effects of embedded pitch adapters on signal formation in n-substrate silicon microstrip sensors with data from beam tests and simulation. According to simulation, the presence of the pitch adapter metal layer changes the electric field inside the sensor, resulting in slowed signal formation on the nearby strips and a pick-up effect on the pitch adapter. This can result in an inefficiency to detect particles passing through the pitch adapter region. All these effects have been observed in the beam test data.	0,1,0,0,0,0
Wikipedia in academia as a teaching tool: from averse to proactive faculty profiles	This study concerned the active use of Wikipedia as a teaching tool in the classroom in higher education, trying to identify different usage profiles and their characterization. A questionnaire survey was administrated to all full-time and part-time teachers at the Universitat Oberta de Catalunya and the Universitat Pompeu Fabra, both in Barcelona, Spain. The questionnaire was designed using the Technology Acceptance Model as a reference, including items about teachers web 2.0 profile, Wikipedia usage, expertise, perceived usefulness, easiness of use, visibility and quality, as well as Wikipedia status among colleagues and incentives to use it more actively. Clustering and statistical analysis were carried out using the k-medoids algorithm and differences between clusters were assessed by means of contingency tables and generalized linear models (logit). The respondents were classified in four clusters, from less to more likely to adopt and use Wikipedia in the classroom, namely averse (25.4%), reluctant (17.9%), open (29.5%) and proactive (27.2%). Proactive faculty are mostly men teaching part-time in STEM fields, mainly engineering, while averse faculty are mostly women teaching full-time in non-STEM fields. Nevertheless, questionnaire items related to visibility, quality, image, usefulness and expertise determine the main differences between clusters, rather than age, gender or domain. Clusters involving a positive view of Wikipedia and at least some frequency of use clearly outnumber those with a strictly negative stance. This goes against the common view that faculty members are mostly sceptical about Wikipedia. Environmental factors such as academic culture and colleagues opinion are more important than faculty personal characteristics, especially with respect to what they think about Wikipedia quality.	1,0,0,0,0,0
DeepFM: A Factorization-Machine based Neural Network for CTR Prediction	Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.	1,0,0,0,0,0
On The Robustness of Epsilon Skew Extension for Burr III Distribution on Real Line	The Burr III distribution is used in a wide variety of fields of lifetime data analysis, reliability theory, and financial literature, etc. It is defined on the positive axis and has two shape parameters, say $c$ and $k$. These shape parameters make the distribution quite flexible. They also control the tail behavior of the distribution. In this study, we extent the Burr III distribution to the real axis and also add a skewness parameter, say $\varepsilon$, with epsilon-skew extension approach. When the parameters $c$ and $k$ have a relation such that $ck \approx 1 $ or $ck < 1 $, it is skewed unimodal. Otherwise, it is skewed bimodal with the same level of peaks on the negative and positive sides of real line. Thus, ESBIII distribution can capture fitting the various data sets even when the number of parameters are three. Location and scale form of this distribution are also given. Some distributional properties of the new distribution are investigated. The maximum likelihood (ML) estimation method for the parameters of ESBIII is considered. The robustness properties of ML estimators are studied and also tail behaviour of ESBIII distribution is examined. The applications on real data are considered to illustrate the modeling capacity of this distribution in the class of bimodal distributions.	0,0,1,1,0,0
On the Power of Truncated SVD for General High-rank Matrix Estimation Problems	We show that given an estimate $\widehat{A}$ that is close to a general high-rank positive semi-definite (PSD) matrix $A$ in spectral norm (i.e., $\|\widehat{A}-A\|_2 \leq \delta$), the simple truncated SVD of $\widehat{A}$ produces a multiplicative approximation of $A$ in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems, which we briefly summarize below ($A$ is an $n\times n$ high-rank PSD matrix and $A_k$ is the best rank-$k$ approximation of $A$): (1) High-rank matrix completion: By observing $\Omega(\frac{n\max\{\epsilon^{-4},k^2\}\mu_0^2\|A\|_F^2\log n}{\sigma_{k+1}(A)^2})$ elements of $A$ where $\sigma_{k+1}\left(A\right)$ is the $\left(k+1\right)$-th singular value of $A$ and $\mu_0$ is the incoherence, the truncated SVD on a zero-filled matrix satisfies $\|\widehat{A}_k-A\|_F \leq (1+O(\epsilon))\|A-A_k\|_F$ with high probability. (2)High-rank matrix de-noising: Let $\widehat{A}=A+E$ where $E$ is a Gaussian random noise matrix with zero mean and $\nu^2/n$ variance on each entry. Then the truncated SVD of $\widehat{A}$ satisfies $\|\widehat{A}_k-A\|_F \leq (1+O(\sqrt{\nu/\sigma_{k+1}(A)}))\|A-A_k\|_F + O(\sqrt{k}\nu)$. (3) Low-rank Estimation of high-dimensional covariance: Given $N$ i.i.d.~samples $X_1,\cdots,X_N\sim\mathcal N_n(0,A)$, can we estimate $A$ with a relative-error Frobenius norm bound? We show that if $N = \Omega\left(n\max\{\epsilon^{-4},k^2\}\gamma_k(A)^2\log N\right)$ for $\gamma_k(A)=\sigma_1(A)/\sigma_{k+1}(A)$, then $\|\widehat{A}_k-A\|_F \leq (1+O(\epsilon))\|A-A_k\|_F$ with high probability, where $\widehat{A}=\frac{1}{N}\sum_{i=1}^N{X_iX_i^\top}$ is the sample covariance.	0,0,1,1,0,0
X-ray luminescence computed tomography using a focused X-ray beam	Due to the low X-ray photon utilization efficiency and low measurement sensitivity of the electron multiplying charge coupled device (EMCCD) camera setup, the collimator based narrow beam X-ray luminescence computed tomography (XLCT) usually requires a long measurement time. In this paper, we, for the first time, report a focused X-ray beam based XLCT imaging system with measurements by a single optical fiber bundle and a photomultiplier tube (PMT). An X-ray tube with a polycapillary lens was used to generate a focused X-ray beam whose X-ray photon density is 1200 times larger than a collimated X-ray beam. An optical fiber bundle was employed to collect and deliver the emitted photons on the phantom surface to the PMT. The total measurement time was reduced to 12.5 minutes. For numerical simulations of both single and six fiber bundle cases, we were able to reconstruct six targets successfully. For the phantom experiment, two targets with an edge-to-edge distance of 0.4 mm and a center-to-center distance of 0.8 mm were successfully reconstructed by the measurement setup with a single fiber bundle and a PMT.	0,1,0,0,0,0
A Decentralized Framework for Real-Time Energy Trading in Distribution Networks with Load and Generation Uncertainty	The proliferation of small-scale renewable generators and price-responsive loads makes it a challenge for distribution network operators (DNOs) to schedule the controllable loads of the load aggregators and the generation of the generators in real-time. Additionally, the high computational burden and violation of the entities' (i.e., load aggregators' and generators') privacy make a centralized framework impractical. In this paper, we propose a decentralized energy trading algorithm that can be executed by the entities in a real-time fashion. To address the privacy issues, the DNO provides the entities with proper control signals using the Lagrange relaxation technique to motivate them towards an operating point with maximum profit for entities. To deal with uncertainty issues, we propose a probabilistic load model and robust framework for renewable generation. The performance of the proposed algorithm is evaluated on an IEEE 123-node test feeder. When compared with a benchmark of not performing load management for the aggregators, the proposed algorithm benefits both the load aggregators and generators by increasing their profit by 17.8%and 10.3%, respectively. When compared with a centralized approach, our algorithm converges to the solution of the DNO's centralized problem with a significantly lower running time in 50 iterations per time slot.	1,0,0,0,0,0
Unsupervised robust nonparametric learning of hidden community properties	We consider learning of fundamental properties of communities in large noisy networks, in the prototypical situation where the nodes or users are split into two classes according to a binary property, e.g., according to their opinions or preferences on a topic. For learning these properties, we propose a nonparametric, unsupervised, and scalable graph scan procedure that is, in addition, robust against a class of powerful adversaries. In our setup, one of the communities can fall under the influence of a knowledgeable adversarial leader, who knows the full network structure, has unlimited computational resources and can completely foresee our planned actions on the network. We prove strong consistency of our results in this setup with minimal assumptions. In particular, the learning procedure estimates the baseline activity of normal users asymptotically correctly with probability 1; the only assumption being the existence of a single implicit community of asymptotically negligible logarithmic size. We provide experiments on real and synthetic data to illustrate the performance of our method, including examples with adversaries.	1,0,0,1,0,0
Staging superstructures in high-$T_c$ Sr/O co-doped La$_{2-x}$Sr$_x$CuO$_{4+y}$	We present high energy X-ray diffraction studies on the structural phases of an optimal high-$T_c$ superconductor La$_{2-x}$Sr$_x$CuO$_{4+y}$ tailored by co-hole-doping. This is specifically done by varying the content of two very different chemical species, Sr and O, respectively, in order to study the influence of each. A superstructure known as staging is observed in all samples, with the staging number $n$ increasing for higher Sr dopings $x$. We find that the staging phases emerge abruptly with temperature, and can be described as a second order phase transition with transition temperatures slightly depending on the Sr doping. The Sr appears to correlate the interstitial oxygen in a way that stabilises the reproducibility of the staging phase both in terms of staging period and volume fraction in a specific sample. The structural details as investigated in this letter appear to have no direct bearing on the electronic phase separation previously observed in the same samples. This provides new evidence that the electronic phase separation is determined by the overall hole concentration rather than specific Sr/O content and concommittant structural details.	0,1,0,0,0,0
Congruences for Restricted Plane Overpartitions Modulo 4 and 8	In 2009, Corteel, Savelief and Vuletić generalized the concept of overpartitions to a new object called plane overpartitions. In recent work, the author considered a restricted form of plane overpartitions called $k$-rowed plane overpartions and proved a method to obtain congruences for these and other types of combinatorial generating functions. In this paper, we prove several restricted and unrestricted plane overpartition congruences modulo $4$ and $8$ using other techniques.	0,0,1,0,0,0
Model equations and structures formation for the media with memory	We propose new types of models of the appearance of small- and large scale structures in media with memory, including a hyperbolic modification of the Navier-Stokes equations and a class of dynamical low-dimensional models with memory effects. On the basis of computer modeling, the formation of the small-scale structures and collapses and the appearance of new chaotic solutions are demonstrated. Possibilities of the application of some proposed models to the description of the burst-type processes and collapses o nthe Sun are discussed.	0,1,0,0,0,0
Highly sensitive atomic based MW interferometry	We theoretically study a scheme to develop an atomic based MW interferometry using the Rydberg states in Rb. Unlike the traditional MW interferometry, this scheme is not based upon the electrical circuits, hence the sensitivity of the phase and the amplitude/strength of the MW field is not limited by the Nyquist thermal noise. Further this system has great advantage due to its very high bandwidth, ranging from radio frequency (RF), micro wave (MW) to terahertz regime. In addition, this is \textbf{orders of magnitude} more sensitive to field strength as compared to the prior demonstrations on the MW electrometry using the Rydberg atomic states. However previously studied atomic systems are only sensitive to the field strength but not to the phase and hence this scheme provides a great opportunity to characterize the MW completely including the propagation direction and the wavefront. This study opens up a new dimension in the Radar technology such as in synthetic aperture radar interferometry. The MW interferometry is based upon a six-level loopy ladder system involving the Rydberg states in which two sub-systems interfere constructively or destructively depending upon the phase between the MW electric fields closing the loop.	0,1,0,0,0,0
Rapid Adaptation with Conditionally Shifted Neurons	We describe a mechanism by which artificial neural networks can learn rapid adaptation - the ability to adapt on the fly, with little data, to new tasks - that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-specific shifts retrieved from a memory module, which is populated rapidly based on limited task experience. On metalearning benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.	1,0,0,1,0,0
Coulomb repulsion of holes and competition between d_{x^2-y^2}-wave and s-wave parings in cuprate superconductors	The effect of the Coulomb repulsion of holes on the Cooper instability in an ensemble of spin-polaron quasiparticles has been analyzed, taking into account the peculiarities of the crystallographic structure of the CuO$_2$ plane, which are associated with the presence of two oxygen ions and one copper ion in the unit cell, as well as the strong spin-fermion coupling. The investigation of the possibility of implementation superconducting phases with d-wave and s-wave pairing of the order parameter symmetry has shown that in the entire doping region only the d-wave pairing satisfies the self-consistency equations, while there is no solution for the s-wave pairing. This result completely corresponds to the experimental data on cuprate HTSC. It has been demonstrated analytically that the intersite Coulomb interaction does not affect the superconducting d-wave pairing, because its Fourier transform $V_q$ does not appear in the kernel of the corresponding integral equation.	0,1,0,0,0,0
Inner Cohomology of the General Linear Group	The main theorem is incorrectly stated.	0,0,1,0,0,0
Note on regions containing eigenvalues of a matrix	By excluding some regions, in which each eigenvalue of a matrix is not contained, from the \alpha\beta-type eigenvalue inclusion region provided by Huang et al.(Electronic Journal of Linear Algebra, 15 (2006) 215-224), a new eigenvalue inclusion region is given. And it is proved that the new region is contained in the \alpha\beta-type eigenvalue inclusion region.	0,0,1,0,0,0
Transductive Boltzmann Machines	We present transductive Boltzmann machines (TBMs), which firstly achieve transductive learning of the Gibbs distribution. While exact learning of the Gibbs distribution is impossible by the family of existing Boltzmann machines due to combinatorial explosion of the sample space, TBMs overcome the problem by adaptively constructing the minimum required sample space from data to avoid unnecessary generalization. We theoretically provide bias-variance decomposition of the KL divergence in TBMs to analyze its learnability, and empirically demonstrate that TBMs are superior to the fully visible Boltzmann machines and popularly used restricted Boltzmann machines in terms of efficiency and effectiveness.	0,0,0,1,0,0
Compositional descriptor-based recommender system accelerating the materials discovery	Structures and properties of many inorganic compounds have been collected historically. However, it only covers a very small portion of possible inorganic crystals, which implies the presence of numerous currently unknown compounds. A powerful machine-learning strategy is mandatory to discover new inorganic compounds from all chemical combinations. Herein we propose a descriptor-based recommender-system approach to estimate the relevance of chemical compositions where stable crystals can be formed [i.e., chemically relevant compositions (CRCs)]. As well as data-driven compositional similarity used in the literature, the use of compositional descriptors as a prior knowledge can accelerate the discovery of new compounds. We validate our recommender systems in two ways. Firstly, one database is used to construct a model, while another is used for the validation. Secondly, we estimate the phase stability for compounds at expected CRCs using density functional theory calculations.	0,1,0,0,0,0
Numerical simulations of magnetic billiards in a convex domain in $\mathbb{R}^2$	We present numerical simulations of magnetic billiards inside a convex domain in the plane.	0,1,1,0,0,0
Iterative Object and Part Transfer for Fine-Grained Recognition	The aim of fine-grained recognition is to identify sub-ordinate categories in images like different species of birds. Existing works have confirmed that, in order to capture the subtle differences across the categories, automatic localization of objects and parts is critical. Most approaches for object and part localization relied on the bottom-up pipeline, where thousands of region proposals are generated and then filtered by pre-trained object/part models. This is computationally expensive and not scalable once the number of objects/parts becomes large. In this paper, we propose a nonparametric data-driven method for object and part localization. Given an unlabeled test image, our approach transfers annotations from a few similar images retrieved in the training set. In particular, we propose an iterative transfer strategy that gradually refine the predicted bounding boxes. Based on the located objects and parts, deep convolutional features are extracted for recognition. We evaluate our approach on the widely-used CUB200-2011 dataset and a new and large dataset called Birdsnap. On both datasets, we achieve better results than many state-of-the-art approaches, including a few using oracle (manually annotated) bounding boxes in the test images.	1,0,0,0,0,0
Extensions of the Benson-Solomon fusion systems	The Benson-Solomon systems comprise the only known family of simple saturated fusion systems at the prime two that do not arise as the fusion system of any finite group. We determine the automorphism groups and the possible almost simple extensions of these systems and of their centric linking systems.	0,0,1,0,0,0
Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction	Future predictions on sequence data (e.g., videos or audios) require the algorithms to capture non-Markovian and compositional properties of high-level semantics. Context-free grammars are natural choices to capture such properties, but traditional grammar parsers (e.g., Earley parser) only take symbolic sentences as inputs. In this paper, we generalize the Earley parser to parse sequence data which is neither segmented nor labeled. This generalized Earley parser integrates a grammar parser with a classifier to find the optimal segmentation and labels, and makes top-down future predictions. Experiments show that our method significantly outperforms other approaches for future human activity prediction.	0,0,0,1,0,0
Construction of curve pairs and their applications	In this study, we introduce a new approach to curve pairs by using integral curves. We consider the direction curve and donor curve to study curve couples such as involute-evolute curves, Mannheim partner curves and Bertrand partner curves. We obtain new methods to construct partner curves of a unit speed curve and give some applications related to helices, slant helices and plane curves.	0,0,1,0,0,0
Randomized Near Neighbor Graphs, Giant Components, and Applications in Data Science	If we pick $n$ random points uniformly in $[0,1]^d$ and connect each point to its $k-$nearest neighbors, then it is well known that there exists a giant connected component with high probability. We prove that in $[0,1]^d$ it suffices to connect every point to $ c_{d,1} \log{\log{n}}$ points chosen randomly among its $ c_{d,2} \log{n}-$nearest neighbors to ensure a giant component of size $n - o(n)$ with high probability. This construction yields a much sparser random graph with $\sim n \log\log{n}$ instead of $\sim n \log{n}$ edges that has comparable connectivity properties. This result has nontrivial implications for problems in data science where an affinity matrix is constructed: instead of picking the $k-$nearest neighbors, one can often pick $k' \ll k$ random points out of the $k-$nearest neighbors without sacrificing efficiency. This can massively simplify and accelerate computation, we illustrate this with several numerical examples.	1,0,0,1,0,0
Using photo-ionisation models to derive carbon and oxygen gas-phase abundances in the rest UV	We present a new method to derive oxygen and carbon abundances using the ultraviolet (UV) lines emitted by the gas-phase ionised by massive stars. The method is based on the comparison of the nebular emission-line ratios with those predicted by a large grid of photo-ionisation models. Given the large dispersion in the O/H - C/O plane, our method firstly fixes C/O using ratios of appropriate emission lines and, in a second step, calculates O/H and the ionisation parameter from carbon lines in the UV. We find abundances totally consistent with those provided by the direct method when we apply this method to a sample of objects with an empirical determination of the electron temperature using optical emission lines. The proposed methodology appears as a powerful tool for systematic studies of nebular abundances in star-forming galaxies at high redshift.	0,1,0,0,0,0
Spontaneous antiferromagnetic order and strain effect on electronic properties of $α$-graphyne	Using hybrid exchange-correlation functional in ab initio density functional theory calculations, we study magnetic properties and strain effect on the electronic properties of $\alpha$-graphyne monolayer. We find that a spontaneous antiferromagnetic (AF) ordering occurs with energy band gap ($\sim$ 0.5 eV) in the equilibrated $\alpha$-graphyne. Bi-axial tensile strain enhances the stability of AF state as well as the staggered spin moment and value of the energy gap. The antiferromagnetic semiconductor phase is quite robust against moderate carrier filling with threshold carrier density up to 1.7$\times$10$^{14}$ electrons/cm$^2$ to destabilize the phase. The spontaneous AF ordering and strain effect in $\alpha$-graphyne can be well described by the framework of the Hubbard model. Our study shows that it is essential to consider the electronic correlation effect properly in $\alpha$-graphyne and may pave an avenue for exploring magnetic ordering in other carbon allotropes with mixed hybridization of s and p orbitals.	0,1,0,0,0,0
Correlations between primes in short intervals on curves over finite fields	We prove an analogue of the Hardy-Littlewood conjecture on the asymptotic distribution of prime constellations in the setting of short intervals in function fields of smooth projective curves over finite fields.	0,0,1,0,0,0
A Nonparametric Method for Producing Isolines of Bivariate Exceedance Probabilities	We present a method for drawing isolines indicating regions of equal joint exceedance probability for bivariate data. The method relies on bivariate regular variation, a dependence framework widely used for extremes. This framework enables drawing isolines corresponding to very low exceedance probabilities and these lines may lie beyond the range of the data. The method we utilize for characterizing dependence in the tail is largely nonparametric. Furthermore, we extend this method to the case of asymptotic independence and propose a procedure which smooths the transition from asymptotic independence in the interior to the first-order behavior on the axes. We propose a diagnostic plot for assessing isoline estimate and choice of smoothing, and a bootstrap procedure to visually assess uncertainty.	0,0,0,1,0,0
Unseen Progenitors of Luminous High-z Quasars in the R_h=ct Universe	Quasars at high redshift provide direct information on the mass growth of supermassive black holes and, in turn, yield important clues about how the Universe evolved since the first (Pop III) stars started forming. Yet even basic questions regarding the seeds of these objects and their growth mechanism remain unanswered. The anticipated launch of eROSITA and ATHENA is expected to facilitate observations of high-redshift quasars needed to resolve these issues. In this paper, we compare accretion-based supermassive black hole growth in the concordance LCDM model with that in the alternative Friedmann-Robertson Walker cosmology known as the R_h=ct universe. Previous work has shown that the timeline predicted by the latter can account for the origin and growth of the > 10^9 M_sol highest redshift quasars better than that of the standard model. Here, we significantly advance this comparison by determining the soft X-ray flux that would be observed for Eddington-limited accretion growth as a function of redshift in both cosmologies. Our results indicate that a clear difference emerges between the two in terms of the number of detectable quasars at redshift z > 6, raising the expectation that the next decade will provide the observational data needed to discriminate between these two models based on the number of detected high-redshift quasar progenitors. For example, while the upcoming ATHENA mission is expected to detect ~0.16 (i.e., essentially zero) quasars at z ~ 7 in R_h=ct, it should detect ~160 in LCDM---a quantitatively compelling difference.	0,1,0,0,0,0
An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients	In this technical report, we consider an approach that combines the PPO objective and K-FAC natural gradient optimization, for which we call PPOKFAC. We perform a range of empirical analysis on various aspects of the algorithm, such as sample complexity, training speed, and sensitivity to batch size and training epochs. We observe that PPOKFAC is able to outperform PPO in terms of sample complexity and speed in a range of MuJoCo environments, while being scalable in terms of batch size. In spite of this, it seems that adding more epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to be worse than its A2C counterpart, ACKTR.	0,0,0,1,0,0
Towards an algebraic natural proofs barrier via polynomial identity testing	We observe that a certain kind of algebraic proof - which covers essentially all known algebraic circuit lower bounds to date - cannot be used to prove lower bounds against VP if and only if what we call succinct hitting sets exist for VP. This is analogous to the Razborov-Rudich natural proofs barrier in Boolean circuit complexity, in that we rule out a large class of lower bound techniques under a derandomization assumption. We also discuss connections between this algebraic natural proofs barrier, geometric complexity theory, and (algebraic) proof complexity.	1,0,1,0,0,0
Positioning services of a travel agency in social networks	In this paper the methods of forming a travel company customer base by means of social networks are observed. These methods are made to involve web-users of the social networks (VK.com and Facebook) for positioning of the service of the travel agency "New Europe" on the Internet. The methods of applying the maintenance activities and interests of web-users are also used. So, the main method of information exchanging in modern network society is on-line social networks. The rapid development and improvement of such information and communication technologies is a key factor in the positioning of the travel agency brand in the global information space. The absence of time and space restrictions and the speed of spreading of the information among an aim audience of social networks create all the conditions for effective popularization of the travel agency "New Europe" and its service in the Internet.	1,0,0,0,0,0
No minimal tall Borel ideal in the Katětov order	Answering a question of the second listed author we show that there is no tall Borel ideal minimal among all tall Borel ideals in the Katětov order.	0,0,1,0,0,0
Human-Centered Autonomous Vehicle Systems: Principles of Effective Shared Autonomy	Building effective, enjoyable, and safe autonomous vehicles is a lot harder than has historically been considered. The reason is that, simply put, an autonomous vehicle must interact with human beings. This interaction is not a robotics problem nor a machine learning problem nor a psychology problem nor an economics problem nor a policy problem. It is all of these problems put into one. It challenges our assumptions about the limitations of human beings at their worst and the capabilities of artificial intelligence systems at their best. This work proposes a set of principles for designing and building autonomous vehicles in a human-centered way that does not run away from the complexity of human nature but instead embraces it. We describe our development of the Human-Centered Autonomous Vehicle (HCAV) as an illustrative case study of implementing these principles in practice.	1,0,0,0,0,0
Bernstein - von Mises theorems for statistical inverse problems I: Schrödinger equation	The inverse problem of determining the unknown potential $f>0$ in the partial differential equation $$\frac{\Delta}{2} u - fu =0 \text{ on } \mathcal O ~~\text{s.t. } u = g \text { on } \partial \mathcal O,$$ where $\mathcal O$ is a bounded $C^\infty$-domain in $\mathbb R^d$ and $g>0$ is a given function prescribing boundary values, is considered. The data consist of the solution $u$ corrupted by additive Gaussian noise. A nonparametric Bayesian prior for the function $f$ is devised and a Bernstein - von Mises theorem is proved which entails that the posterior distribution given the observations is approximated in a suitable function space by an infinite-dimensional Gaussian measure that has a `minimal' covariance structure in an information-theoretic sense. As a consequence the posterior distribution performs valid and optimal frequentist statistical inference on $f$ in the small noise limit.	0,0,1,1,0,0
Understanding news story chains using information retrieval and network clustering techniques	Content analysis of news stories (whether manual or automatic) is a cornerstone of the communication studies field. However, much research is conducted at the level of individual news articles, despite the fact that news events (especially significant ones) are frequently presented as "stories" by news outlets: chains of connected articles covering the same event from different angles. These stories are theoretically highly important in terms of increasing public recall of news items and enhancing the agenda-setting power of the press. Yet thus far, the field has lacked an efficient method for detecting groups of articles which form stories in a way that enables their analysis. In this work, we present a novel, automated method for identifying linked news stories from within a corpus of articles. This method makes use of techniques drawn from the field of information retrieval to identify textual closeness of pairs of articles, and then clustering techniques taken from the field of network analysis to group these articles into stories. We demonstrate the application of the method to a corpus of 61,864 articles, and show how it can efficiently identify valid story clusters within the corpus. We use the results to make observations about the prevalence and dynamics of stories within the UK news media, showing that more than 50% of news production takes place within stories.	1,0,0,0,0,0
Inferactive data analysis	We describe inferactive data analysis, so-named to denote an interactive approach to data analysis with an emphasis on inference after data analysis. Our approach is a compromise between Tukey's exploratory (roughly speaking "model free") and confirmatory data analysis (roughly speaking classical and "model based"), also allowing for Bayesian data analysis. We view this approach as close in spirit to current practice of applied statisticians and data scientists while allowing frequentist guarantees for results to be reported in the scientific literature, or Bayesian results where the data scientist may choose the statistical model (and hence the prior) after some initial exploratory analysis. While this approach to data analysis does not cover every scenario, and every possible algorithm data scientists may use, we see this as a useful step in concrete providing tools (with frequentist statistical guarantees) for current data scientists. The basis of inference we use is selective inference [Lee et al., 2016, Fithian et al., 2014], in particular its randomized form [Tian and Taylor, 2015a]. The randomized framework, besides providing additional power and shorter confidence intervals, also provides explicit forms for relevant reference distributions (up to normalization) through the {\em selective sampler} of Tian et al. [2016]. The reference distributions are constructed from a particular conditional distribution formed from what we call a DAG-DAG -- a Data Analysis Generative DAG. As sampling conditional distributions in DAGs is generally complex, the selective sampler is crucial to any practical implementation of inferactive data analysis. Our principal goal is in reviewing the recent developments in selective inference as well as describing the general philosophy of selective inference.	0,0,1,1,0,0
Accurate ranking of influential spreaders in networks based on dynamically asymmetric link-impact	We propose an efficient and accurate measure for ranking spreaders and identifying the influential ones in spreading processes in networks. While the edges determine the connections among the nodes, their specific role in spreading should be considered explicitly. An edge connecting nodes i and j may differ in its importance for spreading from i to j and from j to i. The key issue is whether node j, after infected by i through the edge, would reach out to other nodes that i itself could not reach directly. It becomes necessary to invoke two unequal weights wij and wji characterizing the importance of an edge according to the neighborhoods of nodes i and j. The total asymmetric directional weights originating from a node leads to a novel measure si which quantifies the impact of the node in spreading processes. A s-shell decomposition scheme further assigns a s-shell index or weighted coreness to the nodes. The effectiveness and accuracy of rankings based on si and the weighted coreness are demonstrated by applying them to nine real-world networks. Results show that they generally outperform rankings based on the nodes' degree and k-shell index, while maintaining a low computational complexity. Our work represents a crucial step towards understanding and controlling the spread of diseases, rumors, information, trends, and innovations in networks.	1,1,0,0,0,0
Exploring Heritability of Functional Brain Networks with Inexact Graph Matching	Data-driven brain parcellations aim to provide a more accurate representation of an individual's functional connectivity, since they are able to capture individual variability that arises due to development or disease. This renders comparisons between the emerging brain connectivity networks more challenging, since correspondences between their elements are not preserved. Unveiling these correspondences is of major importance to keep track of local functional connectivity changes. We propose a novel method based on graph edit distance for the comparison of brain graphs directly in their domain, that can accurately reflect similarities between individual networks while providing the network element correspondences. This method is validated on a dataset of 116 twin subjects provided by the Human Connectome Project.	1,0,0,0,0,0
El Lenguaje Natural como Lenguaje Formal	Formal languages theory is useful for the study of natural language. In particular, it is of interest to study the adequacy of the grammatical formalisms to express syntactic phenomena present in natural language. First, it helps to draw hypothesis about the nature and complexity of the speaker-hearer linguistic competence, a fundamental question in linguistics and other cognitive sciences. Moreover, from an engineering point of view, it allows the knowledge of practical limitations of applications based on those formalisms. In this article I introduce the adequacy problem of grammatical formalisms for natural language, also introducing some formal language theory concepts required for this discussion. Then, I review the formalisms that have been proposed in history, and the arguments that have been given to support or reject their adequacy. ----- La teoría de lenguajes formales es útil para el estudio de los lenguajes naturales. En particular, resulta de interés estudiar la adecuación de los formalismos gramaticales para expresar los fenómenos sintácticos presentes en el lenguaje natural. Primero, ayuda a trazar hipótesis acerca de la naturaleza y complejidad de las competencias lingüísticas de los hablantes-oyentes del lenguaje, un interrogante fundamental de la lingüística y otras ciencias cognitivas. Además, desde el punto de vista de la ingeniería, permite conocer limitaciones prácticas de las aplicaciones basadas en dichos formalismos. En este artículo hago una introducción al problema de la adecuación de los formalismos gramaticales para el lenguaje natural, introduciendo también algunos conceptos de la teoría de lenguajes formales necesarios para esta discusión. Luego, hago un repaso de los formalismos que han sido propuestos a lo largo de la historia, y de los argumentos que se han dado para sostener o refutar su adecuación.	1,0,0,0,0,0
Two bosonic quantum walkers in one-dimensional optical lattices	Dynamical properties of two bosonic quantum walkers in a one-dimensional lattice are studied theoretically. Depending on the initial state, interactions, lattice tilting, and lattice disorder, whole plethora of different behaviors are observed. Particularly, it is shown that two bosons system manifests the many-body localization like behavior in the presence of a quenched disorder. The whole analysis is based on a specific decomposition of the temporal density profile into different contributions from singly and doubly occupied sites. In this way, the role of interactions is extracted. Since the contributions can be directly measured in experiments with ultra-cold atoms in optical lattices, the predictions presented may have some importance for upcoming experiment.	0,1,0,0,0,0
Noise induced transition in Josephson junction with second harmonic	We show a noise-induced transition in Josephson junction with fundamental as well as second harmonic. A periodically modulated multiplicative colored noise can stabilize an unstable configuration in such a system. The stabilization of the unstable configuration has been captured in the effective potential of the system obtained by integrating out the high-frequency components of the noise. This is a classical approach to understand the stability of an unstable configuration due to the presence of such stochasticity in the system and our numerical analysis confirms the prediction from the analytical calculation.	0,1,0,0,0,0
Humanoid Robots as Agents of Human Consciousness Expansion	The "Loving AI" project involves developing software enabling humanoid robots to interact with people in loving and compassionate ways, and to promote people' self-understanding and self-transcendence. Currently the project centers on the Hanson Robotics robot "Sophia" -- specifically, on supplying Sophia with personality content and cognitive, linguistic, perceptual and behavioral content aimed at enabling loving interactions supportive of human self-transcendence. In September 2017 a small pilot study was conducted, involving the Sophia robot leading human subjects through dialogues and exercises focused on meditation, visualization and relaxation. The pilot was an apparent success, qualitatively demonstrating the viability of the approach and the ability of appropriate human-robot interaction to increase human well-being and advance human consciousness.	1,0,0,0,0,0
Siamese Capsule Networks	Capsule Networks have shown encouraging results on \textit{defacto} benchmark computer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from and (3) where point-wise classification is not suitable. Hence, this paper carries out experiments on face verification in both controlled and uncontrolled settings that together address these points. In doing so we introduce \textit{Siamese Capsule Networks}, a new variant that can be used for pairwise learning tasks. The model is trained using contrastive loss with $\ell_2$-normalized capsule encoded pose features. We find that \textit{Siamese Capsule Networks} perform well against strong baselines on both pairwise learning datasets, yielding best results in the few-shot learning setting where image pairs in the test set contain unseen subjects.	0,0,0,1,0,0
Monitoring Telluric Absorption with CAMAL	Ground-based astronomical observations may be limited by telluric water vapor absorption, which is highly variable in time and significantly complicates both spectroscopy and photometry in the near-infrared (NIR). To achieve the sensitivity required to detect Earth-sized exoplanets in the NIR, simultaneous monitoring of precipitable water vapor (PWV) becomes necessary to mitigate the impact of variable telluric lines on radial velocity measurements and transit light curves. To address this issue, we present the Camera for the Automatic Monitoring of Atmospheric Lines (CAMAL), a stand-alone, inexpensive six-inch aperture telescope dedicated to measuring PWV at the Fred Lawrence Whipple Observatory on Mount Hopkins. CAMAL utilizes three narrowband NIR filters to trace the amount of atmospheric water vapor affecting simultaneous observations with the MINiature Exoplanet Radial Velocity Array (MINERVA) and MINERVA-Red telescopes. Here we present the current design of CAMAL, discuss our data analysis methods, and show results from 11 nights of PWV measurements taken with CAMAL. For seven nights of data, we have independent PWV measurements extracted from high-resolution stellar spectra taken with the Tillinghast Reflector Echelle Spectrometer (TRES) also located on Mount Hopkins. We use the TRES spectra to calibrate the CAMAL absolute PWV scale. Comparisons between CAMAL and TRES PWV estimates show excellent agreement, matching to within 1 mm over a 10 mm range in PWV. Analysis of CAMAL's photometric precision propagates to PWV measurements precise to better than 0.5 mm in dry (PWV < 4 mm) conditions. We also find that CAMAL-derived PWVs are highly correlated with those from a GPS-based water vapor monitor located approximately 90 km away at Kitt Peak National Observatory, with a root mean square PWV difference of 0.8 mm.	0,1,0,0,0,0
Finite Sample Complexity of Sequential Monte Carlo Estimators	We present bounds for the finite sample error of sequential Monte Carlo samplers on static spaces. Our approach explicitly relates the performance of the algorithm to properties of the chosen sequence of distributions and mixing properties of the associated Markov kernels. This allows us to give the first finite sample comparison to other Monte Carlo schemes. We obtain bounds for the complexity of sequential Monte Carlo approximations for a variety of target distributions including finite spaces, product measures, and log-concave distributions including Bayesian logistic regression. The bounds obtained are within a logarithmic factor of similar bounds obtainable for Markov chain Monte Carlo.	0,0,0,1,0,0
Fast Information-theoretic Bayesian Optimisation	Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective. We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead. Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space. We demonstrate empirically that FITBO inherits the performance associated with information-theoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.	0,0,0,1,0,0
Cloaking using complementary media for electromagnetic waves	Negative index materials are artificial structures whose refractive index has negative value over some frequency range. The study of these materials has attracted a lot of attention in the scientific community not only because of their many potential interesting applications but also because of challenges in understanding their intriguing properties due to the sign-changing coefficients in equations describing their properties. In this paper, we establish cloaking using complementary media for electromagnetic waves. This confirms and extends the suggestions in two dimensions of Lai et al. for the full Maxwell equations. The analysis is based on the reflecting and removing localized singularity techniques, three-sphere inequalities, and the fact that the Maxwell equations can be reduced to a weakly coupled second order elliptic equations.	0,0,1,0,0,0
Automated Discovery of Process Models from Event Logs: Review and Benchmark	Process mining allows analysts to exploit logs of historical executions of business processes to extract insights regarding the actual performance of these processes. One of the most widely studied process mining operations is automated process discovery. An automated process discovery method takes as input an event log, and produces as output a business process model that captures the control-flow relations between tasks that are observed in or implied by the event log. Various automated process discovery methods have been proposed in the past two decades, striking different tradeoffs between scalability, accuracy and complexity of the resulting models. However, these methods have been evaluated in an ad-hoc manner, employing different datasets, experimental setups, evaluation measures and baselines, often leading to incomparable conclusions and sometimes unreproducible results due to the use of closed datasets. This article provides a systematic review and comparative evaluation of automated process discovery methods, using an open-source benchmark and covering twelve publicly-available real-life event logs, twelve proprietary real-life event logs, and nine quality metrics. The results highlight gaps and unexplored tradeoffs in the field, including the lack of scalability of some methods and a strong divergence in their performance with respect to the different quality metrics used.	1,0,0,0,0,0
Fisher information matrix of binary time series	A common approach to analyzing categorical correlated time series data is to fit a generalized linear model (GLM) with past data as covariate inputs. There remain challenges to conducting inference for short time series length. By treating the historical data as covariate inputs, standard errors of estimates of GLM parameters computed using the empirical Fisher information do not fully account the auto-correlation in the data. To overcome this serious limitation, we derive the exact conditional Fisher information matrix of a general logistic autoregressive model with endogenous covariates for any series length $T$. Moreover, we also develop an iterative computational formula that allows for relatively easy implementation of the proposed estimator. Our simulation studies show that confidence intervals derived using the exact Fisher information matrix tend to be narrower than those utilizing the empirical Fisher information matrix while maintaining type I error rates at or below nominal levels. Further, we establish that the exact Fisher information matrix approaches, as T tends to infinity, the asymptotic Fisher information matrix previously derived for binary time series data. The developed exact conditional Fisher information matrix is applied to time-series data on respiratory rate among a cohort of expectant mothers where it is found to provide narrower confidence intervals for functionals of scientific interest and lead to greater statistical power when compared to the empirical Fisher information matrix.	0,0,1,1,0,0
Prime geodesic theorem of Gallagher type	We reduce the exponent in the error term of the prime geodesic theorem for compact Riemann surfaces from $\frac{3}{4}$ to $\frac{7}{10}$ outside a set of finite logarithmic measure.	0,0,1,0,0,0
Bouncy Hybrid Sampler as a Unifying Device	This work introduces a class of rejection-free Markov chain Monte Carlo (MCMC) samplers, named the Bouncy Hybrid Sampler, which unifies several existing methods from the literature. Examples include the Bouncy Particle Sampler of Peters and de With (2012), Bouchard-Cote et al. (2015) and the Hamiltonian MCMC. Following the introduced general framework, we derive a new sampler called the Quadratic Bouncy Hybrid Sampler. We apply this novel sampler to the problem of sampling from a truncated Gaussian distribution.	0,0,0,1,0,0
Viscosity solutions and the minimal surface system	We give a definition of viscosity solution for the minimal surface system and prove a version of Allard regularity theorem in this setting.	0,0,1,0,0,0
On Geodesic Completeness for Riemannian Metrics on Smooth Probability Densities	The geometric approach to optimal transport and information theory has triggered the interpretation of probability densities as an infinite-dimensional Riemannian manifold. The most studied Riemannian structures are Otto's metric, yielding the $L^2$-Wasserstein distance of optimal mass transport, and the Fisher--Rao metric, predominant in the theory of information geometry. On the space of smooth probability densities, none of these Riemannian metrics are geodesically complete---a property desirable for example in imaging applications. That is, the existence interval for solutions to the geodesic flow equations cannot be extended to the whole real line. Here we study a class of Hamilton--Jacobi-like partial differential equations arising as geodesic flow equations for higher-order Sobolev type metrics on the space of smooth probability densities. We give order conditions for global existence and uniqueness, thereby providing geodesic completeness. The system we study is an interesting example of a flow equation with loss of derivatives, which is well-posed in the smooth category, yet non-parabolic and fully non-linear. On a more general note, the paper establishes a link between geometric analysis on the space of probability densities and analysis of Euler-Arnold equations in topological hydrodynamics.	0,0,1,0,0,0
Bayesian analysis of 210Pb dating	In many studies of environmental change of the past few centuries, 210Pb dating is used to obtain chronologies for sedimentary sequences. One of the most commonly used approaches to estimate the ages of depths in a sequence is to assume a constant rate of supply (CRS) or influx of `unsupported' 210Pb from the atmosphere, together with a constant or varying amount of `supported' 210Pb. Current 210Pb dating models do not use a proper statistical framework and thus provide poor estimates of errors. Here we develop a new model for 210Pb dating, where both ages and values of supported and unsupported 210Pb form part of the parameters. We apply our model to a case study from Canada as well as to some simulated examples. Our model can extend beyond the current CRS approach, deal with asymmetric errors and mix 210Pb with other types of dating, thus obtaining more robust, realistic and statistically better defined estimates.	0,0,0,1,0,0
Small-scale Effects of Thermal Inflation on Halo Abundance at High-$z$, Galaxy Substructure Abundance and 21-cm Power Spectrum	We study the impact of thermal inflation on the formation of cosmological structures and present astrophysical observables which can be used to constrain and possibly probe the thermal inflation scenario. These are dark matter halo abundance at high redshifts, satellite galaxy abundance in the Milky Way, and fluctuation in the 21-cm radiation background before the epoch of reionization. The thermal inflation scenario leaves a characteristic signature on the matter power spectrum by boosting the amplitude at a specific wavenumber determined by the number of e-foldings during thermal inflation ($N_{\rm bc}$), and strongly suppressing the amplitude for modes at smaller scales. For a reasonable range of parameter space, one of the consequences is the suppression of minihalo formation at high redshifts and that of satellite galaxies in the Milky Way. While this effect is substantial, it is degenerate with other cosmological or astrophysical effects. The power spectrum of the 21-cm background probes this impact more directly, and its observation may be the best way to constrain the thermal inflation scenario due to the characteristic signature in the power spectrum. The Square Kilometre Array (SKA) in phase 1 (SKA1) has sensitivity large enough to achieve this goal for models with $N_{\rm bc}\gtrsim 26$ if a 10000-hr observation is performed. The final phase SKA, with anticipated sensitivity about an order of magnitude higher, seems more promising and will cover a wider parameter space.	0,1,0,0,0,0
Neural Semantic Parsing over Multiple Knowledge-bases	A fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form. In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets. We find that we can substantially improve parsing accuracy by training a single sequence-to-sequence model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-of-the-art performance on the Overnight dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters.	1,0,0,0,0,0
Solutions to twisted word equations and equations in virtually free groups	It is well-known that the problem to solve equations in virtually free groups can be reduced to the problem to solve twisted word equations with regular constraints over free monoids with involution. In a first part of the paper we prove that the set of all solutions of such a twisted word equation is an EDT0L language and that the specification of that EDT0L language can be computed in PSPACE. (We give a more precise bound in the paper.) Within the same complexity bound we can decide whether the solution set is empty, finite, or infinite. No PSPACE-algorithm, actually no concrete complexity bound was known for deciding emptiness before. Decidability of finiteness was considered to be an open problem. In the second part we apply the results to the solution set of equations with rational constraints in finitely generated virtually free groups. For each such group we obtain the same results as above for the set of solutions in standard normal forms with respect to some natural set of generators. In particular, for a fixed group we can decide in PSPACE whether the solution set is empty, finite, or infinite. Our results generalize the work by Lohrey and Sénizergues (ICALP 2006) and Dahmani and Guirardel (J. of Topology 2010) with respect to both complexity and expressive power. Neither paper gave any concrete complexity bound and the results in these papers are stated subsets of solutions only, whereas our results concern all solutions. Moreover, we give a formal language characterization of the full solution set as an EDT0L language.	1,0,1,0,0,0
Chance-Constrained AC Optimal Power Flow Integrating HVDC Lines and Controllability	The integration of large-scale renewable generation has major implications on the operation of power systems, two of which we address in this paper. First, system operators have to deal with higher degrees of uncertainty. Second, with abundant potential of renewable generation in remote locations, they need to incorporate the operation of High Voltage Direct Current lines (HVDC). This paper introduces an optimization tool that addresses both challenges by incorporating; the full AC power flow equations and chance constraints to address the uncertainty of renewable infeed, HVDC modeling for point-to-point lines, and optimizing generator and HVDC corrective control policies in reaction to uncertainty. The main contributions are twofold. First, we introduce a HVDC line model and the corresponding HVDC participation factors in a chance-constrained AC-OPF framework. Second, we modify an existing algorithm for solving the chance-constrained AC optimal power flow to allow for optimization of the generation and HVDC participation factors. Using realistic wind forecast data, and a 10 bus system with one HVDC line and two wind farms, we demonstrate the performance of our algorithm and show the benefit of controllability.	1,0,0,0,0,0
Genetic Algorithms for Evolving Computer Chess Programs	This paper demonstrates the use of genetic algorithms for evolving: 1) a grandmaster-level evaluation function, and 2) a search mechanism for a chess program, the parameter values of which are initialized randomly. The evaluation function of the program is evolved by learning from databases of (human) grandmaster games. At first, the organisms are evolved to mimic the behavior of human grandmasters, and then these organisms are further improved upon by means of coevolution. The search mechanism is evolved by learning from tactical test suites. Our results show that the evolved program outperforms a two-time world computer chess champion and is at par with the other leading computer chess programs.	1,0,0,1,0,0
Quantum Critical Behavior in the Asymptotic Limit of High Disorder: Entropy Stabilized NiCoCr0.8 Alloys	The behavior of matter near a quantum critical point (QCP) is one of the most exciting and challenging areas of physics research. Emergent phenomena such as high-temperature superconductivity are linked to the proximity to a QCP. Although significant progress has been made in understanding quantum critical behavior in some low dimensional magnetic insulators, the situation in metallic systems is much less clear. Here we demonstrate that NiCoCrx single crystal alloys are remarkable model systems for investigating QCP physics in a metallic environment. For NiCoCrx alloys with x = 0.8, the critical exponents associated with a ferromagnetic quantum critical point (FQCP) are experimentally determined from low temperature magnetization and heat capacity measurements. For the first time, all of the five critical exponents ( gamma-subT =1/2 , beta-subT = 1, delta = 3/2, nuz-subm = 2, alpha-bar-subT = 0) are in remarkable agreement with predictions of Belitz-Kirkpatrick-Vojta (BKV) theory in the asymptotic limit of high disorder. Using these critical exponents, excellent scaling of the magnetization data is demonstrated with no adjustable parameters. We also find a divergence of the magnetic Gruneisen parameter, consistent with a FQCP. This work therefore demonstrates that entropy stabilized concentrated solid solutions represent a unique platform to study quantum critical behavior in a highly tunable class of materials.	0,1,0,0,0,0
A parametric level-set method for partially discrete tomography	This paper introduces a parametric level-set method for tomographic reconstruction of partially discrete images. Such images consist of a continuously varying background and an anomaly with a constant (known) grey-value. We represent the geometry of the anomaly using a level-set function, which we represent using radial basis functions. We pose the reconstruction problem as a bi-level optimization problem in terms of the background and coefficients for the level-set function. To constrain the background reconstruction we impose smoothness through Tikhonov regularization. The bi-level optimization problem is solved in an alternating fashion; in each iteration we first reconstruct the background and consequently update the level-set function. We test our method on numerical phantoms and show that we can successfully reconstruct the geometry of the anomaly, even from limited data. On these phantoms, our method outperforms Total Variation reconstruction, DART and P-DART.	1,0,0,0,0,0
A Multiple Linear Regression Approach For Estimating the Market Value of Football Players in Forward Position	In this paper, market values of the football players in the forward positions are estimated using multiple linear regression by including the physical and performance factors in 2017-2018 season. Players from 4 major leagues of Europe are examined, and by applying the test for homoscedasticity, a reasonable regression model within 0.10 significance level is built, and the most and the least affecting factors are explained in detail.	0,0,0,1,0,0
Urban Swarms: A new approach for autonomous waste management	Modern cities are growing ecosystems that face new challenges due to the increasing population demands. One of the many problems they face nowadays is waste management, which has become a pressing issue requiring new solutions. Swarm robotics systems have been attracting an increasing amount of attention in the past years and they are expected to become one of the main driving factors for innovation in the field of robotics. The research presented in this paper explores the feasibility of a swarm robotics system in an urban environment. By using bio-inspired foraging methods such as multi-place foraging and stigmergy-based navigation, a swarm of robots is able to improve the efficiency and autonomy of the urban waste management system in a realistic scenario. To achieve this, a diverse set of simulation experiments was conducted using real-world GIS data and implementing different garbage collection scenarios driven by robot swarms. Results presented in this research show that the proposed system outperforms current approaches. Moreover, results not only show the efficiency of our solution, but also give insights about how to design and customize these systems.	1,0,0,0,0,0
Brownian ratchets: How stronger thermal noise can reduce diffusion	We study diffusion properties of an inertial Brownian motor moving on a ratchet substrate, i.e. a periodic structure with broken reflection symmetry. The motor is driven by an unbiased time-periodic symmetric force which takes the system out of thermal equilibrium. For selected parameter sets, the system is in a non-chaotic regime in which we can identify a non-monotonic dependence of the diffusion coefficient on temperature: for low temperature, it initially increases as temperature grows, passes through its local maximum, next starts to diminish reaching its local minimum and finally it monotonically increases in accordance with the Einstein linear relation. Particularly interesting is the temperature interval in which diffusion is suppressed by thermal noise and we explain this effect in terms of transition rates of a three-state stochastic model.	0,1,0,0,0,0
High order conformal symplectic and ergodic schemes for stochastic Langevin equation via generating functions	In this paper, we consider the stochastic Langevin equation with additive noises, which possesses both conformal symplectic geometric structure and ergodicity. We propose a methodology of constructing high weak order conformal symplectic schemes by converting the equation into an equivalent autonomous stochastic Hamiltonian system and modifying the associated generating function. To illustrate this approach, we construct a specific second order numerical scheme, and prove that its symplectic form dissipates exponentially. Moreover, for the linear case, the proposed scheme is also shown to inherit the ergodicity of the original system, and the temporal average of the numerical solution is a proper approximation of the ergodic limit over long time. Numerical experiments are given to verify these theoretical results.	0,0,1,0,0,0
A Novel Partitioning Method for Accelerating the Block Cimmino Algorithm	We propose a novel block-row partitioning method in order to improve the convergence rate of the block Cimmino algorithm for solving general sparse linear systems of equations. The convergence rate of the block Cimmino algorithm depends on the orthogonality among the block rows obtained by the partitioning method. The proposed method takes numerical orthogonality among block rows into account by proposing a row inner-product graph model of the coefficient matrix. In the graph partitioning formulation defined on this graph model, the partitioning objective of minimizing the cutsize directly corresponds to minimizing the sum of inter-block inner products between block rows thus leading to an improvement in the eigenvalue spectrum of the iteration matrix. This in turn leads to a significant reduction in the number of iterations required for convergence. Extensive experiments conducted on a large set of matrices confirm the validity of the proposed method against a state-of-the-art method.	1,0,0,0,0,0
Universality and scaling laws in the cascading failure model with healing	Cascading failures may lead to dramatic collapse in interdependent networks, where the breakdown takes place as a discontinuity of the order parameter. In the cascading failure (CF) model with healing there is a control parameter which at some value suppresses the discontinuity of the order parameter. However, up to this value of the healing parameter the breakdown is a hybrid transition, meaning that, besides this first order character, the transition shows scaling too. In this paper we investigate the question of universality related to the scaling behavior. Recently we showed that the hybrid phase transition in the original CF model has two sets of exponents describing respectively the order parameter and the cascade statistics, which are connected by a scaling law. In the CF model with healing we measure these exponents as a function of the healing parameter. We find two universality classes: In the wide range below the critical healing value the exponents agree with those of the original model, while above this value the model displays trivial scaling meaning that fluctuations follow the central limit theorem.	1,1,0,0,0,0
An Extension of the Method of Brackets. Part 1	The method of brackets is an efficient method for the evaluation of a large class of definite integrals on the half-line. It is based on a small collection of rules, some of which are heuristic. The extension discussed here is based on the concepts of null and divergent series. These are formal representations of functions, whose coefficients $a_{n}$ have meromorphic representations for $n \in \mathbb{C}$, but might vanish or blow up when $n \in \mathbb{N}$. These ideas are illustrated with the evaluation of a variety of entries from the classical table of integrals by Gradshteyn and Ryzhik.	0,0,1,0,0,0
A Scalable Deep Neural Network Architecture for Multi-Building and Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinting	One of the key technologies for future large-scale location-aware services covering a complex of multi-story buildings --- e.g., a big shopping mall and a university campus --- is a scalable indoor localization technique. In this paper, we report the current status of our investigation on the use of deep neural networks (DNNs) for scalable building/floor classification and floor-level position estimation based on Wi-Fi fingerprinting. Exploiting the hierarchical nature of the building/floor estimation and floor-level coordinates estimation of a location, we propose a new DNN architecture consisting of a stacked autoencoder for the reduction of feature space dimension and a feed-forward classifier for multi-label classification of building/floor/location, on which the multi-building and multi-floor indoor localization system based on Wi-Fi fingerprinting is built. Experimental results for the performance of building/floor estimation and floor-level coordinates estimation of a given location demonstrate the feasibility of the proposed DNN-based indoor localization system, which can provide near state-of-the-art performance using a single DNN, for the implementation with lower complexity and energy consumption at mobile devices.	1,0,0,1,0,0
Time-of-Flight Electron Energy Loss Spectroscopy by Longitudinal Phase Space Manipulation with Microwave Cavities	The possibility to perform high-resolution time-resolved electron energy loss spectroscopy has the potential to impact a broad range of research fields. Resolving small energy losses with ultrashort electron pulses, however, is an enormous challenge due to the low average brightness of a pulsed beam. In this letter, we propose to use time-of-flight measurements combined with longitudinal phase space manipulation using resonant microwave cavities. This allows for both an accurate detection of energy losses with a high current throughput, and efficient monochromation. First, a proof-of-principle experiment is presented, showing that with the incorporation of a compression cavity the flight time resolution can be improved significantly. Then, it is shown through simulations that by adding a cavity-based monochromation technique, a full-width-at-half-maximum energy resolution of 22 meV can be achieved with 3.1 ps pulses at a beam energy of 30 keV with currently available technology. By combining state-of-the-art energy resolutions with a pulsed electron beam, the technique proposed here opens up the way to detecting short-lived excitations within the regime of highly collective physics.	0,1,0,0,0,0
On-line Building Energy Optimization using Deep Reinforcement Learning	Unprecedented high volumes of data are becoming available with the growth of the advanced metering infrastructure. These are expected to benefit planning and operation of the future power system, and to help the customers transition from a passive to an active role. In this paper, we explore for the first time in the smart grid context the benefits of using Deep Reinforcement Learning, a hybrid type of methods that combines Reinforcement Learning with Deep Learning, to perform on-line optimization of schedules for building energy management systems. The learning procedure was explored using two methods, Deep Q-learning and Deep Policy Gradient, both of them being extended to perform multiple actions simultaneously. The proposed approach was validated on the large-scale Pecan Street Inc. database. This highly-dimensional database includes information about photovoltaic power generation, electric vehicles as well as buildings appliances. Moreover, these on-line energy scheduling strategies could be used to provide real-time feedback to consumers to encourage more efficient use of electricity.	1,0,1,0,0,0
Centroid estimation based on symmetric KL divergence for Multinomial text classification problem	We define a new method to estimate centroid for text classification based on the symmetric KL-divergence between the distribution of words in training documents and their class centroids. Experiments on several standard data sets indicate that the new method achieves substantial improvements over the traditional classifiers.	0,0,0,1,0,0
Exact partial information decompositions for Gaussian systems based on dependency constraints	The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a theoretical framework to characterize and quantify the structure of multivariate information sharing. A new method (Idep) has recently been proposed for computing a two-predictor PID over discrete spaces. [arXiv:1709.06653] A lattice of maximum entropy probability models is constructed based on marginal dependency constraints, and the unique information that a particular predictor has about the target is defined as the minimum increase in joint predictor-target mutual information when that particular predictor-target marginal dependency is constrained. Here, we apply the Idep approach to Gaussian systems, for which the marginally constrained maximum entropy models are Gaussian graphical models. Closed form solutions for the Idep PID are derived for both univariate and multivariate Gaussian systems. Numerical and graphical illustrations are provided, together with practical and theoretical comparisons of the Idep PID with the minimum mutual information PID (Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method generally produces larger estimates of redundancy and synergy than does the Idep method. In discussion of the practical examples, the PIDs are complemented by the use of deviance tests for the comparison of Gaussian graphical models.	0,0,0,1,1,0
Bernoulli-Carlitz and Cauchy-Carlitz numbers with Stirling-Carlitz numbers	Recently, the Cauchy-Carlitz number was defined as the counterpart of the Bernoulli-Carlitz number. Both numbers can be expressed explicitly in terms of so-called Stirling-Carlitz numbers. In this paper, we study the second analogue of Stirling-Carlitz numbers and give some general formulae, including Bernoulli and Cauchy numbers in formal power series with complex coefficients, and Bernoulli-Carlitz and Cauchy-Carlitz numbers in function fields. We also give some applications of Hasse-Teichmüller derivative to hypergeometric Bernoulli and Cauchy numbers in terms of associated Stirling numbers.	0,0,1,0,0,0
On the Hilbert coefficients, depth of associated graded rings and reduction numbers	Let $(R,\mathfrak{m})$ be a $d$-dimensional Cohen-Macaulay local ring, $I$ an $\mathfrak{m}$-primary ideal of $R$ and $J=(x_1,...,x_d)$ a minimal reduction of $I$. We show that if $J_{d-1}=(x_1,...,x_{d-1})$ and $\sum\limits_{n=1}^\infty\lambda{({I^{n+1}\cap J_{d-1}})/({J{I^n} \cap J_{d-1}})=i}$ where i=0,1, then depth $G(I)\geq{d-i-1}$. Moreover, we prove that if $e_2(I) = \sum_{n=2}^\infty (n-1) \lambda (I^n/JI^{n-1})-2;$ or if $I$ is integrally closed and $e_2(I) = \sum_{n=2}^\infty (n-1)\lambda({I^{n}}/JI^{n-1})-i$ where $i=3,4$, then $e_1(I) = \sum_{n=1}^\infty \lambda(I^n / JI^{n-1})-1.$ In addition, we show that $r(I)$ is independent. Furthermore, we study the independence of $r(I)$ with some other conditions.	0,0,1,0,0,0
A recommender system to restore images with impulse noise	We build a collaborative filtering recommender system to restore images with impulse noise for which the noisy pixels have been previously identified. We define this recommender system in terms of a new color image representation using three matrices that depend on the noise-free pixels of the image to restore, and two parameters: $k$, the number of features; and $\lambda$, the regularization factor. We perform experiments on a well known image database to test our algorithm and we provide image quality statistics for the results obtained. We discuss the roles of bias and variance in the performance of our algorithm as determined by the values of $k$ and $\lambda$, and provide guidance on how to choose the values of these parameters. Finally, we discuss the possibility of using our collaborative filtering recommender system to perform image inpainting and super-resolution.	1,0,0,1,0,0
Bonding charge distribution analysis of molecule by computation of interatomic charge penetration	Charge transfer among individual atoms in a molecule is the key concept in the modern electronic theory of chemical bonding. In this work, we defined an atomic region between two atoms by Slater orbital exponents of valence electrons and suggested a method for analytical calculation of charge penetration between all atoms in a molecule. Computation of charge penetration amount is self-consistently performed until each orbital exponent converges to its certain values respectively. Charge penetration matrix was calculated for ethylene and MgO, and bonding charge and its distribution were analyzed by using the charge penetration matrix and the orbital exponents under the bonding state. These results were compared with those by density function method and showed that this method is a simple and direct method to obtain bonding charge distribution of molecule from atomic orbital functions.	0,1,0,0,0,0
Going Higher in First-Order Quantifier Alternation Hierarchies on Words	We investigate quantifier alternation hierarchies in first-order logic on finite words. Levels in these hierarchies are defined by counting the number of quantifier alternations in formulas. We prove that one can decide membership of a regular language in the levels $\mathcal{B}{\Sigma}_2$ (finite boolean combinations of formulas having only one alternation) and ${\Sigma}_3$ (formulas having only two alternations and beginning with an existential block). Our proofs work by considering a deeper problem, called separation, which, once solved for lower levels, allows us to solve membership for higher levels.	1,0,0,0,0,0
Adaptive Noise Cancellation Using Deep Cerebellar Model Articulation Controller	This paper proposes a deep cerebellar model articulation controller (DCMAC) for adaptive noise cancellation (ANC). We expand upon the conventional CMAC by stacking sin-gle-layer CMAC models into multiple layers to form a DCMAC model and derive a modified backpropagation training algorithm to learn the DCMAC parameters. Com-pared with conventional CMAC, the DCMAC can characterize nonlinear transformations more effectively because of its deep structure. Experimental results confirm that the pro-posed DCMAC model outperforms the CMAC in terms of residual noise in an ANC task, showing that DCMAC provides enhanced modeling capability based on channel characteristics.	1,0,0,0,0,0
AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization	Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine tune parameters such as the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization, which is quite different from the offline and nonconvex setting where adaptive gradient methods shine in practice. We bridge this gap by providing strong theoretical guarantees in batch and stochastic setting, for the convergence of AdaGrad over smooth, nonconvex landscapes, from any initialization of the stepsize, without knowledge of Lipschitz constant of the gradient. We show in the stochastic setting that AdaGrad converges to a stationary point at the optimal $O(1/\sqrt{N})$ rate (up to a $\log(N)$ factor), and in the batch setting, at the optimal $O(1/N)$ rate. Moreover, in both settings, the constant in the rate matches the constant obtained as if the variance of the gradient noise and Lipschitz constant of the gradient were known in advance and used to tune the stepsize, up to a logarithmic factor of the mismatch between the optimal stepsize and the stepsize used to initialize AdaGrad. In particular, our results imply that AdaGrad is robust to both the unknown Lipschitz constant and level of stochastic noise on the gradient, in a near-optimal sense. When there is noise, AdaGrad converges at the rate of $O(1/\sqrt{N})$ with well-tuned stepsize, and when there is not noise, the same algorithm converges at the rate of $O(1/N)$ like well-tuned batch gradient descent.	0,0,0,1,0,0
Towards Wi-Fi AP-Assisted Content Prefetching for On-Demand TV Series: A Reinforcement Learning Approach	The emergence of smart Wi-Fi APs (Access Point), which are equipped with huge storage space, opens a new research area on how to utilize these resources at the edge network to improve users' quality of experience (QoE) (e.g., a short startup delay and smooth playback). One important research interest in this area is content prefetching, which predicts and accurately fetches contents ahead of users' requests to shift the traffic away during peak periods. However, in practice, the different video watching patterns among users, and the varying network connection status lead to the time-varying server load, which eventually makes the content prefetching problem challenging. To understand this challenge, this paper first performs a large-scale measurement study on users' AP connection and TV series watching patterns using real-traces. Then, based on the obtained insights, we formulate the content prefetching problem as a Markov Decision Process (MDP). The objective is to strike a balance between the increased prefetching&storage cost incurred by incorrect prediction and the reduced content download delay because of successful prediction. A learning-based approach is proposed to solve this problem and another three algorithms are adopted as baselines. In particular, first, we investigate the performance lower bound by using a random algorithm, and the upper bound by using an ideal offline approach. Then, we present a heuristic algorithm as another baseline. Finally, we design a reinforcement learning algorithm that is more practical to work in the online manner. Through extensive trace-based experiments, we demonstrate the performance gain of our design. Remarkably, our learning-based algorithm achieves a better precision and hit ratio (e.g., 80%) with about 70% (resp. 50%) cost saving compared to the random (resp. heuristic) algorithm.	1,0,0,0,0,0
Heavy fermion quantum criticality at dilute carrier limit in CeNi$_{2-δ}$(As$_{1-x}$P$_{x}$)$_{2}$	We study the quantum phase transitions in the nickel pnctides, CeNi$_{2-\delta}$(As$_{1-x}$P$_{x}$)$_{2}$ ($\delta$ $\approx$ 0.07-0.22). This series displays the distinct heavy fermion behavior in the rarely studied parameter regime of dilute carrier limit. We systematically investigate the magnetization, specific heat and electrical transport down to low temperatures. Upon increasing the P-content, the antiferromagnetic order of the Ce-4$f$ moment is suppressed continuously and vanishes at $x_c \sim$ 0.55. At this doping, the temperature dependences of the specific heat and longitudinal resistivity display non-Fermi liquid behavior. Both the residual resistivity $\rho_0$ and the Sommerfeld coefficient $\gamma_0$ are sharply peaked around $x_c$. When the P-content reaches close to 100\%, we observe a clear low-temperature crossover into the Fermi liquid regime. In contrast to what happens in the parent compound $x$ = 0.0 as a function of pressure, we find a surprising result that the non-Fermi liquid behavior persists over a nonzero range of doping concentration, $x_c<x<0.9$. In this doping range, at the lowest measured temperatures, the temperature dependence of the specific-heat coefficient is logarithmically divergent and that of the electrical resistivity is linear. We discuss the properties of CeNi$_{2-\delta}$(As$_{1-x}$P$_{x}$)$_{2}$ in comparison with those of its 1111 counterpart, CeNi(As$_{1-x}$P$_{x}$)O. Our results indicate a non-Fermi liquid phase in the global phase diagram of heavy fermion metals.	0,1,0,0,0,0
Relative phantom maps	We define a map $f\colon X\to Y$ to be a phantom map relative to a map $\varphi\colon B\to Y$ if the restriction of $f$ to any finite dimensional skeleton of $X$ lifts to $B$ through $\varphi$, up to homotopy. There are two kinds of maps which are obviously relative phantom maps: (1) the composite of a map $X\to B$ with $\varphi$; (2) a usual phantom map $X\to Y$. A relative phantom map of type (1) is called trivial, and a relative phantom map out of a suspension which is a sum of (1) and (2) is called relatively trivial. We study the (relative) triviality of relative phantom maps from a suspension, and in particular, we give rational homotopy conditions for the (relative) triviality. We also give a rational homotopy condition for the triviality of relative phantom maps from a non-suspension to a finite Postnikov section.	0,0,1,0,0,0
On the essential spectrum of elliptic differential operators	Let $\mathcal{A}$ be a $C^*$-algebra of bounded uniformly continuous functions on $X=\mathbb{R}^d$ such that $\mathcal{A}$ is stable under translations and contains the continuous functions that have a limit at infinity. Denote $\mathcal{A}^\dagger$ the boundary of $X$ in the character space of $\mathcal{A}$. Then the crossed product $\mathscr{A}=\mathcal{A}\rtimes X$ of $\mathcal{A}$ by the natural action of $X$ on $\mathcal{A}$ is a well defined $C^*$-algebra and to each operator $A\in\mathscr{A}$ one may naturally associate a family of bounded operators $A_\varkappa$ on $L^2(X)$ indexed by the characters $\varkappa\in\mathcal{A}^\dagger$. We show that the essential spectrum of $A$ is the union of the spectra of the operators $A_\varkappa$. The applications cover very general classes of singular elliptic operators.	0,0,1,0,0,0
Disturbance propagation, inertia location and slow modes in large-scale high voltage power grids	Conventional generators in power grids are steadily substituted with new renewable sources of electric power. The latter are connected to the grid via inverters and as such have little, if any rotational inertia. The resulting reduction of total inertia raises important issues of power grid stability, especially over short-time scales. We have constructed a model of the synchronous grid of continental Europe with which we numerically investigate frequency deviations as well as rates of change of frequency (RoCoF) following abrupt power losses. The magnitude of RoCoF's and frequency deviations strongly depend on the fault location, and we find the largest effects for faults located on the slowest mode - the Fiedler mode - of the network Laplacian matrix. This mode essentially vanishes over Belgium, Eastern France, Western Germany, northern Italy and Switzerland. Buses inside these regions are only weakly affected by faults occuring outside. Conversely, faults inside these regions have only a local effect and disturb only weakly outside buses. Following this observation, we reduce rotational inertia through three different procedures by either (i) reducing inertia on the Fiedler mode, (ii) reducing inertia homogeneously and (iii) reducing inertia outside the Fiedler mode. We find that procedure (iii) has little effect on disturbance propagation, while procedure (i) leads to the strongest increase of RoCoF and frequency deviations. These results for our model of the European transmission grid are corroborated by numerical investigations on the ERCOT transmission grid.	1,0,0,0,0,0
Sparse Coding Predicts Optic Flow Specificities of Zebrafish Pretectal Neurons	Zebrafish pretectal neurons exhibit specificities for large-field optic flow patterns associated with rotatory or translatory body motion. We investigate the hypothesis that these specificities reflect the input statistics of natural optic flow. Realistic motion sequences were generated using computer graphics simulating self-motion in an underwater scene. Local retinal motion was estimated with a motion detector and encoded in four populations of directionally tuned retinal ganglion cells, represented as two signed input variables. This activity was then used as input into one of two learning networks: a sparse coding network (competitive learning) and backpropagation network (supervised learning). Both simulations develop specificities for optic flow which are comparable to those found in a neurophysiological study (Kubo et al. 2014), and relative frequencies of the various neuronal responses are best modeled by the sparse coding approach. We conclude that the optic flow neurons in the zebrafish pretectum do reflect the optic flow statistics. The predicted vectorial receptive fields show typical optic flow fields but also "Gabor" and dipole-shaped patterns that likely reflect difference fields needed for reconstruction by linear superposition.	0,0,0,0,1,0
Weighted blowup correspondence of orbifold Gromov--Witten invariants and applications	Let $\sf X$ be a symplectic orbifold groupoid with $\sf S$ being a symplectic sub-orbifold groupoid, and $\sf X_{\mathfrak a}$ be the weight-$\mathfrak a$ blowup of $\sf X$ along $\sf S$ with $\sf Z$ being the corresponding exceptional divisor. We show that there is a weighted blowup correspondence between some certain absolute orbifold Gromov--Witten invariants of $\sf X$ relative to $\sf S$ and some certain relative orbifold Gromov--Witten invariants of the pair $(\sf X_{\mathfrak a}|Z)$. As an application, we prove that the symplectic uniruledness of symplectic orbifold groupoids is a weighted blowup invariant.	0,0,1,0,0,0
Comparing People with Bibliometrics	Bibliometric indicators, citation counts and/or download counts are increasingly being used to inform personnel decisions such as hiring or promotions. These statistics are very often misused. Here we provide a guide to the factors which should be considered when using these so-called quantitative measures to evaluate people. Rules of thumb are given for when begin to use bibliometric measures when comparing otherwise similar candidates.	1,1,0,0,0,0
APSYNSIM: An Interactive Tool To Learn Interferometry	The APerture SYNthesis SIMulator is a simple interactive tool to help the students visualize and understand the basics of the Aperture Synthesis technique, applied to astronomical interferometers. The users can load many different interferometers and source models (and also create their own), change the observing parameters (e.g., source coordinates, observing wavelength, antenna location, integration time, etc.), and even deconvolve the interferometric images and corrupt the data with gain errors (amplitude and phase). The program is fully interactive and all the figures are updated in real time. APSYNSIM has already been used in several interferometry schools and has got very positive feedback from the students.	0,1,0,0,0,0
Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization	Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints. In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.	1,0,1,1,0,0
Estimator of Prediction Error Based on Approximate Message Passing for Penalized Linear Regression	We propose an estimator of prediction error using an approximate message passing (AMP) algorithm that can be applied to a broad range of sparse penalties. Following Stein's lemma, the estimator of the generalized degrees of freedom, which is a key quantity for the construction of the estimator of the prediction error, is calculated at the AMP fixed point. The resulting form of the AMP-based estimator does not depend on the penalty function, and its value can be further improved by considering the correlation between predictors. The proposed estimator is asymptotically unbiased when the components of the predictors and response variables are independently generated according to a Gaussian distribution. We examine the behaviour of the estimator for real data under nonconvex sparse penalties, where Akaike's information criterion does not correspond to an unbiased estimator of the prediction error. The model selected by the proposed estimator is close to that which minimizes the true prediction error.	0,0,0,1,0,0
Gas dynamics in strong centrifugal fields	Dynamics of waves generated by scopes in gas centrifuges (GC) for isotope separation is considered. The centrifugal acceleration in the GC reaches values of the order of $10^6$g. The centrifugal and Coriolis forces modify essentially the conventional sound waves. Three families of the waves with different polarisation and dispersion exist in these conditions. Dynamics of the flow in the model GC Iguasu is investigated numerically. Comparison of the results of the numerical modelling of the wave dynamics with the analytical predictions is performed. New phenomena of the resonances in the GC is found. The resonances occur for the waves polarized along the rotational axis having the smallest dumping due to the viscosity.	0,1,0,0,0,0
Latent Association Mining in Binary Data	We consider the problem of identifying groups of mutually associated variables in moderate or high dimensional data. In many cases, ordinary Pearson correlation provides useful information concerning the linear relationship between variables. However, for binary data, ordinary correlation may lose power and may lack interpretability. In this paper, we develop and investigate a new method called Latent Association Mining in Binary Data (LAMB). The LAMB method is built on the assumption that the binary observations represent a random thresholding of a latent continuous variable that may have a complex correlation structure. We consider a new measure of association, latent correlation, that is designed to assess association in the underlying continuous variable, without bias due to the mediating effects of the thresholding procedure. The full LAMB procedure makes use of iterative hypothesis testing to identify groups of latently correlated variables. LAMB is shown to improve power over existing methods in simulated settings, to be computationally efficient for large datasets, and to uncover new meaningful results from common real data types.	0,0,0,1,0,0
Linear-Cost Covariance Functions for Gaussian Random Fields	Gaussian random fields (GRF) are a fundamental stochastic model for spatiotemporal data analysis. An essential ingredient of GRF is the covariance function that characterizes the joint Gaussian distribution of the field. Commonly used covariance functions give rise to fully dense and unstructured covariance matrices, for which required calculations are notoriously expensive to carry out for large data. In this work, we propose a construction of covariance functions that result in matrices with a hierarchical structure. Empowered by matrix algorithms that scale linearly with the matrix dimension, the hierarchical structure is proved to be efficient for a variety of random field computations, including sampling, kriging, and likelihood evaluation. Specifically, with $n$ scattered sites, sampling and likelihood evaluation has an $O(n)$ cost and kriging has an $O(\log n)$ cost after preprocessing, particularly favorable for the kriging of an extremely large number of sites (e.g., predicting on more sites than observed). We demonstrate comprehensive numerical experiments to show the use of the constructed covariance functions and their appealing computation time. Numerical examples on a laptop include simulated data of size up to one million, as well as a climate data product with over two million observations.	0,0,0,1,0,0
A Redshift Survey of the Nearby Galaxy Cluster Abell 2199: Comparison of the Spatial and Kinematic Distributions of Galaxies with the Intracluster Medium	We present the results from an extensive spectroscopic survey of the central region of the nearby galaxy cluster Abell 2199 at $z=0.03$. By combining 775 new redshifts from the MMT/Hectospec observations with the data in the literature, we construct a large sample of 1624 galaxies with measured redshifts at $R<30^\prime$, which results in high spectroscopic completeness at $r_{\rm petro,0}<20.5$ (77%). We use these data to study the kinematics and clustering of galaxies focusing on the comparison with those of the intracluster medium (ICM) from Suzaku X-ray observations. We identify 406 member galaxies of A2199 at $R<30^\prime$ using the caustic technique. The velocity dispersion profile of cluster members appears smoothly connected to the stellar velocity dispersion profile of the cD galaxy. The luminosity function is well fitted with a Schechter function at $M_r<-15$. The radial velocities of cluster galaxies generally agree well with those of the ICM, but there are some regions where the velocity difference between the two is about a few hundred kilometer per second. The cluster galaxies show a hint of global rotation at $R<5^\prime$ with $v_{\rm rot}=300{-}600\,\textrm{km s}^{-1}$, but the ICM in the same region do not show such rotation. We apply a friends-of-friends algorithm to the cluster galaxy sample at $R<60^\prime$ and identify 32 group candidates, and examine the spatial correlation between the galaxy groups and X-ray emission. This extensive survey in the central region of A2199 provides an important basis for future studies of interplay among the galaxies, the ICM and the dark matter in the cluster.	0,1,0,0,0,0
Chomp on numerical semigroups	We consider the two-player game chomp on posets associated to numerical semigroups and show that the analysis of strategies for chomp is strongly related to classical properties of semigroups. We characterize, which player has a winning-strategy for symmetric semigroups, semigroups of maximal embedding dimension and several families of numerical semigroups generated by arithmetic sequences. Furthermore, we show that which player wins on a given numerical semigroup is a decidable question. Finally, we extend several of our results to the more general setting of subsemigroups of $\mathbb{N} \times T$, where $T$ is a finite abelian group.	1,0,1,0,0,0
SMT Queries Decomposition and Caching in Semi-Symbolic Model Checking	In semi-symbolic (control-explicit data-symbolic) model checking the state-space explosion problem is fought by representing sets of states by first-order formulas over the bit-vector theory. In this model checking approach, most of the verification time is spent in an SMT solver on deciding satisfiability of quantified queries, which represent equality of symbolic states. In this paper, we introduce a new scheme for decomposition of symbolic states, which can be used to significantly improve the performance of any semi-symbolic model checker. Using the decomposition, a model checker can issue much simpler and smaller queries to the solver when compared to the original case. Some SMT calls may be even avoided completely, as the satisfaction of some of the simplified formulas can be decided syntactically. Moreover, the decomposition allows for an efficient caching scheme for quantified formulas. To support our theoretical contribution, we show the performance gain of our model checker SymDIVINE on a set of examples from the Software Verification Competition.	1,0,0,0,0,0
Closed-form formulae of hyperbolic metamaterial made by stacked hole-array layers working at terahertz or microwave radiation	A metamaterial made by stacked hole-array layers known as a fishnet metamaterial behaves as a hyperbolic metamaterial at wavelength much longer than hole-array period. However, the analytical formulae of effective parameters of a fishnet metamaterial have not been reported hindering the design of deep-subwavelength imaging devices using this structure. We report the new closed-form formulae of effective parameters comprising anisotropic dispersion relation of a fishnet metamaterial working at terahertz or microwave frequency. These effective parameters of a fishnet metamaterial are consistent with those obtained by quasi-full solutions using known effective parameters of a hole-array layer working at frequency below its spoof plasma frequency with the superlattice period much smaller than the hole-array period. We also theoretically demonstrate the deep-subwavelength focusing at {\lambda}/83 using the composite structure of a slit-array layer and a fishnet metamaterial. It is found that the focused intensity inside a fishnet metamaterial is several times larger than that without the fishnet metamaterial, but the transmitted intensity is still restricted by large-wavevector difference in air and a fishnet metamaterial. Our effective parameters may aid the next-generation deep-subwavelength imaging devices working at terahertz or microwave radiation.	0,1,0,0,0,0
Differentially Private Variational Dropout	Deep neural networks with their large number of parameters are highly flexible learning systems. The high flexibility in such networks brings with some serious problems such as overfitting, and regularization is used to address this problem. A currently popular and effective regularization technique for controlling the overfitting is dropout. Often, large data collections required for neural networks contain sensitive information such as the medical histories of patients, and the privacy of the training data should be protected. In this paper, we modify the recently proposed variational dropout technique which provided an elegant Bayesian interpretation to dropout, and show that the intrinsic noise in the variational dropout can be exploited to obtain a degree of differential privacy. The iterative nature of training neural networks presents a challenge for privacy-preserving estimation since multiple iterations increase the amount of noise added. We overcome this by using a relaxed notion of differential privacy, called concentrated differential privacy, which provides tighter estimates on the overall privacy loss. We demonstrate the accuracy of our privacy-preserving variational dropout algorithm on benchmark datasets.	1,0,0,1,0,0
Reducing variance in importance-weighted cross-validation under covariate shift	Covariate shift classification problems can in principle be tackled by importance-weighting of training samples. However, the sampling variance of the risk estimator is often scaled up dramatically by employing such weighting. One of the consequences of this is that during cross-validation -- when the importance-weighted risk is repeatedly estimated -- suboptimal hyperparameter estimates are produced. We study the sampling variance of the importance-weighted risk estimator as a function of the width of the source distribution. We show that introducing a control variate can reduce its sampling variance, which leads to improved regularization parameter estimates when the training data is smaller in scale than the test data.	1,0,0,1,0,0
Judicious partitions of uniform hypergraphs	The vertices of any graph with $m$ edges may be partitioned into two parts so that each part meets at least $\frac{2m}{3}$ edges. Bollobás and Thomason conjectured that the vertices of any $r$-uniform hypergraph with $m$ edges may likewise be partitioned into $r$ classes such that each part meets at least $\frac{r}{2r-1}m$ edges. In this paper we prove the weaker statement that, for each $r\ge 4$, a partition into $r$ classes may be found in which each class meets at least $\frac{r}{3r-4}m$ edges, a substantial improvement on previous bounds.	0,0,1,0,0,0
Table Space Designs For Implicit and Explicit Concurrent Tabled Evaluation	One of the main advantages of Prolog is its potential for the implicit exploitation of parallelism and, as a high-level language, Prolog is also often used as a means to explicitly control concurrent tasks. Tabling is a powerful implementation technique that overcomes some limitations of traditional Prolog systems in dealing with recursion and redundant sub-computations. Given these advantages, the question that arises is if tabling has also the potential for the exploitation of concurrency/parallelism. On one hand, tabling still exploits a search space as traditional Prolog but, on the other hand, the concurrent model of tabling is necessarily far more complex since it also introduces concurrency on the access to the tables. In this paper, we summarize Yap's main contributions to concurrent tabled evaluation and we describe the design and implementation challenges of several alternative table space designs for implicit and explicit concurrent tabled evaluation which represent different trade-offs between concurrency and memory usage. We also motivate for the advantages of using fixed-size and lock-free data structures, elaborate on the key role that the engine's memory allocator plays on such environments, and discuss how Yap's mode-directed tabling support can be extended to concurrent evaluation. Finally, we present our future perspectives towards an efficient and novel concurrent framework which integrates both implicit and explicit concurrent tabled evaluation in a single Prolog engine. Under consideration in Theory and Practice of Logic Programming (TPLP).	1,0,0,0,0,0
Space Telescope and Optical Reverberation Mapping Project. VII. Understanding the UV anomaly in NGC 5548 with X-Ray Spectroscopy	During the Space Telescope and Optical Reverberation Mapping Project (STORM) observations of NGC 5548, the continuum and emission-line variability became de-correlated during the second half of the 6-month long observing campaign. Here we present Swift and Chandra X-ray spectra of NGC 5548 obtained as a part of the campaign. The Swift spectra show that excess flux (relative to a power-law continuum) in the soft X-ray band appears before the start of the anomalous emission-line behavior, peaks during the period of the anomaly, and then declines. This is a model-independent result suggesting that the soft excess is related to the anomaly. We divide the Swift data into on- and off-anomaly spectra to characterize the soft excess via spectral fitting. The cause of the spectral differences is likely due to a change in the intrinsic spectrum rather than being due to variable obscuration or partial covering. The Chandra spectra have lower signal-to-noise ratios, but are consistent with Swift data. Our preferred model of the soft excess is emission from an optically thick, warm Comptonizing corona, the effective optical depth of which increases during the anomaly. This model simultaneously explains all the three observations: the UV emission line flux decrease, the soft-excess increase, and the emission line anomaly.	0,1,0,0,0,0
A three-dimensional symmetry result for a phase transition equation in the genuinely nonlocal regime	We consider bounded solutions of the nonlocal Allen-Cahn equation $$ (-\Delta)^s u=u-u^3\qquad{\mbox{ in }}{\mathbb{R}}^3,$$ under the monotonicity condition $\partial_{x_3}u>0$ and in the genuinely nonlocal regime in which~$s\in\left(0,\frac12\right)$. Under the limit assumptions $$ \lim_{x_n\to-\infty} u(x',x_n)=-1\quad{\mbox{ and }}\quad \lim_{x_n\to+\infty} u(x',x_n)=1,$$ it has been recently shown that~$u$ is necessarily $1$D, i.e. it depends only on one Euclidean variable. The goal of this paper is to obtain a similar result without assuming such limit conditions. This type of results can be seen as nonlocal counterparts of the celebrated conjecture formulated by Ennio De Giorgi.	0,0,1,0,0,0
Underapproximation of Reach-Avoid Sets for Discrete-Time Stochastic Systems via Lagrangian Methods	We examine Lagrangian techniques for computing underapproximations of finite-time horizon, stochastic reach-avoid level-sets for discrete-time, nonlinear systems. We use the concept of reachability of a target tube in the control literature to define robust reach-avoid sets which are parameterized by the target set, safe set, and the set in which the disturbance is drawn from. We unify two existing Lagrangian approaches to compute these sets and establish that there exists an optimal control policy of the robust reach-avoid sets which is a Markov policy. Based on these results, we characterize the subset of the disturbance space whose corresponding robust reach-avoid set for the given target and safe set is a guaranteed underapproximation of the stochastic reach-avoid level-set of interest. The proposed approach dramatically improves the computational efficiency for obtaining an underapproximation of stochastic reach-avoid level-sets when compared to the traditional approaches based on gridding. Our method, while conservative, does not rely on a grid, implying scalability as permitted by the known computational geometry constraints. We demonstrate the method on two examples: a simple two-dimensional integrator, and a space vehicle rendezvous-docking problem.	1,0,1,0,0,0
Self-Assembled Monolayer Piezoelectrics: Electric-Field Driven Conformational Changes	We demonstrate that an applied electric field causes piezoelectric distortion across single molecular monolayers of oligopeptides. We deposited self-assembled monolayers ~1.5 nm high onto smooth gold surfaces. These monolayers exhibit strong piezoelectric response that varies linearly with applied bias (1-3V), measured using piezoresponse force microscopy (PFM). The response is markedly greater than control experiments with rigid alkanethiols and correlates with surface spectroscopy and theoretical predictions of conformational change from applied electric fields. Unlike existing piezoelectric oxides, our peptide monolayers are intrinsically flexible, easily fabricated, aligned and patterned without poling.	0,1,0,0,0,0
Attractive Heaviside-Maxwellian (Vector) Gravity from Special Relativity and Quantum Field Theory	Adopting two independent approaches (a) Lorentz-invariance of physical laws and (b) local phase invariance of quantum field theory applied to the Dirac Lagrangian for massive electrically neutral Dirac particles, we rediscovered the fundamental field equations of Heaviside Gravity (HG) of 1893 and Maxwellian Gravity (MG), which look different from each other due to a sign difference in some terms of their respective field equations. However, they are shown to represent two mathematical representations of a single physical theory of vector gravity that we name here as Heaviside-Maxwellian Gravity (HMG), in which the speed of gravitational waves in vacuum is uniquely found to be equal to the speed of light in vacuum. We also corrected a sign error in Heaviside's speculative gravitational analogue of the Lorentz force law. This spin-1 HMG is shown to produce attractive force between like masses under static condition, contrary to the prevalent view of field theorists. Galileo's law of universality of free fall is a consequence of HMG, without any initial assumption of the equality of gravitational mass with velocity-dependent mass. We also note a new set of Lorentz-Maxwell's equations having the same physical effects as the standard set - a byproduct of our present study.	0,1,0,0,0,0
Well-balanced mesh-based and meshless schemes for the shallow-water equations	We formulate a general criterion for the exact preservation of the "lake at rest" solution in general mesh-based and meshless numerical schemes for the strong form of the shallow-water equations with bottom topography. The main idea is a careful mimetic design for the spatial derivative operators in the momentum flux equation that is paired with a compatible averaging rule for the water column height arising in the bottom topography source term. We prove consistency of the mimetic difference operators analytically and demonstrate the well-balanced property numerically using finite difference and RBF-FD schemes in the one- and two-dimensional cases.	0,1,1,0,0,0
Goldstone-like phonon modes in a (111)-strained perovskite	Goldstone modes are massless particles resulting from spontaneous symmetry breaking. Although such modes are found in elementary particle physics as well as in condensed matter systems like superfluid helium, superconductors and magnons - structural Goldstone modes are rare. Epitaxial strain in thin films can induce structures and properties not accessible in bulk and has been intensively studied for (001)-oriented perovskite oxides. Here we predict Goldstone-like phonon modes in (111)-strained SrMnO3 by first-principles calculations. Under compressive strain the coupling between two in-plane rotational instabilities give rise to a Mexican hat shaped energy surface characteristic of a Goldstone mode. Conversely, large tensile strain induces in-plane polar instabilities with no directional preference, giving rise to a continuous polar ground state. Such phonon modes with U(1) symmetry could emulate structural condensed matter Higgs modes. The mass of this Higgs boson, given by the shape of the Mexican hat energy surface, can be tuned by strain through proper choice of substrate.	0,1,0,0,0,0
Complementary views on electron spectra: From Fluctuation Diagnostics to real space correlations	We study the relation between the microscopic properties of a many-body system and the electron spectra, experimentally accessible by photoemission. In a recent paper [Phys. Rev. Lett. 114, 236402 (2015)], we introduced the "fluctuation diagnostics" approach, to extract the dominant wave vector dependent bosonic fluctuations from the electronic self-energy. Here, we first reformulate the theory in terms of fermionic modes, to render its connection with resonance valence bond (RVB) fluctuations more transparent. Secondly, by using a large-U expansion, where U is the Coulomb interaction, we relate the fluctuations to real space correlations. Therefore, it becomes possible to study how electron spectra are related to charge, spin, superconductivity and RVB-like real space correlations, broadening the analysis of an earlier work [Phys. Rev. B 89, 245130 (2014)]. This formalism is applied to the pseudogap physics of the two-dimensional Hubbard model, studied in the dynamical cluster approximation. We perform calculations for embedded clusters with up to 32 sites, having three inequivalent K-points at the Fermi surface. We find that as U is increased, correlation functions gradually attain values consistent with an RVB state. This first happens for correlation functions involving the antinodal point and gradually spreads to the nodal point along the Fermi surface. Simultaneously a pseudogap opens up along the Fermi surface. We relate this to a crossover from a Kondo-like state to an RVB-like localized cluster state and to the presence of RVB and spin fluctuations. These changes are caused by a strong momentum dependence in the cluster bath-couplings along the Fermi surface. We also show, from a more algorithmic perspective, how the time-consuming calculations in fluctuation diagnostics can be drastically simplified.	0,1,0,0,0,0
Influence of parameterized small-scale gravity waves on the migrating diurnal tide in Earth's thermosphere	Effects of subgrid-scale gravity waves (GWs) on the diurnal migrating tides are investigated from the mesosphere to the upper thermosphere for September equinox conditions, using a general circulation model coupled with the extended spectral nonlinear GW parameterization of Yiğit et al (2008). Simulations with GW effects cut-off above the turbopause and included in the entire thermosphere have been conducted. GWs appreciably impact the mean circulation and cool the thermosphere down by up to 12-18%. GWs significantly affect the winds modulated by the diurnal migrating tide, in particular in the low-latitude mesosphere and lower thermosphere and in the high-latitude thermosphere. These effects depend on the mutual correlation of the diurnal phases of the GW forcing and tides: GWs can either enhance or reduce the tidal amplitude. In the low-latitude MLT, the correlation between the direction of the deposited GW momentum and the tidal phase is positive due to propagation of a broad spectrum of GW harmonics through the alternating winds. In the Northern Hemisphere high-latitude thermosphere, GWs act against the tide due to an anti-correlation of tidal wind and GW momentum, while in the Southern high-latitudes they weakly enhance the tidal amplitude via a combination of a partial correlation of phases and GW-induced changes of the circulation. The variable nature of GW effects on the thermal tide can be captured in GCMs provided that a GW parameterization (1) considers a broad spectrum of harmonics, (2) properly describes their propagation, and (3) correctly accounts for the physics of wave breaking/saturation.	0,1,0,0,0,0
Minimax Optimal Estimators for Additive Scalar Functionals of Discrete Distributions	In this paper, we consider estimators for an additive functional of $\phi$, which is defined as $\theta(P;\phi)=\sum_{i=1}^k\phi(p_i)$, from $n$ i.i.d. random samples drawn from a discrete distribution $P=(p_1,...,p_k)$ with alphabet size $k$. We propose a minimax optimal estimator for the estimation problem of the additive functional. We reveal that the minimax optimal rate is characterized by the divergence speed of the fourth derivative of $\phi$ if the divergence speed is high. As a result, we show there is no consistent estimator if the divergence speed of the fourth derivative of $\phi$ is larger than $p^{-4}$. Furthermore, if the divergence speed of the fourth derivative of $\phi$ is $p^{4-\alpha}$ for $\alpha \in (0,1)$, the minimax optimal rate is obtained within a universal multiplicative constant as $\frac{k^2}{(n\ln n)^{2\alpha}} + \frac{k^{2-2\alpha}}{n}$.	1,0,1,1,0,0
Control of automated guided vehicles without collision by quantum annealer and digital devices	We formulate an optimization problem to control a large number of automated guided vehicles in a plant without collision. The formulation consists of binary variables. A quadratic cost function over these variables enables us to utilize certain solvers on digital computers and recently developed purpose-specific hardware such as D-Wave 2000Q and the Fujitsu digital annealer. In the present study, we consider an actual plant in Japan, in which vehicles run, and assess efficiency of our formulation for optimizing the vehicles via several solvers. We confirm that our formulation can be a powerful approach for performing smooth control while avoiding collisions between vehicles, as compared to a conventional method. In addition, comparative experiments performed using several solvers reveal that D-Wave 2000Q can be useful as a rapid solver for generating a plan for controlling the vehicles in a short time although it deals only with a small number of vehicles, while a digital computer can rapidly solve the corresponding optimization problem even with a large number of binary variables.	1,0,0,0,0,0
On wrapping the Kalman filter and estimating with the SO(2) group	This paper analyzes directional tracking in 2D with the extended Kalman filter on Lie groups (LG-EKF). The study stems from the problem of tracking objects moving in 2D Euclidean space, with the observer measuring direction only, thus rendering the measurement space and object position on the circle---a non-Euclidean geometry. The problem is further inconvenienced if we need to include higher-order dynamics in the state space, like angular velocity which is a Euclidean variables. The LG-EKF offers a solution to this issue by modeling the state space as a Lie group or combination thereof, e.g., SO(2) or its combinations with Rn. In the present paper, we first derive the LG-EKF on SO(2) and subsequently show that this derivation, based on the mathematically grounded framework of filtering on Lie groups, yields the same result as heuristically wrapping the angular variable within the EKF framework. This result applies only to the SO(2) and SO(2)xRn LG-EKFs and is not intended to be extended to other Lie groups or combinations thereof. In the end, we showcase the SO(2)xR2 LG-EKF, as an example of a constant angular acceleration model, on the problem of speaker tracking with a microphone array for which real-world experiments are conducted and accuracy is evaluated with ground truth data obtained by a motion capture system.	1,0,0,0,0,0
Employing both Gender and Emotion Cues to Enhance Speaker Identification Performance in Emotional Talking Environments	Speaker recognition performance in emotional talking environments is not as high as it is in neutral talking environments. This work focuses on proposing, implementing, and evaluating a new approach to enhance the performance in emotional talking environments. The new proposed approach is based on identifying the unknown speaker using both his/her gender and emotion cues. Both Hidden Markov Models (HMMs) and Suprasegmental Hidden Markov Models (SPHMMs) have been used as classifiers in this work. This approach has been tested on our collected emotional speech database which is composed of six emotions. The results of this work show that speaker identification performance based on using both gender and emotion cues is higher than that based on using gender cues only, emotion cues only, and neither gender nor emotion cues by 7.22%, 4.45%, and 19.56%, respectively. This work also shows that the optimum speaker identification performance takes place when the classifiers are completely biased towards suprasegmental models and no impact of acoustic models in the emotional talking environments. The achieved average speaker identification performance based on the new proposed approach falls within 2.35% of that obtained in subjective evaluation by human judges.	1,0,0,0,0,0
A powerful approach to the study of moderate effect modification in observational studies	Effect modification means the magnitude or stability of a treatment effect varies as a function of an observed covariate. Generally, larger and more stable treatment effects are insensitive to larger biases from unmeasured covariates, so a causal conclusion may be considerably firmer if this pattern is noted if it occurs. We propose a new strategy, called the submax-method, that combines exploratory and confirmatory efforts to determine whether there is stronger evidence of causality - that is, greater insensitivity to unmeasured confounding - in some subgroups of individuals. It uses the joint distribution of test statistics that split the data in various ways based on certain observed covariates. For $L$ binary covariates, the method splits the population $L$ times into two subpopulations, perhaps first men and women, perhaps then smokers and nonsmokers, computing a test statistic from each subpopulation, and appends the test statistic for the whole population, making $2L+1$ test statistics in total. Although $L$ binary covariates define $2^{L}$ interaction groups, only $2L+1$ tests are performed, and at least $L+1$ of these tests use at least half of the data. The submax-method achieves the highest design sensitivity and the highest Bahadur efficiency of its component tests. Moreover, the form of the test is sufficiently tractable that its large sample power may be studied analytically. The simulation suggests that the submax method exhibits superior performance, in comparison with an approach using CART, when there is effect modification of moderate size. Using data from the NHANES I Epidemiologic Follow-Up Survey, an observational study of the effects of physical activity on survival is used to illustrate the method. The method is implemented in the $\texttt{R}$ package $\texttt{submax}$ which contains the NHANES example.	0,0,0,1,0,0
Text-Independent Speaker Verification Using 3D Convolutional Neural Networks	In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN) architecture has been proposed for speaker verification in the text-independent setting. One of the main challenges is the creation of the speaker models. Most of the previously-reported approaches create speaker models based on averaging the extracted features from utterances of the speaker, which is known as the d-vector system. In our paper, we propose an adaptive feature learning by utilizing the 3D-CNNs for direct speaker model creation in which, for both development and enrollment phases, an identical number of spoken utterances per speaker is fed to the network for representing the speakers' utterances and creation of the speaker model. This leads to simultaneously capturing the speaker-related information and building a more robust system to cope with within-speaker variation. We demonstrate that the proposed method significantly outperforms the traditional d-vector verification system. Moreover, the proposed system can also be an alternative to the traditional d-vector system which is a one-shot speaker modeling system by utilizing 3D-CNNs.	1,0,0,0,0,0
Aggregating incoherent agents who disagree	In this paper, we explore how we should aggregate the degrees of belief of of a group of agents to give a single coherent set of degrees of belief, when at least some of those agents might be probabilistically incoherent. There are a number of way of aggregating degrees of belief, and there are a number of ways of fixing incoherent degrees of belief. When we have picked one of each, should we aggregate first and then fix, or fix first and then aggregate? Or should we try to do both at once? And when do these different procedures agree with one another? In this paper, we focus particularly on the final question.	1,0,0,1,0,0
The Origin of Solar Filament Plasma Inferred from in situ Observations of Elemental Abundances	Solar filaments/prominences are one of the most common features in the corona, which may lead to energetic coronal mass ejections (CMEs) and flares when they erupt. Filaments are about one hundred times cooler and denser than the coronal material, and physical understanding of their material origin remains controversial. Two types of scenarios have been proposed: one argues that the filament plasma is brought into the corona from photosphere or chromosphere through a siphon or evaporation/injection process, while the other suggests that the material condenses from the surrounding coronal plasma due to thermal instability. The elemental abundance analysis is a reasonable clue to constrain the models, as the siphon or evaporation/injection model would predict that the filament material abundances are close to the photospheric or chromospheric ones, while the condensation model should have coronal abundances. In this letter, we analyze the elemental abundances of a magnetic cloud that contains the ejected filament material. The corresponding filament eruption occurred on 1998 April 29, accompanying an M6.8 class soft X-ray flare located at the heliographic coordinates S18E20 (NOAA 08210) and a fast halo CME with the linear velocity of 1374 km s$^{-1}$ near the Sun. We find that the abundance ratios of elements with low and high First Ionization Potential such as Fe/O, Mg/O, and Si/O are 0.150, 0.050, and 0.070, respectively, approaching their corresponding photospheric values 0.065, 0.081, and 0.066, which does not support the coronal origin of the filament plasma.	0,1,0,0,0,0
Scaling of the Detonation Product State with Reactant Kinetic Energy	This submissions has been withdrawn by arXiv administrators because the submitter did not have the right to agree to our license.	0,1,0,0,0,0
Estimation of Component Reliability in Coherent Systems	The first step in statistical reliability studies of coherent systems is the estimation of the reliability of each system component. For the cases of parallel and series systems the literature is abundant. It seems that the present paper is the first that presents the general case of component inferences in coherent systems. The failure time model considered here is the three-parameter Weibull distribution. Furthermore, neither independence nor identically distributed failure times are required restrictions. The proposed model is general in the sense that it can be used for any coherent system, from the simplest to the more complex structures. It can be considered for all kinds of censored data; including interval-censored data. An important property obtained for the Weibull model is the fact that the posterior distributions are proper, even for non-informative priors. Using several simulations, the excellent performance of the model is illustrated. As a real example, boys first use of marijuana is considered to show the efficiency of the solution even when censored data occurs.	0,0,0,1,0,0
Exploiting Friction in Torque Controlled Humanoid Robots	A common architecture for torque controlled humanoid robots consists in two nested loops. The outer loop generates desired joint/motor torques, and the inner loop stabilises these desired values. In doing so, the inner loop usually compensates for joint friction phenomena, thus removing their inherent stabilising property that may be also beneficial for high level control objectives. This paper shows how to exploit friction for joint and task space control of humanoid robots. Experiments are carried out using the humanoid robot iCub.	1,0,0,0,0,0
A Unified Neural Network Approach for Estimating Travel Time and Distance for a Taxi Trip	In building intelligent transportation systems such as taxi or rideshare services, accurate prediction of travel time and distance is crucial for customer experience and resource management. Using the NYC taxi dataset, which contains taxi trips data collected from GPS-enabled taxis [23], this paper investigates the use of deep neural networks to jointly predict taxi trip time and distance. We propose a model, called ST-NN (Spatio-Temporal Neural Network), which first predicts the travel distance between an origin and a destination GPS coordinate, then combines this prediction with the time of day to predict the travel time. The beauty of ST-NN is that it uses only the raw trips data without requiring further feature engineering and provides a joint estimate of travel time and distance. We compare the performance of ST-NN to that of state-of-the-art travel time estimation methods, and we observe that the proposed approach generalizes better than state-of-the-art methods. We show that ST-NN approach significantly reduces the mean absolute error for both predicted travel time and distance, about 17% for travel time prediction. We also observe that the proposed approach is more robust to outliers present in the dataset by testing the performance of ST-NN on the datasets with and without outliers.	1,0,0,1,0,0
Simplex Queues for Hot-Data Download	In cloud storage systems, hot data is usually replicated over multiple nodes in order to accommodate simultaneous access by multiple users as well as increase the fault tolerance of the system. Recent cloud storage research has proposed using availability codes, which is a special class of erasure codes, as a more storage efficient way to store hot data. These codes enable data recovery from multiple, small disjoint groups of servers. The number of the recovery groups is referred to as the availability and the size of each group as the locality of the code. Until now, we have very limited knowledge on how code locality and availability affect data access time. Data download from these systems involves multiple fork-join queues operating in-parallel, making the analysis of access time a very challenging problem. In this paper, we present an approximate analysis of data access time in storage systems that employ simplex codes, which are an important and in certain sense optimal class of availability codes. We consider and compare three strategies in assigning download requests to servers; first one aggressively exploits the storage availability for faster download, second one implements only load balancing, and the last one employs storage availability only for hot data download without incurring any negative impact on the cold data download.	1,0,0,0,0,0
Determining rough first order perturbations of the polyharmonic operator	We show that the knowledge of Dirichlet to Neumann map for rough $A$ and $q$ in $(-\Delta)^m +A\cdot D +q$ for $m \geq 2$ for a bounded domain in $\mathbb{R}^n$, $n \geq 3$ determines $A$ and $q$ uniquely. The unique identifiability is proved using property of products of functions in Sobolev spaces and constructing complex geometrical optics solutions with sufficient decay of remainder terms.	0,0,1,0,0,0
An algorithm to reconstruct convex polyhedra from their face normals and areas	A well-known result in the study of convex polyhedra, due to Minkowski, is that a convex polyhedron is uniquely determined (up to translation) by the directions and areas of its faces. The theorem guarantees existence of the polyhedron associated to given face normals and areas, but does not provide a constructive way to find it explicitly. This article provides an algorithm to reconstruct 3D convex polyhedra from their face normals and areas, based on an method by Lasserre to compute the volume of a convex polyhedron in $\mathbb{R}^n$. A Python implementation of the algorithm is available at this https URL.	0,1,0,0,0,0
Closure operators, frames, and neatest representations	Given a poset $P$ and a standard closure operator $\Gamma:\wp(P)\to\wp(P)$ we give a necessary and sufficient condition for the lattice of $\Gamma$-closed sets of $\wp(P)$ to be a frame in terms of the recursive construction of the $\Gamma$-closure of sets. We use this condition to show that given a set $\mathcal{U}$ of distinguished joins from $P$, the lattice of $\mathcal{U}$-ideals of $P$ fails to be a frame if and only if it fails to be $\sigma$-distributive, with $\sigma$ depending on the cardinalities of sets in $\mathcal{U}$. From this we deduce that if a poset has the property that whenever $a\wedge(b\vee c)$ is defined for $a,b,c\in P$ it is necessarily equal to $(a\wedge b)\vee (a\wedge c)$, then it has an $(\omega,3)$-representation. This answers a question from the literature.	0,0,1,0,0,0
Failure of Smooth Pasting Principle and Nonexistence of Equilibrium Stopping Rules under Time-Inconsistency	This paper considers a time-inconsistent stopping problem in which the inconsistency arises from non-constant time preference rates. We show that the smooth pasting principle, the main approach that has been used to construct explicit solutions for conventional time-consistent optimal stopping problems, may fail under time-inconsistency. Specifically, we prove that the smooth pasting principle solves a time-inconsistent problem within the intra-personal game theoretic framework if and only if a certain inequality on the model primitives is satisfied. We show that the violation of this inequality can happen even for very simple non-exponential discount functions. Moreover, we demonstrate that the stopping problem does not admit any intra-personal equilibrium whenever the smooth pasting principle fails. The "negative" results in this paper caution blindly extending the classical approaches for time-consistent stopping problems to their time-inconsistent counterparts.	0,0,0,0,0,1
A level set-based structural optimization code using FEniCS	This paper presents an educational code written using FEniCS, based on the level set method, to perform compliance minimization in structural optimization. We use the concept of distributed shape derivative to compute a descent direction for the compliance, which is defined as a shape functional. The use of the distributed shape derivative is facilitated by FEniCS, which allows to handle complicated partial differential equations with a simple implementation. The code is written for compliance minimization in the framework of linearized elasticity, and can be easily adapted to tackle other functionals and partial differential equations. We also provide an extension of the code for compliant mechanisms. We start by explaining how to compute shape derivatives, and discuss the differences between the distributed and boundary expressions of the shape derivative. Then we describe the implementation in details, and show the application of this code to some classical benchmarks of topology optimization. The code is available at this http URL, and the main file is also given in the appendix.	0,0,1,0,0,0
A New Classification of Technologies	This study here suggests a classification of technologies based on taxonomic characteristics of interaction between technologies in complex systems that is not a studied research field in economics of technical change. The proposed taxonomy here categorizes technologies in four typologies, in a broad analogy with the ecology: 1) technological parasitism is a relationship between two technologies T1 and T2 in a complex system S where one technology T1 benefits from the interaction with T2, whereas T2 has a negative side from interaction with T1; 2) technological commensalism is a relationship between two technologies in S where one technology benefits from the other without affecting it; 3) technological mutualism is a relationship in which each technology benefits from the activity of the other within complex systems; 4) technological symbiosis is a long-term interaction between two (or more) technologies that evolve together in complex systems. This taxonomy systematizes the typologies of interactive technologies within complex systems and predicts their evolutionary pathways that generate stepwise coevolutionary processes of complex systems of technology. This study here begins the process of generalizing, as far as possible, critical typologies of interactive technologies that explain the long-run evolution of technology. The theoretical framework developed here opens the black box of the interaction between technologies that affects, with different types of technologies, the evolutionary pathways of complex systems of technology over time and space. Overall, then, this new theoretical framework may be useful for bringing a new perspective to categorize the gradient of benefit to technologies from interaction with other technologies that can be a ground work for development of more sophisticated concepts to clarify technological and economic change in human society.	1,0,0,0,0,0
Computation of Optimal Transport on Discrete Metric Measure Spaces	In this paper we investigate the numerical approximation of an analogue of the Wasserstein distance for optimal transport on graphs that is defined via a discrete modification of the Benamou--Brenier formula. This approach involves the logarithmic mean of measure densities on adjacent nodes of the graph. For this model a variational time discretization of the probability densities on graph nodes and the momenta on graph edges is proposed. A robust descent algorithm for the action functional is derived, which in particular uses a proximal splitting with an edgewise nonlinear projection on the convex subgraph of the logarithmic mean. Thereby, suitable chosen slack variables avoid a global coupling of probability densities on all graph nodes in the projection step. For the time discrete action functional $\Gamma$--convergence to the time continuous action is established. Numerical results for a selection of test cases show qualitative and quantitative properties of the optimal transport on graphs. Finally, we use our algorithm to implement a JKO scheme for the gradient flow of the entropy in the discrete transportation distance, which is known to coincide with the underlying Markov semigroup, and test our results against a classical backward Euler discretization of this discrete heat flow.	0,0,1,0,0,0
Electronic and atomic kinetics in solids irradiated with free-electron lasers or swift-heavy ions	In this brief review we discuss the transient processes in solids under irradiation with femtosecond X-ray free-electron-laser (FEL) pulses and swift-heavy ions (SHI). Both kinds of irradiation produce highly excited electrons in a target on extremely short timescales. Transfer of the excess electronic energy into the lattice may lead to observable target modifications such as phase transitions and damage formation. Transient kinetics of material excitation and relaxation under FEL or SHI irradiation are comparatively discussed. The same origin for the electronic and atomic relaxation in both cases is demonstrated. Differences in these kinetics introduced by the geometrical effects ({\mu}m-size of a laser spot vs nm-size of an ion track) and initial irradiation (photoabsorption vs an ion impact) are analyzed. The basic mechanisms of electron transport and electron-lattice coupling are addressed. Appropriate models and their limitations are presented. Possibilities of thermal and nonthermal melting of materials under FEL and SHI irradiation are discussed.	0,1,0,0,0,0
Saxion Cosmology for Thermalized Gravitino Dark Matter	In all supersymmetric theories, gravitinos, with mass suppressed by the Planck scale, are an obvious candidate for dark matter; but if gravitinos ever reached thermal equilibrium, such dark matter is apparently either too abundant or too hot, and is excluded. However, in theories with an axion, a saxion condensate is generated during an early era of cosmological history and its late decay dilutes dark matter. We show that such dilution allows previously thermalized gravitinos to account for the observed dark matter over very wide ranges of gravitino mass, keV < $m_{3/2}$ < TeV, axion decay constant, $10^9$ GeV < $f_a$ < $10^{16}$ GeV, and saxion mass, 10 MeV < $m_s$ < 100 TeV. Constraints on this parameter space are studied from BBN, supersymmetry breaking, gravitino and axino production from freeze-in and saxion decay, and from axion production from both misalignment and parametric resonance mechanisms. Large allowed regions of $(m_{3/2}, f_a, m_s)$ remain, but differ for DFSZ and KSVZ theories. Superpartner production at colliders may lead to events with displaced vertices and kinks, and may contain saxions decaying to $(WW,ZZ,hh), gg, \gamma \gamma$ or a pair of Standard Model fermions. Freeze-in may lead to a sub-dominant warm component of gravitino dark matter, and saxion decay to axions may lead to dark radiation.	0,1,0,0,0,0
Real time observation of granular rock analogue material deformation and failure using nonlinear laser interferometry	A better understanding and anticipation of natural processes such as landsliding or seismic fault activity requires detailed theoretical and experimental analysis of rock mechanics and geomaterial dynamics. These last decades, considerable progress has been made towards understanding deformation and fracture process in laboratory experiment on granular rock materials, as the well-known shear banding experiment. One of the reasons for this progress is the continuous improvement in the instrumental techniques of observation. But the lack of real time methods does not allow the detection of indicators of the upcoming fracture process and thus to anticipate the phenomenon. Here, we have performed uniaxial compression experiments to analyse the response of a granular rock material sample to different shocks. We use a novel interferometric laser sensor based on the nonlinear self-mixing interferometry technique to observe in real time the deformations of the sample and assess its usefulness as a diagnostic tool for the analysis of geomaterial dynamics. Due to the high spatial and temporal resolution of this approach, we observe both vibrations processes in response to a dynamic loading and the onset of failure. The latter is preceded by a continuous variation of vibration period of the material. After several shocks, the material response is no longer reversible and we detect a progressive accumulation of irreversible deformation leading to the fracture process. We demonstrate that material failure is anticipated by the critical slowing down of the surface vibrational motion, which may therefore be envisioned as an early warning signal or predictor to the macroscopic failure of the sample. The nonlinear self-mixing interferometry technique is readily extensible to fault propagation measurements. As such, it opens a new window of observation for the study of geomaterial deformation and failure.	0,1,0,0,0,0
A simple script language for choreography of multiple, synchronizing non-anthropomorphic robots	The scripting language described in this document is (in the first place) intended to be used on robots developed by Anja M{\o}lle Lindelof and Henning Christiansen as part of a research project about robots performing on stage. The target robots are expected to appear as familiar domestic objects that take their own life, so to speak, and perhaps perform together with human players, creating at illusion of a communication between them. In the current version, these robots' common behaviour is determined uniquely by a script written in the language described here -- the only possible autonomy for the robots is action to correct dynamically for inaccuracies that arise during a performance. The present work is preliminary and has not been compared to properly to other research work in this area, and the testing is still limited. A first implementation on small Lego Mindstorms based robots is under development by Mads Saustrup Fox as part of his master thesis work.	1,0,0,0,0,0
Hausdorff dimension, projections, intersections, and Besicovitch sets	This is a survey on recent developments on the Hausdorff dimension of projections and intersections for general subsets of Euclidean spaces, with an emphasis on estimates of the Hausdorff dimension of exceptional sets and on restricted projection families. We shall also discuss relations between projections and Hausdorff dimension of Besicovitch sets.	0,0,1,0,0,0
Tuning quantum non-local effects in graphene plasmonics	The response of an electron system to electromagnetic fields with sharp spatial variations is strongly dependent on quantum electronic properties, even in ambient conditions, but difficult to access experimentally. We use propagating graphene plasmons, together with an engineered dielectric-metallic environment, to probe the graphene electron liquid and unveil its detailed electronic response at short wavelengths.The near-field imaging experiments reveal a parameter-free match with the full theoretical quantum description of the massless Dirac electron gas, in which we identify three types of quantum effects as keys to understanding the experimental response of graphene to short-ranged terahertz electric fields. The first type is of single-particle nature and is related to shape deformations of the Fermi surface during a plasmon oscillations. The second and third types are a many-body effect controlled by the inertia and compressibility of the interacting electron liquid in graphene. We demonstrate how, in principle, our experimental approach can determine the full spatiotemporal response of an electron system.	0,1,0,0,0,0
Weighted integral Hankel operators with continuous spectrum	Using the Kato-Rosenblum theorem, we describe the absolutely continuous spectrum of a class of weighted integral Hankel operators in $L^2(\mathbb R_+)$. These self-adjoint operators generalise the explicitly diagonalisable operator with the integral kernel $s^\alpha t^\alpha(s+t)^{-1-2\alpha}$, where $\alpha>-1/2$. Our analysis can be considered as an extension of J.Howland's 1992 paper which dealt with the unweighted case, corresponding to $\alpha=0$.	0,0,1,0,0,0
Deep Learning as a Mixed Convex-Combinatorial Optimization Problem	As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn networks of them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.	1,0,0,0,0,0
SCAV'18: Report of the 2nd International Workshop on Safe Control of Autonomous Vehicles	This report summarizes the discussions, open issues, take-away messages, and conclusions of the 2nd SCAV workshop.	1,0,0,0,0,0
SGDLibrary: A MATLAB library for stochastic gradient descent algorithms	We consider the problem of finding the minimizer of a function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ of the finite-sum form $\min f(w) = 1/n\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent years in the field of machine learning (ML). One promising approach for large-scale data is to use a stochastic optimization algorithm to solve the problem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library of a collection of stochastic optimization algorithms. The purpose of the library is to provide researchers and implementers a comprehensive evaluation environment for the use of these algorithms on various ML problems.	1,0,0,1,0,0
Medoids in almost linear time via multi-armed bandits	Computing the medoid of a large number of points in high-dimensional space is an increasingly common operation in many data science problems. We present an algorithm Med-dit which uses O(n log n) distance evaluations to compute the medoid with high probability. Med-dit is based on a connection with the multi-armed bandit problem. We evaluate the performance of Med-dit empirically on the Netflix-prize and the single-cell RNA-Seq datasets, containing hundreds of thousands of points living in tens of thousands of dimensions, and observe a 5-10x improvement in performance over the current state of the art. Med-dit is available at this https URL	1,0,0,1,0,0
Particular type of gap in the spectrum of multiband superconductors	We show, that in contrast to the free electron model (standard BCS model), a particular gap in the spectrum of multiband superconductors opens at some distance from the Fermi energy, if conduction band is composed of hybridized atomic orbitals of different symmetries. This gap has composite superconducting-hybridization origin, because it exists only if both the superconductivity and the hybridization between different kinds of orbitals are present. So for many classes of superconductors with multiorbital structure such spectrum changes should take place. These particular changes in the spectrum at some distance from the Fermi level result in slow convergence of the spectral weight of the optical conductivity even in quite conventional superconductors with isotropic s-wave pairing mechanism.	0,1,0,0,0,0
RFCDE: Random Forests for Conditional Density Estimation	Random forests is a common non-parametric regression technique which performs well for mixed-type data and irrelevant covariates, while being robust to monotonic variable transformations. Existing random forest implementations target regression or classification. We introduce the RFCDE package for fitting random forest models optimized for nonparametric conditional density estimation, including joint densities for multiple responses. This enables analysis of conditional probability distributions which is useful for propagating uncertainty and of joint distributions that describe relationships between multiple responses and covariates. RFCDE is released under the MIT open-source license and can be accessed at this https URL . Both R and Python versions, which call a common C++ library, are available.	0,0,0,1,0,0
Multitask Learning and Benchmarking with Clinical Time Series Data	Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.	1,0,0,1,0,0
Reliable counting of weakly labeled concepts by a single spiking neuron model	Making an informed, correct and quick decision can be life-saving. It's crucial for animals during an escape behaviour or for autonomous cars during driving. The decision can be complex and may involve an assessment of the amount of threats present and the nature of each threat. Thus, we should expect early sensory processing to supply classification information fast and accurately, even before relying the information to higher brain areas or more complex system components downstream. Today, advanced convolutional artificial neural networks can successfully solve visual detection and classification tasks and are commonly used to build complex decision making systems. However, in order to perform well on these tasks they require increasingly complex, "very deep" model structure, which is costly in inference run-time, energy consumption and number of training samples, only trainable on cloud-computing clusters. A single spiking neuron has been shown to be able to solve recognition tasks for homogeneous Poisson input statistics, a commonly used model for spiking activity in the neocortex. When modeled as leaky integrate and fire with gradient decent learning algorithm it was shown to posses a variety of complex computational capabilities. Here we improve its implementation. We also account for more natural stimulus generated inputs that deviate from this homogeneous Poisson spiking. The improved gradient-based local learning rule allows for significantly better and stable generalization. We also show that with its improved capabilities it can count weakly labeled concepts by applying our model to a problem of multiple instance learning (MIL) with counting where labels are only available for collections of concepts. In this counting MNIST task the neuron exploits the improved implementation and outperforms conventional ConvNet architecture under similar condtions.	0,0,0,0,1,0
Adaptive Network Coding Schemes for Satellite Communications	In this paper, we propose two novel physical layer aware adaptive network coding and coded modulation schemes for time variant channels. The proposed schemes have been applied to different satellite communications scenarios with different Round Trip Times (RTT). Compared to adaptive network coding, and classical non-adaptive network coding schemes for time variant channels, as benchmarks, the proposed schemes demonstrate that adaptation of packet transmission based on the channel variation and corresponding erasures allows for significant gains in terms of throughput, delay and energy efficiency. We shed light on the trade-off between energy efficiency and delay-throughput gains, demonstrating that conservative adaptive approaches that favors less transmission under high erasures, might cause higher delay and less throughput gains in comparison to non-conservative approaches that favor more transmission to account for high erasures.	1,0,0,0,0,0
Symplectic stability on manifolds with cylindrical ends	A famous result of Jurgen Moser states that a symplectic form on a compact manifold cannot be deformed within its cohomology class to an inequivalent symplectic form. It is well known that this does not hold in general for noncompact symplectic manifolds. The notion of Eliashberg-Gromov convex ends provides a natural restricted setting for the study of analogs of Moser's symplectic stability result in the noncompact case, and this has been significantly developed in work of Cieliebak-Eliashberg. Retaining the end structure on the underlying smooth manifold, but dropping the convexity and completeness assumptions on the symplectic forms at infinity we show that symplectic stability holds under a natural growth condition on the path of symplectic forms. The result can be straightforwardly applied as we show through explicit examples.	0,0,1,0,0,0
The Incremental Proximal Method: A Probabilistic Perspective	In this work, we highlight a connection between the incremental proximal method and stochastic filters. We begin by showing that the proximal operators coincide, and hence can be realized with, Bayes updates. We give the explicit form of the updates for the linear regression problem and show that there is a one-to-one correspondence between the proximal operator of the least-squares regression and the Bayes update when the prior and the likelihood are Gaussian. We then carry out this observation to a general sequential setting: We consider the incremental proximal method, which is an algorithm for large-scale optimization, and show that, for a linear-quadratic cost function, it can naturally be realized by the Kalman filter. We then discuss the implications of this idea for nonlinear optimization problems where proximal operators are in general not realizable. In such settings, we argue that the extended Kalman filter can provide a systematic way for the derivation of practical procedures.	0,0,0,1,0,0
Electrostatic interaction between non-identical charged particles at an electrolyte interface	In this thesis we study the lateral electrostatic interaction between a pair of non-identical, moderately charged colloidal particles trapped at an electrolyte interface in the limit of short inter-particle separations. Using a simplified model system we solve the problem analytically within the framework of linearised Poisson-Boltzmann theory and classical density functional theory. In the first step, we calculate the electrostatic potential inside the system exactly as well as within the widely used superposition approximation. Then these results are used to calculate the surface and line interaction energy densities between the particles. Contrary to the case of identical particles, depending upon the parameters of the system, we obtain that both the surface and the line interaction can vary non-monotonically with varying separation between the particles and the superposition approximation fails to predict the correct qualitative behaviours in most cases. Additionally, the superposition approximation is unable to predict the energy contributions quantitatively even at large distances. We also provide expression for the constant (independent of the inter-particle separation) interaction parameters, i.e., the surface tension, the line tension and the interfacial tension. Our results are expected to be of use for modelling particle-interaction at fluid interfaces and, in particular, for emulsion stabilization using oppositely charged particles.	0,1,0,0,0,0
Catalyst design using actively learned machine with non-ab initio input features towards CO2 reduction reactions	In conventional chemisorption model, the d-band center theory (augmented sometimes with the upper edge of d-band for imporved accuarcy) plays a central role in predicting adsorption energies and catalytic activity as a function of d-band center of the solid surfaces, but it requires density functional calculations that can be quite costly for large scale screening purposes of materials. In this work, we propose to use the d-band width of the muffin-tin orbital theory (to account for local coordination environment) plus electronegativity (to account for adsorbate renormalization) as a simple set of alternative descriptors for chemisorption, which do not demand the ab initio calculations. This pair of descriptors are then combined with machine learning methods, namely, artificial neural network (ANN) and kernel ridge regression (KRR), to allow large scale materials screenings. We show, for a toy set of 263 alloy systems, that the CO adsorption energy can be predicted with a remarkably small mean absolute deviation error of 0.05 eV, a significantly improved result as compared to 0.13 eV obtained with descriptors including costly d-band center calculations in literature. We achieved this high accuracy by utilizing an active learning algorithm, without which the accuracy was 0.18 eV otherwise. As a practical application of this machine, we identified Cu3Y@Cu as a highly active and cost-effective electrochemical CO2 reduction catalyst to produce CO with the overpotential 0.37 V lower than Au catalyst.	0,1,0,1,0,0
Virtual Constraints and Hybrid Zero Dynamics for Realizing Underactuated Bipedal Locomotion	Underactuation is ubiquitous in human locomotion and should be ubiquitous in bipedal robotic locomotion as well. This chapter presents a coherent theory for the design of feedback controllers that achieve stable walking gaits in underactuated bipedal robots. Two fundamental tools are introduced, virtual constraints and hybrid zero dynamics. Virtual constraints are relations on the state variables of a mechanical model that are imposed through a time-invariant feedback controller. One of their roles is to synchronize the robot's joints to an internal gait phasing variable. A second role is to induce a low dimensional system, the zero dynamics, that captures the underactuated aspects of a robot's model, without any approximations. To enhance intuition, the relation between physical constraints and virtual constraints is first established. From here, the hybrid zero dynamics of an underactuated bipedal model is developed, and its fundamental role in the design of asymptotically stable walking motions is established. The chapter includes numerous references to robots on which the highlighted techniques have been implemented.	1,0,1,0,0,0
A Hybrid MILP and IPM for Dynamic Economic Dispatch with Valve Point Effect	Dynamic economic dispatch with valve-point effect (DED-VPE) is a non-convex and non-differentiable optimization problem which is difficult to solve efficiently. In this paper, a hybrid mixed integer linear programming (MILP) and interior point method (IPM), denoted by MILP-IPM, is proposed to solve such a DED-VPE problem, where the complicated transmission loss is also included. Due to the non-differentiable characteristic of DED-VPE, the classical derivative-based optimization methods can not be used any more. With the help of model reformulation, a differentiable non-linear programming (NLP) formulation which can be directly solved by IPM is derived. However, if the DED-VPE is solved by IPM in a single step, the optimization will easily trap in a poor local optima due to its non-convex and multiple local minima characteristics. To exploit a better solution, an MILP method is required to solve the DED-VPE without transmission loss, yielding a good initial point for IPM to improve the quality of the solution. Simulation results demonstrate the validity and effectiveness of the proposed MILP-IPM in solving DED-VPE.	0,0,1,0,0,0
An initial-boundary value problem of the general three-component nonlinear Schrodinger equation with a 4x4 Lax pair on a finite interval	We investigate the initial-boundary value problem for the general three-component nonlinear Schrodinger (gtc-NLS) equation with a 4x4 Lax pair on a finite interval by extending the Fokas unified approach. The solutions of the gtc-NLS equation can be expressed in terms of the solutions of a 4x4 matrix Riemann-Hilbert (RH) problem formulated in the complex k-plane. Moreover, the relevant jump matrices of the RH problem can be explicitly found via the three spectral functions arising from the initial data, the Dirichlet-Neumann boundary data. The global relation is also established to deduce two distinct but equivalent types of representations (i.e., one by using the large k of asymptotics of the eigenfunctions and another one in terms of the Gelfand-Levitan-Marchenko (GLM) method) for the Dirichlet and Neumann boundary value problems. Moreover, the relevant formulae for boundary value problems on the finite interval can reduce to ones on the half-line as the length of the interval approaches to infinity. Finally, we also give the linearizable boundary conditions for the GLM representation.	0,1,1,0,0,0
EmbedInsight: Automated Grading of Embedded Systems Assignments	Grading in embedded systems courses typically requires a face-to-face appointment between the student and the instructor because of experimental setups that are only available in laboratory facilities. Such a manual grading process is an impediment to both students and instructors. Students have to wait for several days to get feedback, and instructors may spend valuable time evaluating trivial aspects of the assignment. As seen with software courses, an automated grading system can significantly improve the insights available to the instructor and encourage students to learn quickly with iterative testing. We have designed and implemented EmbedInsight, an automated grading system for embedded system courses that accommodates a wide variety of experimental setups and is scalable to MOOC-style courses. EmbedInsight employs a modular web services design that separates the user interface and the experimental setup that evaluates student assignments. We deployed and evaluated EmbedInsight for our university embedded systems course. We show that our system scales well to a large number of submissions, and students are satisfied with their overall experience.	1,0,0,0,0,0
Wasserstein Identity Testing	Uniformity testing and the more general identity testing are well studied problems in distributional property testing. Most previous work focuses on testing under $L_1$-distance. However, when the support is very large or even continuous, testing under $L_1$-distance may require a huge (even infinite) number of samples. Motivated by such issues, we consider the identity testing in Wasserstein distance (a.k.a. transportation distance and earthmover distance) on a metric space (discrete or continuous). In this paper, we propose the Wasserstein identity testing problem (Identity Testing in Wasserstein distance). We obtain nearly optimal worst-case sample complexity for the problem. Moreover, for a large class of probability distributions satisfying the so-called "Doubling Condition", we provide nearly instance-optimal sample complexity.	1,0,1,0,0,0
Econometric Modeling of Regional Electricity Spot Prices in the Australian Market	Wholesale electricity markets are increasingly integrated via high voltage interconnectors, and inter-regional trade in electricity is growing. To model this, we consider a spatial equilibrium model of price formation, where constraints on inter-regional flows result in three distinct equilibria in prices. We use this to motivate an econometric model for the distribution of observed electricity spot prices that captures many of their unique empirical characteristics. The econometric model features supply and inter-regional trade cost functions, which are estimated using Bayesian monotonic regression smoothing methodology. A copula multivariate time series model is employed to capture additional dependence -- both cross-sectional and serial-- in regional prices. The marginal distributions are nonparametric, with means given by the regression means. The model has the advantage of preserving the heavy right-hand tail in the predictive densities of price. We fit the model to half-hourly spot price data in the five interconnected regions of the Australian national electricity market. The fitted model is then used to measure how both supply and price shocks in one region are transmitted to the distribution of prices in all regions in subsequent periods. Finally, to validate our econometric model, we show that prices forecast using the proposed model compare favorably with those from some benchmark alternatives.	0,0,0,1,0,0
Toward Optimal Run Racing: Application to Deep Learning Calibration	This paper aims at one-shot learning of deep neural nets, where a highly parallel setting is considered to address the algorithm calibration problem - selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art with no extra hyper-parameter.	1,0,0,0,0,0
An Arcsine Law for Markov Random Walks	The classic arcsine law for the number $N_{n}^{>}:=n^{-1}\sum_{k=1}^{n}\mathbf{1}_{\{S_{k}>0\}}$ of positive terms, as $n\to\infty$, in an ordinary random walk $(S_{n})_{n\ge 0}$ is extended to the case when this random walk is governed by a positive recurrent Markov chain $(M_{n})_{n\ge 0}$ on a countable state space $\mathcal{S}$, that is, for a Markov random walk $(M_{n},S_{n})_{n\ge 0}$ with positive recurrent discrete driving chain. More precisely, it is shown that $n^{-1}N_{n}^{>}$ converges in distribution to a generalized arcsine law with parameter $\rho\in [0,1]$ (the classic arcsine law if $\rho=1/2$) iff the Spitzer condition $$ \lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^{n}\mathbb{P}_{i}(S_{n}>0)\ =\ \rho $$ holds true for some and then all $i\in\mathcal{S}$, where $\mathbb{P}_{i}:=\mathbb{P}(\cdot|M_{0}=i)$ for $i\in\mathcal{S}$. It is also proved, under an extra assumption on the driving chain if $0<\rho<1$, that this condition is equivalent to the stronger variant $$ \lim_{n\to\infty}\mathbb{P}_{i}(S_{n}>0)\ =\ \rho. $$ For an ordinary random walk, this was shown by Doney for $0<\rho<1$ and by Bertoin and Doney for $\rho\in\{0,1\}$.	0,0,1,0,0,0
Removal of Batch Effects using Generative Adversarial Networks	Many biological data analysis processes like Cytometry or Next Generation Sequencing (NGS) produce massive amounts of data which needs to be processed in batches for down-stream analysis. Such datasets are prone to technical variations due to difference in handling the batches possibly at different times, by different experimenters or under other different conditions. This adds variation to the batches coming from the same source sample. These variations are known as Batch Effects. It is possible that these variations and natural variations due to biology confound but such situations can be avoided by performing experiments in a carefully planned manner. Batch effects can hamper down-stream analysis and may also cause results to be inconclusive. Thus, it is essential to correct for these effects. Some recent methods propose deep learning based solution to solve this problem. We demonstrate that this can be solved using a novel Generative Adversarial Networks (GANs) based framework. The advantage of using this framework over other prior approaches is that here we do not require to choose a reproducing kernel and define its parameters.We demonstrate results of our framework on a Mass Cytometry dataset.	1,0,0,1,0,0
Artificial intelligence in peer review: How can evolutionary computation support journal editors?	With the volume of manuscripts submitted for publication growing every year, the deficiencies of peer review (e.g. long review times) are becoming more apparent. Editorial strategies, sets of guidelines designed to speed up the process and reduce editors workloads, are treated as trade secrets by publishing houses and are not shared publicly. To improve the effectiveness of their strategies, editors in small publishing groups are faced with undertaking an iterative trial-and-error approach. We show that Cartesian Genetic Programming, a nature-inspired evolutionary algorithm, can dramatically improve editorial strategies. The artificially evolved strategy reduced the duration of the peer review process by 30%, without increasing the pool of reviewers (in comparison to a typical human-developed strategy). Evolutionary computation has typically been used in technological processes or biological ecosystems. Our results demonstrate that genetic programs can improve real-world social systems that are usually much harder to understand and control than physical systems.	1,0,0,0,0,0
Quantification of market efficiency based on informational-entropy	Since the 1960s, the question whether markets are efficient or not is controversially discussed. One reason for the difficulty to overcome the controversy is the lack of a universal, but also precise, quantitative definition of efficiency that is able to graduate between different states of efficiency. The main purpose of this article is to fill this gap by developing a measure for the efficiency of markets that fulfill all the stated requirements. It is shown that the new definition of efficiency, based on informational-entropy, is equivalent to the two most used definitions of efficiency from Fama and Jensen. The new measure therefore enables steps to settle the dispute over the state of efficiency in markets. Moreover, it is shown that inefficiency in a market can either arise from the possibility to use information to predict an event with higher than chance level, or can emerge from wrong pricing/ quotes that do not reflect the right probabilities of possible events. Finally, the calculation of efficiency is demonstrated on a simple game (of coin tossing), to show how one could exactly quantify the efficiency in any market-like system, if all probabilities are known.	0,0,0,0,0,1
Neighborhood selection with application to social networks	The topic of this paper is modeling and analyzing dependence in stochastic social networks. Using a latent variable block model allows the analysis of dependence between blocks via the analysis of a latent graphical model. Our approach to the analysis of the graphical model then is based on the idea underlying the neighborhood selection scheme put forward by Meinshausen and Bühlmann (2006). However, because of the latent nature of our model, estimates have to be used in lieu of the unobserved variables. This leads to a novel analysis of graphical models under uncertainty, in the spirit of Rosenbaum et al. (2010), or Belloni et al. (2017). Lasso-based selectors, and a class of Dantzig-type selectors are studied.	0,0,1,1,0,0
Inhomogeneous hard-core bosonic mixture with checkerboard supersolid phase: Quantum and thermal phase diagram	We introduce an inhomogeneous bosonic mixture composed of two kinds of hard-core and semi-hard-core bosons with different nilpotency conditions and demonstrate that in contrast with the standard hard-core Bose-Hubbard model, our bosonic mixture with nearest- and next-nearest-neighbor interactions on a square lattice develops the checkerboard supersolid phase characterized by the simultaneous superfluid and checkerboard solid orders. Our bosonic mixture is created from a two-orbital Bose-Hubbard model including two kinds of bosons: a single-orbital boson and a two-orbital boson. By mapping the bosonic mixture to an anisotropic inhomogeneous spin model in the presence of a magnetic field, we study the ground-state phase diagram of the model by means of cluster mean field theory and linear spin-wave theory and show that various phases such as solid, superfluid, supersolid, and Mott insulator appear in the phase diagram of the mixture. Competition between the interactions and magnetic field causes the mixture to undergo different kinds of first- and second-order phase transitions. By studying the behavior of the spin-wave excitations, we find the reasons of all first- and second-order phase transitions. We also obtain the temperature phase diagram of the system using cluster mean field theory. We show that the checkerboard supersolid phase persists at finite temperature comparable with the interaction energies of bosons.	0,1,0,0,0,0
Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a Hyperspectral Unmixing Method Dealing with Intra-class Variability	Blind source separation is a common processing tool to analyse the constitution of pixels of hyperspectral images. Such methods usually suppose that pure pixel spectra (endmembers) are the same in all the image for each class of materials. In the framework of remote sensing, such an assumption is no more valid in the presence of intra-class variabilities due to illumination conditions, weathering, slight variations of the pure materials, etc... In this paper, we first describe the results of investigations highlighting intra-class variability measured in real images. Considering these results, a new formulation of the linear mixing model is presented leading to two new methods. Unconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation method based on the assumption of a linear mixing model, which can deal with intra-class variability. To overcome UP-NMF limitations an extended method is proposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each sensed spectrum, these extended versions of NMF extract a corresponding set of source spectra. A constraint is set to limit the spreading of each source's estimates in IP-NMF. The methods are tested on a semi-synthetic data set built with spectra extracted from a real hyperspectral image and then numerically mixed. We thus demonstrate the interest of our methods for realistic source variabilities. Finally, IP-NMF is tested on a real data set and it is shown to yield better performance than state of the art methods.	1,1,0,1,0,0
Statistics of turbulence in the energy-containing range of Taylor-Couette compared to canonical wall-bounded flows	Considering structure functions of the streamwise velocity component in a framework akin to the extended self-similarity hypothesis (ESS), de Silva \textit{et al.} (\textit{J. Fluid Mech.}, vol. 823,2017, pp. 498-510) observed that remarkably the \textit{large-scale} (energy-containing range) statistics in canonical wall bounded flows exhibit universal behaviour. In the present study, we extend this universality, which was seen to encompass also flows at moderate Reynolds number, to Taylor-Couette flow. In doing so, we find that also the transversal structure function of the spanwise velocity component exhibits the same universal behaviour across all flow types considered. We further demonstrate that these observations are consistent with predictions developed based on an attached-eddy hypothesis. These considerations also yield a possible explanation for the efficacy of the ESS framework by showing that it relaxes the self-similarity assumption for the attached eddy contributions. By taking the effect of streamwise alignment into account, the attached eddy model predicts different behaviour for structure functions in the streamwise and in the spanwise directions and that this effect cancels in the ESS-framework --- both consistent with the data. Moreover, it is demonstrated here that also the additive constants, which were previously believed to be flow dependent, are indeed universal at least in turbulent boundary layers and pipe flow where high-Reynolds number data are currently available.	0,1,0,0,0,0
Learning to Use Learners' Advice	In this paper, we study a variant of the framework of online learning using expert advice with limited/bandit feedback. We consider each expert as a learning entity, seeking to more accurately reflecting certain real-world applications. In our setting, the feedback at any time $t$ is limited in a sense that it is only available to the expert $i^t$ that has been selected by the central algorithm (forecaster), \emph{i.e.}, only the expert $i^t$ receives feedback from the environment and gets to learn at time $t$. We consider a generic black-box approach whereby the forecaster does not control or know the learning dynamics of the experts apart from knowing the following no-regret learning property: the average regret of any expert $j$ vanishes at a rate of at least $O(t_j^{\regretRate-1})$ with $t_j$ learning steps where $\regretRate \in [0, 1]$ is a parameter. In the spirit of competing against the best action in hindsight in multi-armed bandits problem, our goal here is to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to "guide" the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, \emph{i.e.}, not allowing the selected expert $i^t$ to learn at time $t$ for some time steps. Then, we design a novel no-regret learning algorithm \algo for this problem setting by carefully guiding the feedbacks observed by experts. We prove that \algo achieves the worst-case expected cumulative regret of $O(\Time^\frac{1}{2 - \regretRate})$ after $\Time$ time steps.	1,0,0,0,0,0
NeuroRule: A Connectionist Approach to Data Mining	Classification, which involves finding rules that partition a given data set into disjoint groups, is one class of data mining problems. Approaches proposed so far for mining classification rules for large databases are mainly decision tree based symbolic learning methods. The connectionist approach based on neural networks has been thought not well suited for data mining. One of the major reasons cited is that knowledge generated by neural networks is not explicitly represented in the form of rules suitable for verification or interpretation by humans. This paper examines this issue. With our newly developed algorithms, rules which are similar to, or more concise than those generated by the symbolic methods can be extracted from the neural networks. The data mining process using neural networks with the emphasis on rule extraction is described. Experimental results and comparison with previously published works are presented.	1,0,0,0,0,0
Increasing the Reusability of Enforcers with Lifecycle Events	Runtime enforcement can be effectively used to improve the reliability of software applications. However, it often requires the definition of ad hoc policies and enforcement strategies, which might be expensive to identify and implement. This paper discusses how to exploit lifecycle events to obtain useful enforcement strategies that can be easily reused across applications, thus reducing the cost of adoption of the runtime enforcement technology. The paper finally sketches how this idea can be used to define libraries that can automatically overcome problems related to applications misusing them.	1,0,0,0,0,0
Transverse spinning of light with globally unique handedness	Access to the transverse spin of light has unlocked new regimes in topological photonics and optomechanics. To achieve the transverse spin of nonzero longitudinal fields, various platforms that derive transversely confined waves based on focusing, interference, or evanescent waves have been suggested. Nonetheless, because of the transverse confinement inherently accompanying sign reversal of the field derivative, the resulting transverse spin handedness experiences spatial inversion, which leads to a mismatch between the densities of the wavefunction and its spin component and hinders the global observation of the transverse spin. Here, we reveal a globally pure transverse spin in which the wavefunction density signifies the spin distribution, by employing inverse molding of the eigenmode in the spin basis. Starting from the target spin profile, we analytically obtain the potential landscape and then show that the elliptic-hyperbolic transition around the epsilon-near-zero permittivity allows for the global conservation of transverse spin handedness across the topological interface between anisotropic metamaterials. Extending to the non-Hermitian regime, we also develop annihilated transverse spin modes to cover the entire Poincare sphere of the meridional plane. Our results enable the complete transfer of optical energy to transverse spinning motions and realize the classical analogy of 3-dimensional quantum spin states.	0,1,0,0,0,0
Causal Inference with Two Versions of Treatment	Causal effects are commonly defined as comparisons of the potential outcomes under treatment and control, but this definition is threatened by the possibility that the treatment or control condition is not well-defined, existing instead in more than one version. A simple, widely applicable analysis is proposed to address the possibility that the treatment or control condition exists in two versions with two different treatment effects. This analysis loses no power in the main comparison of treatment and control, provides additional information about version effects, and controls the family-wise error rate in several comparisons. The method is motivated and illustrated using an on-going study of the possibility that repeated head trauma in high school football causes an increase in risk of early on-set dementia.	0,0,0,1,0,0
A Multi-task Selected Learning Approach for Solving New Type 3D Bin Packing Problem	This paper studies a new type of 3D bin packing problem (BPP), in which a number of cuboid-shaped items must be put into a bin one by one orthogonally. The objective is to find a way to place these items that can minimize the surface area of the bin. This problem is based on the fact that there is no fixed-sized bin in many real business scenarios and the cost of a bin is proportional to its surface area. Based on previous research on 3D BPP, the surface area is determined by the sequence, spatial locations and orientations of items. It is a new NP-hard combinatorial optimization problem on unfixed-sized bin packing, for which we propose a multi-task framework based on Selected Learning, generating the sequence and orientations of items packed into the bin simultaneously. During training steps, Selected Learning chooses one of loss functions derived from Deep Reinforcement Learning and Supervised Learning corresponding to the training procedure. Numerical results show that the method proposed significantly outperforms Lego baselines by a substantial gain of 7.52%. Moreover, we produce large scale 3D Bin Packing order data set for studying bin packing problems and will release it to the research community.	0,0,0,1,0,0
From synaptic interactions to collective dynamics in random neuronal networks models: critical role of eigenvectors and transient behavior	The study of neuronal interactions is currently at the center of several neuroscience big collaborative projects (including the Human Connectome, the Blue Brain, the Brainome, etc.) which attempt to obtain a detailed map of the entire brain matrix. Under certain constraints, mathematical theory can advance predictions of the expected neural dynamics based solely on the statistical properties of such synaptic interaction matrix. This work explores the application of free random variables (FRV) to the study of large synaptic interaction matrices. Besides recovering in a straightforward way known results on eigenspectra of neural networks, we extend them to heavy-tailed distributions of interactions. More importantly, we derive analytically the behavior of eigenvector overlaps, which determine stability of the spectra. We observe that upon imposing the neuronal excitation/inhibition balance, although the eigenvalues remain unchanged, their stability dramatically decreases due to strong non-orthogonality of associated eigenvectors. It leads us to the conclusion that the understanding of the temporal evolution of asymmetric neural networks requires considering the entangled dynamics of both eigenvectors and eigenvalues, which might bear consequences for learning and memory processes in these models. Considering the success of FRV analysis in a wide variety of branches disciplines, we hope that the results presented here foster additional application of these ideas in the area of brain sciences.	0,0,0,0,1,0
Connectivity jamming game for physical layer attack in peer to peer networks	Because of the open access nature of wireless communications, wireless networks can suffer from malicious activity, such as jamming attacks, aimed at undermining the network's ability to sustain communication links and acceptable throughput. One important consideration when designing networks is to appropriately tune the network topology and its connectivity so as to support the communication needs of those participating in the network. This paper examines the problem of interference attacks that are intended to harm connectivity and throughput, and illustrates the method of mapping network performance parameters into the metric of topographic connectivity. Specifically, this paper arrives at anti-jamming strategies aimed at coping with interference attacks through a unified stochastic game. In such a framework, an entity trying to protect a network faces a dilemma: (i) the underlying motivations for the adversary can be quite varied, which depends largely on the network's characteristics such as power and distance; (ii) the metrics for such an attack can be incomparable (e.g., network connectivity and total throughput). To deal with the problem of such incomparable metrics, this paper proposes using the attack's expected duration as a unifying metric to compare distinct attack metrics because a longer-duration of unsuccessful attack assumes a higher cost. Based on this common metric, a mechanism of maxmin selection for an attack prevention strategy is suggested.	1,0,0,0,0,0
Covering and separation of Chebyshev points for non-integrable Riesz potentials	For Riesz $s$-potentials $K(x,y)=|x-y|^{-s}$, $s>0$, we investigate separation and covering properties of $N$-point configurations $\omega^*_N=\{x_1, \ldots, x_N\}$ on a $d$-dimensional compact set $A\subset \mathbb{R}^\ell$ for which the minimum of $\sum_{j=1}^N K(x, x_j)$ is maximal. Such configurations are called $N$-point optimal Riesz $s$-polarization (or Chebyshev) configurations. For a large class of $d$-dimensional sets $A$ we show that for $s>d$ the configurations $\omega^*_N$ have the optimal order of covering. Furthermore, for these sets we investigate the asymptotics as $N\to \infty$ of the best covering constant. For these purposes we compare best-covering configurations with optimal Riesz $s$-polarization configurations and determine the $s$-th root asymptotic behavior (as $s\to \infty$) of the maximal $s$-polarization constants. In addition, we introduce the notion of "weak separation" for point configurations and prove this property for optimal Riesz $s$-polarization configurations on $A$ for $s>\text{dim}(A)$, and for $d-1\leqslant s < d$ on the sphere $\mathbb{S}^d$.	0,0,1,0,0,0
Localized magnetic moments with tunable spin exchange in a gas of ultracold fermions	We report on the experimental realization of a state-dependent lattice for a two-orbital fermionic quantum gas with strong interorbital spin exchange. In our state-dependent lattice, the ground and metastable excited electronic states of $^{173}$Yb take the roles of itinerant and localized magnetic moments, respectively. Repulsive on-site interactions in conjunction with the tunnel mobility lead to spin exchange between mobile and localized particles, modeling the coupling term in the well-known Kondo Hamiltonian. In addition, we find that this exchange process can be tuned resonantly by varying the on-site confinement. We attribute this to a resonant coupling to center-of-mass excited bound states of one interorbital scattering channel.	0,1,0,0,0,0
Quantifying macroeconomic expectations in stock markets using Google Trends	Among other macroeconomic indicators, the monthly release of U.S. unemployment rate figures in the Employment Situation report by the U.S. Bureau of Labour Statistics gets a lot of media attention and strongly affects the stock markets. I investigate whether a profitable investment strategy can be constructed by predicting the likely changes in U.S. unemployment before the official news release using Google query volumes for related search terms. I find that massive new data sources of human interaction with the Internet not only improves U.S. unemployment rate predictability, but can also enhance market timing of trading strategies when considered jointly with macroeconomic data. My results illustrate the potential of combining extensive behavioural data sets with economic data to anticipate investor expectations and stock market moves.	0,0,0,0,0,1
More lessons from the six box toy experiment	Following a paper in which the fundamental aspects of probabilistic inference were introduced by means of a toy experiment, details of the analysis of simulated long sequences of extractions are shown here. In fact, the striking performance of probability-based inference and forecasting, compared to those obtained by simple `rules', might impress those practitioners who are usually underwhelmed by the philosophical foundation of the different methods. The analysis of the sequences also shows how the smallness of the probability of what has been actually observed, given the hypotheses of interest, is irrelevant for the purpose of inference.	0,1,1,0,0,0
The effect of phase change on stability of convective flow in a layer of volatile liquid driven by a horizontal temperature gradient	Buoyancy-thermocapillary convection in a layer of volatile liquid driven by a horizontal temperature gradient arises in a variety of situations. Recent studies have shown that the composition of the gas phase, which is typically a mixture of vapour and air, has a noticeable effect on the critical Marangoni number describing the onset of convection as well as on the observed convection pattern. Specifically, as the total pressure or, equivalently, the average concentration of air is decreased, the threshold of the instability leading to the emergence of convective rolls is found to increase rather significantly. We present a linear stability analysis of the problem which shows that this trend can be readily understood by considering the transport of heat and vapour through the gas phase. In particular, we show that transport in the gas phase has a noticeable effect even at atmospheric conditions, when phase change is greatly suppressed.	0,1,0,0,0,0
On Bayesian Exponentially Embedded Family for Model Order Selection	In this paper, we derive a Bayesian model order selection rule by using the exponentially embedded family method, termed Bayesian EEF. Unlike many other Bayesian model selection methods, the Bayesian EEF can use vague proper priors and improper noninformative priors to be objective in the elicitation of parameter priors. Moreover, the penalty term of the rule is shown to be the sum of half of the parameter dimension and the estimated mutual information between parameter and observed data. This helps to reveal the EEF mechanism in selecting model orders and may provide new insights into the open problems of choosing an optimal penalty term for model order selection and choosing a good prior from information theoretic viewpoints. The important example of linear model order selection is given to illustrate the algorithms and arguments. Lastly, the Bayesian EEF that uses Jeffreys prior coincides with the EEF rule derived by frequentist strategies. This shows another interesting relationship between the frequentist and Bayesian philosophies for model selection.	0,0,0,1,0,0
Rokhlin dimension for compact quantum group actions	We show that, for a given compact or discrete quantum group $G$, the class of actions of $G$ on C*-algebras is first-order axiomatizable in the logic for metric structures. As an application, we extend the notion of Rokhlin property for $G$-C*-algebra, introduced by Barlak, Szabó, and Voigt in the case when $G$ is second countable and coexact, to an arbitrary compact quantum group $G$. All the the preservations and rigidity results for Rokhlin actions of second countable coexact compact quantum groups obtained by Barlak, Szabó, and Voigt are shown to hold in this general context. As a further application, we extend the notion of equivariant order zero dimension for equivariant *-homomorphisms, introduced in the classical setting by the first and third authors, to actions of compact quantum groups. This allows us to define the Rokhlin dimension of an action of a compact quantum group on a C*-algebra, recovering the Rokhlin property as Rokhlin dimension zero. We conclude by establishing a preservation result for finite nuclear dimension and finite decomposition rank when passing to fixed point algebras and crossed products by compact quantum group actions with finite Rokhlin dimension.	0,0,1,0,0,0
Noisy Networks for Exploration	We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.	1,0,0,1,0,0
Pili mediated intercellular forces shape heterogeneous bacterial microcolonies prior to multicellular differentiation	Microcolonies are aggregates of a few dozen to a few thousand cells exhibited by many bacteria. The formation of microcolonies is a crucial step towards the formation of more mature bacterial communities known as biofilms, but also marks a significant change in bacterial physiology. Within a microcolony, bacteria forgo a single cell lifestyle for a communal lifestyle hallmarked by high cell density and physical interactions between cells potentially altering their behaviour. It is thus crucial to understand how initially identical single cells start to behave differently while assembling in these tight communities. Here we show that cells in the microcolonies formed by the human pathogen Neisseria gonorrhoeae (Ng) present differential motility behaviors within an hour upon colony formation. Observation of merging microcolonies and tracking of single cells within microcolonies reveal a heterogeneous motility behavior: cells close to the surface of the microcolony exhibit a much higher motility compared to cells towards the center. Numerical simulations of a biophysical model for the microcolonies at the single cell level suggest that the emergence of differential behavior within a multicellular microcolony of otherwise identical cells is of mechanical origin. It could suggest a route toward further bacterial differentiation and ultimately mature biofilms.	0,1,0,0,0,0
Extrasolar Planets and Their Host Stars	In order to understand the exoplanet, you need to understand its parent star. Astrophysical parameters of extrasolar planets are directly and indirectly dependent on the properties of their respective host stars. These host stars are very frequently the only visible component in the systems. This book describes our work in the field of characterization of exoplanet host stars using interferometry to determine angular diameters, trigonometric parallax to determine physical radii, and SED fitting to determine effective temperatures and luminosities. The interferometry data are based on our decade-long survey using the CHARA Array. We describe our methods and give an update on the status of the field, including a table with the astrophysical properties of all stars with high-precision interferometric diameters out to 150 pc (status Nov 2016). In addition, we elaborate in more detail on a number of particularly significant or important exoplanet systems, particularly with respect to (1) insights gained from transiting exoplanets, (2) the determination of system habitable zones, and (3) the discrepancy between directly determined and model-based stellar radii. Finally, we discuss current and future work including the calibration of semi-empirical methods based on interferometric data.	0,1,0,0,0,0
VAE with a VampPrior	Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call "Variational Mixture of Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.	1,0,0,1,0,0
Particle-hole symmetry and composite fermions in fractional quantum Hall states	We study fractional quantum Hall states at filling fractions in the Jain sequences using the framework of composite Dirac fermions. Synthesizing previous work, we write down an effective field theory consistent with all symmetry requirements, including Galilean invariance and particle-hole symmetry. Employing a Fermi liquid description, we demonstrate the appearance of the Girvin--Macdonlald--Platzman algebra and compute the dispersion relation of neutral excitations and various response functions. Our results satisfy requirements of particle-hole symmetry. We show that while the dispersion relation obtained from the HLR theory is particle-hole symmetric, correlation functions obtained from HLR are not. The results of the Dirac theory are shown to be consistent with the Haldane bound on the projected structure factor, while those of the HLR theory violate it.	0,1,0,0,0,0
Formal Synthesis of Control Strategies for Positive Monotone Systems	We design controllers from formal specifications for positive discrete-time monotone systems that are subject to bounded disturbances. Such systems are widely used to model the dynamics of transportation and biological networks. The specifications are described using signal temporal logic (STL), which can express a broad range of temporal properties. We formulate the problem as a mixed-integer linear program (MILP) and show that under the assumptions made in this paper, which are not restrictive for traffic applications, the existence of open-loop control policies is sufficient and almost necessary to ensure the satisfaction of STL formulas. We establish a relation between satisfaction of STL formulas in infinite time and set-invariance theories and provide an efficient method to compute robust control invariant sets in high dimensions. We also develop a robust model predictive framework to plan controls optimally while ensuring the satisfaction of the specification. Illustrative examples and a traffic management case study are included.	1,0,1,0,0,0
Randomized Load Balancing on Networks with Stochastic Inputs	Iterative load balancing algorithms for indivisible tokens have been studied intensively in the past. Complementing previous worst-case analyses, we study an average-case scenario where the load inputs are drawn from a fixed probability distribution. For cycles, tori, hypercubes and expanders, we obtain almost matching upper and lower bounds on the discrepancy, the difference between the maximum and the minimum load. Our bounds hold for a variety of probability distributions including the uniform and binomial distribution but also distributions with unbounded range such as the Poisson and geometric distribution. For graphs with slow convergence like cycles and tori, our results demonstrate a substantial difference between the convergence in the worst- and average-case. An important ingredient in our analysis is new upper bound on the t-step transition probability of a general Markov chain, which is derived by invoking the evolving set process.	1,0,0,0,0,0
Defect entropies and enthalpies in Barium Fluoride	Various experimental techniques, have revealed that the predominant intrinsic point defects in BaF$_2$ are anion Frenkel defects. Their formation enthalpy and entropy as well as the corresponding parameters for the fluorine vacancy and fluorine interstitial motion have been determined. In addition, low temperature dielectric relaxation measurements in BaF$_2$ doped with uranium leads to the parameters {\tau}$_0$, E in the Arrhenius relation {\tau}={\tau}$_0$exp(E/kBT) for the relaxation time {\tau}. For the relaxation peak associated with a single tetravalent uranium, the migration entropy deduced from the pre-exponential factor {\tau}$_0$, is smaller than the anion Frenkel defect formation entropy by almost two orders of magnitude. We show that, despite their great variation, the defect entropies and enthalpies are interconnected through a model based on anharmonic properties of the bulk material that have been recently studied by employing density-functional theory and density-functional perturbation theory.	0,1,0,0,0,0
Collapsed Tetragonal Phase Transition in LaRu$_2$P$_2$	The structural properties of LaRu$_2$P$_2$ under external pressure have been studied up to 14 GPa, employing high-energy x-ray diffraction in a diamond-anvil pressure cell. At ambient conditions, LaRu$_2$P$_2$ (I4/mmm) has a tetragonal structure with a bulk modulus of $B=105(2)$ GPa and exhibits superconductivity at $T_c= 4.1$ K. With the application of pressure, LaRu$_2$P$_2$ undergoes a phase transition to a collapsed tetragonal (cT) state with a bulk modulus of $B=175(5)$ GPa. At the transition, the c-lattice parameter exhibits a sharp decrease with a concurrent increase of the a-lattice parameter. The cT phase transition in LaRu$_2$P$_2$ is consistent with a second order transition, and was found to be temperature dependent, increasing from $P=3.9(3)$ GPa at 160 K to $P=4.6(3)$ GPa at 300 K. In total, our data are consistent with the cT transition being near, but slightly above 2 GPa at 5 K. Finally, we compare the effect of physical and chemical pressure in the RRu$_2$P$_2$ (R = Y, La-Er, Yb) isostructural series of compounds and find them to be analogous.	0,1,0,0,0,0
Spontaneous domain formation in disordered copolymers as a mechanism for chromosome structuring	Motivated by the problem of domain formation in chromosomes, we studied a co--polymer model where only a subset of the monomers feel attractive interactions. These monomers are displaced randomly from a regularly-spaced pattern, thus introducing some quenched disorder in the system. Previous work has shown that in the case of regularly-spaced interacting monomers this chain can fold into structures characterized by multiple distinct domains of consecutive segments. In each domain, attractive interactions are balanced by the entropy cost of forming loops. We show by advanced replica-exchange simulations that adding disorder in the position of the interacting monomers further stabilizes these domains. The model suggests that the partitioning of the chain into well-defined domains of consecutive monomers is a spontaneous property of heteropolymers. In the case of chromosomes, evolution could have acted on the spacing of interacting monomers to modulate in a simple way the underlying domains for functional reasons.	0,0,0,0,1,0
Acyclic cluster algebras, reflection groups, and curves on a punctured disc	We establish a bijective correspondence between certain non-self-intersecting curves in an $n$-punctured disc and positive ${\mathbf c}$-vectors of acyclic cluster algebras whose quivers have multiple arrows between every pair of vertices. As a corollary, we obtain a proof of a conjecture by K.-H. Lee and K. Lee (arXiv:1703.09113) on the combinatorial description of real Schur roots for acyclic quivers with multiple arrows, and give a combinatorial characterization of seeds in terms of curves in an $n$-punctured disc.	0,0,1,0,0,0
Automatic Estimation of Fetal Abdominal Circumference from Ultrasound Images	Ultrasound diagnosis is routinely used in obstetrics and gynecology for fetal biometry, and owing to its time-consuming process, there has been a great demand for automatic estimation. However, the automated analysis of ultrasound images is complicated because they are patient-specific, operator-dependent, and machine-specific. Among various types of fetal biometry, the accurate estimation of abdominal circumference (AC) is especially difficult to perform automatically because the abdomen has low contrast against surroundings, non-uniform contrast, and irregular shape compared to other parameters.We propose a method for the automatic estimation of the fetal AC from 2D ultrasound data through a specially designed convolutional neural network (CNN), which takes account of doctors' decision process, anatomical structure, and the characteristics of the ultrasound image. The proposed method uses CNN to classify ultrasound images (stomach bubble, amniotic fluid, and umbilical vein) and Hough transformation for measuring AC. We test the proposed method using clinical ultrasound data acquired from 56 pregnant women. Experimental results show that, with relatively small training samples, the proposed CNN provides sufficient classification results for AC estimation through the Hough transformation. The proposed method automatically estimates AC from ultrasound images. The method is quantitatively evaluated, and shows stable performance in most cases and even for ultrasound images deteriorated by shadowing artifacts. As a result of experiments for our acceptance check, the accuracies are 0.809 and 0.771 with the expert 1 and expert 2, respectively, while the accuracy between the two experts is 0.905. However, for cases of oversized fetus, when the amniotic fluid is not observed or the abdominal area is distorted, it could not correctly estimate AC.	1,0,0,1,0,0
Koszul duality for Lie algebroids	This paper studies the role of dg-Lie algebroids in derived deformation theory. More precisely, we provide an equivalence between the homotopy theories of formal moduli problems and dg-Lie algebroids over a commutative dg-algebra of characteristic zero. At the level of linear objects, we show that the category of representations of a dg-Lie algebroid is an extension of the category of quasi-coherent sheaves on the corresponding formal moduli problem. We describe this extension geometrically in terms of pro-coherent sheaves.	0,0,1,0,0,0
Chaotic Dynamics Enhance the Sensitivity of Inner Ear Hair Cells	Hair cells of the auditory and vestibular systems are capable of detecting sounds that induce sub-nanometer vibrations of the hair bundle, below the stochastic noise levels of the surrounding fluid. Hair cells of certain species are also known to oscillate without external stimulation, indicating the presence of an underlying active mechanism. We previously demonstrated that these spontaneous oscillations exhibit chaotic dynamics. By varying the Calcium concentration and the viscosity of the Endolymph solution, we are able to modulate the degree of chaos in the hair cell dynamics. We find that the hair cell is most sensitive to a stimulus of small amplitude when it is poised in the weakly chaotic regime. Further, we show that the response time to a force step decreases with increasing levels of chaos. These results agree well with our numerical simulations of a chaotic Hopf oscillator and suggest that chaos may be responsible for the extreme sensitivity and temporal resolution of hair cells.	0,0,0,0,1,0
Causal Discovery in the Presence of Measurement Error: Identifiability Conditions	Measurement error in the observed values of the variables can greatly change the output of various causal discovery methods. This problem has received much attention in multiple fields, but it is not clear to what extent the causal model for the measurement-error-free variables can be identified in the presence of measurement error with unknown variance. In this paper, we study precise sufficient identifiability conditions for the measurement-error-free causal model and show what information of the causal model can be recovered from observed data. In particular, we present two different sets of identifiability conditions, based on the second-order statistics and higher-order statistics of the data, respectively. The former was inspired by the relationship between the generating model of the measurement-error-contaminated data and the factor analysis model, and the latter makes use of the identifiability result of the over-complete independent component analysis problem.	1,0,0,1,0,0
TFDASH: A Fairness, Stability, and Efficiency Aware Rate Control Approach for Multiple Clients over DASH	Dynamic adaptive streaming over HTTP (DASH) has recently been widely deployed in the Internet and adopted in the industry. It, however, does not impose any adaptation logic for selecting the quality of video fragments requested by clients and suffers from lackluster performance with respect to a number of desirable properties: efficiency, stability, and fairness when multiple players compete for a bottleneck link. In this paper, we propose a throughput-friendly DASH (TFDASH) rate control scheme for video streaming with multiple clients over DASH to well balance the trade-offs among efficiency, stability, and fairness. The core idea behind guaranteeing fairness and high efficiency (bandwidth utilization) is to avoid OFF periods during the downloading process for all clients, i.e., the bandwidth is in perfect-subscription or over-subscription with bandwidth utilization approach to 100\%. We also propose a dual-threshold buffer model to solve the instability problem caused by the above idea. As a result, by integrating these novel components, we also propose a probability-driven rate adaption logic taking into account several key factors that most influence visual quality, including buffer occupancy, video playback quality, video bit-rate switching frequency and amplitude, to guarantee high-quality video streaming. Our experiments evidently demonstrate the superior performance of the proposed method.	1,0,0,0,0,0
Towards Classification of Web ontologies using the Horizontal and Vertical Segmentation	The new era of the Web is known as the semantic Web or the Web of data. The semantic Web depends on ontologies that are seen as one of its pillars. The bigger these ontologies, the greater their exploitation. However, when these ontologies become too big other problems may appear, such as the complexity to charge big files in memory, the time it needs to download such files and especially the time it needs to make reasoning on them. We discuss in this paper approaches for segmenting such big Web ontologies as well as its usefulness. The segmentation method extracts from an existing ontology a segment that represents a layer or a generation in the existing ontology; i.e. a horizontally extraction. The extracted segment should be itself an ontology.	1,0,0,0,0,0
Ensemble Sampling	Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.	1,0,0,1,0,0
Evolution of eccentricity and inclination of hot protoplanets embedded in radiative discs	We study the evolution of the eccentricity and inclination of protoplanetary embryos and low-mass protoplanets (from a fraction of an Earth mass to a few Earth masses) embedded in a protoplanetary disc, by means of three dimensional hydrodynamics calculations with radiative transfer in the diffusion limit. When the protoplanets radiate in the surrounding disc the energy released by the accretion of solids, their eccentricity and inclination experience a growth toward values which depend on the luminosity to mass ratio of the planet, which are comparable to the disc's aspect ratio and which are reached over timescales of a few thousand years. This growth is triggered by the appearance of a hot, under-dense region in the vicinity of the planet. The growth rate of the eccentricity is typically three times larger than that of the inclination. In long term calculations, we find that the excitation of eccentricity and the excitation of inclination are not independent. In the particular case in which a planet has initially a very small eccentricity and inclination, the eccentricity largely overruns the inclination. When the eccentricity reaches its asymptotic value, the growth of inclination is quenched, yielding an eccentric orbit with a very low inclination. As a side result, we find that the eccentricity and inclination of non-luminous planets are damped more vigorously in radiative discs than in isothermal discs.	0,1,0,0,0,0
Discrete Dynamic Causal Modeling and Its Relationship with Directed Information	This paper explores the discrete Dynamic Causal Modeling (DDCM) and its relationship with Directed Information (DI). We prove the conditional equivalence between DDCM and DI in characterizing the causal relationship between two brain regions. The theoretical results are demonstrated using fMRI data obtained under both resting state and stimulus based state. Our numerical analysis is consistent with that reported in previous study.	0,0,0,1,0,0
Streaming Algorithm for Euler Characteristic Curves of Multidimensional Images	We present an efficient algorithm to compute Euler characteristic curves of gray scale images of arbitrary dimension. In various applications the Euler characteristic curve is used as a descriptor of an image. Our algorithm is the first streaming algorithm for Euler characteristic curves. The usage of streaming removes the necessity to store the entire image in RAM. Experiments show that our implementation handles terabyte scale images on commodity hardware. Due to lock-free parallelism, it scales well with the number of processor cores. Our software---CHUNKYEuler---is available as open source on Bitbucket. Additionally, we put the concept of the Euler characteristic curve in the wider context of computational topology. In particular, we explain the connection with persistence diagrams.	1,0,1,0,0,0
Crystal structure, site selectivity, and electronic structure of layered chalcogenide LaOBiPbS3	We have investigated the crystal structure of LaOBiPbS3 using neutron diffraction and synchrotron X-ray diffraction. From structural refinements, we found that the two metal sites, occupied by Bi and Pb, were differently surrounded by the sulfur atoms. Calculated bond valence sum suggested that one metal site was nearly trivalent and the other was nearly divalent. Neutron diffraction also revealed site selectivity of Bi and Pb in the LaOBiPbS3 structure. These results suggested that the crystal structure of LaOBiPbS3 can be regarded as alternate stacks of the rock-salt-type Pb-rich sulfide layers and the LaOBiS2-type Bi-rich layers. From band calculations for an ideal (LaOBiS2)(PbS) system, we found that the S bands of the PbS layer were hybridized with the Bi bands of the BiS plane at around the Fermi energy, which resulted in the electronic characteristics different from that of LaOBiS2. Stacking the rock-salt type sulfide (chalcogenide) layers and the BiS2-based layered structure could be a new strategy to exploration of new BiS2-based layered compounds, exotic two-dimensional electronic states, or novel functionality.	0,1,0,0,0,0
K-closedness of weighted Hardy spaces on the two-dimensional torus	It is proved that, under certain restrictions on weights, a pair of weighted Hardy spaces on the two-dimensional torus is K-closed in the pair of the corresponding weighted Lebesgue spaces. By now, K-closedness of Hardy spaces on the two-dimensional torus was considered either in the case of no weights or in the case of weights that split into a product of two functions of one variable (the so-called "split weights"). Here the case of certain nonsplit weights is studied.	0,0,1,0,0,0
An empirical evaluation of alternative methods of estimation for Permutation Entropy in time series with tied values	Bandt and Pompe introduced Permutation Entropy in 2002 for Time Series where equal values, xt1 = xt2, t1 = t2, were neglected and only inequalities between the xt were considered. Since then, this measure has been modified and extended, in particular in cases when the amount of equal values in the series can not be neglected, (i.e. heart rate variability (HRV) time series). We review the different existing methodologies that treats this subject by classifying them according to their different strategies. In addition, a novel Bayesian Missing Data Imputation is presented that proves to outperform the existing methodologies that deals with type of time series. All this facts are illustrated by simulations and also by distinguishing patients suffering from Congestive Heart Failure from a (healthy) control group using HRV time series	0,0,0,1,0,0
Reconstructing a $f(R)$ theory from the $α$-Attractors	We show an analogy at high curvature between a $f(R) = R + aR^{n - 1} + bR^2$ theory and the $\alpha$-Attractors. We calculate the expressions of the parameters $a$, $b$ and $n$ as functions of $\alpha$ and the predictions of the model $f(R) = R + aR^{n - 1} + bR^2$ on the scalar spectral index $n_{\rm s}$ and the tensor-to-scalar ratio $r$. We find that the power law correction $R^{n - 1}$ allows for a production of gravitational waves enhanced with respect to the one in the Starobinsky model, while maintaining a viable prediction on $n_{\rm s}$. We numerically reconstruct the full $\alpha$-Attractors class of models testing the goodness of our high-energy approximation $f(R) = R + aR^{n - 1} + bR^2$. Moreover, we also investigate the case of a single power law $f(R) = \gamma R^{2 - \delta}$ theory, with $\gamma$ and $\delta$ free parameters. We calculate analytically the predictions of this model on the scalar spectral index $n_{\rm s}$ and the tensor-to-scalar ratio $r$ and the values of $\delta$ which are allowed from the current observational results. We find that $-0.015 < \delta < 0.016$, confirming once again the excellent agreement between the Starobinsky model and observation.	0,1,0,0,0,0
An Efficiently Searchable Encrypted Data Structure for Range Queries	At CCS 2015 Naveed et al. presented first attacks on efficiently searchable encryption, such as deterministic and order-preserving encryption. These plaintext guessing attacks have been further improved in subsequent work, e.g. by Grubbs et al. in 2016. Such cryptanalysis is crucially important to sharpen our understanding of the implications of security models. In this paper we present an efficiently searchable, encrypted data structure that is provably secure against these and even more powerful chosen plaintext attacks. Our data structure supports logarithmic-time search with linear space complexity. The indices of our data structure can be used to search by standard comparisons and hence allow easy retrofitting to existing database management systems. We implemented our scheme and show that its search time overhead is only 10 milliseconds compared to non-secure search.	1,0,0,0,0,0
Persistence-like distance on Tamarkin's category and symplectic displacement energy	We introduce a persistence-like pseudo-distance on Tamarkin's category and prove that the distance between an object and its Hamiltonian deformation is at most the Hofer norm of the Hamiltonian function. Using the distance, we show a quantitative version of Tamarkin's non-displaceability theorem, which gives a lower bound of the displacement energy of compact subsets in a cotangent bundle.	0,0,1,0,0,0
Restoring a smooth function from its noisy integrals	Numerical (and experimental) data analysis often requires the restoration of a smooth function from a set of sampled integrals over finite bins. We present the bin hierarchy method that efficiently computes the maximally smooth function from the sampled integrals using essentially all the information contained in the data. We perform extensive tests with different classes of functions and levels of data quality, including Monte Carlo data suffering from a severe sign problem and physical data for the Green's function of the Fröhlich polaron.	0,1,0,1,0,0
Macro-molecular data storage with petabyte/cm^3 density, highly parallel read/write operations, and genuine 3D storage capability	Digital information can be encoded in the building-block sequence of macro-molecules, such as RNA and single-stranded DNA. Methods of "writing" and "reading" macromolecular strands are currently available, but they are slow and expensive. In an ideal molecular data storage system, routine operations such as write, read, erase, store, and transfer must be done reliably and at high speed within an integrated chip. As a first step toward demonstrating the feasibility of this concept, we report preliminary results of DNA readout experiments conducted in miniaturized chambers that are scalable to even smaller dimensions. We show that translocation of a single-stranded DNA molecule (consisting of 50 adenosine bases followed by 100 cytosine bases) through an ion-channel yields a characteristic signal that is attributable to the 2-segment structure of the molecule. We also examine the dependence of the rate and speed of molecular translocation on the adjustable parameters of the experiment.	1,1,0,0,0,0
Multiresolution Tensor Decomposition for Multiple Spatial Passing Networks	This article is motivated by soccer positional passing networks collected across multiple games. We refer to these data as replicated spatial passing networks---to accurately model such data it is necessary to take into account the spatial positions of the passer and receiver for each passing event. This spatial registration and replicates that occur across games represent key differences with usual social network data. As a key step before investigating how the passing dynamics influence team performance, we focus on developing methods for summarizing different team's passing strategies. Our proposed approach relies on a novel multiresolution data representation framework and Poisson nonnegative block term decomposition model, which automatically produces coarse-to-fine low-rank network motifs. The proposed methods are applied to detailed passing record data collected from the 2014 FIFA World Cup.	1,0,0,1,0,0
Experimental Tests of Spirituality	We currently harness technologies that could shed new light on old philosophical questions, such as whether our mind entails anything beyond our body or whether our moral values reflect universal truth.	0,0,0,0,1,0
Volumetric Super-Resolution of Multispectral Data	Most multispectral remote sensors (e.g. QuickBird, IKONOS, and Landsat 7 ETM+) provide low-spatial high-spectral resolution multispectral (MS) or high-spatial low-spectral resolution panchromatic (PAN) images, separately. In order to reconstruct a high-spatial/high-spectral resolution multispectral image volume, either the information in MS and PAN images are fused (i.e. pansharpening) or super-resolution reconstruction (SRR) is used with only MS images captured on different dates. Existing methods do not utilize temporal information of MS and high spatial resolution of PAN images together to improve the resolution. In this paper, we propose a multiframe SRR algorithm using pansharpened MS images, taking advantage of both temporal and spatial information available in multispectral imagery, in order to exceed spatial resolution of given PAN images. We first apply pansharpening to a set of multispectral images and their corresponding PAN images captured on different dates. Then, we use the pansharpened multispectral images as input to the proposed wavelet-based multiframe SRR method to yield full volumetric SRR. The proposed SRR method is obtained by deriving the subband relations between multitemporal MS volumes. We demonstrate the results on Landsat 7 ETM+ images comparing our method to conventional techniques.	1,0,0,0,0,0
Graph Theoretical Models of Closed n-Dimensional Manifolds: Digital Models of a Moebius Strip, a Torus, a Projective Plane a Klein Bottle and n-Dimensional Spheres	In this paper, we show how to construct graph theoretical models of n-dimensional continuous objects and manifolds. These models retain topological properties of their continuous counterparts. An LCL collection of n-cells in Euclidean space is introduced and investigated. If an LCL collection of n-cells is a cover of a continuous n-dimensional manifold then the intersection graph of this cover is a digital closed n-dimensional manifold with the same topology as its continuous counterpart. As an example, we prove that the digital model of a continuous n-dimensional sphere is a digital n-sphere with at least 2n+2 points, the digital model of a continuous projective plane is a digital projective plane with at least eleven points, the digital model of a continuous Klein bottle is the digital Klein bottle with at least sixteen points, the digital model of a continuous torus is the digital torus with at least sixteen points and the digital model of a continuous Moebius band is the digital Moebius band with at least twelve points.	1,0,1,0,0,0
Decoupled Access-Execute on ARM big.LITTLE	Energy-efficiency plays a significant role given the battery lifetime constraints in embedded systems and hand-held devices. In this work we target the ARM big.LITTLE, a heterogeneous platform that is dominant in the mobile and embedded market, which allows code to run transparently on different microarchitectures with individual energy and performance characteristics. It allows to se more energy efficient cores to conserve power during simple tasks and idle times and switch over to faster, more power hungry cores when performance is needed. This proposal explores the power-savings and the performance gains that can be achieved by utilizing the ARM big.LITTLE core in combination with Decoupled Access-Execute (DAE). DAE is a compiler technique that splits code regions into two distinct phases: a memory-bound Access phase and a compute-bound Execute phase. By scheduling the memory-bound phase on the LITTLE core, and the compute-bound phase on the big core, we conserve energy while caching data from main memory and perform computations at maximum performance. Our preliminary findings show that applying DAE on ARM big.LITTLE has potential. By prefetching data in Access we can achieve an IPC improvement of up to 37% in the Execute phase, and manage to shift more than half of the program runtime to the LITTLE core. We also provide insight into advantages and disadvantages of our approach, present preliminary results and discuss potential solutions to overcome locking overhead.	1,0,0,0,0,0
A cost effective and reliable environment monitoring system for HPC applications	We present a slow control system to gather all relevant environment information necessary to effectively and reliably run an HPC (High Performance Computing) system at a high value over price ratio. The scalable and reliable overall concept is presented as well as a newly developed hardware device for sensor read out. This device incorporates a Raspberry Pi, an Arduino and PoE (Power over Ethernet) functionality in a compact form factor. The system is in use at the 2 PFLOPS cluster of the Johannes Gutenberg-University and Helmholtz-Institute in Mainz.	1,0,0,0,0,0
Two-Person Zero-Sum Games with Unbounded Payoff Functions and Uncertain Expected Payoffs	This paper provides sufficient conditions for the existence of values and solutions for two-person zero-sum one-step games with possibly noncompact action sets for both players and possibly unbounded payoff functions, which may be neither convex nor concave. For such games payoffs may not be defined for some pairs of strategies. In addition to the existence of values and solutions, this paper investigates continuity properties of the value functions and solution multifunctions for families of games with possibly noncompact action sets and unbounded payoff functions, when action sets and payoffs depend on a parameter.	0,0,1,0,0,0
Exploring a search for long-duration transient gravitational waves associated with magnetar bursts	Soft gamma repeaters and anomalous X-ray pulsars are thought to be magnetars, neutron stars with strong magnetic fields of order $\mathord{\sim} 10^{13}$--$10^{15} \, \mathrm{gauss}$. These objects emit intermittent bursts of hard X-rays and soft gamma rays. Quasiperiodic oscillations in the X-ray tails of giant flares imply the existence of neutron star oscillation modes which could emit gravitational waves powered by the magnetar's magnetic energy reservoir. We describe a method to search for transient gravitational-wave signals associated with magnetar bursts with durations of 10s to 1000s of seconds. The sensitivity of this method is estimated by adding simulated waveforms to data from the sixth science run of Laser Interferometer Gravitational-wave Observatory (LIGO). We find a search sensitivity in terms of the root sum square strain amplitude of $h_{\mathrm{rss}} = 1.3 \times 10^{-21} \, \mathrm{Hz}^{-1/2}$ for a half sine-Gaussian waveform with a central frequency $f_0 = 150 \, \mathrm{Hz}$ and a characteristic time $\tau = 400 \, \mathrm{s}$. This corresponds to a gravitational wave energy of $E_{\mathrm{GW}} = 4.3 \times 10^{46} \, \mathrm{erg}$, the same order of magnitude as the 2004 giant flare which had an estimated electromagnetic energy of $E_{\mathrm{EM}} = \mathord{\sim} 1.7 \times 10^{46} (d/ 8.7 \, \mathrm{kpc})^2 \, \mathrm{erg}$, where $d$ is the distance to SGR 1806-20. We present an extrapolation of these results to Advanced LIGO, estimating a sensitivity to a gravitational wave energy of $E_{\mathrm{GW}} = 3.2 \times 10^{43} \, \mathrm{erg}$ for a magnetar at a distance of $1.6 \, \mathrm{kpc}$. These results suggest this search method can probe significantly below the energy budgets for magnetar burst emission mechanisms such as crust cracking and hydrodynamic deformation.	0,1,0,0,0,0
Autonomous Extracting a Hierarchical Structure of Tasks in Reinforcement Learning and Multi-task Reinforcement Learning	Reinforcement learning (RL), while often powerful, can suffer from slow learning speeds, particularly in high dimensional spaces. The autonomous decomposition of tasks and use of hierarchical methods hold the potential to significantly speed up learning in such domains. This paper proposes a novel practical method that can autonomously decompose tasks, by leveraging association rule mining, which discovers hidden relationship among entities in data mining. We introduce a novel method called ARM-HSTRL (Association Rule Mining to extract Hierarchical Structure of Tasks in Reinforcement Learning). It extracts temporal and structural relationships of sub-goals in RL, and multi-task RL. In particular,it finds sub-goals and relationship among them. It is shown the significant efficiency and performance of the proposed method in two main topics of RL.	1,0,0,0,0,0
An alternative quadratic formula	The classical quadratic formula and some of its lesser known variants for solving the quadratic equation are reviewed. Then, a new formula for the roots of a quadratic polynomial is presented.	0,0,1,0,0,0
On the Limiting Stokes' Wave of Extreme Height in Arbitrary Water Depth	As mentioned by Schwartz (1974) and Cokelet (1977), it was failed to gain convergent results of limiting Stokes' waves in extremely shallow water by means of perturbation methods even with the aid of extrapolation techniques such as Padé approximant. Especially, it is extremely difficult for traditional analytic/numerical approaches to present the wave profile of limiting waves with a sharp crest of $120^\circ$ included angle first mentioned by Stokes in 1880s. Thus, traditionally, different wave models are used for waves in different water depths. In this paper, by means of the homotopy analysis method (HAM), an analytic approximation method for highly nonlinear equations, we successfully gain convergent results (and especially the wave profiles) of the limiting Stokes' waves with this kind of sharp crest in arbitrary water depth, even including solitary waves of extreme form in extremely shallow water, without using any extrapolation techniques. Therefore, in the frame of the HAM, the Stokes' wave can be used as a unified theory for all kinds of waves, including periodic waves in deep and intermediate depth, cnoidal waves in shallow water and solitary waves in extremely shallow water.	0,1,0,0,0,0
Correspondences without a Core	We study the formal properties of correspondences of curves without a core, focusing on the case of étale correspondences. The motivating examples come from Hecke correspondences of Shimura curves. Given a correspondence without a core, we construct an infinite graph $\mathcal{G}_{gen}$ together with a large group of "algebraic" automorphisms $A$. The graph $\mathcal{G}_{gen}$ measures the "generic dynamics" of the correspondence. We construct specialization maps $\mathcal{G}_{gen}\rightarrow\mathcal{G}_{phys}$ to the "physical dynamics" of the correspondence. We also prove results on the number of bounded étale orbits, in particular generalizing a recent theorem of Hallouin and Perret. We use a variety of techniques: Galois theory, the theory of groups acting on infinite graphs, and finite group schemes.	0,0,1,0,0,0
Some Remarks about the Complexity of Epidemics Management	Recent outbreaks of Ebola, H1N1 and other infectious diseases have shown that the assumptions underlying the established theory of epidemics management are too idealistic. For an improvement of procedures and organizations involved in fighting epidemics, extended models of epidemics management are required. The necessary extensions consist in a representation of the management loop and the potential frictions influencing the loop. The effects of the non-deterministic frictions can be taken into account by including the measures of robustness and risk in the assessment of management options. Thus, besides of the increased structural complexity resulting from the model extensions, the computational complexity of the task of epidemics management - interpreted as an optimization problem - is increased as well. This is a serious obstacle for analyzing the model and may require an additional pre-processing enabling a simplification of the analysis process. The paper closes with an outlook discussing some forthcoming problems.	0,1,0,0,0,0
Wasserstein Soft Label Propagation on Hypergraphs: Algorithm and Generalization Error Bounds	Inspired by recent interests of developing machine learning and data mining algorithms on hypergraphs, we investigate in this paper the semi-supervised learning algorithm of propagating "soft labels" (e.g. probability distributions, class membership scores) over hypergraphs, by means of optimal transportation. Borrowing insights from Wasserstein propagation on graphs [Solomon et al. 2014], we re-formulate the label propagation procedure as a message-passing algorithm, which renders itself naturally to a generalization applicable to hypergraphs through Wasserstein barycenters. Furthermore, in a PAC learning framework, we provide generalization error bounds for propagating one-dimensional distributions on graphs and hypergraphs using 2-Wasserstein distance, by establishing the \textit{algorithmic stability} of the proposed semi-supervised learning algorithm. These theoretical results also shed new lights upon deeper understandings of the Wasserstein propagation on graphs.	0,0,0,1,0,0
The difficulty of folding self-folding origami	Why is it difficult to refold a previously folded sheet of paper? We show that even crease patterns with only one designed folding motion inevitably contain an exponential number of `distractor' folding branches accessible from a bifurcation at the flat state. Consequently, refolding a sheet requires finding the ground state in a glassy energy landscape with an exponential number of other attractors of higher energy, much like in models of protein folding (Levinthal's paradox) and other NP-hard satisfiability (SAT) problems. As in these problems, we find that refolding a sheet requires actuation at multiple carefully chosen creases. We show that seeding successful folding in this way can be understood in terms of sub-patterns that fold when cut out (`folding islands'). Besides providing guidelines for the placement of active hinges in origami applications, our results point to fundamental limits on the programmability of energy landscapes in sheets.	0,1,0,0,0,0
Indefinite Kernel Logistic Regression	Traditionally, kernel learning methods requires positive definitiveness on the kernel, which is too strict and excludes many sophisticated similarities, that are indefinite, in multimedia area. To utilize those indefinite kernels, indefinite learning methods are of great interests. This paper aims at the extension of the logistic regression from positive semi-definite kernels to indefinite kernels. The model, called indefinite kernel logistic regression (IKLR), keeps consistency to the regular KLR in formulation but it essentially becomes non-convex. Thanks to the positive decomposition of an indefinite matrix, IKLR can be transformed into a difference of two convex models, which follows the use of concave-convex procedure. Moreover, we employ an inexact solving scheme to speed up the sub-problem and develop a concave-inexact-convex procedure (CCICP) algorithm with theoretical convergence analysis. Systematical experiments on multi-modal datasets demonstrate the superiority of the proposed IKLR method over kernel logistic regression with positive definite kernels and other state-of-the-art indefinite learning based algorithms.	1,0,0,1,0,0
Single-Crystal N-polar GaN p-n Diodes by Plasma-Assisted Molecular Beam Epitaxy	N-polar GaN p-n diodes are realized on single-crystal N-polar GaN bulk wafers by plasma-assisted molecular beam epitaxy growth. The current-voltage characteristics show high-quality rectification and electroluminescence characteristics with a high on/off current ratio and interband photon emission. The measured electroluminescence spectrum is dominated by strong near-band edge emission, while deep level luminescence is greatly suppressed. A very low dislocation density leads to a high reverse breakdown electric field. The low leakage current N-polar diodes open up several potential applications in polarization-engineered photonic and electronic devices.	0,1,0,0,0,0
The kinematics of the white dwarf population from the SDSS DR12	We use the Sloan Digital Sky Survey Data Release 12, which is the largest available white dwarf catalog to date, to study the evolution of the kinematical properties of the population of white dwarfs in the Galactic disc. We derive masses, ages, photometric distances and radial velocities for all white dwarfs with hydrogen-rich atmospheres. For those stars for which proper motions from the USNO-B1 catalog are available the true three-dimensional components of the stellar space velocity are obtained. This subset of the original sample comprises 20,247 objects, making it the largest sample of white dwarfs with measured three-dimensional velocities. Furthermore, the volume probed by our sample is large, allowing us to obtain relevant kinematical information. In particular, our sample extends from a Galactocentric radial distance $R_{\rm G}=7.8$~kpc to 9.3~kpc, and vertical distances from the Galactic plane ranging from $Z=-0.5$~kpc to 0.5~kpc. We examine the mean components of the stellar three-dimensional velocities, as well as their dispersions with respect to the Galactocentric and vertical distances. We confirm the existence of a mean Galactocentric radial velocity gradient, $\partial\langle V_{\rm R}\rangle/\partial R_{\rm G}=-3\pm5$~km~s$^{-1}$~kpc$^{-1}$. We also confirm North-South differences in $\langle V_{\rm z}\rangle$. Specifically, we find that white dwarfs with $Z>0$ (in the North Galactic hemisphere) have $\langle V_{\rm z}\rangle<0$, while the reverse is true for white dwarfs with $Z<0$. The age-velocity dispersion relation derived from the present sample indicates that the Galactic population of white dwarfs may have experienced an additional source of heating, which adds to the secular evolution of the Galactic disc.	0,1,0,0,0,0
Low Rank Matrix Recovery with Simultaneous Presence of Outliers and Sparse Corruption	We study a data model in which the data matrix D can be expressed as D = L + S + C, where L is a low rank matrix, S an element-wise sparse matrix and C a matrix whose non-zero columns are outlying data points. To date, robust PCA algorithms have solely considered models with either S or C, but not both. As such, existing algorithms cannot account for simultaneous element-wise and column-wise corruptions. In this paper, a new robust PCA algorithm that is robust to simultaneous types of corruption is proposed. Our approach hinges on the sparse approximation of a sparsely corrupted column so that the sparse expansion of a column with respect to the other data points is used to distinguish a sparsely corrupted inlier column from an outlying data point. We also develop a randomized design which provides a scalable implementation of the proposed approach. The core idea of sparse approximation is analyzed analytically where we show that the underlying ell_1-norm minimization can obtain the representation of an inlier in presence of sparse corruptions.	1,0,0,1,0,0
Dark Matter in the Local Group of Galaxies	We describe the neutrino flavor (e = electron, u = muon, t = tau) masses as m(i=e;u;t)= m + [Delta]mi with |[Delta]mij|/m < 1 and probably |[Delta]mij|/m << 1. The quantity m is the degenerate neutrino mass. Because neutrino flavor is not a quantum number, this degenerate mass appears in the neutrino equation of state. We apply a Monte Carlo computational physics technique to the Local Group (LG) of galaxies to determine an approximate location for a Dark Matter embedding condensed neutrino object(CNO). The calculation is based on the rotational properties of the only spiral galaxies within the LG: M31, M33 and the Milky Way. CNOs could be the Dark Matter everyone is looking for and we estimate the CNO embedding the LG to have a mass 5.17x10^15 Mo and a radius 1.316 Mpc, with the estimated value of m ~= 0.8 eV/c2. The up-coming KATRIN experiment will either be the definitive result or eliminate condensed neutrinos as a Dark Matter candidate.	0,1,0,0,0,0
Asymptotic behaviour of the Christoffel functions on the Unit Ball in the presence of a Mass on the Sphere	We present a family of mutually orthogonal polynomials on the unit ball with respect to an inner product which includes a mass uniformly distributed on the sphere. First, connection formulas relating these multivariate orthogonal polynomials and the classical ball polynomials are obtained. Then, using the representation formula for these polynomials in terms of spherical harmonics analytic properties will be deduced. Finally, we analyze the asymptotic behaviour of the Christoffel functions.	0,0,1,0,0,0
Making the Dzyaloshinskii-Moriya interaction visible	Brillouin light spectroscopy is a powerful and robust technique for measuring the interfacial Dzyaloshinskii-Moriya interaction in thin films with broken inversion symmetry. Here we show that the magnon visibility, i.e. the intensity of the inelastically scattered light, strongly depends on the thickness of the dielectric seed material - SiO$_2$. By using both, analytical thin-film optics and numerical calculations, we reproduce the experimental data. We therefore provide a guideline for the maximization of the signal by adapting the substrate properties to the geometry of the measurement. Such a boost-up of the signal eases the magnon visualization in ultrathin magnetic films, speeds-up the measurement and increases the reliability of the data.	0,1,0,0,0,0
TRAMP: Tracking by a Real-time AMbisonic-based Particle filter	This article presents a multiple sound source localization and tracking system, fed by the Eigenmike array. The First Order Ambisonics (FOA) format is used to build a pseudointensity-based spherical histogram, from which the source position estimates are deduced. These instantaneous estimates are processed by a wellknown tracking system relying on a set of particle filters. While the novelty within localization and tracking is incremental, the fully-functional, complete and real-time running system based on these algorithms is proposed for the first time. As such, it could serve as an additional baseline method of the LOCATA challenge.	1,0,0,0,0,0
3D spatial exploration by E. coli echoes motor temporal variability	Unraveling bacterial strategies for spatial exploration is crucial to understand the complexity of the organi- zation of life. Currently, a cornerstone for quantitative modeling of bacterial transport, is their run-and-tumble strategy to explore their environment. For Escherichia coli, the run time distribution was reported to follow a Poisson process with a single characteristic time related to the rotational switching of the flagellar motor. Direct measurements on flagellar motors show, on the contrary, heavy-tailed distributions of rotation times stemming from the intrinsic noise in the chemotactic mechanism. The crucial role of stochasticity on the chemotactic response has also been highlighted by recent modeling, suggesting its determinant influence on motility. In stark contrast with the accepted vision of run-and-tumble, here we report a large behavioral variability of wild-type E. coli, revealed in their three-dimensional trajectories. At short times, a broad distribution of run times is measured on a population and attributed to the slow fluctuations of a signaling protein triggering the flagellar motor reversal. Over long times, individual bacteria undergo significant changes in motility. We demonstrate that such a large distribution introduces measurement biases in most practical situations. These results reconcile the notorious conundrum between run time observations and motor switching statistics. We finally propose that statistical modeling of transport properties currently undertaken in the emerging framework of active matter studies should be reconsidered under the scope of this large variability of motility features.	0,0,0,0,1,0
Surface plasmons in superintense laser-solid interactions	We review studies of superintense laser interaction with solid targets where the generation of propagating surface plasmons (or surface waves) plays a key role. These studies include the onset of plasma instabilities at the irradiated surface, the enhancement of secondary emissions (protons, electrons, and photons as high harmonics in the XUV range) in femtosecond interactions with grating targets, and the generation of unipolar current pulses with picosecond duration. The experimental results give evidence of the existence of surface plasmons in the nonlinear regime of relativistic electron dynamics. These findings open up a route to the improvement of ultrashort laser-driven sources of energetic radiation and, more in general, to the extension of plasmonics in a high field regime.	0,1,0,0,0,0
Riemannian Gaussian distributions on the space of positive-definite quaternion matrices	Recently, Riemannian Gaussian distributions were defined on spaces of positive-definite real and complex matrices. The present paper extends this definition to the space of positive-definite quaternion matrices. In order to do so, it develops the Riemannian geometry of the space of positive-definite quaternion matrices, which is shown to be a Riemannian symmetric space of non-positive curvature. The paper gives original formulae for the Riemannian metric of this space, its geodesics, and distance function. Then, it develops the theory of Riemannian Gaussian distributions, including the exact expression of their probability density, their sampling algorithm and statistical inference.	0,0,1,1,0,0
The multidimensional truncated Moment Problem: Atoms, Determinacy, and Core Variety	This paper is about the moment problem on a finite-dimensional vector space of continuous functions. We investigate the structure of the convex cone of moment functionals (supporting hyperplanes, exposed faces, inner points) and treat various important special topics on moment functionals (determinacy, set of atoms of representing measures, core variety).	0,0,1,0,0,0
Near-UV OH Prompt Emission in the Innermost Coma of 103P/Hartley 2	The Deep Impact spacecraft fly-by of comet 103P/Hartley 2 occurred on 2010 November 4, one week after perihelion with a closest approach (CA) distance of about 700 km. We used narrowband images obtained by the Medium Resolution Imager (MRI) onboard the spacecraft to study the gas and dust in the innermost coma. We derived an overall dust reddening of 15\%/100 nm between 345 and 749 nm and identified a blue enhancement in the dust coma in the sunward direction within 5 km from the nucleus, which we interpret as a localized enrichment in water ice. OH column density maps show an anti-sunward enhancement throughout the encounter except for the highest resolution images, acquired at CA, where a radial jet becomes visible in the innermost coma, extending up to 12 km from the nucleus. The OH distribution in the inner coma is very different from that expected for a fragment species. Instead, it correlates well with the water vapor map derived by the HRI-IR instrument onboard Deep Impact \citep{AHearn2011}. Radial profiles of the OH column density and derived water production rates show an excess of OH emission during CA that cannot be explained with pure fluorescence. We attribute this excess to a prompt emission process where photodissociation of H$_2$O directly produces excited OH*($A^2\it{\Sigma}^+$) radicals. Our observations provide the first direct imaging of Near-UV prompt emission of OH. We therefore suggest the use of a dedicated filter centered at 318.8 nm to directly trace the water in the coma of comets.	0,1,0,0,0,0
Photographic dataset: playing cards	This is a photographic dataset collected for testing image processing algorithms. The idea is to have images that can exploit the properties of total variation, therefore a set of playing cards was distributed on the scene. The dataset is made available at www.fips.fi/photographic_dataset2.php	1,1,0,0,0,0
A simultaneous generalization of the theorems of Chevalley-Warning and Morlaye	Inspired by recent work of I. Baoulina, we give a simultaneous generalization of the theorems of Chevalley-Warning and Morlaye.	0,0,1,0,0,0
Sparse Algorithm for Robust LSSVM in Primal Space	As enjoying the closed form solution, least squares support vector machine (LSSVM) has been widely used for classification and regression problems having the comparable performance with other types of SVMs. However, LSSVM has two drawbacks: sensitive to outliers and lacking sparseness. Robust LSSVM (R-LSSVM) overcomes the first partly via nonconvex truncated loss function, but the current algorithms for R-LSSVM with the dense solution are faced with the second drawback and are inefficient for training large-scale problems. In this paper, we interpret the robustness of R-LSSVM from a re-weighted viewpoint and give a primal R-LSSVM by the representer theorem. The new model may have sparse solution if the corresponding kernel matrix has low rank. Then approximating the kernel matrix by a low-rank matrix and smoothing the loss function by entropy penalty function, we propose a convergent sparse R-LSSVM (SR-LSSVM) algorithm to achieve the sparse solution of primal R-LSSVM, which overcomes two drawbacks of LSSVM simultaneously. The proposed algorithm has lower complexity than the existing algorithms and is very efficient for training large-scale problems. Many experimental results illustrate that SR-LSSVM can achieve better or comparable performance with less training time than related algorithms, especially for training large scale problems.	1,0,0,1,0,0
Darboux and Binary Darboux Transformations for Discrete Integrable Systems. II. Discrete Potential mKdV Equation	The paper presents two results. First it is shown how the discrete potential modified KdV equation and its Lax pairs in matrix form arise from the Hirota-Miwa equation by a 2-periodic reduction. Then Darboux transformations and binary Darboux transformations are derived for the discrete potential modified KdV equation and it is shown how these may be used to construct exact solutions.	0,1,0,0,0,0
Sharp Threshold of Blow-up and Scattering for the fractional Hartree equation	We consider the fractional Hartree equation in the $L^2$-supercritical case, and we find a sharp threshold of the scattering versus blow-up dichotomy for radial data: If $ M[u_{0}]^{\frac{s-s_c}{s_c}}E[u_{0}<M[Q]^{\frac{s-s_c}{s_c}}E[Q]$ and $M[u_{0}]^{\frac{s-s_c}{s_c}}\|u_{0}\|^2_{\dot H^s}<M[Q]^{\frac{s-s_c}{s_c}}\| Q\|^2_{\dot H^s}$, then the solution $u(t)$ is globally well-posed and scatters; if $ M[u_{0}]^{\frac{s-s_c}{s_c}}E[u_{0}]<M[Q]^{\frac{s-s_c}{s_c}}E[Q]$ and $M[u_{0}]^{\frac{s-s_c}{s_c}}\|u_{0}\|^2_{\dot H^s}>M[Q]^{\frac{s-s_c}{s_c}}\| Q\|^2_{\dot H^s}$, the solution $u(t)$ blows up in finite time. This condition is sharp in the sense that the solitary wave solution $e^{it}Q(x)$ is global but not scattering, which satisfies the equality in the above conditions. Here, $Q$ is the ground-state solution for the fractional Hartree equation.	0,0,1,0,0,0
Free Cooling of a Granular Gas in Three Dimensions	Granular gases as dilute ensembles of particles in random motion are not only at the basis of elementary structure-forming processes in the universe and involved in many industrial and natural phenomena, but also excellent models to study fundamental statistical dynamics. A vast number of theoretical and numerical investigations have dealt with this apparently simple non-equilibrium system. The essential difference to molecular gases is the energy dissipation in particle collisions, a subtle distinction with immense impact on their global dynamics. Its most striking manifestation is the so-called granular cooling, the gradual loss of mechanical energy in absence of external excitation. We report an experimental study of homogeneous cooling of three-dimensional (3D) granular gases in microgravity. Surprisingly, the asymptotic scaling $E(t)\propto t^{-2}$ obtained by Haff's minimal model [J. Fluid Mech. 134, 401 (1983)] proves to be robust, despite the violation of several of its central assumptions. The shape anisotropy of the grains influences the characteristic time of energy loss quantitatively, but not qualitatively. We compare kinetic energies in the individual degrees of freedom, and find a slight predominance of the translational motions. In addition, we detect a certain preference of the grains to align with their long axis in flight direction, a feature known from active matter or animal flocks, and the onset of clustering.	0,1,0,0,0,0
Commuting graphs on Coxeter groups, Dynkin diagrams and finite subgroups of $SL(2,\mathbb{C})$	For a group $H$ and a non empty subset $\Gamma\subseteq H$, the commuting graph $G=\mathcal{C}(H,\Gamma)$ is the graph with $\Gamma$ as the node set and where any $x,y \in \Gamma$ are joined by an edge if $x$ and $y$ commute in $H$. We prove that any simple graph can be obtained as a commuting graph of a Coxeter group, solving the realizability problem in this setup. In particular we can recover every Dynkin diagram of ADE type as a commuting graph. Thanks to the relation between the ADE classification and finite subgroups of $\SL(2,\C)$, we are able to rephrase results from the {\em McKay correspondence} in terms of generators of the corresponding Coxeter groups. We finish the paper studying commuting graphs $\mathcal{C}(H,\Gamma)$ for every finite subgroup $H\subset\SL(2,\C)$ for different subsets $\Gamma\subseteq H$, and investigating metric properties of them when $\Gamma=H$.	0,0,1,0,0,0
QCRI Machine Translation Systems for IWSLT 16	This paper describes QCRI's machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic->English and English->Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic->English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs.	1,0,0,0,0,0
A robotic vision system to measure tree traits	The autonomous measurement of tree traits, such as branching structure, branch diameters, branch lengths, and branch angles, is required for tasks such as robotic pruning of trees as well as structural phenotyping. We propose a robotic vision system called the Robotic System for Tree Shape Estimation (RoTSE) to determine tree traits in field settings. The process is composed of the following stages: image acquisition with a mobile robot unit, segmentation, reconstruction, curve skeletonization, conversion to a graph representation, and then computation of traits. Quantitative and qualitative results on apple trees are shown in terms of accuracy, computation time, and robustness. Compared to ground truth measurements, the RoTSE produced the following estimates: branch diameter (mean-squared error $0.99$ mm), branch length (mean-squared error $45.64$ mm), and branch angle (mean-squared error $10.36$ degrees). The average run time was 8.47 minutes when the voxel resolution was $3$ mm$^3$.	1,0,0,0,0,0
On stabilization of solutions of nonlinear parabolic equations with a gradient term	For parabolic equations of the form $$ \frac{\partial u}{\partial t} - \sum_{i,j=1}^n a_{ij} (x, u) \frac{\partial^2 u}{\partial x_i \partial x_j} + f (x, u, D u) = 0 \quad \mbox{in } {\mathbb R}_+^{n+1}, $$ where ${\mathbb R}_+^{n+1} = {\mathbb R}^n \times (0, \infty)$, $n \ge 1$, $D = (\partial / \partial x_1, \ldots, \partial / \partial x_n)$ is the gradient operator, and $f$ is some function, we obtain conditions guaranteeing that every solution tends to zero as $t \to \infty$.	0,0,1,0,0,0
Gradient Descent using Duality Structures	Gradient descent is commonly used to solve optimization problems arising in machine learning, such as training neural networks. Although it seems to be effective for many different neural network training problems, it is unclear if the effectiveness of gradient descent can be explained using existing performance guarantees for the algorithm. We argue that existing analyses of gradient descent rely on assumptions that are too strong to be applicable in the case of multi-layer neural networks. To address this, we propose an algorithm, duality structure gradient descent (DSGD), that is amenable to a non-asymptotic performance analysis, under mild assumptions on the training set and network architecture. The algorithm can be viewed as a form of layer-wise coordinate descent, where at each iteration the algorithm chooses one layer of the network to update. The decision of what layer to update is done in a greedy fashion, based on a rigorous lower bound of the function decrease for each possible choice of layer. In the analysis, we bound the time required to reach approximate stationary points, in both the deterministic and stochastic settings. The convergence is measured in terms of a Finsler geometry that is derived from the network architecture and designed to confirm a Lipschitz-like property on the gradient of the training objective function. Numerical experiments in both the full batch and mini-batch settings suggest that the algorithm is a promising step towards methods for training neural networks that are both rigorous and efficient.	1,0,0,0,0,0
Understanding the Feedforward Artificial Neural Network Model From the Perspective of Network Flow	In recent years, deep learning based on artificial neural network (ANN) has achieved great success in pattern recognition. However, there is no clear understanding of such neural computational models. In this paper, we try to unravel "black-box" structure of Ann model from network flow. Specifically, we consider the feed forward Ann as a network flow model, which consists of many directional class-pathways. Each class-pathway encodes one class. The class-pathway of a class is obtained by connecting the activated neural nodes in each layer from input to output, where activation value of neural node (node-value) is defined by the weights of each layer in a trained ANN-classifier. From the perspective of the class-pathway, training an ANN-classifier can be regarded as the formulation process of class-pathways of different classes. By analyzing the the distances of each two class-pathways in a trained ANN-classifiers, we try to answer the questions, why the classifier performs so? At last, from the neural encodes view, we define the importance of each neural node through the class-pathways, which is helpful to optimize the structure of a classifier. Experiments for two types of ANN model including multi-layer MLP and CNN verify that the network flow based on class-pathway is a reasonable explanation for ANN models.	1,0,0,0,0,0
Switching between Limit Cycles in a Model of Running Using Exponentially Stabilizing Discrete Control Lyapunov Function	This paper considers the problem of switching between two periodic motions, also known as limit cycles, to create agile running motions. For each limit cycle, we use a control Lyapunov function to estimate the region of attraction at the apex of the flight phase. We switch controllers at the apex, only if the current state of the robot is within the region of attraction of the subsequent limit cycle. If the intersection between two limit cycles is the null set, then we construct additional limit cycles till we are able to achieve sufficient overlap of the region of attraction between sequential limit cycles. Additionally, we impose an exponential convergence condition on the control Lyapunov function that allows us to rapidly transition between limit cycles. Using the approach we demonstrate switching between 5 limit cycles in about 5 steps with the speed changing from 2 m/s to 5 m/s.	1,0,0,0,0,0
Statistical inference for misspecified ergodic Lévy driven stochastic differential equation models	This paper deals with the estimation problem of misspecified ergodic Lévy driven stochastic differential equation models based on high-frequency samples. We utilize the widely applicable and tractable Gaussian quasi-likelihood approach which focuses on (conditional) mean and variance structure. It is shown that the corresponding Gaussian quasi-likelihood estimators of drift and scale parameters satisfy tail probability estimates and asymptotic normality at the same rate as correctly specified case. In this process, extended Poisson equation for time-homogeneous Feller Markov processes plays an important role to handle misspecification effect. Our result confirms the practical usefulness of the Gaussian quasi-likelihood approach for SDE models, more firmly.	0,0,1,1,0,0
Design Considerations for Proposed Fermilab Integrable RCS	Integrable optics is an innovation in particle accelerator design that provides strong nonlinear focusing while avoiding parametric resonances. One promising application of integrable optics is to overcome the traditional limits on accelerator intensity imposed by betatron tune-spread and collective instabilities. The efficacy of high-intensity integrable accelerators will be undergo comprehensive testing over the next several years at the Fermilab Integrable Optics Test Accelerator (IOTA) and the University of Maryland Electron Ring (UMER). We propose an integrable Rapid-Cycling Synchrotron (iRCS) as a replacement for the Fermilab Booster to achieve multi-MW beam power for the Fermilab high-energy neutrino program. We provide a overview of the machine parameters and discuss an approach to lattice optimization. Integrable optics requires arcs with integer-pi phase advance followed by drifts with matched beta functions. We provide an example integrable lattice with features of a modern RCS - long dispersion-free drifts, low momentum compaction, superperiodicity, chromaticity correction, separate-function magnets, and bounded beta functions.	0,1,0,0,0,0
Nematic phase with colossal magnetoresistance and orbital polarons in manganite La$_{1-x}$Sr$_x$MnO$_3$	The origin of colossal magnetoresistance (CMR) is still controversial. The spin dynamics of La$_{1-x}$Sr$_x$MnO$_3$ is revisited along the Mn-O-Mn direction at $x\leq 0.5$, $T\leq T_C$ with a new study at $x$=0.4. A new lattice dynamics study is also reported at $x_0$=0.2,representative of the optimal doping for CMR. In large-$q$ wavevector range, typical of the scale of polarons, spin dynamics exhibits a discrete spectrum, $E^n_{\rm mag}$ with $n$ equal to the degeneracy of orbital-pseudospin transitions and energy values in coincidence with the phonon ones. It corresponds to the spin-orbital excitation spectrum of short life-time polarons, in which the orbital pseudospin degeneracy is lift by phonons. For $x\neq x_0$, its q-range reveals a $\ell \approx 1.7a$ size of polarons with a dimension $2d$ at $x=1/8$ partly increasing to $\approx$ $3d$ at $x=0.3$. At $x_0=0.2$ ($T<T_C$) two distinct $q$ and energy ranges appear separated by $\Delta E(q_0\approx 0.35)=3meV$. The same $\Delta E(q_0)$ value separates two unusual transverse acoustic branches ($T>T_C$). Both characterize a nematic-phase defined by chains of "orbital polarons" of $2a$ size, distant from $3a$, typical of $x_0=1/6$. It could explain CMR.	0,1,0,0,0,0
Simulating optical coherence tomography for observing nerve activity: a finite difference time domain bi-dimensional model	We present a finite difference time domain (FDTD) model for computation of A line scans in time domain optical coherence tomography (OCT). By simulating only the end of the two arms of the interferometer and computing the interference signal in post processing, it is possible to reduce the computational time required by the simulations and, thus, to simulate much bigger environments. Moreover, it is possible to simulate successive A lines and thus obtaining a cross section of the sample considered. In this paper we present the model applied to two different samples: a glass rod filled with water-sucrose solution at different concentrations and a peripheral nerve. This work demonstrates the feasibility of using OCT for non-invasive, direct optical monitoring of peripheral nerve activity, which is a long-sought goal of neuroscience.	0,1,0,0,0,0
A free energy landscape of the capture of CO2 by frustrated Lewis pairs	Frustrated Lewis pairs (FLPs) are known for its ability to capture CO2. Although many FLPs have been reported experimentally and several theoretical studies have been carried out to address the reaction mechanism, the individual roles of Lewis acids and bases of FLP in the capture of CO2 is still unclear. In this study, we employed density functional theory (DFT) based metadynamics simulations to investigate the complete path for the capture of CO2 by tBu3P/B(C6F5)3 pair, and to understand the role of the Lewis acid and base. Interestingly, we have found out that the Lewis acids play more important role than Lewis bases. Specifically, the Lewis acids are crucial for catalytical properties and are responsible for both kinetic and thermodynamics control. The Lewis bases, however, have less impact on the catalytic performance and are mainly responsible for the formation of FLP systems. Based on these findings, we propose a thumb of rule for the future synthesis of FLP-based catalyst for the utilization of CO2.	0,1,0,0,0,0
Hydrodynamic stability in the presence of a stochastic forcing:a case study in convection	We investigate the stability of a statistically stationary conductive state for Rayleigh-Bénard convection between stress-free plates that arises due to a bulk stochastic internal heating. This setup may be seen as a generalization to a stochastic setting of the seminal 1916 study of Lord Rayleigh. Our results indicate that stochastic forcing at small magnitude has a stabilizing effect, while strong stochastic forcing has a destabilizing effect. The methodology put forth in this article, which combines rigorous analysis with careful computation, also provides an approach to hydrodynamic stability for a variety of systems subject to a large scale stochastic forcing.	0,1,1,0,0,0
Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent	Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms---which build a small, weighted subset of the data that approximates the full dataset---are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty. To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally. GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors. The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.	0,0,0,1,0,0
Critical exponent $ω$ in the Gross-Neveu-Yukawa model at $O(1/N)$	The critcal exponent $\omega$ is evaluated at $O(1/N)$ in $d$-dimensions in the Gross-Neveu model using the large $N$ critical point formalism. It is shown to be in agreement with the recently determined three loop $\beta$-functions of the Gross-Neveu-Yukawa model in four dimensions. The same exponent is computed for the chiral Gross-Neveu and non-abelian Nambu-Jona-Lasinio universality classes.	0,1,0,0,0,0
Empirical likelihood inference for partial functional linear regression models based on B spline	In this paper, we apply empirical likelihood method to inference for the regression parameters in the partial functional linear regression models based on B spline. We prove that the empirical log likelihood ratio for the regression parameters converges in law to a weighted sum of independent chi square distributions and run simulations to assess the finite sample performance of our method.	0,0,0,1,0,0
Drug Selection via Joint Push and Learning to Rank	Selecting the right drugs for the right patients is a primary goal of precision medicine. In this manuscript, we consider the problem of cancer drug selection in a learning-to-rank framework. We have formulated the cancer drug selection problem as to accurately predicting 1). the ranking positions of sensitive drugs and 2). the ranking orders among sensitive drugs in cancer cell lines based on their responses to cancer drugs. We have developed a new learning-to-rank method, denoted as pLETORg , that predicts drug ranking structures in each cell line via using drug latent vectors and cell line latent vectors. The pLETORg method learns such latent vectors through explicitly enforcing that, in the drug ranking list of each cell line, the sensitive drugs are pushed above insensitive drugs, and meanwhile the ranking orders among sensitive drugs are correct. Genomics information on cell lines is leveraged in learning the latent vectors. Our experimental results on a benchmark cell line-drug response dataset demonstrate that the new pLETORg significantly outperforms the state-of-the-art method in prioritizing new sensitive drugs.	0,0,0,1,0,0
Recurrent Deep Embedding Networks for Genotype Clustering and Ethnicity Prediction	The understanding of variations in genome sequences assists us in identifying people who are predisposed to common diseases, solving rare diseases, and finding the corresponding population group of the individuals from a larger population group. Although classical machine learning techniques allow researchers to identify groups (i.e. clusters) of related variables, the accuracy, and effectiveness of these methods diminish for large and high-dimensional datasets such as the whole human genome. On the other hand, deep neural network architectures (the core of deep learning) can better exploit large-scale datasets to build complex models. In this paper, we use the K-means clustering approach for scalable genomic data analysis aiming towards clustering genotypic variants at the population scale. Finally, we train a deep belief network (DBN) for predicting the geographic ethnicity. We used the genotype data from the 1000 Genomes Project, which covers the result of genome sequencing for 2504 individuals from 26 different ethnic origins and comprises 84 million variants. Our experimental results, with a focus on accuracy and scalability, show the effectiveness and superiority compared to the state-of-the-art.	0,0,0,0,1,0
Ordinary differential equations in algebras of generalized functions	A local existence and uniqueness theorem for ODEs in the special algebra of generalized functions is established, as well as versions including parameters and dependence on initial values in the generalized sense. Finally, a Frobenius theorem is proved. In all these results, composition of generalized functions is based on the notion of c-boundedness.	0,0,1,0,0,0
Spin-Frustrated Pyrochlore Chains in the Volcanic Mineral Kamchatkite (KCu3OCl(SO4)2)	Search of new frustrated magnetic systems is of a significant importance for physics studying the condensed matter. The platform for geometric frustration of magnetic systems can be provided by copper oxocentric tetrahedra (OCu4) forming the base of crystalline structures of copper minerals from Tolbachik volcanos in Kamchatka. The present work was devoted to a new frustrated antiferromagnetic - kamchatkite (KCu3OCl(SO4)2). The calculation of the sign and strength of magnetic couplings in KCu3OCl(SO4)2 has been performed on the basis of structural data by the phenomenological crystal chemistry method with taking into account corrections on the Jahn-Teller orbital degeneracy of Cu2. It has been established that kamchatkite (KCu3OCl(SO4)2) contains AFM spin-frustrated chains of the pyrochlore type composed of cone-sharing Cu4 tetrahedra. Strong AFM intrachain and interchain couplings compete with each other. Frustration of magnetic couplings in tetrahedral chains is combined with the presence of electric polarization.	0,1,0,0,0,0
A Distributed Algorithm for Computing a Common Fixed Point of a Finite Family of Paracontractions	A distributed algorithm is described for finding a common fixed point of a family of m>1 nonlinear maps M_i : R^n -> R^n assuming that each map is a paracontraction and that at least one such common fixed point exists. The common fixed point is simultaneously computed by m agents assuming each agent i knows only M_i, the current estimates of the fixed point generated by its neighbors, and nothing more. Each agent recursively updates its estimate of a fixed point by utilizing the current estimates generated by each of its neighbors. Neighbor relations are characterized by a time-varying directed graph N(t). It is shown under suitably general conditions on N(t), that the algorithm causes all agents estimates to converge to the same common fixed point of the m nonlinear maps.	1,0,1,0,0,0
Monte Carlo determination of the low-energy constants for a two-dimensional spin-1 Heisenberg model with spatial anisotropy	The low-energy constants, namely the spin stiffness $\rho_s$, the staggered magnetization density ${\cal M}_s$ per area, and the spinwave velocity $c$ of the two-dimensional (2D) spin-1 Heisenberg model on the square and rectangular lattices are determined using the first principles Monte Carlo method. In particular, the studied models have antiferromagnetic couplings $J_1$ and $J_2$ in the spatial 1- and 2-directions, respectively. For each considered $J_2/J_1$, the aspect ratio of the corresponding linear box sizes $L_2/L_1$ used in the simulations is adjusted so that the squares of the two spatial winding numbers take the same values. In addition, the relevant finite-volume and -temperature predictions from magnon chiral perturbation theory are employed in extracting the numerical values of these low-energy constants. Our results of $\rho_{s1}$ are in quantitative agreement with those obtained by the series expansion method over a broad range of $J_2/J_1$. This in turn provides convincing numerical evidence for the quantitative correctness of our approach. The ${\cal M}_s$ and $c$ presented here for the spatially anisotropic models are new and can be used as benchmarks for future related studies.	0,1,0,0,0,0
HybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning	In this paper, we introduce a new model for leveraging unlabeled data to improve generalization performances of image classifiers: a two-branch encoder-decoder architecture called HybridNet. The first branch receives supervision signal and is dedicated to the extraction of invariant class-related representations. The second branch is fully unsupervised and dedicated to model information discarded by the first branch to reconstruct input data. To further support the expected behavior of our model, we propose an original training objective. It favors stability in the discriminative branch and complementarity between the learned representations in the two branches. HybridNet is able to outperform state-of-the-art results on CIFAR-10, SVHN and STL-10 in various semi-supervised settings. In addition, visualizations and ablation studies validate our contributions and the behavior of the model on both CIFAR-10 and STL-10 datasets.	0,0,0,1,0,0
Asymmetry-Induced Synchronization in Oscillator Networks	A scenario has recently been reported in which in order to stabilize complete synchronization of an oscillator network---a symmetric state---the symmetry of the system itself has to be broken by making the oscillators nonidentical. But how often does such behavior---which we term asymmetry-induced synchronization (AISync)---occur in oscillator networks? Here we present the first general scheme for constructing AISync systems and demonstrate that this behavior is the norm rather than the exception in a wide class of physical systems that can be seen as multilayer networks. Since a symmetric network in complete synchrony is the basic building block of cluster synchronization in more general networks, AISync should be common also in facilitating cluster synchronization by breaking the symmetry of the cluster subnetworks.	0,1,0,0,0,0
Machine learning based localization and classification with atomic magnetometers	We demonstrate identification of position, material, orientation and shape of objects imaged by an $^{85}$Rb atomic magnetometer performing electromagnetic induction imaging supported by machine learning. Machine learning maximizes the information extracted from the images created by the magnetometer, demonstrating the use of hidden data. Localization 2.6 times better than the spatial resolution of the imaging system and successful classification up to 97$\%$ are obtained. This circumvents the need of solving the inverse problem, and demonstrates the extension of machine learning to diffusive systems such as low-frequency electrodynamics in media. Automated collection of task-relevant information from quantum-based electromagnetic imaging will have a relevant impact from biomedicine to security.	0,1,0,0,0,0
Magnetism and charge density waves in RNiC$_2$ (R = Ce, Pr, Nd)	We have compared the magnetic, transport, galvanomagnetic and specific heat properties of CeNiC$_2$, PrNiC$_2$ and NdNiC$_2$ to study the interplay between charge density waves and magnetism in these compounds. The negative magnetoresistance in NdNiC$_2$ is discussed in terms of the partial destruction of charge density waves and an irreversible phase transition stabilized by the field induced ferromagnetic transformation is reported. For PrNiC$_2$ we demonstrate that the magnetic field initially weakens the CDW state, due to the Zeeman splitting of conduction bands. However, the Fermi surface nesting is enhanced at a temperature related to the magnetic anomaly.	0,1,0,0,0,0
Fairer and more accurate, but for whom?	Complex statistical machine learning models are increasingly being used or considered for use in high-stakes decision-making pipelines in domains such as financial services, health care, criminal justice and human services. These models are often investigated as possible improvements over more classical tools such as regression models or human judgement. While the modeling approach may be new, the practice of using some form of risk assessment to inform decisions is not. When determining whether a new model should be adopted, it is therefore essential to be able to compare the proposed model to the existing approach across a range of task-relevant accuracy and fairness metrics. Looking at overall performance metrics, however, may be misleading. Even when two models have comparable overall performance, they may nevertheless disagree in their classifications on a considerable fraction of cases. In this paper we introduce a model comparison framework for automatically identifying subgroups in which the differences between models are most pronounced. Our primary focus is on identifying subgroups where the models differ in terms of fairness-related quantities such as racial or gender disparities. We present experimental results from a recidivism prediction task and a hypothetical lending example.	1,0,0,1,0,0
Parallel Implementation of Efficient Search Schemes for the Inference of Cancer Progression Models	The emergence and development of cancer is a consequence of the accumulation over time of genomic mutations involving a specific set of genes, which provides the cancer clones with a functional selective advantage. In this work, we model the order of accumulation of such mutations during the progression, which eventually leads to the disease, by means of probabilistic graphic models, i.e., Bayesian Networks (BNs). We investigate how to perform the task of learning the structure of such BNs, according to experimental evidence, adopting a global optimization meta-heuristics. In particular, in this work we rely on Genetic Algorithms, and to strongly reduce the execution time of the inference -- which can also involve multiple repetitions to collect statistically significant assessments of the data -- we distribute the calculations using both multi-threading and a multi-node architecture. The results show that our approach is characterized by good accuracy and specificity; we also demonstrate its feasibility, thanks to a 84x reduction of the overall execution time with respect to a traditional sequential implementation.	1,0,0,1,0,0
Continuous CM-regularity of semihomogeneous vector bundles	We show that if $X$ is an abelian variety of dimension $g \geq 1$ and ${\mathcal E}$ is an M-regular coherent sheaf on $X$, the Castelnuovo-Mumford regularity of ${\mathcal E}$ with respect to an ample and globally generated line bundle ${\mathcal O}(1)$ on $X$ is at most $g$, and that equality is obtained when ${\mathcal E}^{\vee}(1)$ is continuously globally generated. As an application, we give a numerical characterization of ample semihomogeneous vector bundles for which this bound is attained.	0,0,1,0,0,0
Pixel-Level Statistical Analyses of Prescribed Fire Spread	Wildland fire dynamics is a complex turbulent dimensional process. Cellular automata (CA) is an efficient tool to predict fire dynamics, but the main parameters of the method are challenging to estimate. To overcome this challenge, we compute statistical distributions of the key parameters of a CA model using infrared images from controlled burns. Moreover, we apply this analysis to different spatial scales and compare the experimental results to a simple statistical model. By performing this analysis and making this comparison, several capabilities and limitations of CA are revealed.	0,1,0,0,0,0
Next Steps for the Colorado Risk-Limiting Audit (CORLA) Program	Colorado conducted risk-limiting tabulation audits (RLAs) across the state in 2017, including both ballot-level comparison audits and ballot-polling audits. Those audits only covered contests restricted to a single county; methods to efficiently audit contests that cross county boundaries and combine ballot polling and ballot-level comparisons have not been available. Colorado's current audit software (RLATool) needs to be improved to audit these contests that cross county lines and to audit small contests efficiently. This paper addresses these needs. It presents extremely simple but inefficient methods, more efficient methods that combine ballot polling and ballot-level comparisons using stratified samples, and methods that combine ballot-level comparison and variable-size batch comparison audits in a way that does not require stratified sampling. We conclude with some recommendations, and illustrate our recommended method using examples that compare them to existing approaches. Exemplar open-source code and interactive Jupyter notebooks are provided that implement the methods and allow further exploration.	0,0,0,1,0,0
Adaptive Grasp Control through Multi-Modal Interactions for Assistive Prosthetic Devices	The hand is one of the most complex and important parts of the human body. The dexterity provided by its multiple degrees of freedom enables us to perform many of the tasks of daily living which involve grasping and manipulating objects of interest. Contemporary prosthetic devices for people with transradial amputations or wrist disarticulation vary in complexity, from passive prosthetics to complex devices that are body or electrically driven. One of the important challenges in developing smart prosthetic hands is to create devices which are able to mimic all activities that a person might perform and address the needs of a wide variety of users. The approach explored here is to develop algorithms that permit a device to adapt its behavior to the preferences of the operator through interactions with the wearer. This device uses multiple sensing modalities including muscle activity from a myoelectric armband, visual information from an on-board camera, tactile input through a touchscreen interface, and speech input from an embedded microphone. Presented within this paper are the design, software and controls of a platform used to evaluate this architecture as well as results from experiments deigned to quantify the performance.	1,0,0,0,0,0
The Salesman's Improved Tours for Fundamental Classes	Finding the exact integrality gap $\alpha$ for the LP relaxation of the metric Travelling Salesman Problem (TSP) has been an open problem for over thirty years, with little progress made. It is known that $4/3 \leq \alpha \leq 3/2$, and a famous conjecture states $\alpha = 4/3$. For this problem, essentially two "fundamental" classes of instances have been proposed. This fundamental property means that in order to show that the integrality gap is at most $\rho$ for all instances of metric TSP, it is sufficient to show it only for the instances in the fundamental class. However, despite the importance and the simplicity of such classes, no apparent effort has been deployed for improving the integrality gap bounds for them. In this paper we take a natural first step in this endeavour, and consider the $1/2$-integer points of one such class. We successfully improve the upper bound for the integrality gap from $3/2$ to $10/7$ for a superclass of these points, as well as prove a lower bound of $4/3$ for the superclass. Our methods involve innovative applications of tools from combinatorial optimization which have the potential to be more broadly applied.	1,0,0,0,0,0
Global geometry and $C^1$ convex extensions of $1$-jets	Let $E$ be an arbitrary subset of $\mathbb{R}^n$ (not necessarily bounded), and $f:E\to\mathbb{R}$, $G:E\to\mathbb{R}^n$ be functions. We provide necessary and sufficient conditions for the $1$-jet $(f,G)$ to have an extension $(F, \nabla F)$ with $F:\mathbb{R}^n\to\mathbb{R}$ convex and of class $C^{1}$. Besides, if $G$ is bounded we can take $F$ so that $\textrm{Lip}(F)\lesssim \|G\|_{\infty}$. As an application we also solve a similar problem about finding convex hypersurfaces of class $C^1$ with prescribed normals at the points of an arbitrary subset of $\mathbb{R}^n$.	0,0,1,0,0,0
Min-max formulas and other properties of certain classes of nonconvex effective Hamiltonians	This paper is the first attempt to systematically study properties of the effective Hamiltonian $\overline{H}$ arising in the periodic homogenization of some coercive but nonconvex Hamilton-Jacobi equations. Firstly, we introduce a new and robust decomposition method to obtain min-max formulas for a class of nonconvex $\overline{H}$. Secondly, we analytically and numerically investigate other related interesting phenomena, such as "quasi-convexification" and breakdown of symmetry, of $\overline{H}$ from other typical nonconvex Hamiltonians. Finally, in the appendix, we show that our new method and those a priori formulas from the periodic setting can be used to obtain stochastic homogenization for same class of nonconvex Hamilton-Jacobi equations. Some conjectures and problems are also proposed.	0,0,1,0,0,0
On the error term of a lattice counting problem, II	Under the Riemann Hypothesis, we improve the error term in the asymptotic formula related to the counting lattice problem studied in a first part of this work. The improvement comes from the use of Weyl's bound for exponential sums of polynomials and a device due to Popov allowing us to get an improved main term in the sums of certain fractional parts of polynomials.	0,0,1,0,0,0
Quantum Singwi-Tosi-Land-Sjoelander approach for interacting inhomogeneous systems under electromagnetic fields: Comparison with exact results	For inhomogeneous interacting electronic systems under a time-dependent electromagnetic perturbation, we derive the linear equation for response functions in a quantum mechanical manner. It is a natural extension of the original semi-classical Singwi-Tosi-Land-Sjoelander (STLS) approach for an electron gas. The factorization ansatz for the two-particle distribution is an indispensable ingredient in the STLS approaches for determination of the response function and the pair correlation function. In this study, we choose an analytically solvable interacting two-electron system as the target for which we examine the validity of the approximation. It is demonstrated that the STLS response function reproduces well the exact one for low-energy excitations. The interaction energy contributed from the STLS response function is also discussed.	0,1,0,0,0,0
Composite Behavioral Modeling for Identity Theft Detection in Online Social Networks	In this work, we aim at building a bridge from poor behavioral data to an effective, quick-response, and robust behavior model for online identity theft detection. We concentrate on this issue in online social networks (OSNs) where users usually have composite behavioral records, consisting of multi-dimensional low-quality data, e.g., offline check-ins and online user generated content (UGC). As an insightful result, we find that there is a complementary effect among different dimensions of records for modeling users' behavioral patterns. To deeply exploit such a complementary effect, we propose a joint model to capture both online and offline features of a user's composite behavior. We evaluate the proposed joint model by comparing with some typical models on two real-world datasets: Foursquare and Yelp. In the widely-used setting of theft simulation (simulating thefts via behavioral replacement), the experimental results show that our model outperforms the existing ones, with the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively. Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False Positive Rate) below $1\%$. It is worth mentioning that these performances can be achieved by examining only one composite behavior (visiting a place and posting a tip online simultaneously) per authentication, which guarantees the low response latency of our method. This study would give the cybersecurity community new insights into whether and how a real-time online identity authentication can be improved via modeling users' composite behavioral patterns.	1,0,0,0,0,0
A Simplified Approach to Analyze Complementary Sensitivity Trade-offs in Continuous-Time and Discrete-Time Systems	A simplified approach is proposed to investigate the continuous-time and discrete-time complementary sensitivity Bode integrals (CSBIs) in this note. For continuous-time feedback systems with unbounded frequency domain, the CSBI weighted by $1/\omega^2$ is considered, where this simplified method reveals a more explicit relationship between the value of CSBI and the structure of the open-loop transfer function. With a minor modification of this method, the CSBI of discrete-time system is derived, and illustrative examples are provided. Compared with the existing results on CSBI, neither Cauchy integral theorem nor Poisson integral formula are used throughout the analysis, and the analytic constraint on the integrand is removed.	1,0,0,0,0,0
$\left( β, \varpi \right)$-stability for cross-validation and the choice of the number of folds	In this paper, we introduce a new concept of stability for cross-validation, called the $\left( \beta, \varpi \right)$-stability, and use it as a new perspective to build the general theory for cross-validation. The $\left( \beta, \varpi \right)$-stability mathematically connects the generalization ability and the stability of the cross-validated model via the Rademacher complexity. Our result reveals mathematically the effect of cross-validation from two sides: on one hand, cross-validation picks the model with the best empirical generalization ability by validating all the alternatives on test sets; on the other hand, cross-validation may compromise the stability of the model selection by causing subsampling error. Moreover, the difference between training and test errors in q\textsuperscript{th} round, sometimes referred to as the generalization error, might be autocorrelated on q. Guided by the ideas above, the $\left( \beta, \varpi \right)$-stability help us derivd a new class of Rademacher bounds, referred to as the one-round/convoluted Rademacher bounds, for the stability of cross-validation in both the i.i.d.\ and non-i.i.d.\ cases. For both light-tail and heavy-tail losses, the new bounds quantify the stability of the one-round/average test error of the cross-validated model in terms of its one-round/average training error, the sample sizes $n$, number of folds $K$, the tail property of the loss (encoded as Orlicz-$\Psi_\nu$ norms) and the Rademacher complexity of the model class $\Lambda$. The new class of bounds not only quantitatively reveals the stability of the generalization ability of the cross-validated model, it also shows empirically the optimal choice for number of folds $K$, at which the upper bound of the one-round/average test error is lowest, or, to put it in another way, where the test error is most stable.	1,0,1,1,0,0
On Microtargeting Socially Divisive Ads: A Case Study of Russia-Linked Ad Campaigns on Facebook	Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are "divisive": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.	1,0,0,0,0,0
The Morphospace of Consciousness	We construct a complexity-based morphospace to study systems-level properties of conscious & intelligent systems. The axes of this space label 3 complexity types: autonomous, cognitive & social. Given recent proposals to synthesize consciousness, a generic complexity-based conceptualization provides a useful framework for identifying defining features of conscious & synthetic systems. Based on current clinical scales of consciousness that measure cognitive awareness and wakefulness, we take a perspective on how contemporary artificially intelligent machines & synthetically engineered life forms measure on these scales. It turns out that awareness & wakefulness can be associated to computational & autonomous complexity respectively. Subsequently, building on insights from cognitive robotics, we examine the function that consciousness serves, & argue the role of consciousness as an evolutionary game-theoretic strategy. This makes the case for a third type of complexity for describing consciousness: social complexity. Having identified these complexity types, allows for a representation of both, biological & synthetic systems in a common morphospace. A consequence of this classification is a taxonomy of possible conscious machines. We identify four types of consciousness, based on embodiment: (i) biological consciousness, (ii) synthetic consciousness, (iii) group consciousness (resulting from group interactions), & (iv) simulated consciousness (embodied by virtual agents within a simulated reality). This taxonomy helps in the investigation of comparative signatures of consciousness across domains, in order to highlight design principles necessary to engineer conscious machines. This is particularly relevant in the light of recent developments at the crossroads of cognitive neuroscience, biomedical engineering, artificial intelligence & biomimetics.	1,1,0,0,0,0
Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms	We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL	1,0,0,1,0,0
Choreographic and Somatic Approaches for the Development of Expressive Robotic Systems	As robotic systems are moved out of factory work cells into human-facing environments questions of choreography become central to their design, placement, and application. With a human viewer or counterpart present, a system will automatically be interpreted within context, style of movement, and form factor by human beings as animate elements of their environment. The interpretation by this human counterpart is critical to the success of the system's integration: knobs on the system need to make sense to a human counterpart; an artificial agent should have a way of notifying a human counterpart of a change in system state, possibly through motion profiles; and the motion of a human counterpart may have important contextual clues for task completion. Thus, professional choreographers, dance practitioners, and movement analysts are critical to research in robotics. They have design methods for movement that align with human audience perception, can identify simplified features of movement for human-robot interaction goals, and have detailed knowledge of the capacity of human movement. This article provides approaches employed by one research lab, specific impacts on technical and artistic projects within, and principles that may guide future such work. The background section reports on choreography, somatic perspectives, improvisation, the Laban/Bartenieff Movement System, and robotics. From this context methods including embodied exercises, writing prompts, and community building activities have been developed to facilitate interdisciplinary research. The results of this work is presented as an overview of a smattering of projects in areas like high-level motion planning, software development for rapid prototyping of movement, artistic output, and user studies that help understand how people interpret movement. Finally, guiding principles for other groups to adopt are posited.	1,0,0,0,0,0
A Formal Approach to Exploiting Multi-Stage Attacks based on File-System Vulnerabilities of Web Applications (Extended Version)	Web applications require access to the file-system for many different tasks. When analyzing the security of a web application, secu- rity analysts should thus consider the impact that file-system operations have on the security of the whole application. Moreover, the analysis should take into consideration how file-system vulnerabilities might in- teract with other vulnerabilities leading an attacker to breach into the web application. In this paper, we first propose a classification of file- system vulnerabilities, and then, based on this classification, we present a formal approach that allows one to exploit file-system vulnerabilities. We give a formal representation of web applications, databases and file- systems, and show how to reason about file-system vulnerabilities. We also show how to combine file-system vulnerabilities and SQL-Injection vulnerabilities for the identification of complex, multi-stage attacks. We have developed an automatic tool that implements our approach and we show its efficiency by discussing several real-world case studies, which are witness to the fact that our tool can generate, and exploit, complex attacks that, to the best of our knowledge, no other state-of-the-art-tool for the security of web applications can find.	1,0,0,0,0,0
